{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_final_amostra_03_nov_22_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/defesa/PSD_histogram_final_amostra_03_nov_22_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from sklearn.metrics import r2_score\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZEvJvfoibE4",
        "outputId": "0a849f5b-8c8a-40ed-92bc-f5667781791f"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mahotas in /usr/local/lib/python3.7/dist-packages (1.4.11)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT"
      },
      "source": [
        "import mahotas.features.texture as mht\n",
        "import mahotas.features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VcTdaNVh9EE",
        "outputId": "ad09bb1d-9e4d-45bf-f26a-89a97279f001"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'marquesgabi_fev_2020' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v7SRrc8mH2N",
        "outputId": "37671e25-b731-401f-9b4c-b1d9dc49fa7c"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip' \n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Doutorado' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqIYzUcnrdMp",
        "outputId": "3a219e79-169d-4eb9-f266-8db4e2c6d3b0"
      },
      "source": [
        "labels =[]\n",
        "with zipfile.ZipFile(Transfere, \"r\") as f:\n",
        "  for f in f.namelist():\n",
        "    labels.append(f)\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Fotos_Grandes-3cdAmostra/Q6-8-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-4.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[4] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHgqAnaFyCjp",
        "outputId": "b0fdc09d-1293-4b12-ba95-946b754b588e"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MarquesGabi_Routines' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from segment_filter_not_conclude import Segmenta  # got image provided segmented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN5MN5a_v4np",
        "outputId": "6eef089c-9d61-46b9-e599-85c5df6e9aee"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     180  145.719513  150.013351  ...  101.761482  138.450378  107.141251\n",
            "1     191  166.213730  169.232727  ...  157.971970  161.132874  156.439804\n",
            "2     187  244.189804  187.140671  ...  163.978333  162.558334  165.163589\n",
            "3     144  200.745377  148.200607  ...  156.721451  151.279327  140.120377\n",
            "4     109  184.045959  183.017609  ...  192.236679  196.926514  203.937225\n",
            "5     166  142.251556  138.279709  ...  140.374939  140.610107  158.666992\n",
            "6     127   66.022690   66.545479  ...  145.395187  153.718643  155.329788\n",
            "7     113  173.522049  174.167496  ...  175.562225  173.831619  157.014801\n",
            "8     199  144.099640  142.534622  ...    1.000000    0.814979    0.000000\n",
            "9     181  158.586319  147.640015  ...    1.230060    0.129025    1.333323\n",
            "10    179   69.376610   68.343681  ...    1.000000    1.000000    1.127587\n",
            "11    103  165.650391  189.363068  ...  114.038834  113.347061  126.937775\n",
            "12    170  198.000153  200.873230  ...   86.931908   75.574951   50.199169\n",
            "13    126  176.000000  163.012360  ...  174.987656  186.469147  185.481476\n",
            "14    159  129.392426  127.839165  ...   85.470001   91.879517  100.635567\n",
            "15    119  204.712814  207.653976  ...  134.110733  132.439453  128.245682\n",
            "16    161   72.877129   96.930054  ...  161.028366  156.413986  157.037811\n",
            "17    144  178.679779  180.694458  ...  155.527786  150.525452  152.331009\n",
            "18    182  165.029602  195.905334  ...  126.662735  107.071014  101.982262\n",
            "19    159   73.675171   82.796249  ...  129.980423  119.410667  110.276764\n",
            "20    149    1.283636    1.538670  ...  114.486343  207.709381  239.392120\n",
            "21    194  184.890823  108.140068  ...  100.602081  101.990524   98.531082\n",
            "22    117  114.901604  116.087944  ...  101.969757  109.328369  106.056831\n",
            "23    169  108.163010   79.929550  ...  129.401489  130.961761  140.439865\n",
            "24    199  106.462860  177.305359  ...  139.074799  135.975967  139.406769\n",
            "25    126  243.790131  240.160507  ...   93.481483  107.271606  106.432098\n",
            "26    155  123.280792  112.884125  ...   94.379524   58.692329   11.038128\n",
            "27    108  169.049377  172.770905  ...  207.559677  198.950623  184.030182\n",
            "28    168  129.416672  144.666672  ...  190.750000   88.361115   81.500000\n",
            "29    121  144.276077  153.585419  ...  142.851105  134.317871  129.777191\n",
            "30    136   99.134956   94.203293  ...    0.898789    0.060554    0.660035\n",
            "31    145   43.272152   60.903831  ...  165.895706  162.168228  160.466339\n",
            "32    171  168.788101  144.322525  ...  188.822342  195.656647  175.751312\n",
            "33    123  145.053085  140.328247  ...   73.881027   74.638649   73.521317\n",
            "34    178    0.224467    1.092539  ...   70.846863   69.556000   67.378746\n",
            "35    194  103.849495  124.696762  ...    1.443724    0.217345    1.330322\n",
            "36    138  125.695229  106.990753  ...    0.403487    0.461458    1.446965\n",
            "37    136  137.164368  138.429062  ...  172.725784  168.855560  177.711090\n",
            "38    174  129.991425  125.565750  ...    1.060906    0.171357    1.353283\n",
            "39    194  106.796471  120.867027  ...  191.888168  189.230194  137.177048\n",
            "40    104  171.536987  176.902374  ...  182.976334  180.387604  183.288483\n",
            "41    116  252.277039  252.398331  ...    0.003567    0.697979    1.491082\n",
            "42    193   98.371262  111.789337  ...    0.995194    0.496121    0.784021\n",
            "43    127  165.931732  160.096664  ...  182.653473  198.177567  210.999817\n",
            "44    194   77.472839   85.598251  ...   90.324142   92.372505   92.152924\n",
            "45    112  234.687500  231.500000  ...  175.500000  172.437500  167.562500\n",
            "46    100   61.923199   92.396797  ...  148.910400  150.431992  150.150391\n",
            "47    101  160.026367  159.789536  ...  201.371140  208.273804  208.251648\n",
            "48    141    1.046527    1.826669  ...  120.947632  134.325241  135.299728\n",
            "49    156  166.301132  179.980301  ...  147.673248  144.969757  155.364243\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2emP4rNjQy",
        "outputId": "f605377f-e9cf-449b-cc78-b443d53f85f8"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MarquesGabi_Routines' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIFPGE_-vx3T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "Description = '3 layers of Convolution: 64, 128, 256 '\n",
        "N1 = 20\n",
        "N2 = 20\n",
        "\n",
        "# make the CNN\n",
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=64, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=N1))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=N2))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbPQ1FSRG6A",
        "outputId": "74e7018f-154b-4a57-eab4-a2cb0449142f"
      },
      "source": [
        "\n",
        "# training the model\n",
        "r = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), \n",
        "              epochs=200, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 3s 162ms/step - loss: 0.6682 - accuracy: 0.7085 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.3534 - accuracy: 0.8513 - val_loss: 0.6928 - val_accuracy: 0.5102\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.2552 - accuracy: 0.8921 - val_loss: 0.6927 - val_accuracy: 0.5102\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.1808 - accuracy: 0.9329 - val_loss: 0.6927 - val_accuracy: 0.5102\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.1008 - accuracy: 0.9708 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0801 - accuracy: 0.9621 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.1240 - accuracy: 0.9563 - val_loss: 0.6921 - val_accuracy: 0.5102\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.1209 - accuracy: 0.9504 - val_loss: 0.6942 - val_accuracy: 0.5102\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0660 - accuracy: 0.9767 - val_loss: 0.6972 - val_accuracy: 0.5102\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0446 - accuracy: 0.9825 - val_loss: 0.6942 - val_accuracy: 0.5102\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.0249 - accuracy: 0.9971 - val_loss: 0.6941 - val_accuracy: 0.5102\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.6952 - val_accuracy: 0.5102\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0229 - accuracy: 0.9913 - val_loss: 0.6994 - val_accuracy: 0.5102\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0141 - accuracy: 0.9971 - val_loss: 0.7152 - val_accuracy: 0.5102\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.7149 - val_accuracy: 0.5102\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 0.7123 - val_accuracy: 0.5102\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.7411 - val_accuracy: 0.5102\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.7447 - val_accuracy: 0.5102\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.7184 - val_accuracy: 0.5102\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.7664 - val_accuracy: 0.5102\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.8099 - val_accuracy: 0.5102\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.8349 - val_accuracy: 0.5102\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.8508 - val_accuracy: 0.5102\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.8808 - val_accuracy: 0.5102\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.9447 - val_accuracy: 0.5102\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.0108 - val_accuracy: 0.5102\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.0534 - val_accuracy: 0.5102\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 1.1476 - val_accuracy: 0.5102\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.1520 - val_accuracy: 0.5102\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.1299 - val_accuracy: 0.5102\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 1.2241 - val_accuracy: 0.5102\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.2356 - val_accuracy: 0.5102\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.2441 - val_accuracy: 0.5102\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0105 - accuracy: 0.9942 - val_loss: 1.0232 - val_accuracy: 0.5102\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 1.0495 - val_accuracy: 0.5102\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 2.0471 - val_accuracy: 0.5102\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0049 - accuracy: 0.9971 - val_loss: 2.0758 - val_accuracy: 0.5102\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0078 - accuracy: 0.9971 - val_loss: 2.6097 - val_accuracy: 0.5102\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 3.5036 - val_accuracy: 0.5102\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 3.7278 - val_accuracy: 0.5102\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 4.3456 - val_accuracy: 0.5102\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 3.2560 - val_accuracy: 0.5102\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 6.1577e-04 - accuracy: 1.0000 - val_loss: 2.2008 - val_accuracy: 0.5102\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 2.2918 - val_accuracy: 0.5102\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 5.1101e-04 - accuracy: 1.0000 - val_loss: 2.4350 - val_accuracy: 0.5102\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 3.2432 - val_accuracy: 0.5102\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 4.6367e-04 - accuracy: 1.0000 - val_loss: 3.8515 - val_accuracy: 0.5102\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0030 - accuracy: 0.9971 - val_loss: 4.1154 - val_accuracy: 0.5102\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 2.8383 - val_accuracy: 0.5102\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0442 - accuracy: 0.9942 - val_loss: 16.7317 - val_accuracy: 0.5102\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0164 - accuracy: 0.9971 - val_loss: 41.4794 - val_accuracy: 0.5102\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0306 - accuracy: 0.9854 - val_loss: 39.6789 - val_accuracy: 0.5102\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0401 - accuracy: 0.9913 - val_loss: 67.9976 - val_accuracy: 0.5102\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 0.0520 - accuracy: 0.9708 - val_loss: 61.2677 - val_accuracy: 0.5102\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0155 - accuracy: 0.9971 - val_loss: 37.5464 - val_accuracy: 0.5102\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0134 - accuracy: 0.9942 - val_loss: 37.3499 - val_accuracy: 0.5102\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 0.0111 - accuracy: 0.9971 - val_loss: 39.0417 - val_accuracy: 0.5102\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0077 - accuracy: 0.9971 - val_loss: 33.7729 - val_accuracy: 0.5102\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0119 - accuracy: 0.9971 - val_loss: 28.7425 - val_accuracy: 0.5102\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 0.0040 - accuracy: 0.9971 - val_loss: 20.0273 - val_accuracy: 0.5102\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0109 - accuracy: 0.9942 - val_loss: 34.3350 - val_accuracy: 0.5102\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 80.5686 - val_accuracy: 0.5102\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 59.7320 - val_accuracy: 0.5102\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0078 - accuracy: 0.9942 - val_loss: 46.2012 - val_accuracy: 0.5102\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 24.7808 - val_accuracy: 0.5102\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 4.6843e-04 - accuracy: 1.0000 - val_loss: 29.8430 - val_accuracy: 0.5102\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 32.2415 - val_accuracy: 0.5102\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 32.8234 - val_accuracy: 0.5102\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 3.9147e-04 - accuracy: 1.0000 - val_loss: 33.2589 - val_accuracy: 0.5102\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 8.4437e-04 - accuracy: 1.0000 - val_loss: 31.3014 - val_accuracy: 0.5102\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 1.9976e-04 - accuracy: 1.0000 - val_loss: 28.2984 - val_accuracy: 0.5102\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 7.2684e-04 - accuracy: 1.0000 - val_loss: 26.2511 - val_accuracy: 0.5102\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 3.5177e-04 - accuracy: 1.0000 - val_loss: 23.2081 - val_accuracy: 0.5102\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 8.6344e-05 - accuracy: 1.0000 - val_loss: 20.5804 - val_accuracy: 0.5102\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 9.6126e-05 - accuracy: 1.0000 - val_loss: 18.7916 - val_accuracy: 0.5102\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 3.5051e-04 - accuracy: 1.0000 - val_loss: 14.7463 - val_accuracy: 0.5102\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 2.0222e-04 - accuracy: 1.0000 - val_loss: 12.6472 - val_accuracy: 0.5102\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 2.2045e-04 - accuracy: 1.0000 - val_loss: 10.9336 - val_accuracy: 0.5102\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 5.3603e-04 - accuracy: 1.0000 - val_loss: 9.8574 - val_accuracy: 0.5102\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 1.0759e-04 - accuracy: 1.0000 - val_loss: 6.5460 - val_accuracy: 0.5102\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 2.8407e-04 - accuracy: 1.0000 - val_loss: 6.8248 - val_accuracy: 0.5102\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 2.1908e-04 - accuracy: 1.0000 - val_loss: 5.6023 - val_accuracy: 0.5102\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 2.7813e-04 - accuracy: 1.0000 - val_loss: 5.3115 - val_accuracy: 0.5102\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 8.3745e-05 - accuracy: 1.0000 - val_loss: 5.0522 - val_accuracy: 0.5102\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 1s 138ms/step - loss: 1.6669e-04 - accuracy: 1.0000 - val_loss: 4.7205 - val_accuracy: 0.5102\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.2725 - val_accuracy: 0.5102\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 1s 138ms/step - loss: 1.0810e-04 - accuracy: 1.0000 - val_loss: 3.3106 - val_accuracy: 0.5102\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 1.2132e-04 - accuracy: 1.0000 - val_loss: 2.8926 - val_accuracy: 0.5170\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 1.3585e-04 - accuracy: 1.0000 - val_loss: 2.6031 - val_accuracy: 0.5170\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 1.5897e-04 - accuracy: 1.0000 - val_loss: 2.8369 - val_accuracy: 0.5170\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 1s 138ms/step - loss: 1.0494e-04 - accuracy: 1.0000 - val_loss: 2.4499 - val_accuracy: 0.5510\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 8.6833e-05 - accuracy: 1.0000 - val_loss: 2.0372 - val_accuracy: 0.5850\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 8.4297e-05 - accuracy: 1.0000 - val_loss: 1.5463 - val_accuracy: 0.6259\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 6.2805e-05 - accuracy: 1.0000 - val_loss: 0.9805 - val_accuracy: 0.7075\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 3.6383e-04 - accuracy: 1.0000 - val_loss: 1.0355 - val_accuracy: 0.7007\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 1.4954e-04 - accuracy: 1.0000 - val_loss: 0.9007 - val_accuracy: 0.7211\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 7.6139e-05 - accuracy: 1.0000 - val_loss: 0.4341 - val_accuracy: 0.8571\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 5.3337e-05 - accuracy: 1.0000 - val_loss: 0.2850 - val_accuracy: 0.9184\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 1.3411e-04 - accuracy: 1.0000 - val_loss: 0.2034 - val_accuracy: 0.9524\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 7.9037e-05 - accuracy: 1.0000 - val_loss: 0.1647 - val_accuracy: 0.9660\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 1.4060e-04 - accuracy: 1.0000 - val_loss: 0.1578 - val_accuracy: 0.9660\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 5.2734e-05 - accuracy: 1.0000 - val_loss: 0.3507 - val_accuracy: 0.9184\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 1.1256e-04 - accuracy: 1.0000 - val_loss: 0.4317 - val_accuracy: 0.9116\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 7.1444e-05 - accuracy: 1.0000 - val_loss: 0.6244 - val_accuracy: 0.8844\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 2.3485e-04 - accuracy: 1.0000 - val_loss: 0.7868 - val_accuracy: 0.8776\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 3.7567e-05 - accuracy: 1.0000 - val_loss: 0.9483 - val_accuracy: 0.8299\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 4.7537e-05 - accuracy: 1.0000 - val_loss: 0.8321 - val_accuracy: 0.8503\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 5.3986e-04 - accuracy: 1.0000 - val_loss: 3.5533 - val_accuracy: 0.5850\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 5.1434 - val_accuracy: 0.5102\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 1.1786e-04 - accuracy: 1.0000 - val_loss: 6.1199 - val_accuracy: 0.5102\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 1.4726e-04 - accuracy: 1.0000 - val_loss: 6.6042 - val_accuracy: 0.5102\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 1.3521e-04 - accuracy: 1.0000 - val_loss: 6.3912 - val_accuracy: 0.5102\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 4.5700e-04 - accuracy: 1.0000 - val_loss: 8.5510 - val_accuracy: 0.5102\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 1.0914e-04 - accuracy: 1.0000 - val_loss: 12.7713 - val_accuracy: 0.5102\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 3.2160e-05 - accuracy: 1.0000 - val_loss: 14.1315 - val_accuracy: 0.5102\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 5.0388e-05 - accuracy: 1.0000 - val_loss: 13.7290 - val_accuracy: 0.5102\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 6.0535e-05 - accuracy: 1.0000 - val_loss: 12.9410 - val_accuracy: 0.5102\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 7.5304e-05 - accuracy: 1.0000 - val_loss: 12.0868 - val_accuracy: 0.5102\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 4.2359e-05 - accuracy: 1.0000 - val_loss: 11.3112 - val_accuracy: 0.5102\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 2.2706e-05 - accuracy: 1.0000 - val_loss: 10.1437 - val_accuracy: 0.5102\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 4.0407e-04 - accuracy: 1.0000 - val_loss: 7.2780 - val_accuracy: 0.5102\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 3.7907e-04 - accuracy: 1.0000 - val_loss: 15.4377 - val_accuracy: 0.5102\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 3.8441e-05 - accuracy: 1.0000 - val_loss: 18.0827 - val_accuracy: 0.5102\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 6.9373e-05 - accuracy: 1.0000 - val_loss: 17.3129 - val_accuracy: 0.5102\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 22.8815 - val_accuracy: 0.5102\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 5.7259e-04 - accuracy: 1.0000 - val_loss: 31.7472 - val_accuracy: 0.5102\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 8.5950e-05 - accuracy: 1.0000 - val_loss: 31.5450 - val_accuracy: 0.5102\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 1.4595e-04 - accuracy: 1.0000 - val_loss: 26.9450 - val_accuracy: 0.5102\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 4.2436e-05 - accuracy: 1.0000 - val_loss: 21.9687 - val_accuracy: 0.5102\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 1.1791e-04 - accuracy: 1.0000 - val_loss: 17.9244 - val_accuracy: 0.5102\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 8.3164e-04 - accuracy: 1.0000 - val_loss: 7.2165 - val_accuracy: 0.5102\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0055 - accuracy: 0.9971 - val_loss: 0.5029 - val_accuracy: 0.7211\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0054 - accuracy: 0.9971 - val_loss: 51.9270 - val_accuracy: 0.5102\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0100 - accuracy: 0.9942 - val_loss: 121.6105 - val_accuracy: 0.5102\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.2082 - accuracy: 0.9563 - val_loss: 66.0188 - val_accuracy: 0.5102\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.1232 - accuracy: 0.9534 - val_loss: 185.7665 - val_accuracy: 0.5102\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0793 - accuracy: 0.9621 - val_loss: 294.0480 - val_accuracy: 0.5102\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.1093 - accuracy: 0.9504 - val_loss: 386.1337 - val_accuracy: 0.5102\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0284 - accuracy: 0.9883 - val_loss: 80.4682 - val_accuracy: 0.5102\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0126 - accuracy: 0.9942 - val_loss: 101.5260 - val_accuracy: 0.5102\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 100.6500 - val_accuracy: 0.5102\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 90.2125 - val_accuracy: 0.5102\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 82.0628 - val_accuracy: 0.5102\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 71.3844 - val_accuracy: 0.5102\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 2.8185e-04 - accuracy: 1.0000 - val_loss: 67.0189 - val_accuracy: 0.5102\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 6.8668e-04 - accuracy: 1.0000 - val_loss: 61.2569 - val_accuracy: 0.5102\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 55.1342 - val_accuracy: 0.5102\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 2s 155ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 49.0853 - val_accuracy: 0.5102\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 49.3178 - val_accuracy: 0.5102\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 3.3860e-04 - accuracy: 1.0000 - val_loss: 49.2159 - val_accuracy: 0.5102\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 3.9049e-04 - accuracy: 1.0000 - val_loss: 45.4315 - val_accuracy: 0.5102\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 3.5557e-04 - accuracy: 1.0000 - val_loss: 40.5483 - val_accuracy: 0.5102\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 2.1743e-04 - accuracy: 1.0000 - val_loss: 36.3610 - val_accuracy: 0.5102\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 6.4987e-04 - accuracy: 1.0000 - val_loss: 31.3907 - val_accuracy: 0.5102\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 4.6750e-04 - accuracy: 1.0000 - val_loss: 28.6504 - val_accuracy: 0.5102\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 3.4918e-04 - accuracy: 1.0000 - val_loss: 25.5124 - val_accuracy: 0.5102\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 2.6739e-04 - accuracy: 1.0000 - val_loss: 22.4244 - val_accuracy: 0.5102\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 2.2175e-04 - accuracy: 1.0000 - val_loss: 19.5556 - val_accuracy: 0.5102\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 1.7051e-04 - accuracy: 1.0000 - val_loss: 16.9469 - val_accuracy: 0.5102\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 1.2294e-04 - accuracy: 1.0000 - val_loss: 14.4532 - val_accuracy: 0.5102\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 7.4860e-04 - accuracy: 1.0000 - val_loss: 11.4251 - val_accuracy: 0.5102\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 3.8470e-04 - accuracy: 1.0000 - val_loss: 11.7505 - val_accuracy: 0.5102\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 1.3656e-04 - accuracy: 1.0000 - val_loss: 10.5157 - val_accuracy: 0.5102\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 6.0998e-04 - accuracy: 1.0000 - val_loss: 8.8691 - val_accuracy: 0.5102\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 1.3588e-04 - accuracy: 1.0000 - val_loss: 7.4848 - val_accuracy: 0.5102\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 5.8806 - val_accuracy: 0.5170\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 1.7728e-04 - accuracy: 1.0000 - val_loss: 7.1072 - val_accuracy: 0.5170\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 8.5483e-04 - accuracy: 1.0000 - val_loss: 7.6363 - val_accuracy: 0.5170\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 2.8488e-04 - accuracy: 1.0000 - val_loss: 7.7507 - val_accuracy: 0.5170\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 2.0454e-04 - accuracy: 1.0000 - val_loss: 8.9652 - val_accuracy: 0.5102\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 1.5208e-04 - accuracy: 1.0000 - val_loss: 8.6443 - val_accuracy: 0.5102\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 2.9612e-04 - accuracy: 1.0000 - val_loss: 7.5343 - val_accuracy: 0.5170\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 7.6451e-05 - accuracy: 1.0000 - val_loss: 6.3238 - val_accuracy: 0.5306\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 2.4053e-04 - accuracy: 1.0000 - val_loss: 4.7915 - val_accuracy: 0.5442\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 3.1311e-04 - accuracy: 1.0000 - val_loss: 3.6922 - val_accuracy: 0.5714\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 8.2138e-05 - accuracy: 1.0000 - val_loss: 2.8829 - val_accuracy: 0.6190\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 1.3272e-04 - accuracy: 1.0000 - val_loss: 1.8574 - val_accuracy: 0.6939\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 9.7538e-05 - accuracy: 1.0000 - val_loss: 1.2625 - val_accuracy: 0.7415\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 2.7538e-05 - accuracy: 1.0000 - val_loss: 0.9019 - val_accuracy: 0.7823\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 6.3327e-04 - accuracy: 1.0000 - val_loss: 0.4005 - val_accuracy: 0.8776\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 3.2095e-05 - accuracy: 1.0000 - val_loss: 0.2396 - val_accuracy: 0.9252\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 1.3476e-04 - accuracy: 1.0000 - val_loss: 0.2006 - val_accuracy: 0.9388\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 2.5838e-04 - accuracy: 1.0000 - val_loss: 0.1765 - val_accuracy: 0.9524\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 1.5689e-04 - accuracy: 1.0000 - val_loss: 0.1841 - val_accuracy: 0.9388\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 3.7787e-04 - accuracy: 1.0000 - val_loss: 0.1907 - val_accuracy: 0.9388\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 1.8504e-04 - accuracy: 1.0000 - val_loss: 0.1795 - val_accuracy: 0.9524\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 1.1684e-04 - accuracy: 1.0000 - val_loss: 0.2188 - val_accuracy: 0.9524\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 2.1210e-04 - accuracy: 1.0000 - val_loss: 0.3779 - val_accuracy: 0.9048\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 1.7264e-04 - accuracy: 1.0000 - val_loss: 0.4120 - val_accuracy: 0.9048\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 6.6240e-05 - accuracy: 1.0000 - val_loss: 0.3919 - val_accuracy: 0.9048\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 9.9252e-05 - accuracy: 1.0000 - val_loss: 0.3736 - val_accuracy: 0.9116\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 2.8225e-04 - accuracy: 1.0000 - val_loss: 0.3779 - val_accuracy: 0.9184\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 1.9734e-04 - accuracy: 1.0000 - val_loss: 0.3305 - val_accuracy: 0.9388\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 3.6192e-04 - accuracy: 1.0000 - val_loss: 0.1882 - val_accuracy: 0.9524\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 2.6524e-04 - accuracy: 1.0000 - val_loss: 0.5089 - val_accuracy: 0.8980\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 5.4993e-05 - accuracy: 1.0000 - val_loss: 0.6232 - val_accuracy: 0.8776\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 1.2889e-04 - accuracy: 1.0000 - val_loss: 0.5635 - val_accuracy: 0.8912\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 7.5317e-05 - accuracy: 1.0000 - val_loss: 0.4271 - val_accuracy: 0.9116\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 2.8707e-04 - accuracy: 1.0000 - val_loss: 0.2704 - val_accuracy: 0.9320\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 1.5979e-04 - accuracy: 1.0000 - val_loss: 0.2433 - val_accuracy: 0.9388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDVY6HbxMOlH",
        "outputId": "0703b7f7-1750-4a12-ac3e-ed71dae3d906"
      },
      "source": [
        "# pred_test= model.predict_classes(X_test)\n",
        "pred_test = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict   0   1\n",
            "Actual         \n",
            "0        65   7\n",
            "1         2  73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7pT2q7traXg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6023b097-e2f9-4212-81e3-46816ea72ef2"
      },
      "source": [
        "print(METRICS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.90      0.94        72\n",
            "           1       0.91      0.97      0.94        75\n",
            "\n",
            "    accuracy                           0.94       147\n",
            "   macro avg       0.94      0.94      0.94       147\n",
            "weighted avg       0.94      0.94      0.94       147\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElpxWbBnpgLX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "08911fd8-ca67-4801-c8a6-2710f505f02c"
      },
      "source": [
        "'''\n",
        "#X =np.array(df.copy())/255.0 \n",
        "X =np.array(df.copy())\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)\n",
        "model = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh', solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  \n",
        "prediction = model.predict(X_test)  \n",
        "y =np.copy(y_test)\n",
        "data = {'y_true': y_test,'y_predict': prediction}  \n",
        "# este dado esta no formato de dicionario\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)\n",
        "print(METRICS)\n",
        "#X =np.array(df.copy())/255.0 X =np.array(df_all.copy())X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)model = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh',                       solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  prediction = model.predict(X_test)  y =np.copy(y_test)data = {'y_true': y_test,'y_predict': prediction}  # este dado esta no formato de dicionariodf = pd.DataFrame(data, columns=['y_true','y_predict'])confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])print(confusion_matrix)y_true = df['y_true']y_pred = df['y_predict']\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n#X =np.array(df.copy())/255.0 \\nX =np.array(df.copy())\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)\\nmodel = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh', solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  \\nprediction = model.predict(X_test)  \\ny =np.copy(y_test)\\ndata = {'y_true': y_test,'y_predict': prediction}  \\n# este dado esta no formato de dicionario\\ndf = pd.DataFrame(data, columns=['y_true','y_predict'])\\nconfusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\\nprint(confusion_matrix)\\ny_true = df['y_true']\\ny_pred = df['y_predict']  \\nMETRICS=sklearn.metrics.classification_report(y_true, y_pred)\\nprint(METRICS)\\n#X =np.array(df.copy())/255.0 X =np.array(df_all.copy())X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)model = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh',                       solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  prediction = model.predict(X_test)  y =np.copy(y_test)data = {'y_true': y_test,'y_predict': prediction}  # este dado esta no formato de dicionariodf = pd.DataFrame(data, columns=['y_true','y_predict'])confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])print(confusion_matrix)y_true = df['y_true']y_pred = df['y_predict']\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iFNNrlWV9tH",
        "outputId": "bc0f9fd3-25c3-4ea3-a849-a31c64349293"
      },
      "source": [
        "pred_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
              "       1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv5I61yhPQmk",
        "outputId": "dc590f22-2b65-4019-e404-5a15c3a68da5"
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[4] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  #prediction = model.predict_classes(result)\n",
        "  prediction= np.argmax(model.predict(result), axis=-1)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "3   140.0   94.599998  104.799995  ...  155.080002  150.959991  145.440002\n",
            "7   199.0   69.949287   81.643486  ...  159.012558  160.725571  165.267303\n",
            "8   182.0   98.171608   99.284035  ...  157.224869  124.544388   58.508873\n",
            "9   194.0  250.763199  251.585587  ...  188.782745  181.773499  184.002014\n",
            "14  161.0  162.155014  169.920624  ...  161.387512  150.964081  148.801514\n",
            "16  174.0  188.315613  177.896042  ...  137.941757  132.752289  140.082718\n",
            "17  154.0  182.008270  192.487625  ...  230.157028  244.355377  249.247955\n",
            "18  167.0    1.842590    3.006490  ...  111.883438  123.575790  123.373558\n",
            "20  164.0  102.621651  109.965508  ...  178.772156  175.673996  166.965485\n",
            "21  135.0   69.492668   85.651016  ...  142.175400  171.969437  178.589020\n",
            "22  198.0  172.701447  188.410049  ...  174.727997  174.466461  164.208954\n",
            "26  159.0  207.110413  208.191452  ...  179.635300  194.136978  196.263306\n",
            "27  145.0  188.609756  196.827408  ...  178.847672  182.614029  188.185394\n",
            "28  184.0  188.550568  196.688553  ...  122.038269  142.218796  150.217865\n",
            "29  115.0  179.464493  171.713852  ...  119.055420  108.113190  102.942833\n",
            "35  126.0  153.518524  168.975296  ...  224.049377  236.666672  207.679016\n",
            "38  105.0   89.026680   93.995560  ...  224.902252  211.724472  203.662231\n",
            "39  100.0  194.009598  206.065582  ...  160.860809  184.048004  194.152008\n",
            "42  119.0  150.505188  154.795837  ...  202.480957  207.885803  214.986145\n",
            "45  135.0   66.012787   64.711823  ...  133.686676  137.628860  142.052887\n",
            "47  115.0  166.883774  187.980927  ...  113.374657  112.922340  111.572319\n",
            "49  138.0  163.782379  163.377853  ...  136.303711  128.187988  127.320107\n",
            "0   197.0  113.658043  121.195831  ...  194.157013  185.237396  170.283997\n",
            "1   161.0   77.187149  156.321381  ...  111.750488   91.618149   84.113419\n",
            "3   152.0  165.874649  170.925217  ...   30.540857   46.436981   97.727150\n",
            "7   192.0  139.643219  150.088531  ...    1.241753    1.057726    0.805555\n",
            "8   135.0  129.329102  127.730965  ...  158.419678  163.002625  176.205139\n",
            "9   147.0  151.129257  146.965988  ...  139.467117  132.911575  120.158737\n",
            "10  120.0  183.218903  180.833344  ...  190.596649  197.250015  200.662231\n",
            "11  163.0  176.162308  192.455551  ...  184.329208  194.913361  199.286499\n",
            "15  110.0  174.585449  178.934555  ...  241.521652  237.130554  221.839325\n",
            "17  133.0  107.623260  104.016632  ...  148.498627  154.775604  154.925217\n",
            "18  142.0  113.138077  146.580643  ...   97.683990   87.888321   59.514980\n",
            "19  193.0  242.474472  236.527176  ...  131.888245  130.304626   96.654922\n",
            "20  150.0  189.953598  197.028107  ...  147.900269  150.984192  150.835220\n",
            "22  173.0   93.941628  101.156967  ...    1.349895    0.569548    0.511744\n",
            "28  193.0  239.535767  230.744705  ...  250.424805  234.524765  205.759384\n",
            "30  116.0  137.068970  136.275864  ...  140.983368  143.828766  146.529129\n",
            "33  175.0  147.680008  136.441589  ...  247.799988  236.601593  225.855988\n",
            "35  108.0   66.858704   67.779144  ...  148.334702  146.791489  149.644714\n",
            "36  134.0  195.213196  206.989532  ...  158.888855  164.503235  176.093338\n",
            "40  141.0  105.493690  112.362061  ...  149.746140  150.584778  151.818420\n",
            "42  128.0  147.305664  148.994141  ...  196.482422  207.094727  195.316406\n",
            "49  113.0   73.981049   67.345749  ...  158.392914  173.734680  185.558624\n",
            "\n",
            "[44 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRbWgmX_LFH",
        "outputId": "86f44ee1-6f58-4080-859c-40be44bba421"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "from GetBetterSegm import GetBetter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'marquesgabi_paper_fev_2021' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAG_I6FwCvFr",
        "outputId": "1d01d461-e4e9-4f94-8d89-c6416fab152f"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "#!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd marquesgabi_out_2020\n",
        "#%cd Doutorado\n",
        "#PSD_imageJ = 'Amostra7.csv' \n",
        "#PSD_new = pd.read_csv(PSD_imageJ,sep=';')\n",
        "#encoding='utf8'\n",
        "\n",
        "PSD_imageJ = 'Areas_ImageJ.csv'\n",
        "PSD_new = pd.read_csv(PSD_imageJ)\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'marquesgabi_out_2020' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020\n",
            "   Juntas   Area\n",
            "0       1  2.001\n",
            "1       2  0.820\n",
            "2       3  1.270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tEPjIBnv_xM",
        "outputId": "b00574e3-aa2e-4166-aa76-b45c4bdc2e46"
      },
      "source": [
        "PSD_new.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(95, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "PekBHQOT_6CP",
        "outputId": "91c7af43-e156-4157-bd00-c4f0e5f274f7"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>140.0</td>\n",
              "      <td>94.599998</td>\n",
              "      <td>104.799995</td>\n",
              "      <td>109.360001</td>\n",
              "      <td>107.599998</td>\n",
              "      <td>104.720001</td>\n",
              "      <td>97.919998</td>\n",
              "      <td>90.320000</td>\n",
              "      <td>85.159996</td>\n",
              "      <td>86.759995</td>\n",
              "      <td>95.599998</td>\n",
              "      <td>103.000000</td>\n",
              "      <td>110.799995</td>\n",
              "      <td>117.040001</td>\n",
              "      <td>120.439995</td>\n",
              "      <td>111.119995</td>\n",
              "      <td>75.559998</td>\n",
              "      <td>33.639999</td>\n",
              "      <td>34.719997</td>\n",
              "      <td>27.400000</td>\n",
              "      <td>49.200001</td>\n",
              "      <td>62.119999</td>\n",
              "      <td>65.959999</td>\n",
              "      <td>66.919998</td>\n",
              "      <td>68.680000</td>\n",
              "      <td>71.400002</td>\n",
              "      <td>62.919998</td>\n",
              "      <td>64.320000</td>\n",
              "      <td>72.119995</td>\n",
              "      <td>100.239998</td>\n",
              "      <td>107.320000</td>\n",
              "      <td>111.320000</td>\n",
              "      <td>111.479996</td>\n",
              "      <td>101.279999</td>\n",
              "      <td>97.879997</td>\n",
              "      <td>90.559998</td>\n",
              "      <td>83.439995</td>\n",
              "      <td>93.000000</td>\n",
              "      <td>103.639999</td>\n",
              "      <td>109.279999</td>\n",
              "      <td>...</td>\n",
              "      <td>106.479996</td>\n",
              "      <td>107.759995</td>\n",
              "      <td>108.639999</td>\n",
              "      <td>117.159996</td>\n",
              "      <td>127.159996</td>\n",
              "      <td>142.679993</td>\n",
              "      <td>152.519989</td>\n",
              "      <td>154.440002</td>\n",
              "      <td>154.879990</td>\n",
              "      <td>157.000000</td>\n",
              "      <td>159.639999</td>\n",
              "      <td>154.599991</td>\n",
              "      <td>131.479996</td>\n",
              "      <td>131.000000</td>\n",
              "      <td>130.759995</td>\n",
              "      <td>131.520004</td>\n",
              "      <td>131.360001</td>\n",
              "      <td>127.279999</td>\n",
              "      <td>126.919998</td>\n",
              "      <td>128.000000</td>\n",
              "      <td>127.399994</td>\n",
              "      <td>119.599998</td>\n",
              "      <td>116.399994</td>\n",
              "      <td>112.079994</td>\n",
              "      <td>106.079994</td>\n",
              "      <td>106.239998</td>\n",
              "      <td>108.239998</td>\n",
              "      <td>111.239998</td>\n",
              "      <td>107.040001</td>\n",
              "      <td>105.399994</td>\n",
              "      <td>105.279999</td>\n",
              "      <td>112.439995</td>\n",
              "      <td>132.599991</td>\n",
              "      <td>150.759995</td>\n",
              "      <td>155.479996</td>\n",
              "      <td>156.599991</td>\n",
              "      <td>154.919998</td>\n",
              "      <td>155.080002</td>\n",
              "      <td>150.959991</td>\n",
              "      <td>145.440002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>199.0</td>\n",
              "      <td>69.949287</td>\n",
              "      <td>81.643486</td>\n",
              "      <td>107.144165</td>\n",
              "      <td>111.906517</td>\n",
              "      <td>108.416084</td>\n",
              "      <td>113.340179</td>\n",
              "      <td>136.402222</td>\n",
              "      <td>165.753708</td>\n",
              "      <td>157.173203</td>\n",
              "      <td>150.569366</td>\n",
              "      <td>151.194351</td>\n",
              "      <td>167.711060</td>\n",
              "      <td>179.245926</td>\n",
              "      <td>160.467850</td>\n",
              "      <td>143.909119</td>\n",
              "      <td>136.633026</td>\n",
              "      <td>133.548401</td>\n",
              "      <td>135.412445</td>\n",
              "      <td>141.338440</td>\n",
              "      <td>144.703110</td>\n",
              "      <td>143.970703</td>\n",
              "      <td>146.903458</td>\n",
              "      <td>132.437546</td>\n",
              "      <td>120.644730</td>\n",
              "      <td>137.572708</td>\n",
              "      <td>164.816605</td>\n",
              "      <td>161.962997</td>\n",
              "      <td>157.071869</td>\n",
              "      <td>176.374237</td>\n",
              "      <td>161.341019</td>\n",
              "      <td>110.165688</td>\n",
              "      <td>105.967575</td>\n",
              "      <td>103.322166</td>\n",
              "      <td>113.065849</td>\n",
              "      <td>135.239899</td>\n",
              "      <td>153.760757</td>\n",
              "      <td>151.544388</td>\n",
              "      <td>151.784546</td>\n",
              "      <td>152.592148</td>\n",
              "      <td>...</td>\n",
              "      <td>202.189285</td>\n",
              "      <td>195.843216</td>\n",
              "      <td>187.662216</td>\n",
              "      <td>191.851151</td>\n",
              "      <td>198.245087</td>\n",
              "      <td>206.753311</td>\n",
              "      <td>201.158249</td>\n",
              "      <td>179.313797</td>\n",
              "      <td>151.607452</td>\n",
              "      <td>147.617706</td>\n",
              "      <td>152.914795</td>\n",
              "      <td>158.681976</td>\n",
              "      <td>143.478973</td>\n",
              "      <td>147.558960</td>\n",
              "      <td>129.150406</td>\n",
              "      <td>125.749367</td>\n",
              "      <td>128.752823</td>\n",
              "      <td>126.385971</td>\n",
              "      <td>128.153702</td>\n",
              "      <td>129.525635</td>\n",
              "      <td>130.238602</td>\n",
              "      <td>130.094208</td>\n",
              "      <td>142.351669</td>\n",
              "      <td>165.169327</td>\n",
              "      <td>160.093903</td>\n",
              "      <td>157.297867</td>\n",
              "      <td>159.077805</td>\n",
              "      <td>179.868088</td>\n",
              "      <td>207.742172</td>\n",
              "      <td>220.065842</td>\n",
              "      <td>228.598923</td>\n",
              "      <td>230.526505</td>\n",
              "      <td>213.086334</td>\n",
              "      <td>174.931488</td>\n",
              "      <td>163.038040</td>\n",
              "      <td>159.192032</td>\n",
              "      <td>161.763870</td>\n",
              "      <td>159.012558</td>\n",
              "      <td>160.725571</td>\n",
              "      <td>165.267303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>182.0</td>\n",
              "      <td>98.171608</td>\n",
              "      <td>99.284035</td>\n",
              "      <td>102.644974</td>\n",
              "      <td>109.260353</td>\n",
              "      <td>121.911247</td>\n",
              "      <td>102.952667</td>\n",
              "      <td>75.449715</td>\n",
              "      <td>108.195274</td>\n",
              "      <td>146.739655</td>\n",
              "      <td>147.863922</td>\n",
              "      <td>144.686386</td>\n",
              "      <td>134.532547</td>\n",
              "      <td>124.485207</td>\n",
              "      <td>120.378700</td>\n",
              "      <td>135.514801</td>\n",
              "      <td>139.301788</td>\n",
              "      <td>166.224854</td>\n",
              "      <td>166.485229</td>\n",
              "      <td>144.562149</td>\n",
              "      <td>91.236687</td>\n",
              "      <td>76.331367</td>\n",
              "      <td>73.562134</td>\n",
              "      <td>109.899414</td>\n",
              "      <td>138.313614</td>\n",
              "      <td>139.106522</td>\n",
              "      <td>136.739655</td>\n",
              "      <td>135.733749</td>\n",
              "      <td>169.343216</td>\n",
              "      <td>99.153862</td>\n",
              "      <td>95.272202</td>\n",
              "      <td>96.550301</td>\n",
              "      <td>105.319534</td>\n",
              "      <td>110.278107</td>\n",
              "      <td>88.337280</td>\n",
              "      <td>94.218941</td>\n",
              "      <td>135.650894</td>\n",
              "      <td>152.704147</td>\n",
              "      <td>153.923080</td>\n",
              "      <td>151.266281</td>\n",
              "      <td>...</td>\n",
              "      <td>1.153846</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.325444</td>\n",
              "      <td>1.491124</td>\n",
              "      <td>1.881657</td>\n",
              "      <td>17.928995</td>\n",
              "      <td>110.142014</td>\n",
              "      <td>130.254440</td>\n",
              "      <td>188.858002</td>\n",
              "      <td>165.431946</td>\n",
              "      <td>136.414215</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.467456</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.005917</td>\n",
              "      <td>1.112426</td>\n",
              "      <td>0.189349</td>\n",
              "      <td>1.408284</td>\n",
              "      <td>1.254438</td>\n",
              "      <td>0.124260</td>\n",
              "      <td>1.331361</td>\n",
              "      <td>1.331361</td>\n",
              "      <td>0.124260</td>\n",
              "      <td>1.254438</td>\n",
              "      <td>1.408284</td>\n",
              "      <td>0.189349</td>\n",
              "      <td>1.112426</td>\n",
              "      <td>1.153846</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.591716</td>\n",
              "      <td>1.585799</td>\n",
              "      <td>1.408284</td>\n",
              "      <td>3.242604</td>\n",
              "      <td>24.142012</td>\n",
              "      <td>112.846153</td>\n",
              "      <td>157.224869</td>\n",
              "      <td>124.544388</td>\n",
              "      <td>58.508873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>194.0</td>\n",
              "      <td>250.763199</td>\n",
              "      <td>251.585587</td>\n",
              "      <td>252.215607</td>\n",
              "      <td>239.231018</td>\n",
              "      <td>134.646179</td>\n",
              "      <td>129.693466</td>\n",
              "      <td>128.624390</td>\n",
              "      <td>101.492493</td>\n",
              "      <td>81.565620</td>\n",
              "      <td>120.658623</td>\n",
              "      <td>134.171082</td>\n",
              "      <td>138.377090</td>\n",
              "      <td>138.047821</td>\n",
              "      <td>132.786896</td>\n",
              "      <td>146.387909</td>\n",
              "      <td>150.686966</td>\n",
              "      <td>134.283859</td>\n",
              "      <td>124.488571</td>\n",
              "      <td>169.519272</td>\n",
              "      <td>187.792007</td>\n",
              "      <td>191.211273</td>\n",
              "      <td>193.126892</td>\n",
              "      <td>206.833755</td>\n",
              "      <td>214.691132</td>\n",
              "      <td>210.821228</td>\n",
              "      <td>156.749374</td>\n",
              "      <td>119.312454</td>\n",
              "      <td>159.827286</td>\n",
              "      <td>248.969147</td>\n",
              "      <td>245.508438</td>\n",
              "      <td>240.822281</td>\n",
              "      <td>166.145691</td>\n",
              "      <td>100.297585</td>\n",
              "      <td>112.028908</td>\n",
              "      <td>116.699005</td>\n",
              "      <td>114.995529</td>\n",
              "      <td>114.063652</td>\n",
              "      <td>125.978851</td>\n",
              "      <td>131.763412</td>\n",
              "      <td>...</td>\n",
              "      <td>131.618759</td>\n",
              "      <td>135.363480</td>\n",
              "      <td>137.651382</td>\n",
              "      <td>155.842484</td>\n",
              "      <td>191.678589</td>\n",
              "      <td>198.680191</td>\n",
              "      <td>205.466354</td>\n",
              "      <td>196.050446</td>\n",
              "      <td>182.598236</td>\n",
              "      <td>181.672318</td>\n",
              "      <td>186.686020</td>\n",
              "      <td>193.898270</td>\n",
              "      <td>61.347210</td>\n",
              "      <td>70.234238</td>\n",
              "      <td>75.930908</td>\n",
              "      <td>103.080231</td>\n",
              "      <td>111.554459</td>\n",
              "      <td>113.857788</td>\n",
              "      <td>112.256561</td>\n",
              "      <td>108.490585</td>\n",
              "      <td>104.332970</td>\n",
              "      <td>99.727173</td>\n",
              "      <td>88.721222</td>\n",
              "      <td>86.232742</td>\n",
              "      <td>108.709839</td>\n",
              "      <td>127.165359</td>\n",
              "      <td>137.385788</td>\n",
              "      <td>135.747986</td>\n",
              "      <td>138.457733</td>\n",
              "      <td>142.802307</td>\n",
              "      <td>142.020813</td>\n",
              "      <td>154.194290</td>\n",
              "      <td>194.259079</td>\n",
              "      <td>194.893616</td>\n",
              "      <td>203.146439</td>\n",
              "      <td>195.966705</td>\n",
              "      <td>189.535217</td>\n",
              "      <td>188.782745</td>\n",
              "      <td>181.773499</td>\n",
              "      <td>184.002014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>161.0</td>\n",
              "      <td>162.155014</td>\n",
              "      <td>169.920624</td>\n",
              "      <td>181.338379</td>\n",
              "      <td>181.172028</td>\n",
              "      <td>178.311920</td>\n",
              "      <td>170.066162</td>\n",
              "      <td>159.570892</td>\n",
              "      <td>155.801514</td>\n",
              "      <td>158.534973</td>\n",
              "      <td>155.914948</td>\n",
              "      <td>157.871445</td>\n",
              "      <td>171.429123</td>\n",
              "      <td>182.784500</td>\n",
              "      <td>186.470718</td>\n",
              "      <td>188.332718</td>\n",
              "      <td>188.325165</td>\n",
              "      <td>178.139908</td>\n",
              "      <td>168.147446</td>\n",
              "      <td>164.364853</td>\n",
              "      <td>161.310028</td>\n",
              "      <td>164.909271</td>\n",
              "      <td>168.349731</td>\n",
              "      <td>171.043472</td>\n",
              "      <td>176.589798</td>\n",
              "      <td>184.162582</td>\n",
              "      <td>188.638962</td>\n",
              "      <td>90.455574</td>\n",
              "      <td>118.716454</td>\n",
              "      <td>168.211716</td>\n",
              "      <td>172.884705</td>\n",
              "      <td>182.064270</td>\n",
              "      <td>185.202286</td>\n",
              "      <td>183.113434</td>\n",
              "      <td>172.599258</td>\n",
              "      <td>164.561447</td>\n",
              "      <td>164.315704</td>\n",
              "      <td>164.236282</td>\n",
              "      <td>165.865799</td>\n",
              "      <td>169.557678</td>\n",
              "      <td>...</td>\n",
              "      <td>146.391296</td>\n",
              "      <td>123.608704</td>\n",
              "      <td>85.877121</td>\n",
              "      <td>76.122879</td>\n",
              "      <td>99.156906</td>\n",
              "      <td>116.049164</td>\n",
              "      <td>120.807175</td>\n",
              "      <td>158.913055</td>\n",
              "      <td>163.017029</td>\n",
              "      <td>158.472610</td>\n",
              "      <td>147.652176</td>\n",
              "      <td>146.646515</td>\n",
              "      <td>192.482056</td>\n",
              "      <td>192.527405</td>\n",
              "      <td>197.155014</td>\n",
              "      <td>187.627609</td>\n",
              "      <td>177.009460</td>\n",
              "      <td>179.232513</td>\n",
              "      <td>174.190918</td>\n",
              "      <td>183.956528</td>\n",
              "      <td>197.608704</td>\n",
              "      <td>221.967865</td>\n",
              "      <td>221.028351</td>\n",
              "      <td>203.485825</td>\n",
              "      <td>167.381851</td>\n",
              "      <td>98.542534</td>\n",
              "      <td>133.296783</td>\n",
              "      <td>141.882797</td>\n",
              "      <td>157.877136</td>\n",
              "      <td>175.291122</td>\n",
              "      <td>178.584122</td>\n",
              "      <td>165.994339</td>\n",
              "      <td>141.103973</td>\n",
              "      <td>130.517960</td>\n",
              "      <td>133.612488</td>\n",
              "      <td>167.000000</td>\n",
              "      <td>171.224960</td>\n",
              "      <td>161.387512</td>\n",
              "      <td>150.964081</td>\n",
              "      <td>148.801514</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Width           0           1  ...         781         782         783\n",
              "3   140.0   94.599998  104.799995  ...  155.080002  150.959991  145.440002\n",
              "7   199.0   69.949287   81.643486  ...  159.012558  160.725571  165.267303\n",
              "8   182.0   98.171608   99.284035  ...  157.224869  124.544388   58.508873\n",
              "9   194.0  250.763199  251.585587  ...  188.782745  181.773499  184.002014\n",
              "14  161.0  162.155014  169.920624  ...  161.387512  150.964081  148.801514\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "VaZPe_AxNBK9",
        "outputId": "d17ea49e-03e0-4a8a-f8ca-5bc6413e7b5a"
      },
      "source": [
        "PSD_new.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Juntas</th>\n",
              "      <th>Area</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1.162</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Juntas   Area\n",
              "0       1  2.001\n",
              "1       2  0.820\n",
              "2       3  1.270\n",
              "3       4  0.958\n",
              "4       5  1.162"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "#lost_value = float(PSD_new.columns[1])\n",
        "\n",
        "# Area = np.array(PSD_new.iloc[:,1])\n",
        "Area = PSD_new['Area'].values\n",
        "# Area = np.concatenate( (Area, [lost_value] ) )\n",
        "# Area = np.concatenate( (Area, [lost_value] ) )\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfk_fNXGDK5_"
      },
      "source": [
        "wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        "wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        "X = pd.DataFrame([Diam1,Diameter_All])\n",
        "wts = pd.DataFrame([wt1,wt2])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "OieAXw_by3nz",
        "outputId": "e16f1072-f80e-45a2-fe1a-ce71d4349ba4"
      },
      "source": [
        "A = plt.hist(X,weights=wts,bins=7)\n",
        "plt.legend(['True','CNN'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fdad6538410>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpdrvEySy8Ij"
      },
      "source": [
        "B = A[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUhGZHT8y9Or",
        "outputId": "50424485-2ac5-4dca-ae72-b252c95d1912"
      },
      "source": [
        "Novo = []\n",
        "k = 0\n",
        "soma = 0\n",
        "for i in B:\n",
        "  if(k<4):\n",
        "    Novo.append(i)\n",
        "  else:\n",
        "    soma = soma + i\n",
        "  k = k + 1\n",
        "Novo.append(soma)\n",
        "print(Novo)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14.736842105263156, 24.2105263157895, 42.1052631578948, 14.736842105263179, 4.21052631578948]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "yMK89w-fzCVe",
        "outputId": "cc56dfd3-8cfb-4605-87d7-556ad12d2b2c"
      },
      "source": [
        "# Freq1 = [19.12043703, 29.22484843, 19.35872174, 20.82190224, 11.47409056] # avarage 4 samples\n",
        "Freq1 = [20.69301557, 28.55598044, 18.50768331, 22.7106327, 8.905907357] # avarage 10 samples\n",
        "#Freq2 = [16.93792791, 31.38008965, 24.93810752, 18.56158392, 6.233810752, 0.4]\n",
        "Freq2 = [16.93792791, 31.38008965, 24.93810752, 18.56158392, 6.633810752]\n",
        "Freq3 = Novo\n",
        "barWidth = 0.25\n",
        "\n",
        "br1 = range(len(Freq1))\n",
        "# Set position of bar on X axis\n",
        "br2 = [x + barWidth for x in br1]\n",
        "br3 = [x + barWidth for x in br2]\n",
        "# labels = [0.8, 1.0, 1.2, 1.4, 1.6, 1.8]\n",
        "labels = [0.8, 1.0, 1.2, 1.4, 1.6]\n",
        "\n",
        "xx=[]\n",
        "for a in labels:\n",
        "  xx.append(str(a))\n",
        "plt.bar(br1, Freq1 , color=\"green\", align=\"center\", width=0.3, tick_label= xx) \n",
        "plt.bar(br2, Freq2 , color=\"red\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.bar(br3, Freq3 , color=\"blue\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.legend(['CNN 1','CNN 2','True'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fdb40a01e50>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUXklEQVR4nO3df5BdZX3H8fe3m+BagUbCBjNZ6UbEkoTKAguxA1ogxcHUqWKkA6U2qZlJnTZOU7VFSKckrR2akR8BFTtRmEREhUEpyCAV+VEVRNwNaQyktQihbhrIJtFWWhMIfPvHvYtLcjf37t5fOdn3a+ZO7jnnufd8z93sZ8+efc7zRGYiSSqeX2l3AZKk8THAJamgDHBJKigDXJIKygCXpIKa1MqdHX300dnT09PKXUpS4Q0MDOzIzK5917c0wHt6eujv72/lLiWp8CLimUrrvYQiSQVlgEtSQRngklRQLb0GLkn7evHFFxkcHGT37t3tLqXtOjs76e7uZvLkyTW1N8AltdXg4CBHHHEEPT09RES7y2mbzGTnzp0MDg4yc+bMml7jJRRJbbV7926mTp06ocMbICKYOnXqmH4TMcAltd1ED+9hY/0cDHBJKiivgUs6qMTKxp6N5+XV5zx49tlnWbZsGT/4wQ+YMmUKxxxzDKtXr+awww5j5syZXHfddXz4wx8GYOnSpfT19bFo0SIWLVrEvffey1NPPcVrXvMaduzYQV9fH1u2bNlvHx/84Ae56667mDZtGps2bWrIsXkGLlUR0biHDj6Zyfnnn89ZZ53Fj3/8YwYGBrjiiit47rnnAJg2bRrXXnstL7zwQsXXd3R0cOONN1bdz6JFi7jnnnsaWrsBLmlCe+CBB5g8eTIf+tCHXll30kkn8fa3vx2Arq4u5s2bx7p16yq+ftmyZVxzzTXs3bv3gPt5xzvewVFHHdW4wjHAJU1wmzZt4tRTTz1gm0suuYQrr7ySl156ab9txx57LGeeeSY33XRTs0oclQEuSVW86U1vYu7cuXzpS1+quP3SSy/lk5/8JC+//HJL6zLAJU1oc+bMYWBgoGq7yy67jFWrVlFpIvjjjz+e3t5ebr311maUOCoDXNKEds4557Bnzx7WrFnzyrqNGzfyne9851XtTjjhBGbPns3Xv/71iu+zfPlyrrzyyqbWui+7EUo6qNTS7a+RIoLbb7+dZcuWsWrVKjo7O+np6WH16tX7tV2+fDknn3xyxfeZM2cOp5xyCuvXr6+4/aKLLuLBBx9kx44ddHd3s3LlShYvXlxf7ZV+HajYMKID6Ae2Zua7I2Im8BVgKjAAfCAzK/ezKevr60sndFDRNLL7X43fbhPK5s2bmTVrVrvLOGhU+jwiYiAz+/ZtO5ZLKH8ObB6xvAq4JjPfDPwUqO9HiSRpTGoK8IjoBn4X+Hx5OYBzgNvKTdYB721GgZKkymo9A18N/BUw3EdmKvCzzBzuuT4IzKj0wohYEhH9EdE/NDRUV7GSpF+qGuAR8W5ge2ZW72dTQWauycy+zOzr6tpvUmVJ0jjV0gvlDOD3ImI+0AkcCVwLTImISeWz8G5ga/PKlCTtq+oZeGZempndmdkDXAjcn5kXAw8A7y83Wwjc0bQqJUn7qedGnkuAj0TEk5Suid/QmJIkTWiNHP6xxj6gzz77LBdeeCHHHXccp556KvPnz+dHP/oRW7ZsISL41Kc+9UrbpUuXsnbtWqA0wuCMGTPYs2cPADt27KCnp2e/9//JT37C2WefzezZs5kzZw7XXntt3R8TjDHAM/PBzHx3+flTmXl6Zr45My/IzD0NqUiSWqgVw8lOmjSJq666iieeeIJHHnmEz3zmMzzxxBN11+6t9JImtFYMJzt9+nROOeUUAI444ghmzZrF1q31/9nQAJc0obV6ONktW7bw2GOPMXfu3HHVO5IBLklVNGo42eeff54FCxawevVqjjzyyLrrMsAlTWitGk72xRdfZMGCBVx88cW8733vq6vmYQa4pAmtFcPJZiaLFy9m1qxZfOQjH2lY7Qa4pINLZmMfVQwPJ/utb32L4447jjlz5nDppZfyhje8Yb+2y5cvZ3BwsOL7DA8nW8lDDz3ETTfdxP33309vby+9vb3cfffdY/tcKtVe63CyjeBwsioih5NtLoeTfbVmDScrSTqIGOCSVFAGuCQVlAEuSQVlgEtSQRngklRQtUzoIEkt08hum1C96+bOnTuZN28eUBpWtqOjg+HZwx599FEOO+ywxhbUQAa4pAlt6tSpbNiwAYAVK1Zw+OGH87GPfeyV7Xv37mXSpIMzKg/OqiSpjRYtWkRnZyePPfYYZ5xxBkceeeSrgv3EE0/krrvuoqenhy9+8Ytcd911vPDCC8ydO5frr7+ejo6OltRZy6TGnRHxaET8a0Q8HhEry+vXRsTTEbGh/OhtfrmS1BqDg4M8/PDDXH311aO22bx5M7fccgsPPfQQGzZsoKOjg5tvvrllNdZyBr4HOCczn4+IycB3I+Ib5W1/mZm3Na88SWqPCy64oOqZ9H333cfAwACnnXYaAL/4xS+YNm1aK8oDagjwLA2W8nx5cXL54YgOkg5pr3vd6155PmnSpFeN9b17926gNMrgwoULueKKK1peH9TYjTAiOiJiA7AduDczv1/e9PcRsTEiromI14zy2iUR0R8R/UNDQw0qW5Jap6enh/Xr1wOwfv16nn76aQDmzZvHbbfdxvbt2wHYtWsXzzzzTMvqqinAM/OlzOwFuoHTI+JE4FLgBOA04ChKs9RXeu2azOzLzL7hrjmSNJoWjyZbkwULFrBr1y7mzJnDpz/9ad7ylrcAMHv2bD7xiU/wzne+k7e+9a2ce+65bNu2rTE7rcGYh5ONiL8B/i8zrxyx7izgY8Mz1o/G4WRVRA4n21wOJ/tqDR1ONiK6ImJK+flrgXOBf4uI6eV1AbwX2NSA2iVJNaqlF8p0YF1EdFAK/Fsz866IuD8iuoAANgAfamKdkqR91NILZSNwcoX15zSlIkkTTmYSjb6HvoDGeknbwawktVVnZyc7d+4cc3gdajKTnTt30tnZWfNrvJVeUlt1d3czODiI3YxLP8y6u7trbm+AS2qryZMnM3PmzHaXUUheQpGkgjLAJamgDHBJKigDXJIKygCXpIIywCWpoAxwSSooA1ySCsobeSYCx0OVDkmegUtSQRngklRQBrgkFVQtM/J0RsSjEfGvEfF4RKwsr58ZEd+PiCcj4paIOKz55UqShtVyBr4HOCczTwJ6gfMi4m3AKuCazHwz8FNgcfPKlCTtq2qAZ8nz5cXJ5UcC5wC3ldevozQvpiSpRWq6Bh4RHRGxAdgO3Av8GPhZZu4tNxkEZozy2iUR0R8R/Q7YLkmNU1OAZ+ZLmdkLdAOnAyfUuoPMXJOZfZnZ19XVNc4yJUn7GlMvlMz8GfAA8FvAlIgYvhGoG9ja4NokSQdQSy+UroiYUn7+WuBcYDOlIH9/udlC4I5mFSlJ2l8tt9JPB9ZFRAelwL81M++KiCeAr0TEJ4DHgBuaWKckaR9VAzwzNwInV1j/FKXr4ZKkNvBOTEkqKEcjLIhYOf4RBR0/UDo0eQYuSQVlgEtSQRngklRQBrgkFZQBLkkFZYBLUkEZ4JJUUAa4JBWUAS5JBWWAS1JBGeCSVFAGuCQVlAEuSQVlgEtSQdUypdobI+KBiHgiIh6PiD8vr18REVsjYkP5Mb/55UqShtUyHvhe4KOZuT4ijgAGIuLe8rZrMvPK5pUnSRpNLVOqbQO2lZ//PCI2AzOaXZgk6cDGdA08InoozY/5/fKqpRGxMSJujIjXj/KaJRHRHxH9Q0NDdRUrjUtEfQ/pIFVzgEfE4cBXgWWZ+T/AZ4HjgF5KZ+hXVXpdZq7JzL7M7Ovq6mpAyZIkqDHAI2IypfC+OTO/BpCZz2XmS5n5MvA5nKFeklqqll4oAdwAbM7Mq0esnz6i2fnApsaXJ0kaTS29UM4APgD8MCI2lNddBlwUEb2UJj3fAvxJUypU28XIee3rvCScWb2NpNrU0gvlu1T+tr278eVIkmrlnZiSVFAGuCQVlAEuSQVlgEtSQRngklRQtXQjlNQmsbJxt/Ln5fbhPNR4Bi5JBWWAS1JBGeCSVFAGuCQVlAEuSQVlgEtSQRngklRQBrgkFZQBLkkFVfVOzIh4I/AF4BhKkzesycxrI+Io4Bagh9KEDr+fmT9tVqHekSZJr1bLGfhe4KOZORt4G/BnETEb+DhwX2YeD9xXXpYktUjVAM/MbZm5vvz858BmYAbwHmBdudk64L3NKlKStL8xXQOPiB7gZOD7wDGZua286VlKl1gqvWZJRPRHRP/Q0FAdpUqSRqo5wCPicOCrwLLM/J+R2zIzgYoXljNzTWb2ZWZfV1dXXcVKkn6ppgCPiMmUwvvmzPxaefVzETG9vH06sL05JUqSKqka4BERwA3A5sy8esSmO4GF5ecLgTsaX54kaTS1TOhwBvAB4IcRsaG87jLgH4BbI2Ix8Azw+80pUZJUSdUAz8zvAqN1wp7X2HIkSbXyTkxJKigDXJIKykmNddCrdxiFiTxwQq4YsbCizuEociJ/kgcnz8AlqaAMcEkqKANckgrKAJekgjLAJamgDHBJKqgJ043Q7lSSDjWegUtSQRngklRQBrgkFZQBLkkFZYBLUkEZ4JJUULVMqXZjRGyPiE0j1q2IiK0RsaH8mN/cMiVJ+6rlDHwtcF6F9ddkZm/5cXdjy5IkVVM1wDPz28CuFtQiSRqDeq6BL42IjeVLLK8frVFELImI/ojoHxoaqmN3kqSRxhvgnwWOA3qBbcBVozXMzDWZ2ZeZfV1dXePcXfsF+ctHUNdDKgr/3x/cxhXgmflcZr6UmS8DnwNOb2xZkqRqxhXgETF9xOL5wKbR2kqSmqPqaIQR8WXgLODoiBgELgfOioheSvPFbgH+pIk1SpIqqBrgmXlRhdU3NKEWSdIYeCemJBWUAS5JBWWAS1JBGeCSVFAGuCQVlAEuSQVlgEtSQRngklRQBrgkFZQBLkkFZYBLUkEZ4JJUUAa4JBWUAS5JBWWAS1JBVQ3w8qTF2yNi04h1R0XEvRHxH+V/R53UWJLUHLWcga8Fzttn3ceB+zLzeOC+8rIkqYWqBnhmfhvYtc/q9wDrys/XAe9tcF2SpCrGew38mMzcVn7+LHBMg+qRJNWo7j9iZmZSmty4oohYEhH9EdE/NDRU7+4kSWXjDfDnImI6QPnf7aM1zMw1mdmXmX1dXV3j3J0kaV/jDfA7gYXl5wuBOxpTjiSpVrV0I/wy8D3gNyJiMCIWA/8AnBsR/wH8TnlZktRCk6o1yMyLRtk0r8G1SJLGoGqAS1K7xMpo2Hvl5aP2tSgsb6WXpIIywCWpoAxwSSooA1ySCsoAl6SCMsAlqaAMcEkqKANckgrKAJekgjLAJamgvJVe0iErV4xYWFHnbfl58N2K7xm4JBWUAS5JBWWAS1JBGeCSVFB1/REzIrYAPwdeAvZmZl8jipIkVdeIXihnZ+aOBryPJGkMvIQiSQVVb4An8M2IGIiIJZUaRMSSiOiPiP6hoaE6dydJGlZvgJ+ZmacA7wL+LCLesW+DzFyTmX2Z2dfV1VXn7iRJw+oK8MzcWv53O3A7cHojipIkVTfuAI+I10XEEcPPgXcCmxpVmCTpwOrphXIMcHtEDL/PlzLznoZUJUkHgSBHLtSlGUOpjDvAM/Mp4KQG1iJJGgO7EUpSQRngklRQBrgkFZQBLkkFZYBLUkEZ4JJUUAa4JBWUAS5JBWWAS1JBGeCSVFAGuCQVlAEuSQVlgEtSQRngklRQBrgkFZQBLkkFVVeAR8R5EfHvEfFkRHy8UUVJkqqrZ07MDuAzlGaknw1cFBGzG1WYJOnA6jkDPx14MjOfyswXgK8A72lMWZKkauqZ1HgG8JMRy4PA3H0bRcQSYEl58fmI+Pc69jludc5HWus7HA3sqPpO9RczJi06dqjh+Ft97ODXvkXv4Ne+2jvVV8yvV1pZT4DXJDPXAGuavZ+DQUT0Z2Zfu+tol4l8/BP52GFiH387j72eSyhbgTeOWO4ur5MktUA9Af4D4PiImBkRhwEXAnc2pixJUjXjvoSSmXsjYinwz0AHcGNmPt6wyoppQlwqOoCJfPwT+dhhYh9/2449MrNd+5Yk1cE7MSWpoAxwSSooA3wcqg0hEBHHRsQDEfFYRGyMiPntqLMZIuLGiNgeEZtG2R4RcV35s9kYEae0usZmqeHYLy4f8w8j4uGIOKnVNTZTteMf0e60iNgbEe9vVW3NVsuxR8RZEbEhIh6PiH9pRV0G+BjVOITAXwO3ZubJlHrnXN/aKptqLXDeAba/Czi+/FgCfLYFNbXKWg587E8Dv52Zvwn8HYfeH/bWcuDjH/7+WAV8sxUFtdBaDnDsETGF0vf572XmHOCCVhRlgI9dLUMIJHBk+fmvAf/VwvqaKjO/Dew6QJP3AF/IkkeAKRExvTXVNVe1Y8/MhzPzp+XFRyjdG3HIqOFrD/Bh4KvA9uZX1Do1HPsfAF/LzP8st2/J8RvgY1dpCIEZ+7RZAfxhRAwCd1P6Tz1R1PL5TASLgW+0u4hWiogZwPkcWr911eotwOsj4sGIGIiIP2rFTpt+K/0EdRGwNjOviojfAm6KiBMz8+V2F6bmi4izKQX4me2upcVWA5dk5svRjoFP2msScCowD3gt8L2IeCQzf9TsnWpsahlCYDHl62WZ+b2I6KQ04M0h9WvlKCb0EAsR8Vbg88C7MnNnu+tpsT7gK+XwPhqYHxF7M/Of2ltWSwwCOzPzf4H/jYhvAycBTQ1wL6GMXS1DCPwnpZ/ERMQsoBMYammV7XMn8Efl3ihvA/47M7e1u6hWiIhjga8BH2j2mdfBKDNnZmZPZvYAtwF/OkHCG+AO4MyImBQRv0ppZNbNzd6pZ+BjNNoQAhHxt0B/Zt4JfBT4XET8BaU/aC7KQ+SW14j4MnAWcHT5Gv/lwGSAzPxHStf85wNPAv8H/HF7Km28Go79b4CpwPXls9C9h9IIfTUc/yGr2rFn5uaIuAfYCLwMfD4zD9jdsiF1HSK5IkkTjpdQJKmgDHBJKigDXJIKygCXpIIywCWpoAxwSSooA1ySCur/AaH2WnyM8BF9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}