{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_final_amostra_03_nov_22_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/defesa/PSD_histogram_final_amostra_03_nov_22_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from sklearn.metrics import r2_score\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZEvJvfoibE4"
      },
      "source": [
        "#!pip install mahotas"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT",
        "outputId": "30f4fc3c-6090-4a1c-cbb0-1b7507fb6cf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "'''\n",
        "import mahotas.features.texture as mht\n",
        "import mahotas.features\n",
        "'''"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nimport mahotas.features.texture as mht\\nimport mahotas.features\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "_VcTdaNVh9EE",
        "outputId": "c38c23e3-440f-4ccc-8d48-072b337ee303"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas\n",
        "'''"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\\n%cd marquesgabi_fev_2020\\nimport Go2BlackWhite\\nimport Go2Mahotas\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UZ30b1EVQhq"
      },
      "source": [
        "def BlackWhite(Transfere,Size):\n",
        "\n",
        "  img_name=[]\n",
        "  xw=[]\n",
        "  ww=[]\n",
        "\n",
        "  with zipfile.ZipFile(Transfere, \"r\") as f:\n",
        "    for name in f.namelist():\n",
        "      img_name.append(name)\n",
        "      #xw.append(cv2.imread(name))\n",
        "      xw.append(cv2.resize(cv2.imread(name),(Size,Size)))\n",
        "\n",
        "  nrow=len(img_name)\n",
        "  ncol=Size*Size\n",
        "  pw=np.zeros((nrow,ncol))\n",
        "  #pw=[]\n",
        "  for i in range(nrow):\n",
        "    ww.append(cv2.cvtColor(np.array(xw[i]), cv2.COLOR_BGR2GRAY))\n",
        "    pw[i,:]=ww[i].ravel()\n",
        "  return ww,img_name"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v7SRrc8mH2N",
        "outputId": "1ed91e61-a7a4-49ba-dfc5-6502ed1212ba"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip' \n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Doutorado'...\n",
            "remote: Enumerating objects: 464, done.\u001b[K\n",
            "remote: Counting objects: 100% (214/214), done.\u001b[K\n",
            "remote: Compressing objects: 100% (210/210), done.\u001b[K\n",
            "remote: Total 464 (delta 102), reused 4 (delta 3), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (464/464), 166.12 MiB | 29.08 MiB/s, done.\n",
            "Resolving deltas: 100% (225/225), done.\n",
            "/content/Doutorado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqIYzUcnrdMp",
        "outputId": "af587028-9044-48f3-c3ea-c8bbfbeae5a5"
      },
      "source": [
        "labels =[]\n",
        "with zipfile.ZipFile(Transfere, \"r\") as f:\n",
        "  for f in f.namelist():\n",
        "    labels.append(f)\n",
        "print(labels)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Fotos_Grandes-3cdAmostra/Q6-8-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-4.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "# ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "ww,img_name=BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[4] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHgqAnaFyCjp",
        "outputId": "6a6ce319-b7de-4738-a687-dd852a818614"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 167, done.\u001b[K\n",
            "remote: Counting objects: 100% (167/167), done.\u001b[K\n",
            "remote: Compressing objects: 100% (165/165), done.\u001b[K\n",
            "remote: Total 167 (delta 66), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (167/167), 211.71 MiB | 22.15 MiB/s, done.\n",
            "Resolving deltas: 100% (66/66), done.\n",
            "Checking out files: 100% (48/48), done.\n",
            "/content/Doutorado/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from segment_filter_not_conclude import Segmenta  # got image provided segmented"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN5MN5a_v4np",
        "outputId": "2bccb532-8a28-4189-91ef-48b5d1d36107"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     102  215.197632  212.785492  ...  155.264145  154.048843  160.749329\n",
            "1     150  174.930847  174.756638  ...   98.711288   98.479462  118.279289\n",
            "2     131  172.534821  168.727341  ...  170.653381  179.598862  186.003937\n",
            "3     146  101.802582  106.869583  ...   60.719646   65.152939   70.389191\n",
            "4     193   86.418404   98.353859  ...  213.624954  227.768829  217.027878\n",
            "5     135  136.020020  142.663971  ...  188.428085  201.377228  206.629990\n",
            "6     113  230.574677  238.703247  ...   79.136269   80.043854   79.697388\n",
            "7     116  102.853745  106.343636  ...  120.500587  123.022591  153.556488\n",
            "8     146   89.538368   92.093079  ...  153.708771  148.396896  140.676117\n",
            "9     104  161.587296  161.399429  ...  186.337296  184.593201  182.754456\n",
            "10    116   16.825207   34.185490  ...    1.000000    1.000000    1.000000\n",
            "11    117  130.671860  130.837524  ...  170.094666  175.737381  179.494995\n",
            "12    112  164.687500  141.625000  ...  184.625000  168.625000  132.812500\n",
            "13    100  153.062393  151.081604  ...  136.209595  150.974396  153.263992\n",
            "14    146  134.788879  139.727524  ...  109.141861  112.779694  112.727524\n",
            "15    107  129.186218   97.909424  ...    5.361691    0.499520    1.474452\n",
            "16    172  110.650085  109.514877  ...    0.998919    0.173607    1.352082\n",
            "17    132   84.879707   97.358139  ...    1.287420    0.348026    0.469238\n",
            "18    178  179.212738  168.077042  ...    1.020578    1.060094    0.924000\n",
            "19    130  186.579407  169.462250  ...  142.512665  146.566620  146.635742\n",
            "20    126  115.000000  106.358032  ...    0.135802    0.580247    1.469136\n",
            "21    126  145.716064  141.543213  ...    0.160494    0.604938    1.493827\n",
            "22    182  222.816589  217.443817  ...  166.295853  175.248535  175.473389\n",
            "23    148  133.189209  131.872177  ...  145.751633  145.273193  148.068665\n",
            "24    149  186.657135  173.357376  ...  148.586914  149.579041  156.261078\n",
            "25    190  167.990570  142.895844  ...  146.535614  160.164627  168.869354\n",
            "26    124  162.294479  169.497391  ...  145.340271   78.827263  103.787720\n",
            "27    198  112.599625   95.791756  ...  166.357513  152.414536  145.240982\n",
            "28    117    1.967492    1.047045  ...  122.558258  120.952225  118.547966\n",
            "29    136  199.044983  207.711945  ...  129.493073  130.848618  125.602951\n",
            "30    110  229.259491  230.329895  ...  209.346436  203.479675  193.942474\n",
            "31    198  114.930405  121.638901  ...   45.100704   32.284866   21.474541\n",
            "32    173    1.216947    1.482843  ...   65.173439   44.231449   21.286545\n",
            "33    148  163.934265  162.556625  ...    0.576333    0.360117    1.414171\n",
            "34    176   98.319214  121.196800  ...  144.650818  144.662704  155.576447\n",
            "35    157  148.995529  138.781494  ...  170.389481  174.694504  175.253311\n",
            "36    167    1.525118    1.201764  ...   70.179077   75.234001   88.933243\n",
            "37    100  137.358398  141.737595  ...    1.000000    1.000000    1.000000\n",
            "38    176  218.828506  219.571793  ...    0.425620    1.062500    1.204545\n",
            "39    130  223.708633  225.377762  ...  183.569000  193.293503  190.383209\n",
            "40    108  137.319626  115.331955  ...   97.781891  102.961586  116.976685\n",
            "41    164  105.237350  110.982750  ...  129.599640  126.339081  136.915527\n",
            "42    183  144.550568  138.607254  ...  187.841507  186.756149  170.617844\n",
            "43    160  179.605606  226.061234  ...  175.910629  179.736862  185.132507\n",
            "44    177  196.253433  197.553757  ...  146.641571  146.546387  148.202667\n",
            "45    103   99.827606  100.045143  ...    0.047507    0.929305    1.617589\n",
            "46    148  134.194305  145.788910  ...   99.219879  100.235214   97.269554\n",
            "47    137  244.543976  230.557510  ...  223.113220  202.341934   84.405243\n",
            "48    122   40.522709   31.741734  ...    0.069067    0.659231    1.511690\n",
            "49    123  174.965576  173.460922  ...  171.965164  163.377167  150.669922\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2emP4rNjQy",
        "outputId": "b85f8435-aea9-4df5-c08b-a5c813c178ec"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 167, done.\u001b[K\n",
            "remote: Counting objects: 100% (167/167), done.\u001b[K\n",
            "remote: Compressing objects: 100% (165/165), done.\u001b[K\n",
            "remote: Total 167 (delta 66), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (167/167), 211.71 MiB | 21.33 MiB/s, done.\n",
            "Resolving deltas: 100% (66/66), done.\n",
            "Checking out files: 100% (48/48), done.\n",
            "/content/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIFPGE_-vx3T"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "Description = '3 layers of Convolution: 64, 128, 256 '\n",
        "N1 = 20\n",
        "N2 = 20\n",
        "\n",
        "# make the CNN\n",
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=64, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=N1))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=N2))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbPQ1FSRG6A",
        "outputId": "6aa006a8-7191-4fa8-ff32-08c680fbf59b"
      },
      "source": [
        "\n",
        "# training the model\n",
        "r = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), \n",
        "              epochs=200, batch_size=32)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 2s 152ms/step - loss: 0.6241 - accuracy: 0.6997 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.3616 - accuracy: 0.8426 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.1834 - accuracy: 0.9475 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.1792 - accuracy: 0.9096 - val_loss: 0.6930 - val_accuracy: 0.4898\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0897 - accuracy: 0.9738 - val_loss: 0.6929 - val_accuracy: 0.4898\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0648 - accuracy: 0.9825 - val_loss: 0.6926 - val_accuracy: 0.5102\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0337 - accuracy: 0.9883 - val_loss: 0.6926 - val_accuracy: 0.5102\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0435 - accuracy: 0.9883 - val_loss: 0.6924 - val_accuracy: 0.5102\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0499 - accuracy: 0.9796 - val_loss: 0.6925 - val_accuracy: 0.4966\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0492 - accuracy: 0.9883 - val_loss: 0.6930 - val_accuracy: 0.4898\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0191 - accuracy: 0.9971 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0187 - accuracy: 0.9971 - val_loss: 0.6924 - val_accuracy: 0.4898\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0149 - accuracy: 0.9971 - val_loss: 0.6921 - val_accuracy: 0.5102\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.6922 - val_accuracy: 0.5102\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.6936 - val_accuracy: 0.5102\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0106 - accuracy: 0.9942 - val_loss: 0.6953 - val_accuracy: 0.5102\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0057 - accuracy: 0.9971 - val_loss: 0.7030 - val_accuracy: 0.5102\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 0.0079 - accuracy: 0.9971 - val_loss: 0.7043 - val_accuracy: 0.5102\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.7045 - val_accuracy: 0.5102\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.7125 - val_accuracy: 0.5102\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0065 - accuracy: 0.9971 - val_loss: 0.7351 - val_accuracy: 0.5102\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.7563 - val_accuracy: 0.5102\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.7632 - val_accuracy: 0.5102\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.7649 - val_accuracy: 0.5102\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.7869 - val_accuracy: 0.5102\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.8180 - val_accuracy: 0.5102\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 8.5563e-04 - accuracy: 1.0000 - val_loss: 0.8253 - val_accuracy: 0.5102\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.8278 - val_accuracy: 0.5102\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 8.2744e-04 - accuracy: 1.0000 - val_loss: 0.8398 - val_accuracy: 0.5102\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 4.8404e-04 - accuracy: 1.0000 - val_loss: 0.8431 - val_accuracy: 0.5102\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 9.2414e-04 - accuracy: 1.0000 - val_loss: 0.8320 - val_accuracy: 0.5102\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 5.5684e-04 - accuracy: 1.0000 - val_loss: 0.8206 - val_accuracy: 0.5102\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.8352 - val_accuracy: 0.5102\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0040 - accuracy: 0.9971 - val_loss: 0.9041 - val_accuracy: 0.5102\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.9597 - val_accuracy: 0.5102\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.0024 - val_accuracy: 0.5102\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.0172 - val_accuracy: 0.5102\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 5.2524e-04 - accuracy: 1.0000 - val_loss: 0.9750 - val_accuracy: 0.5102\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.9984 - val_accuracy: 0.5102\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 5.8519e-04 - accuracy: 1.0000 - val_loss: 0.9143 - val_accuracy: 0.5102\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 5.5783e-04 - accuracy: 1.0000 - val_loss: 0.9186 - val_accuracy: 0.5102\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 4.7552e-04 - accuracy: 1.0000 - val_loss: 0.9343 - val_accuracy: 0.5102\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 4.2066e-04 - accuracy: 1.0000 - val_loss: 0.9315 - val_accuracy: 0.5102\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 2.1508e-04 - accuracy: 1.0000 - val_loss: 0.9802 - val_accuracy: 0.5102\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.5765 - val_accuracy: 0.5102\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 5.9951e-04 - accuracy: 1.0000 - val_loss: 1.6309 - val_accuracy: 0.5102\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 2.2309e-04 - accuracy: 1.0000 - val_loss: 1.4466 - val_accuracy: 0.5102\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 8.3299e-04 - accuracy: 1.0000 - val_loss: 1.2559 - val_accuracy: 0.5102\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 1.0691e-04 - accuracy: 1.0000 - val_loss: 1.2581 - val_accuracy: 0.5102\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 4.1179e-04 - accuracy: 1.0000 - val_loss: 1.2571 - val_accuracy: 0.5102\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 1.9977e-04 - accuracy: 1.0000 - val_loss: 1.2482 - val_accuracy: 0.5102\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 3.3787e-04 - accuracy: 1.0000 - val_loss: 1.0144 - val_accuracy: 0.5102\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 3.4259e-04 - accuracy: 1.0000 - val_loss: 0.7618 - val_accuracy: 0.5170\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 1.3627e-04 - accuracy: 1.0000 - val_loss: 0.7534 - val_accuracy: 0.5238\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 2s 151ms/step - loss: 2.8510e-04 - accuracy: 1.0000 - val_loss: 0.6705 - val_accuracy: 0.5374\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 2s 189ms/step - loss: 2.7858e-04 - accuracy: 1.0000 - val_loss: 0.8065 - val_accuracy: 0.5238\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 2s 202ms/step - loss: 8.0476e-04 - accuracy: 1.0000 - val_loss: 0.9657 - val_accuracy: 0.5442\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 1.4428e-04 - accuracy: 1.0000 - val_loss: 1.1062 - val_accuracy: 0.5442\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 3.6082e-04 - accuracy: 1.0000 - val_loss: 1.1817 - val_accuracy: 0.5442\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 1.8961e-04 - accuracy: 1.0000 - val_loss: 0.9537 - val_accuracy: 0.6531\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 5.0269e-04 - accuracy: 1.0000 - val_loss: 0.9001 - val_accuracy: 0.6871\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 9.1786e-05 - accuracy: 1.0000 - val_loss: 0.9009 - val_accuracy: 0.6667\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 1.4738e-04 - accuracy: 1.0000 - val_loss: 0.9086 - val_accuracy: 0.6531\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 2.8669e-04 - accuracy: 1.0000 - val_loss: 1.3739 - val_accuracy: 0.5170\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 6.2011e-05 - accuracy: 1.0000 - val_loss: 1.6624 - val_accuracy: 0.5102\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 1.8641e-04 - accuracy: 1.0000 - val_loss: 2.0736 - val_accuracy: 0.5102\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 9.9216e-05 - accuracy: 1.0000 - val_loss: 2.3243 - val_accuracy: 0.5102\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 1.2196e-04 - accuracy: 1.0000 - val_loss: 2.3334 - val_accuracy: 0.5102\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 2.2116e-04 - accuracy: 1.0000 - val_loss: 2.4605 - val_accuracy: 0.5170\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 4.3695e-05 - accuracy: 1.0000 - val_loss: 2.4175 - val_accuracy: 0.5170\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 8.4992e-05 - accuracy: 1.0000 - val_loss: 2.2586 - val_accuracy: 0.5238\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 1.8761e-04 - accuracy: 1.0000 - val_loss: 1.9429 - val_accuracy: 0.5238\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 3.6391e-04 - accuracy: 1.0000 - val_loss: 1.4427 - val_accuracy: 0.5646\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 9.7855e-05 - accuracy: 1.0000 - val_loss: 1.1453 - val_accuracy: 0.6327\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 2.1138e-04 - accuracy: 1.0000 - val_loss: 0.6548 - val_accuracy: 0.7211\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 9.3816e-05 - accuracy: 1.0000 - val_loss: 0.3676 - val_accuracy: 0.8435\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 4.1256e-05 - accuracy: 1.0000 - val_loss: 0.2656 - val_accuracy: 0.9048\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 1.1496e-04 - accuracy: 1.0000 - val_loss: 0.2690 - val_accuracy: 0.9116\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.1685e-04 - accuracy: 1.0000 - val_loss: 0.3548 - val_accuracy: 0.8912\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.2990e-04 - accuracy: 1.0000 - val_loss: 0.4523 - val_accuracy: 0.8776\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.0986e-04 - accuracy: 1.0000 - val_loss: 0.4951 - val_accuracy: 0.8503\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 9.4932e-05 - accuracy: 1.0000 - val_loss: 0.4369 - val_accuracy: 0.8776\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 1.2020e-04 - accuracy: 1.0000 - val_loss: 0.5895 - val_accuracy: 0.8435\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 1.2155e-04 - accuracy: 1.0000 - val_loss: 0.3437 - val_accuracy: 0.9048\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 7.9372e-05 - accuracy: 1.0000 - val_loss: 0.3146 - val_accuracy: 0.9252\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 4.5824e-05 - accuracy: 1.0000 - val_loss: 0.2896 - val_accuracy: 0.9252\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 3.8136e-05 - accuracy: 1.0000 - val_loss: 0.2693 - val_accuracy: 0.9252\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 4.4682e-05 - accuracy: 1.0000 - val_loss: 0.2680 - val_accuracy: 0.9252\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.3272e-04 - accuracy: 1.0000 - val_loss: 0.2116 - val_accuracy: 0.9388\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 1.4753e-04 - accuracy: 1.0000 - val_loss: 0.1912 - val_accuracy: 0.9524\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 6.4871e-04 - accuracy: 1.0000 - val_loss: 4.0357 - val_accuracy: 0.5102\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 1.4053e-04 - accuracy: 1.0000 - val_loss: 16.7816 - val_accuracy: 0.5102\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 1.2203e-04 - accuracy: 1.0000 - val_loss: 18.8045 - val_accuracy: 0.5102\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 1.0677e-04 - accuracy: 1.0000 - val_loss: 16.9351 - val_accuracy: 0.5102\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 2.1068e-04 - accuracy: 1.0000 - val_loss: 13.9856 - val_accuracy: 0.5102\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 6.8618e-05 - accuracy: 1.0000 - val_loss: 12.3108 - val_accuracy: 0.5102\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 1.8163e-04 - accuracy: 1.0000 - val_loss: 9.5929 - val_accuracy: 0.5102\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 4.3703e-05 - accuracy: 1.0000 - val_loss: 8.1502 - val_accuracy: 0.5102\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 1.1684e-04 - accuracy: 1.0000 - val_loss: 6.3497 - val_accuracy: 0.5102\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 5.2256e-05 - accuracy: 1.0000 - val_loss: 4.6979 - val_accuracy: 0.5102\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 1.0135e-04 - accuracy: 1.0000 - val_loss: 3.0997 - val_accuracy: 0.5102\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 4.5523e-05 - accuracy: 1.0000 - val_loss: 2.5336 - val_accuracy: 0.5170\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 2.2308e-04 - accuracy: 1.0000 - val_loss: 1.6736 - val_accuracy: 0.5578\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 6.5932e-05 - accuracy: 1.0000 - val_loss: 1.2592 - val_accuracy: 0.5918\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 6.0452e-05 - accuracy: 1.0000 - val_loss: 1.4234 - val_accuracy: 0.5782\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 9.1198e-05 - accuracy: 1.0000 - val_loss: 1.7237 - val_accuracy: 0.5782\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 6.1964e-05 - accuracy: 1.0000 - val_loss: 2.1214 - val_accuracy: 0.5510\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 6.7306e-05 - accuracy: 1.0000 - val_loss: 2.2598 - val_accuracy: 0.5510\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 7.6580e-05 - accuracy: 1.0000 - val_loss: 2.2723 - val_accuracy: 0.5442\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 4.0895e-05 - accuracy: 1.0000 - val_loss: 2.0290 - val_accuracy: 0.5646\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 9.4665e-05 - accuracy: 1.0000 - val_loss: 2.1841 - val_accuracy: 0.5714\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 1.9004e-05 - accuracy: 1.0000 - val_loss: 2.2168 - val_accuracy: 0.6054\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 4.0357e-05 - accuracy: 1.0000 - val_loss: 1.9203 - val_accuracy: 0.6327\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 2.3423e-04 - accuracy: 1.0000 - val_loss: 0.2622 - val_accuracy: 0.9252\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 1.3830e-04 - accuracy: 1.0000 - val_loss: 0.2718 - val_accuracy: 0.9184\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 2.0649e-05 - accuracy: 1.0000 - val_loss: 1.7549 - val_accuracy: 0.6803\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 7.2648e-05 - accuracy: 1.0000 - val_loss: 1.6353 - val_accuracy: 0.6735\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 4.7195e-05 - accuracy: 1.0000 - val_loss: 1.3019 - val_accuracy: 0.7007\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 5.5200e-05 - accuracy: 1.0000 - val_loss: 0.8603 - val_accuracy: 0.7551\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 1.0082e-04 - accuracy: 1.0000 - val_loss: 2.0076 - val_accuracy: 0.5986\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 5.2395e-05 - accuracy: 1.0000 - val_loss: 3.0572 - val_accuracy: 0.5714\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 2.7772e-04 - accuracy: 1.0000 - val_loss: 3.7615 - val_accuracy: 0.5102\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 26.9081 - val_accuracy: 0.5102\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 3.8500e-04 - accuracy: 1.0000 - val_loss: 32.4000 - val_accuracy: 0.5102\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 32.5231 - val_accuracy: 0.5102\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 25.2567 - val_accuracy: 0.5102\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0113 - accuracy: 0.9971 - val_loss: 35.8477 - val_accuracy: 0.5102\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0160 - accuracy: 0.9913 - val_loss: 77.7147 - val_accuracy: 0.5102\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.1969 - accuracy: 0.9563 - val_loss: 55.1391 - val_accuracy: 0.5102\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.2096 - accuracy: 0.9213 - val_loss: 285.4879 - val_accuracy: 0.4898\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.1482 - accuracy: 0.9563 - val_loss: 54.7040 - val_accuracy: 0.4898\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0879 - accuracy: 0.9738 - val_loss: 143.6662 - val_accuracy: 0.4898\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0234 - accuracy: 0.9913 - val_loss: 116.2338 - val_accuracy: 0.4898\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 102.2408 - val_accuracy: 0.5102\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0385 - accuracy: 0.9767 - val_loss: 62.4017 - val_accuracy: 0.4898\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0203 - accuracy: 0.9913 - val_loss: 12.7160 - val_accuracy: 0.5102\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0223 - accuracy: 0.9913 - val_loss: 100.8353 - val_accuracy: 0.5102\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0138 - accuracy: 0.9971 - val_loss: 192.6704 - val_accuracy: 0.5102\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.1013 - accuracy: 0.9796 - val_loss: 33.3711 - val_accuracy: 0.5102\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0223 - accuracy: 0.9942 - val_loss: 89.9593 - val_accuracy: 0.5102\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 77.2011 - val_accuracy: 0.5102\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 83.4676 - val_accuracy: 0.5102\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 87.3504 - val_accuracy: 0.5102\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 7.0670e-04 - accuracy: 1.0000 - val_loss: 78.6637 - val_accuracy: 0.5102\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 5.5951e-04 - accuracy: 1.0000 - val_loss: 68.7122 - val_accuracy: 0.5102\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 60.1429 - val_accuracy: 0.5102\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 4.1297e-04 - accuracy: 1.0000 - val_loss: 51.9345 - val_accuracy: 0.5102\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 4.1313e-04 - accuracy: 1.0000 - val_loss: 45.3079 - val_accuracy: 0.5102\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 3.3650e-04 - accuracy: 1.0000 - val_loss: 39.7355 - val_accuracy: 0.5102\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 35.0441 - val_accuracy: 0.5102\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 9.1762e-04 - accuracy: 1.0000 - val_loss: 30.5495 - val_accuracy: 0.5102\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 4.4376e-04 - accuracy: 1.0000 - val_loss: 26.7152 - val_accuracy: 0.5102\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 22.6480 - val_accuracy: 0.5102\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 2.1445e-04 - accuracy: 1.0000 - val_loss: 18.9130 - val_accuracy: 0.5102\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0053 - accuracy: 0.9971 - val_loss: 33.9567 - val_accuracy: 0.5102\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 70.3857 - val_accuracy: 0.5102\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 7.4300e-04 - accuracy: 1.0000 - val_loss: 78.0897 - val_accuracy: 0.5102\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 71.4314 - val_accuracy: 0.5102\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 6.0857e-04 - accuracy: 1.0000 - val_loss: 62.7619 - val_accuracy: 0.5102\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 8.1708e-04 - accuracy: 1.0000 - val_loss: 55.9573 - val_accuracy: 0.5102\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.9869e-04 - accuracy: 1.0000 - val_loss: 56.1145 - val_accuracy: 0.5102\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 3.5312e-04 - accuracy: 1.0000 - val_loss: 51.3868 - val_accuracy: 0.5102\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 3.7866e-04 - accuracy: 1.0000 - val_loss: 45.4916 - val_accuracy: 0.5102\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 2.6489e-04 - accuracy: 1.0000 - val_loss: 39.7013 - val_accuracy: 0.5102\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 2.0330e-04 - accuracy: 1.0000 - val_loss: 34.5213 - val_accuracy: 0.5102\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.5175e-04 - accuracy: 1.0000 - val_loss: 29.9985 - val_accuracy: 0.5102\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 7.8033e-05 - accuracy: 1.0000 - val_loss: 26.0810 - val_accuracy: 0.5102\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 2.5996e-04 - accuracy: 1.0000 - val_loss: 22.9588 - val_accuracy: 0.5102\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 3.4323e-04 - accuracy: 1.0000 - val_loss: 20.6382 - val_accuracy: 0.5102\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 4.1278e-04 - accuracy: 1.0000 - val_loss: 18.8834 - val_accuracy: 0.5102\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 2.2785e-04 - accuracy: 1.0000 - val_loss: 17.1999 - val_accuracy: 0.5102\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 2.5041e-04 - accuracy: 1.0000 - val_loss: 15.2226 - val_accuracy: 0.5102\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 1.9642e-04 - accuracy: 1.0000 - val_loss: 13.5805 - val_accuracy: 0.5102\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 8.1082e-05 - accuracy: 1.0000 - val_loss: 11.9228 - val_accuracy: 0.5102\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 2.0825e-04 - accuracy: 1.0000 - val_loss: 10.4358 - val_accuracy: 0.5102\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 6.8172e-05 - accuracy: 1.0000 - val_loss: 8.9918 - val_accuracy: 0.5102\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.6420e-04 - accuracy: 1.0000 - val_loss: 7.3817 - val_accuracy: 0.5170\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 2.5347e-04 - accuracy: 1.0000 - val_loss: 5.8887 - val_accuracy: 0.5238\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 9.7889e-04 - accuracy: 1.0000 - val_loss: 4.1746 - val_accuracy: 0.5306\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 7.9617e-04 - accuracy: 1.0000 - val_loss: 5.4298 - val_accuracy: 0.5170\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 2.4977e-04 - accuracy: 1.0000 - val_loss: 5.0644 - val_accuracy: 0.5238\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 8.1061e-05 - accuracy: 1.0000 - val_loss: 4.1314 - val_accuracy: 0.5306\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 5.6946e-05 - accuracy: 1.0000 - val_loss: 3.1840 - val_accuracy: 0.6122\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.1562e-04 - accuracy: 1.0000 - val_loss: 2.6784 - val_accuracy: 0.6463\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.6150e-04 - accuracy: 1.0000 - val_loss: 2.6925 - val_accuracy: 0.6395\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 4.3173e-05 - accuracy: 1.0000 - val_loss: 2.3344 - val_accuracy: 0.6463\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.2572e-04 - accuracy: 1.0000 - val_loss: 1.8276 - val_accuracy: 0.7007\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 8.6354e-05 - accuracy: 1.0000 - val_loss: 1.3454 - val_accuracy: 0.7687\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 1.9409e-04 - accuracy: 1.0000 - val_loss: 1.0094 - val_accuracy: 0.7959\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 3.9865e-05 - accuracy: 1.0000 - val_loss: 0.9169 - val_accuracy: 0.8163\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 6.6080e-05 - accuracy: 1.0000 - val_loss: 0.6872 - val_accuracy: 0.8367\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 9.7278e-05 - accuracy: 1.0000 - val_loss: 0.5188 - val_accuracy: 0.8776\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 3.5784e-05 - accuracy: 1.0000 - val_loss: 0.3731 - val_accuracy: 0.9048\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 9.5759e-05 - accuracy: 1.0000 - val_loss: 0.3499 - val_accuracy: 0.9252\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 3.3418e-05 - accuracy: 1.0000 - val_loss: 0.3044 - val_accuracy: 0.9252\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 6.3582e-05 - accuracy: 1.0000 - val_loss: 0.2834 - val_accuracy: 0.9388\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 5.8083e-05 - accuracy: 1.0000 - val_loss: 0.2471 - val_accuracy: 0.9592\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 3.9426e-05 - accuracy: 1.0000 - val_loss: 0.2205 - val_accuracy: 0.9592\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 7.0579e-05 - accuracy: 1.0000 - val_loss: 0.1923 - val_accuracy: 0.9592\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 8.4349e-05 - accuracy: 1.0000 - val_loss: 0.1520 - val_accuracy: 0.9864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDVY6HbxMOlH",
        "outputId": "c29d3647-e9f0-4fe3-a51b-b957acc3130d"
      },
      "source": [
        "# pred_test= model.predict_classes(X_test)\n",
        "pred_test = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict   0   1\n",
            "Actual         \n",
            "0        71   1\n",
            "1         1  74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7pT2q7traXg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e84e5db-d469-40f4-f952-86aec8c3e120"
      },
      "source": [
        "print(METRICS)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99        72\n",
            "           1       0.99      0.99      0.99        75\n",
            "\n",
            "    accuracy                           0.99       147\n",
            "   macro avg       0.99      0.99      0.99       147\n",
            "weighted avg       0.99      0.99      0.99       147\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElpxWbBnpgLX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "3062c1b9-c1f4-46e9-ca5c-7b72091a0cf2"
      },
      "source": [
        "'''\n",
        "#X =np.array(df.copy())/255.0 \n",
        "X =np.array(df.copy())\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)\n",
        "model = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh', solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  \n",
        "prediction = model.predict(X_test)  \n",
        "y =np.copy(y_test)\n",
        "data = {'y_true': y_test,'y_predict': prediction}  \n",
        "# este dado esta no formato de dicionario\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)\n",
        "print(METRICS)\n",
        "#X =np.array(df.copy())/255.0 X =np.array(df_all.copy())X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)model = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh',                       solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  prediction = model.predict(X_test)  y =np.copy(y_test)data = {'y_true': y_test,'y_predict': prediction}  # este dado esta no formato de dicionariodf = pd.DataFrame(data, columns=['y_true','y_predict'])confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])print(confusion_matrix)y_true = df['y_true']y_pred = df['y_predict']\n",
        "'''"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n#X =np.array(df.copy())/255.0 \\nX =np.array(df.copy())\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)\\nmodel = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh', solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  \\nprediction = model.predict(X_test)  \\ny =np.copy(y_test)\\ndata = {'y_true': y_test,'y_predict': prediction}  \\n# este dado esta no formato de dicionario\\ndf = pd.DataFrame(data, columns=['y_true','y_predict'])\\nconfusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\\nprint(confusion_matrix)\\ny_true = df['y_true']\\ny_pred = df['y_predict']  \\nMETRICS=sklearn.metrics.classification_report(y_true, y_pred)\\nprint(METRICS)\\n#X =np.array(df.copy())/255.0 X =np.array(df_all.copy())X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)model = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh',                       solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  prediction = model.predict(X_test)  y =np.copy(y_test)data = {'y_true': y_test,'y_predict': prediction}  # este dado esta no formato de dicionariodf = pd.DataFrame(data, columns=['y_true','y_predict'])confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])print(confusion_matrix)y_true = df['y_true']y_pred = df['y_predict']\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iFNNrlWV9tH",
        "outputId": "9bab4b65-511a-401d-b073-0949ed948f46"
      },
      "source": [
        "pred_test"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv5I61yhPQmk",
        "outputId": "4cfa8d5a-d152-4ddb-904e-945d897905e4"
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[4] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  #prediction = model.predict_classes(result)\n",
        "  prediction= np.argmax(model.predict(result), axis=-1)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "1   178.0   99.614960   93.996727  ...  218.879196  216.848007  207.590347\n",
            "2   180.0   41.040497   32.472099  ...  173.566925  153.957520  159.266174\n",
            "3   144.0  189.334106  175.109589  ...  129.371918   92.259262   81.466820\n",
            "4   113.0   55.608746   50.869919  ...  125.337143  119.127731  135.359222\n",
            "6   147.0    0.226757    0.702948  ...  135.902496  130.657608  126.639465\n",
            "9   187.0  164.918869  161.003799  ...  154.471115  164.435379  165.120911\n",
            "10  172.0  128.931870  129.601410  ...  213.281784  219.322891  222.270416\n",
            "11  178.0    0.238228    1.451711  ...  186.795120  187.726181  183.753204\n",
            "12  181.0  146.996597  146.538849  ...  143.309830  166.768539  160.995758\n",
            "13  173.0  211.757462  211.826096  ...    1.349895    0.943767    0.494905\n",
            "15  117.0  138.449127  144.195786  ...  243.661041  227.910660  198.758331\n",
            "16  124.0  132.386047  130.844940  ...  243.952118  247.009354  243.445358\n",
            "17  145.0  189.044891  231.974457  ...  145.450119  157.149200  161.519714\n",
            "19  112.0  163.625000  159.937500  ...  144.312500  182.062500  194.625000\n",
            "20  186.0  198.077698  202.219925  ...   93.706795  108.357391  112.147423\n",
            "23  160.0  143.394363  142.638748  ...  148.014374  144.629364  155.885620\n",
            "24  196.0   49.959183   52.122448  ...  252.795914  250.816315  243.326523\n",
            "25  190.0  100.585159  110.492287  ...    1.316454    0.159224    1.316454\n",
            "28  127.0   58.677040   69.952011  ...    0.895654    1.812388    1.166718\n",
            "29  104.0  150.073975  149.297348  ...  172.692322  166.421616  146.488174\n",
            "31  198.0  180.814911  158.181305  ...    0.498521    1.265381    0.770125\n",
            "32  128.0  158.348633  156.666992  ...  168.480469  182.395508  190.087891\n",
            "33  177.0  114.716675  112.963219  ...  129.142700  125.508530  124.915207\n",
            "34  200.0  227.111588  215.221603  ...  132.283997  120.749214  119.276398\n",
            "35  129.0  126.342651  125.393311  ...  188.159302  187.628616  184.029205\n",
            "36  103.0  142.192001  133.983505  ...   63.093979   28.248089    1.163917\n",
            "37  189.0  197.131683  207.395050  ...  161.892990  163.028809  167.481491\n",
            "38  155.0    3.847617    2.376400  ...  123.224648  129.089462  128.302032\n",
            "39  191.0  146.731216  145.984955  ...  127.699875   73.155586    8.374195\n",
            "40  183.0  158.576141  124.002335  ...    1.000000    1.000000    1.000000\n",
            "41  141.0  166.768066  175.605453  ...  161.698456  161.223480  164.050903\n",
            "42  132.0  161.226822  153.041351  ...  173.716248  183.421494  196.605133\n",
            "43  185.0  186.814880  197.464828  ...  139.092651  154.862396  143.886887\n",
            "44  125.0  123.564369  119.551811  ...   90.905411   75.144066   74.164742\n",
            "46  179.0  179.942230  195.902344  ...    0.739927    1.212041    1.000000\n",
            "47  182.0  143.059174  136.207108  ...  152.982254  153.704147  141.278122\n",
            "48  168.0  135.555557  144.277786  ...    1.055556    0.583333    0.000000\n",
            "49  169.0  149.557877  145.456848  ...    1.000000    1.000000    1.000000\n",
            "\n",
            "[38 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRbWgmX_LFH",
        "outputId": "a3df9079-7380-4beb-ccd8-063b35c329cb"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "from GetBetterSegm import GetBetter"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marquesgabi_paper_fev_2021'...\n",
            "remote: Enumerating objects: 741, done.\u001b[K\n",
            "remote: Counting objects: 100% (502/502), done.\u001b[K\n",
            "remote: Compressing objects: 100% (500/500), done.\u001b[K\n",
            "remote: Total 741 (delta 319), reused 0 (delta 0), pack-reused 239\u001b[K\n",
            "Receiving objects: 100% (741/741), 5.89 MiB | 11.04 MiB/s, done.\n",
            "Resolving deltas: 100% (456/456), done.\n",
            "/content/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAG_I6FwCvFr",
        "outputId": "7fa6c01a-a462-4e3b-c240-a4c65888f39a"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "#!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd marquesgabi_out_2020\n",
        "#%cd Doutorado\n",
        "#PSD_imageJ = 'Amostra7.csv' \n",
        "#PSD_new = pd.read_csv(PSD_imageJ,sep=';')\n",
        "#encoding='utf8'\n",
        "\n",
        "PSD_imageJ = 'Areas_ImageJ.csv'\n",
        "PSD_new = pd.read_csv(PSD_imageJ)\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marquesgabi_out_2020'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 146 (delta 75), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (146/146), 1.00 MiB | 5.80 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "/content/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020\n",
            "   Juntas   Area\n",
            "0       1  2.001\n",
            "1       2  0.820\n",
            "2       3  1.270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tEPjIBnv_xM",
        "outputId": "31e7d991-f2ee-4a21-ebfd-a98f7b4d306c"
      },
      "source": [
        "PSD_new.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(95, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "PekBHQOT_6CP",
        "outputId": "a6165805-97bf-47b8-dcc6-9ec48107bd62"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>178.0</td>\n",
              "      <td>99.614960</td>\n",
              "      <td>93.996727</td>\n",
              "      <td>130.426849</td>\n",
              "      <td>158.160736</td>\n",
              "      <td>159.325363</td>\n",
              "      <td>141.862396</td>\n",
              "      <td>132.532272</td>\n",
              "      <td>133.724792</td>\n",
              "      <td>137.618988</td>\n",
              "      <td>153.171082</td>\n",
              "      <td>155.668488</td>\n",
              "      <td>122.435051</td>\n",
              "      <td>79.575188</td>\n",
              "      <td>84.258179</td>\n",
              "      <td>101.909996</td>\n",
              "      <td>113.836517</td>\n",
              "      <td>114.847244</td>\n",
              "      <td>122.582123</td>\n",
              "      <td>119.721886</td>\n",
              "      <td>124.789688</td>\n",
              "      <td>129.324478</td>\n",
              "      <td>130.393021</td>\n",
              "      <td>124.445786</td>\n",
              "      <td>116.302612</td>\n",
              "      <td>103.795364</td>\n",
              "      <td>143.769867</td>\n",
              "      <td>147.586548</td>\n",
              "      <td>146.585159</td>\n",
              "      <td>89.916435</td>\n",
              "      <td>85.408173</td>\n",
              "      <td>92.625061</td>\n",
              "      <td>134.579346</td>\n",
              "      <td>152.607254</td>\n",
              "      <td>143.143799</td>\n",
              "      <td>132.356018</td>\n",
              "      <td>134.462585</td>\n",
              "      <td>136.806229</td>\n",
              "      <td>147.740692</td>\n",
              "      <td>136.072723</td>\n",
              "      <td>...</td>\n",
              "      <td>217.075745</td>\n",
              "      <td>239.894592</td>\n",
              "      <td>253.040924</td>\n",
              "      <td>253.315887</td>\n",
              "      <td>253.734909</td>\n",
              "      <td>251.217804</td>\n",
              "      <td>241.313721</td>\n",
              "      <td>206.714569</td>\n",
              "      <td>200.205292</td>\n",
              "      <td>196.401001</td>\n",
              "      <td>194.510941</td>\n",
              "      <td>194.822784</td>\n",
              "      <td>248.914047</td>\n",
              "      <td>253.493546</td>\n",
              "      <td>253.901321</td>\n",
              "      <td>254.010651</td>\n",
              "      <td>254.899536</td>\n",
              "      <td>250.349579</td>\n",
              "      <td>190.613678</td>\n",
              "      <td>127.426468</td>\n",
              "      <td>118.342140</td>\n",
              "      <td>113.775795</td>\n",
              "      <td>120.666206</td>\n",
              "      <td>120.468002</td>\n",
              "      <td>114.313744</td>\n",
              "      <td>121.397690</td>\n",
              "      <td>179.415497</td>\n",
              "      <td>215.830353</td>\n",
              "      <td>226.960114</td>\n",
              "      <td>251.448456</td>\n",
              "      <td>254.189270</td>\n",
              "      <td>253.363373</td>\n",
              "      <td>253.003693</td>\n",
              "      <td>230.450714</td>\n",
              "      <td>194.432556</td>\n",
              "      <td>214.073990</td>\n",
              "      <td>213.077667</td>\n",
              "      <td>218.879196</td>\n",
              "      <td>216.848007</td>\n",
              "      <td>207.590347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>180.0</td>\n",
              "      <td>41.040497</td>\n",
              "      <td>32.472099</td>\n",
              "      <td>55.886917</td>\n",
              "      <td>69.962967</td>\n",
              "      <td>68.264206</td>\n",
              "      <td>65.537773</td>\n",
              "      <td>60.066181</td>\n",
              "      <td>50.174324</td>\n",
              "      <td>38.357037</td>\n",
              "      <td>32.342720</td>\n",
              "      <td>51.010868</td>\n",
              "      <td>78.011864</td>\n",
              "      <td>89.095314</td>\n",
              "      <td>91.962479</td>\n",
              "      <td>92.213333</td>\n",
              "      <td>92.115562</td>\n",
              "      <td>87.661736</td>\n",
              "      <td>79.932846</td>\n",
              "      <td>73.065689</td>\n",
              "      <td>77.139267</td>\n",
              "      <td>75.639511</td>\n",
              "      <td>73.310623</td>\n",
              "      <td>86.599014</td>\n",
              "      <td>106.709145</td>\n",
              "      <td>138.413345</td>\n",
              "      <td>149.496811</td>\n",
              "      <td>148.000992</td>\n",
              "      <td>149.050385</td>\n",
              "      <td>86.892342</td>\n",
              "      <td>87.596550</td>\n",
              "      <td>100.619263</td>\n",
              "      <td>112.051369</td>\n",
              "      <td>112.192108</td>\n",
              "      <td>97.122971</td>\n",
              "      <td>58.486919</td>\n",
              "      <td>40.030621</td>\n",
              "      <td>52.455811</td>\n",
              "      <td>95.693329</td>\n",
              "      <td>123.073090</td>\n",
              "      <td>...</td>\n",
              "      <td>158.461746</td>\n",
              "      <td>153.356049</td>\n",
              "      <td>161.709641</td>\n",
              "      <td>171.316559</td>\n",
              "      <td>171.984711</td>\n",
              "      <td>160.334335</td>\n",
              "      <td>153.799530</td>\n",
              "      <td>166.001968</td>\n",
              "      <td>168.138275</td>\n",
              "      <td>163.808411</td>\n",
              "      <td>143.995560</td>\n",
              "      <td>151.581253</td>\n",
              "      <td>158.066193</td>\n",
              "      <td>157.717056</td>\n",
              "      <td>179.814331</td>\n",
              "      <td>189.044464</td>\n",
              "      <td>181.090378</td>\n",
              "      <td>163.142242</td>\n",
              "      <td>152.929413</td>\n",
              "      <td>153.415802</td>\n",
              "      <td>155.186188</td>\n",
              "      <td>172.602966</td>\n",
              "      <td>189.062241</td>\n",
              "      <td>176.136307</td>\n",
              "      <td>169.348648</td>\n",
              "      <td>173.449387</td>\n",
              "      <td>175.857803</td>\n",
              "      <td>172.311615</td>\n",
              "      <td>162.752609</td>\n",
              "      <td>156.710617</td>\n",
              "      <td>158.786179</td>\n",
              "      <td>170.223724</td>\n",
              "      <td>168.900742</td>\n",
              "      <td>152.739761</td>\n",
              "      <td>143.064209</td>\n",
              "      <td>162.243469</td>\n",
              "      <td>174.830124</td>\n",
              "      <td>173.566925</td>\n",
              "      <td>153.957520</td>\n",
              "      <td>159.266174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>144.0</td>\n",
              "      <td>189.334106</td>\n",
              "      <td>175.109589</td>\n",
              "      <td>145.048615</td>\n",
              "      <td>102.171295</td>\n",
              "      <td>105.125786</td>\n",
              "      <td>106.908958</td>\n",
              "      <td>105.022369</td>\n",
              "      <td>99.218376</td>\n",
              "      <td>98.385803</td>\n",
              "      <td>121.030869</td>\n",
              "      <td>139.206787</td>\n",
              "      <td>150.442139</td>\n",
              "      <td>145.221466</td>\n",
              "      <td>141.430557</td>\n",
              "      <td>143.198303</td>\n",
              "      <td>156.185211</td>\n",
              "      <td>161.245377</td>\n",
              "      <td>167.760803</td>\n",
              "      <td>177.029327</td>\n",
              "      <td>177.816360</td>\n",
              "      <td>194.298630</td>\n",
              "      <td>206.317917</td>\n",
              "      <td>214.554016</td>\n",
              "      <td>227.742294</td>\n",
              "      <td>161.412033</td>\n",
              "      <td>71.929787</td>\n",
              "      <td>83.848770</td>\n",
              "      <td>81.543213</td>\n",
              "      <td>183.054794</td>\n",
              "      <td>162.363434</td>\n",
              "      <td>128.533966</td>\n",
              "      <td>98.596458</td>\n",
              "      <td>104.067902</td>\n",
              "      <td>105.603394</td>\n",
              "      <td>99.958336</td>\n",
              "      <td>114.904335</td>\n",
              "      <td>173.118057</td>\n",
              "      <td>175.777786</td>\n",
              "      <td>174.371918</td>\n",
              "      <td>...</td>\n",
              "      <td>84.233803</td>\n",
              "      <td>72.059418</td>\n",
              "      <td>79.066360</td>\n",
              "      <td>111.785492</td>\n",
              "      <td>125.613426</td>\n",
              "      <td>130.318680</td>\n",
              "      <td>137.138123</td>\n",
              "      <td>145.575623</td>\n",
              "      <td>145.729935</td>\n",
              "      <td>134.672073</td>\n",
              "      <td>129.236115</td>\n",
              "      <td>128.368057</td>\n",
              "      <td>123.806335</td>\n",
              "      <td>123.173607</td>\n",
              "      <td>129.885040</td>\n",
              "      <td>130.467590</td>\n",
              "      <td>129.141983</td>\n",
              "      <td>128.220673</td>\n",
              "      <td>123.057106</td>\n",
              "      <td>124.566360</td>\n",
              "      <td>106.110336</td>\n",
              "      <td>111.278549</td>\n",
              "      <td>113.724548</td>\n",
              "      <td>122.501541</td>\n",
              "      <td>162.899689</td>\n",
              "      <td>174.790115</td>\n",
              "      <td>174.614960</td>\n",
              "      <td>169.456024</td>\n",
              "      <td>163.780090</td>\n",
              "      <td>144.540115</td>\n",
              "      <td>134.204483</td>\n",
              "      <td>143.908188</td>\n",
              "      <td>155.760040</td>\n",
              "      <td>163.098007</td>\n",
              "      <td>163.407425</td>\n",
              "      <td>151.565582</td>\n",
              "      <td>141.388885</td>\n",
              "      <td>129.371918</td>\n",
              "      <td>92.259262</td>\n",
              "      <td>81.466820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>113.0</td>\n",
              "      <td>55.608746</td>\n",
              "      <td>50.869919</td>\n",
              "      <td>45.180904</td>\n",
              "      <td>38.793560</td>\n",
              "      <td>115.065468</td>\n",
              "      <td>153.107758</td>\n",
              "      <td>165.894897</td>\n",
              "      <td>171.432068</td>\n",
              "      <td>174.725647</td>\n",
              "      <td>168.803360</td>\n",
              "      <td>160.592209</td>\n",
              "      <td>154.942749</td>\n",
              "      <td>152.810165</td>\n",
              "      <td>151.845001</td>\n",
              "      <td>147.584686</td>\n",
              "      <td>137.870224</td>\n",
              "      <td>110.525887</td>\n",
              "      <td>80.399788</td>\n",
              "      <td>67.402069</td>\n",
              "      <td>67.330956</td>\n",
              "      <td>61.584774</td>\n",
              "      <td>61.885502</td>\n",
              "      <td>71.630669</td>\n",
              "      <td>71.947685</td>\n",
              "      <td>75.757454</td>\n",
              "      <td>93.839378</td>\n",
              "      <td>107.489548</td>\n",
              "      <td>110.920822</td>\n",
              "      <td>90.849396</td>\n",
              "      <td>84.273636</td>\n",
              "      <td>73.269402</td>\n",
              "      <td>61.674133</td>\n",
              "      <td>90.864822</td>\n",
              "      <td>140.749878</td>\n",
              "      <td>160.668335</td>\n",
              "      <td>167.092346</td>\n",
              "      <td>170.097275</td>\n",
              "      <td>170.064835</td>\n",
              "      <td>165.942200</td>\n",
              "      <td>...</td>\n",
              "      <td>96.855515</td>\n",
              "      <td>151.440369</td>\n",
              "      <td>178.466125</td>\n",
              "      <td>179.616241</td>\n",
              "      <td>166.098907</td>\n",
              "      <td>143.913147</td>\n",
              "      <td>144.581406</td>\n",
              "      <td>145.205109</td>\n",
              "      <td>139.256165</td>\n",
              "      <td>129.981827</td>\n",
              "      <td>131.227341</td>\n",
              "      <td>140.202194</td>\n",
              "      <td>57.134544</td>\n",
              "      <td>65.304955</td>\n",
              "      <td>81.706009</td>\n",
              "      <td>89.917305</td>\n",
              "      <td>87.301979</td>\n",
              "      <td>91.094131</td>\n",
              "      <td>92.335098</td>\n",
              "      <td>87.350067</td>\n",
              "      <td>93.234161</td>\n",
              "      <td>98.791992</td>\n",
              "      <td>107.959976</td>\n",
              "      <td>118.019341</td>\n",
              "      <td>120.043465</td>\n",
              "      <td>118.034531</td>\n",
              "      <td>120.738739</td>\n",
              "      <td>113.991158</td>\n",
              "      <td>88.666611</td>\n",
              "      <td>159.286865</td>\n",
              "      <td>182.493225</td>\n",
              "      <td>181.356567</td>\n",
              "      <td>161.535370</td>\n",
              "      <td>129.838440</td>\n",
              "      <td>139.741089</td>\n",
              "      <td>143.810333</td>\n",
              "      <td>139.704742</td>\n",
              "      <td>125.337143</td>\n",
              "      <td>119.127731</td>\n",
              "      <td>135.359222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>147.0</td>\n",
              "      <td>0.226757</td>\n",
              "      <td>0.702948</td>\n",
              "      <td>1.750567</td>\n",
              "      <td>1.083900</td>\n",
              "      <td>0.045351</td>\n",
              "      <td>0.902494</td>\n",
              "      <td>1.807256</td>\n",
              "      <td>0.854875</td>\n",
              "      <td>0.083900</td>\n",
              "      <td>1.131519</td>\n",
              "      <td>1.702948</td>\n",
              "      <td>0.655329</td>\n",
              "      <td>0.274376</td>\n",
              "      <td>1.244898</td>\n",
              "      <td>0.834467</td>\n",
              "      <td>6.716554</td>\n",
              "      <td>41.768711</td>\n",
              "      <td>69.065765</td>\n",
              "      <td>134.517014</td>\n",
              "      <td>147.519287</td>\n",
              "      <td>137.911591</td>\n",
              "      <td>123.585037</td>\n",
              "      <td>119.666672</td>\n",
              "      <td>115.526085</td>\n",
              "      <td>110.090706</td>\n",
              "      <td>106.897964</td>\n",
              "      <td>102.192749</td>\n",
              "      <td>95.793648</td>\n",
              "      <td>0.226757</td>\n",
              "      <td>0.702948</td>\n",
              "      <td>1.750567</td>\n",
              "      <td>1.083900</td>\n",
              "      <td>0.045351</td>\n",
              "      <td>0.902494</td>\n",
              "      <td>1.807256</td>\n",
              "      <td>0.854875</td>\n",
              "      <td>0.083900</td>\n",
              "      <td>1.131519</td>\n",
              "      <td>1.702948</td>\n",
              "      <td>...</td>\n",
              "      <td>24.297052</td>\n",
              "      <td>69.997734</td>\n",
              "      <td>79.154198</td>\n",
              "      <td>96.482994</td>\n",
              "      <td>118.780060</td>\n",
              "      <td>127.582764</td>\n",
              "      <td>138.972794</td>\n",
              "      <td>147.408173</td>\n",
              "      <td>149.167816</td>\n",
              "      <td>141.090714</td>\n",
              "      <td>130.308395</td>\n",
              "      <td>126.453522</td>\n",
              "      <td>0.235828</td>\n",
              "      <td>0.712018</td>\n",
              "      <td>1.759637</td>\n",
              "      <td>1.092971</td>\n",
              "      <td>0.056689</td>\n",
              "      <td>0.913832</td>\n",
              "      <td>1.818594</td>\n",
              "      <td>0.866213</td>\n",
              "      <td>0.092971</td>\n",
              "      <td>1.140590</td>\n",
              "      <td>1.712018</td>\n",
              "      <td>0.664399</td>\n",
              "      <td>0.283447</td>\n",
              "      <td>1.331066</td>\n",
              "      <td>1.984127</td>\n",
              "      <td>1.571429</td>\n",
              "      <td>27.399097</td>\n",
              "      <td>71.394562</td>\n",
              "      <td>70.759644</td>\n",
              "      <td>51.689346</td>\n",
              "      <td>69.278915</td>\n",
              "      <td>93.492058</td>\n",
              "      <td>113.716560</td>\n",
              "      <td>131.321991</td>\n",
              "      <td>138.655334</td>\n",
              "      <td>135.902496</td>\n",
              "      <td>130.657608</td>\n",
              "      <td>126.639465</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Width           0           1  ...         781         782         783\n",
              "1  178.0   99.614960   93.996727  ...  218.879196  216.848007  207.590347\n",
              "2  180.0   41.040497   32.472099  ...  173.566925  153.957520  159.266174\n",
              "3  144.0  189.334106  175.109589  ...  129.371918   92.259262   81.466820\n",
              "4  113.0   55.608746   50.869919  ...  125.337143  119.127731  135.359222\n",
              "6  147.0    0.226757    0.702948  ...  135.902496  130.657608  126.639465\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "VaZPe_AxNBK9",
        "outputId": "0bfec215-0bfe-4295-8ec7-af3856df0d2a"
      },
      "source": [
        "PSD_new.head()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Juntas</th>\n",
              "      <th>Area</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1.162</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Juntas   Area\n",
              "0       1  2.001\n",
              "1       2  0.820\n",
              "2       3  1.270\n",
              "3       4  0.958\n",
              "4       5  1.162"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "#lost_value = float(PSD_new.columns[1])\n",
        "\n",
        "# Area = np.array(PSD_new.iloc[:,1])\n",
        "Area = PSD_new['Area'].values\n",
        "# Area = np.concatenate( (Area, [lost_value] ) )\n",
        "# Area = np.concatenate( (Area, [lost_value] ) )\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfk_fNXGDK5_"
      },
      "source": [
        "wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        "wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        "X = pd.DataFrame([Diam1,Diameter_All])\n",
        "wts = pd.DataFrame([wt1,wt2])\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "OieAXw_by3nz",
        "outputId": "04c33527-7dc5-4aeb-af51-535f36a411e1"
      },
      "source": [
        "A = plt.hist(X,weights=wts,bins=7)\n",
        "plt.legend(['True','CNN'])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fbf386bd0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpdrvEySy8Ij"
      },
      "source": [
        "B = A[0][0]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUhGZHT8y9Or",
        "outputId": "6f1b9e20-0675-4a32-83b1-17c842290af5"
      },
      "source": [
        "Novo = []\n",
        "k = 0\n",
        "soma = 0\n",
        "for i in B:\n",
        "  if(k<4):\n",
        "    Novo.append(i)\n",
        "  else:\n",
        "    soma = soma + i\n",
        "  k = k + 1\n",
        "Novo.append(soma)\n",
        "print(Novo)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14.736842105263156, 24.2105263157895, 42.1052631578948, 14.736842105263179, 4.21052631578948]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "yMK89w-fzCVe",
        "outputId": "1ce55d10-1660-4d2c-be92-fc87291ac376"
      },
      "source": [
        "# Freq1 = [19.12043703, 29.22484843, 19.35872174, 20.82190224, 11.47409056] # avarage 4 samples\n",
        "Freq1 = [20.69301557, 28.55598044, 18.50768331, 22.7106327, 8.905907357] # avarage 10 samples\n",
        "#Freq2 = [16.93792791, 31.38008965, 24.93810752, 18.56158392, 6.233810752, 0.4]\n",
        "Freq2 = [16.93792791, 31.38008965, 24.93810752, 18.56158392, 6.633810752]\n",
        "Freq3 = Novo\n",
        "barWidth = 0.25\n",
        "\n",
        "br1 = range(len(Freq1))\n",
        "# Set position of bar on X axis\n",
        "br2 = [x + barWidth for x in br1]\n",
        "br3 = [x + barWidth for x in br2]\n",
        "# labels = [0.8, 1.0, 1.2, 1.4, 1.6, 1.8]\n",
        "labels = [0.8, 1.0, 1.2, 1.4, 1.6]\n",
        "\n",
        "xx=[]\n",
        "for a in labels:\n",
        "  xx.append(str(a))\n",
        "plt.bar(br1, Freq1 , color=\"green\", align=\"center\", width=0.3, tick_label= xx) \n",
        "plt.bar(br2, Freq2 , color=\"red\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.bar(br3, Freq3 , color=\"blue\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.legend(['CNN 1','CNN 2','True'])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fbf3832d910>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}