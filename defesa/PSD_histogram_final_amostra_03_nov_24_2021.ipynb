{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_final_amostra_03_nov_24_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/defesa/PSD_histogram_final_amostra_03_nov_24_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from sklearn.metrics import r2_score\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZEvJvfoibE4"
      },
      "source": [
        "#!pip install mahotas"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "54e440cc-d590-4c1b-92df-c88aabec0edc"
      },
      "source": [
        "'''\n",
        "import mahotas.features.texture as mht\n",
        "import mahotas.features\n",
        "'''"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nimport mahotas.features.texture as mht\\nimport mahotas.features\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "_VcTdaNVh9EE",
        "outputId": "025fa143-e12a-4ba9-954d-d3e850d15370"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas\n",
        "'''"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\\n%cd marquesgabi_fev_2020\\nimport Go2BlackWhite\\nimport Go2Mahotas\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UZ30b1EVQhq"
      },
      "source": [
        "def BlackWhite(Transfere,Size):\n",
        "\n",
        "  img_name=[]\n",
        "  xw=[]\n",
        "  ww=[]\n",
        "\n",
        "  with zipfile.ZipFile(Transfere, \"r\") as f:\n",
        "    for name in f.namelist():\n",
        "      img_name.append(name)\n",
        "      #xw.append(cv2.imread(name))\n",
        "      xw.append(cv2.resize(cv2.imread(name),(Size,Size)))\n",
        "\n",
        "  nrow=len(img_name)\n",
        "  ncol=Size*Size\n",
        "  pw=np.zeros((nrow,ncol))\n",
        "  #pw=[]\n",
        "  for i in range(nrow):\n",
        "    ww.append(cv2.cvtColor(np.array(xw[i]), cv2.COLOR_BGR2GRAY))\n",
        "    pw[i,:]=ww[i].ravel()\n",
        "  return ww,img_name"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v7SRrc8mH2N",
        "outputId": "bd60179b-5d5c-4e0e-f625-412a2b6a08da"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip' \n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Doutorado' already exists and is not an empty directory.\n",
            "/content/Doutorado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqIYzUcnrdMp",
        "outputId": "dc68dcc3-a02a-49b0-bc06-76a9f50daa91"
      },
      "source": [
        "labels =[]\n",
        "with zipfile.ZipFile(Transfere, \"r\") as f:\n",
        "  for f in f.namelist():\n",
        "    labels.append(f)\n",
        "print(labels)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Fotos_Grandes-3cdAmostra/Q6-8-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-4.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "# ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "ww,img_name=BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[4] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHgqAnaFyCjp",
        "outputId": "05711edb-469c-4870-f7ea-5bf925c561d6"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MarquesGabi_Routines' already exists and is not an empty directory.\n",
            "/content/Doutorado/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDEGUiuubwuZ"
      },
      "source": [
        "FILE='SugarSample03.zip'\n",
        "img_name=[]\n",
        "x_original = [] \n",
        "\n",
        "data_file ='xls'\n",
        "\n",
        "\n",
        "file_name = zipfile.ZipFile(FILE, 'r')\n",
        "file_name.extractall()\n",
        "\n",
        "k = 0\n",
        "with zipfile.ZipFile(FILE, \"r\") as f:\n",
        "    for name in f.namelist():\n",
        "      if(name[-3:] == data_file):\n",
        "        #df =pd.read_csv(name)\n",
        "        if( k > 0):\n",
        "          df_old = df.copy()\n",
        "        df = pd.read_excel(name)\n",
        "        df = df.drop(labels=[0], axis=0)\n",
        "        if(k > 0):\n",
        "          df = pd.concat( [df, df_old], ignore_index = True)\n",
        "        k = k + 1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbQ0tal0etXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d4b34d-3762-443a-c06d-a58a78eddfe1"
      },
      "source": [
        "f.namelist()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Results_03_02.xls', 'Results_03_03.xls', 'Results_03_01.xls']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMBJ6C-YdF3q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "58f90c4e-5d1f-4366-849d-de3d890efa90"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Area</th>\n",
              "      <th>Mean</th>\n",
              "      <th>Min</th>\n",
              "      <th>Max</th>\n",
              "      <th>Major</th>\n",
              "      <th>Minor</th>\n",
              "      <th>Angle</th>\n",
              "      <th>Feret</th>\n",
              "      <th>FeretX</th>\n",
              "      <th>FeretY</th>\n",
              "      <th>FeretAngle</th>\n",
              "      <th>MinFeret</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>1.288</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>1.383</td>\n",
              "      <td>1.185</td>\n",
              "      <td>5.847</td>\n",
              "      <td>1.636</td>\n",
              "      <td>767</td>\n",
              "      <td>213</td>\n",
              "      <td>18.157</td>\n",
              "      <td>1.161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0.407</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>0.814</td>\n",
              "      <td>0.637</td>\n",
              "      <td>62.186</td>\n",
              "      <td>0.877</td>\n",
              "      <td>283</td>\n",
              "      <td>234</td>\n",
              "      <td>59.036</td>\n",
              "      <td>0.667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>0.592</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>0.925</td>\n",
              "      <td>0.815</td>\n",
              "      <td>117.923</td>\n",
              "      <td>1.078</td>\n",
              "      <td>633</td>\n",
              "      <td>154</td>\n",
              "      <td>122.335</td>\n",
              "      <td>0.802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>1.391</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>1.435</td>\n",
              "      <td>1.235</td>\n",
              "      <td>29.966</td>\n",
              "      <td>1.564</td>\n",
              "      <td>1321</td>\n",
              "      <td>333</td>\n",
              "      <td>53.253</td>\n",
              "      <td>1.165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>0.549</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>0.923</td>\n",
              "      <td>0.758</td>\n",
              "      <td>136.396</td>\n",
              "      <td>1.024</td>\n",
              "      <td>370</td>\n",
              "      <td>254</td>\n",
              "      <td>118.237</td>\n",
              "      <td>0.738</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Area  Mean  Min  Max  ...  Feret  FeretX  FeretY  FeretAngle  MinFeret\n",
              "0  2  1.288   255  255  255  ...  1.636     767     213      18.157     1.161\n",
              "1  3  0.407   255  255  255  ...  0.877     283     234      59.036     0.667\n",
              "2  4  0.592   255  255  255  ...  1.078     633     154     122.335     0.802\n",
              "3  5  1.391   255  255  255  ...  1.564    1321     333      53.253     1.165\n",
              "4  6  0.549   255  255  255  ...  1.024     370     254     118.237     0.738\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from Segment_Filter import Segmenta  # got image provided segmented"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5MN5a_v4np",
        "outputId": "4815886c-526b-4621-aae6-4c44b56ebebc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     139  232.854614  238.962769  ...  175.873611  174.873901  183.617035\n",
            "1     170  167.081131  162.162354  ...    1.759585    0.744360    0.383668\n",
            "2     189  149.964325  152.647461  ...  164.183823  174.434830  225.307251\n",
            "3     173  184.651276  187.003036  ...  172.154724  207.330383  213.416229\n",
            "4     188   59.334087  106.062462  ...    1.000000    1.000000    1.000000\n",
            "5     178  108.079796  112.934998  ...  113.183449  110.443130  121.657623\n",
            "6     154  208.173569  196.404968  ...   16.652895   24.198349   27.471075\n",
            "7     182   89.023674  103.698227  ...    1.254438    0.124260    1.331361\n",
            "8     163  148.851257  132.954529  ...  168.847488  160.830582  155.921570\n",
            "9     130  149.223434  162.546021  ...  228.065109  221.590775  216.742966\n",
            "10    152  199.560928  196.948044  ...  155.852493  153.283234  138.699448\n",
            "11    189  170.436218  191.736618  ...  158.744858  147.950623  137.761322\n",
            "12    159  116.027092   89.169609  ...  144.214218  140.967560  134.888763\n",
            "13    103  105.871048  116.892448  ...  217.607407  172.199738  108.349792\n",
            "14    170   88.560005  106.848450  ...  124.498970  140.724304  144.077667\n",
            "15    160    0.155625    1.280625  ...  249.499390  253.582489  252.796860\n",
            "16    114    0.875039    1.843336  ...  176.609421  175.289322  156.821472\n",
            "17    126  144.185196  184.160492  ...  154.283951  141.827164  113.679008\n",
            "18    177   93.686295  109.832542  ...  164.725250  161.576660  157.452438\n",
            "19    146  196.603668  218.324448  ...  105.953461   98.246201  103.376999\n",
            "20    181  198.160477  176.968079  ...  141.398651  138.433609  139.368439\n",
            "21    158  100.151260  101.948395  ...    2.244192    1.271431    0.145329\n",
            "22    143  118.617432  124.097603  ...  179.697433  174.070999  160.772964\n",
            "23    162  127.164307  147.997711  ...  139.710098  138.272064  143.033539\n",
            "24    145  150.630966  133.635391  ...  122.929176  133.876755  137.088272\n",
            "25    190  118.594452  111.632675  ...    1.386149    0.193352    1.333518\n",
            "26    186  119.005096   89.629204  ...    1.312984    0.142097    1.323737\n",
            "27    173   29.288881   45.193192  ...    1.033379    0.174847    1.354572\n",
            "28    149  253.480652  252.767548  ...  148.155304  136.294678  144.143234\n",
            "29    189  141.485596  154.533600  ...  249.869690  246.272964  247.984894\n",
            "30    173  139.655075  133.624954  ...  105.818192  107.578995  106.998413\n",
            "31    169   70.056442   67.450645  ...    0.916845    0.189174    1.358811\n",
            "32    157  252.547531  239.343597  ...  138.845795  126.726028   83.344238\n",
            "33    153  229.066208  202.797424  ...    1.118331    1.363664    1.218933\n",
            "34    109  140.647339  143.394409  ...    1.000000    1.000000    1.000000\n",
            "35    186   69.752808   70.929588  ...    1.011446    1.193548    1.000000\n",
            "36    133  243.703613  252.218826  ...  212.803329  185.218842  158.614960\n",
            "37    153  152.231506  159.582428  ...   68.446114  100.326324  111.637367\n",
            "38    111  165.243317  162.950012  ...  189.901398  197.036499  199.743286\n",
            "39    123  130.664093   96.017250  ...  156.072845  133.760468  120.320053\n",
            "40    153  117.580170  123.216805  ...  139.447525  141.805161  141.276230\n",
            "41    112  172.250000  163.062500  ...  143.062500  145.312500  157.750000\n",
            "42    114  135.328094  143.547867  ...  127.352417  123.959373  122.992928\n",
            "43    197  167.343018  166.836884  ...  128.975479  136.252548  149.221558\n",
            "44    146  245.976349  133.179947  ...  126.427849  133.400070  130.164566\n",
            "45    140  149.479996  145.279999  ...    0.440000    0.440000    1.440000\n",
            "46    101  148.842087  158.768372  ...  144.967255  136.672577  130.370453\n",
            "47    141    1.635179    0.628087  ...  104.143860  102.725716  113.047028\n",
            "48    141   66.731056   62.331673  ...    0.371812    0.578945    2.088829\n",
            "49    110  138.626114  143.406937  ...  127.970901  130.944138  137.677353\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR2emP4rNjQy",
        "outputId": "f5de547b-1f4a-4f73-f3b3-9ca9ee99e2c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MarquesGabi_Routines' already exists and is not an empty directory.\n",
            "/content/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIFPGE_-vx3T"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "Description = '3 layers of Convolution: 64, 128, 256 '\n",
        "N1 = 20\n",
        "N2 = 20\n",
        "\n",
        "# make the CNN\n",
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=64, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=N1))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=N2))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpbPQ1FSRG6A",
        "outputId": "1461be1a-cb84-4e9a-d055-760ab0e111f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "# training the model\n",
        "r = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), \n",
        "              epochs=200, batch_size=32)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 2s 152ms/step - loss: 0.5981 - accuracy: 0.7376 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.3143 - accuracy: 0.8776 - val_loss: 0.6933 - val_accuracy: 0.5102\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.2752 - accuracy: 0.8863 - val_loss: 0.6934 - val_accuracy: 0.5102\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.2258 - accuracy: 0.9155 - val_loss: 0.6935 - val_accuracy: 0.5102\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.1363 - accuracy: 0.9359 - val_loss: 0.6941 - val_accuracy: 0.5102\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0934 - accuracy: 0.9650 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0478 - accuracy: 0.9883 - val_loss: 0.6956 - val_accuracy: 0.5102\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0360 - accuracy: 0.9854 - val_loss: 0.6956 - val_accuracy: 0.5102\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0236 - accuracy: 0.9942 - val_loss: 0.6961 - val_accuracy: 0.5102\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0223 - accuracy: 0.9913 - val_loss: 0.6996 - val_accuracy: 0.5102\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0194 - accuracy: 0.9971 - val_loss: 0.6979 - val_accuracy: 0.5102\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0394 - accuracy: 0.9854 - val_loss: 0.7187 - val_accuracy: 0.5102\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0398 - accuracy: 0.9825 - val_loss: 0.7138 - val_accuracy: 0.5102\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0368 - accuracy: 0.9825 - val_loss: 0.6999 - val_accuracy: 0.5102\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0391 - accuracy: 0.9913 - val_loss: 0.7051 - val_accuracy: 0.5102\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0234 - accuracy: 0.9942 - val_loss: 0.7206 - val_accuracy: 0.5102\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0192 - accuracy: 0.9942 - val_loss: 0.7248 - val_accuracy: 0.5102\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0120 - accuracy: 0.9971 - val_loss: 0.7198 - val_accuracy: 0.5102\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.7267 - val_accuracy: 0.5102\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.7327 - val_accuracy: 0.5102\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.7240 - val_accuracy: 0.5102\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.7193 - val_accuracy: 0.5102\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.7237 - val_accuracy: 0.5102\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.7028 - val_accuracy: 0.5102\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.7034 - val_accuracy: 0.5102\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7020 - val_accuracy: 0.5102\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0045 - accuracy: 0.9971 - val_loss: 0.7490 - val_accuracy: 0.5102\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.8094 - val_accuracy: 0.5102\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.8210 - val_accuracy: 0.5102\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 6.2585e-04 - accuracy: 1.0000 - val_loss: 0.7946 - val_accuracy: 0.5102\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.8089 - val_accuracy: 0.5102\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.8412 - val_accuracy: 0.5102\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 7.3385e-04 - accuracy: 1.0000 - val_loss: 0.8728 - val_accuracy: 0.5102\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 7.1500e-04 - accuracy: 1.0000 - val_loss: 0.8659 - val_accuracy: 0.5102\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.8351 - val_accuracy: 0.5102\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 8.9232e-04 - accuracy: 1.0000 - val_loss: 0.8382 - val_accuracy: 0.5102\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 5.6829e-04 - accuracy: 1.0000 - val_loss: 0.8159 - val_accuracy: 0.5102\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 3.2127e-04 - accuracy: 1.0000 - val_loss: 0.8301 - val_accuracy: 0.5102\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.8106 - val_accuracy: 0.5102\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.8058 - val_accuracy: 0.5102\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 3.6606e-04 - accuracy: 1.0000 - val_loss: 0.9392 - val_accuracy: 0.5102\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 5.0362e-04 - accuracy: 1.0000 - val_loss: 1.0399 - val_accuracy: 0.5102\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 5.6828e-04 - accuracy: 1.0000 - val_loss: 1.0303 - val_accuracy: 0.5102\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 8.5187e-04 - accuracy: 1.0000 - val_loss: 1.0312 - val_accuracy: 0.5102\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.8529 - val_accuracy: 0.5102\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 8.9060e-04 - accuracy: 1.0000 - val_loss: 0.8920 - val_accuracy: 0.5102\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 5.1538e-04 - accuracy: 1.0000 - val_loss: 1.2192 - val_accuracy: 0.5102\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 3.7694e-04 - accuracy: 1.0000 - val_loss: 1.6668 - val_accuracy: 0.5102\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 2.1074e-04 - accuracy: 1.0000 - val_loss: 1.8663 - val_accuracy: 0.5102\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 4.9924e-04 - accuracy: 1.0000 - val_loss: 1.8873 - val_accuracy: 0.5102\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 6.9790e-04 - accuracy: 1.0000 - val_loss: 1.7832 - val_accuracy: 0.5102\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 2.6401e-04 - accuracy: 1.0000 - val_loss: 1.8174 - val_accuracy: 0.5102\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 1.6202 - val_accuracy: 0.5102\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 5.3593e-04 - accuracy: 1.0000 - val_loss: 2.8509 - val_accuracy: 0.5102\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 9.3062e-05 - accuracy: 1.0000 - val_loss: 3.4193 - val_accuracy: 0.5102\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 1.6999e-04 - accuracy: 1.0000 - val_loss: 3.3083 - val_accuracy: 0.5102\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 6.1267e-04 - accuracy: 1.0000 - val_loss: 2.6886 - val_accuracy: 0.5102\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 3.5065e-04 - accuracy: 1.0000 - val_loss: 2.0587 - val_accuracy: 0.5102\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 1.3370e-04 - accuracy: 1.0000 - val_loss: 1.8700 - val_accuracy: 0.5170\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 1.5574e-04 - accuracy: 1.0000 - val_loss: 1.7351 - val_accuracy: 0.5238\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 3.6215e-04 - accuracy: 1.0000 - val_loss: 4.6421 - val_accuracy: 0.5102\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 6.2838e-04 - accuracy: 1.0000 - val_loss: 5.2431 - val_accuracy: 0.5102\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 2.4547e-04 - accuracy: 1.0000 - val_loss: 5.1547 - val_accuracy: 0.5102\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 1.4851e-04 - accuracy: 1.0000 - val_loss: 5.1122 - val_accuracy: 0.5102\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 3.5378e-04 - accuracy: 1.0000 - val_loss: 3.8282 - val_accuracy: 0.5238\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 2.1169e-04 - accuracy: 1.0000 - val_loss: 2.3462 - val_accuracy: 0.5578\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 1.2561e-04 - accuracy: 1.0000 - val_loss: 1.7074 - val_accuracy: 0.6327\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 9.7153e-05 - accuracy: 1.0000 - val_loss: 1.5106 - val_accuracy: 0.6735\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 9.5829e-05 - accuracy: 1.0000 - val_loss: 1.3678 - val_accuracy: 0.6803\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 9.1289e-05 - accuracy: 1.0000 - val_loss: 1.1685 - val_accuracy: 0.7211\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 6.4069e-05 - accuracy: 1.0000 - val_loss: 0.8872 - val_accuracy: 0.7483\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 1.0493e-04 - accuracy: 1.0000 - val_loss: 0.8334 - val_accuracy: 0.7551\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 3.7457e-05 - accuracy: 1.0000 - val_loss: 0.7500 - val_accuracy: 0.7687\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 1.4668e-04 - accuracy: 1.0000 - val_loss: 0.5358 - val_accuracy: 0.8231\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 2.9299e-04 - accuracy: 1.0000 - val_loss: 1.2921 - val_accuracy: 0.7279\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 9.9926e-05 - accuracy: 1.0000 - val_loss: 4.9103 - val_accuracy: 0.5102\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 3.3433e-04 - accuracy: 1.0000 - val_loss: 4.9638 - val_accuracy: 0.5102\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 6.1254e-05 - accuracy: 1.0000 - val_loss: 4.1760 - val_accuracy: 0.5102\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 1.8713e-04 - accuracy: 1.0000 - val_loss: 3.4235 - val_accuracy: 0.5238\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 4.9750e-05 - accuracy: 1.0000 - val_loss: 2.7829 - val_accuracy: 0.5510\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 4.8630e-05 - accuracy: 1.0000 - val_loss: 2.3710 - val_accuracy: 0.5714\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 5.3197e-04 - accuracy: 1.0000 - val_loss: 2.5902 - val_accuracy: 0.5646\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 1.3628e-04 - accuracy: 1.0000 - val_loss: 4.4623 - val_accuracy: 0.5306\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 2.7217e-04 - accuracy: 1.0000 - val_loss: 5.3565 - val_accuracy: 0.5306\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 2.7431e-05 - accuracy: 1.0000 - val_loss: 5.3979 - val_accuracy: 0.5374\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 3.7933e-05 - accuracy: 1.0000 - val_loss: 4.8521 - val_accuracy: 0.5646\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 5.9453e-05 - accuracy: 1.0000 - val_loss: 3.9909 - val_accuracy: 0.5850\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 4.7519e-05 - accuracy: 1.0000 - val_loss: 3.1063 - val_accuracy: 0.6327\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 1.0520e-04 - accuracy: 1.0000 - val_loss: 2.3658 - val_accuracy: 0.6667\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 9.8775e-05 - accuracy: 1.0000 - val_loss: 0.6800 - val_accuracy: 0.8299\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 1.0685e-04 - accuracy: 1.0000 - val_loss: 0.6001 - val_accuracy: 0.8367\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 3.1912e-04 - accuracy: 1.0000 - val_loss: 0.7628 - val_accuracy: 0.7891\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 45.6980 - val_accuracy: 0.5102\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 92.1785 - val_accuracy: 0.5102\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0067 - accuracy: 0.9971 - val_loss: 91.9459 - val_accuracy: 0.5102\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0569 - accuracy: 0.9854 - val_loss: 30.4212 - val_accuracy: 0.5102\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.1025 - accuracy: 0.9738 - val_loss: 27.8402 - val_accuracy: 0.4898\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0863 - accuracy: 0.9708 - val_loss: 137.4002 - val_accuracy: 0.5102\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.1318 - accuracy: 0.9446 - val_loss: 122.6914 - val_accuracy: 0.5102\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.1157 - accuracy: 0.9679 - val_loss: 242.4309 - val_accuracy: 0.5102\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0614 - accuracy: 0.9883 - val_loss: 138.9317 - val_accuracy: 0.5102\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0173 - accuracy: 0.9971 - val_loss: 112.7647 - val_accuracy: 0.5102\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0137 - accuracy: 0.9971 - val_loss: 69.3808 - val_accuracy: 0.5102\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0195 - accuracy: 0.9913 - val_loss: 43.7188 - val_accuracy: 0.5102\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 48.8376 - val_accuracy: 0.5102\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 36.9950 - val_accuracy: 0.5102\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 9.1333e-04 - accuracy: 1.0000 - val_loss: 29.9581 - val_accuracy: 0.5102\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 4.7094e-04 - accuracy: 1.0000 - val_loss: 25.5203 - val_accuracy: 0.5102\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 5.9539e-04 - accuracy: 1.0000 - val_loss: 21.7166 - val_accuracy: 0.5102\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 13.5267 - val_accuracy: 0.5102\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 12.5789 - val_accuracy: 0.5102\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 9.8452e-04 - accuracy: 1.0000 - val_loss: 13.1888 - val_accuracy: 0.5102\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 12.3909 - val_accuracy: 0.5102\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 8.9433e-04 - accuracy: 1.0000 - val_loss: 13.1831 - val_accuracy: 0.5102\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 8.9757e-04 - accuracy: 1.0000 - val_loss: 14.2816 - val_accuracy: 0.5102\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 9.3673e-05 - accuracy: 1.0000 - val_loss: 13.4634 - val_accuracy: 0.5102\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 6.5642e-04 - accuracy: 1.0000 - val_loss: 11.7755 - val_accuracy: 0.5102\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 6.5500e-05 - accuracy: 1.0000 - val_loss: 10.3394 - val_accuracy: 0.5102\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0328 - accuracy: 0.9913 - val_loss: 8.0069 - val_accuracy: 0.5102\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0847 - accuracy: 0.9767 - val_loss: 4.3908 - val_accuracy: 0.5102\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0355 - accuracy: 0.9854 - val_loss: 37.6820 - val_accuracy: 0.5102\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0670 - accuracy: 0.9767 - val_loss: 150.3076 - val_accuracy: 0.5102\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0476 - accuracy: 0.9854 - val_loss: 162.7545 - val_accuracy: 0.5102\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0126 - accuracy: 0.9971 - val_loss: 148.4538 - val_accuracy: 0.5102\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0339 - accuracy: 0.9854 - val_loss: 159.5013 - val_accuracy: 0.5102\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0110 - accuracy: 0.9942 - val_loss: 151.7710 - val_accuracy: 0.5102\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 140.1172 - val_accuracy: 0.5102\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0045 - accuracy: 0.9971 - val_loss: 113.8932 - val_accuracy: 0.5102\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 87.1680 - val_accuracy: 0.5102\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 5.8619e-04 - accuracy: 1.0000 - val_loss: 73.8371 - val_accuracy: 0.5102\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 63.0558 - val_accuracy: 0.5102\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 55.7126 - val_accuracy: 0.5102\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 68.3001 - val_accuracy: 0.5102\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 1.8612e-04 - accuracy: 1.0000 - val_loss: 67.0047 - val_accuracy: 0.5102\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 8.9047e-04 - accuracy: 1.0000 - val_loss: 58.1701 - val_accuracy: 0.5102\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 5.0200e-05 - accuracy: 1.0000 - val_loss: 49.7428 - val_accuracy: 0.5102\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 1.4184e-04 - accuracy: 1.0000 - val_loss: 43.1572 - val_accuracy: 0.5102\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 5.4385e-04 - accuracy: 1.0000 - val_loss: 37.6087 - val_accuracy: 0.5102\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 2.5993e-04 - accuracy: 1.0000 - val_loss: 32.6830 - val_accuracy: 0.5102\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 2.8421e-04 - accuracy: 1.0000 - val_loss: 28.1845 - val_accuracy: 0.5102\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0026 - accuracy: 0.9971 - val_loss: 25.2471 - val_accuracy: 0.5102\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 5.2754e-04 - accuracy: 1.0000 - val_loss: 23.4291 - val_accuracy: 0.5102\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 19.7893 - val_accuracy: 0.5102\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 2.3442e-04 - accuracy: 1.0000 - val_loss: 16.1506 - val_accuracy: 0.5102\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 1.8759e-04 - accuracy: 1.0000 - val_loss: 13.2237 - val_accuracy: 0.5102\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 1.8624e-04 - accuracy: 1.0000 - val_loss: 11.0993 - val_accuracy: 0.5102\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 1.8356e-04 - accuracy: 1.0000 - val_loss: 9.1127 - val_accuracy: 0.5102\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 8.8825e-04 - accuracy: 1.0000 - val_loss: 3.5005 - val_accuracy: 0.5986\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 1.6700e-04 - accuracy: 1.0000 - val_loss: 1.0984 - val_accuracy: 0.7619\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 1.4957e-04 - accuracy: 1.0000 - val_loss: 0.6157 - val_accuracy: 0.8639\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 5.7140e-04 - accuracy: 1.0000 - val_loss: 3.6389 - val_accuracy: 0.5510\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 4.0896e-05 - accuracy: 1.0000 - val_loss: 5.1729 - val_accuracy: 0.5170\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 2.6680e-05 - accuracy: 1.0000 - val_loss: 4.9602 - val_accuracy: 0.5170\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 4.4014 - val_accuracy: 0.5238\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 1.4335e-04 - accuracy: 1.0000 - val_loss: 3.7436 - val_accuracy: 0.5442\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 5.6401e-04 - accuracy: 1.0000 - val_loss: 3.1026 - val_accuracy: 0.5850\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 1.2115e-04 - accuracy: 1.0000 - val_loss: 2.7365 - val_accuracy: 0.6122\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 7.1822e-05 - accuracy: 1.0000 - val_loss: 2.2154 - val_accuracy: 0.7075\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 5.5343e-05 - accuracy: 1.0000 - val_loss: 1.8115 - val_accuracy: 0.7211\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 6.4994e-05 - accuracy: 1.0000 - val_loss: 1.5297 - val_accuracy: 0.7347\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 2.7027e-04 - accuracy: 1.0000 - val_loss: 1.2850 - val_accuracy: 0.7415\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 9.8841e-05 - accuracy: 1.0000 - val_loss: 1.0457 - val_accuracy: 0.7959\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 1.9628e-04 - accuracy: 1.0000 - val_loss: 0.8111 - val_accuracy: 0.8367\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 3.1622e-05 - accuracy: 1.0000 - val_loss: 0.6429 - val_accuracy: 0.8707\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 1.6903e-04 - accuracy: 1.0000 - val_loss: 0.5531 - val_accuracy: 0.8980\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 3.1715e-04 - accuracy: 1.0000 - val_loss: 0.4917 - val_accuracy: 0.8980\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 1.0500e-04 - accuracy: 1.0000 - val_loss: 0.5093 - val_accuracy: 0.8980\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 6.7987e-04 - accuracy: 1.0000 - val_loss: 0.4924 - val_accuracy: 0.8980\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 1.5851e-05 - accuracy: 1.0000 - val_loss: 0.6840 - val_accuracy: 0.8367\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 3.1036e-04 - accuracy: 1.0000 - val_loss: 0.5669 - val_accuracy: 0.8776\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 3.1575e-05 - accuracy: 1.0000 - val_loss: 0.4559 - val_accuracy: 0.8980\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 6.1315e-05 - accuracy: 1.0000 - val_loss: 0.3988 - val_accuracy: 0.9184\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 2.0008e-04 - accuracy: 1.0000 - val_loss: 0.2643 - val_accuracy: 0.9320\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 9.5524e-05 - accuracy: 1.0000 - val_loss: 0.2590 - val_accuracy: 0.9320\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 3.6040e-05 - accuracy: 1.0000 - val_loss: 0.2492 - val_accuracy: 0.9320\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 9.6868e-05 - accuracy: 1.0000 - val_loss: 0.2403 - val_accuracy: 0.9456\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 5.6187e-05 - accuracy: 1.0000 - val_loss: 0.2419 - val_accuracy: 0.9524\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 1.2443e-05 - accuracy: 1.0000 - val_loss: 0.2380 - val_accuracy: 0.9524\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 8.5432e-05 - accuracy: 1.0000 - val_loss: 0.2363 - val_accuracy: 0.9592\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 6.4987e-05 - accuracy: 1.0000 - val_loss: 0.2411 - val_accuracy: 0.9524\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 6.2773e-05 - accuracy: 1.0000 - val_loss: 0.2400 - val_accuracy: 0.9456\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 2.2732e-04 - accuracy: 1.0000 - val_loss: 0.2328 - val_accuracy: 0.9524\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 5.4474e-05 - accuracy: 1.0000 - val_loss: 0.2265 - val_accuracy: 0.9456\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 9.8191e-05 - accuracy: 1.0000 - val_loss: 0.2308 - val_accuracy: 0.9524\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 2.1587e-04 - accuracy: 1.0000 - val_loss: 0.2429 - val_accuracy: 0.9592\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 1.8516e-05 - accuracy: 1.0000 - val_loss: 0.2374 - val_accuracy: 0.9592\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 3.1647e-05 - accuracy: 1.0000 - val_loss: 0.2245 - val_accuracy: 0.9592\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 5.9604e-05 - accuracy: 1.0000 - val_loss: 0.2162 - val_accuracy: 0.9592\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 1.5553e-04 - accuracy: 1.0000 - val_loss: 0.2279 - val_accuracy: 0.9592\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 2.6537e-05 - accuracy: 1.0000 - val_loss: 0.2350 - val_accuracy: 0.9592\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 9.4273e-05 - accuracy: 1.0000 - val_loss: 0.2410 - val_accuracy: 0.9592\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 7.9916e-05 - accuracy: 1.0000 - val_loss: 0.2340 - val_accuracy: 0.9592\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 1.5526e-05 - accuracy: 1.0000 - val_loss: 0.2304 - val_accuracy: 0.9592\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 5.9692e-05 - accuracy: 1.0000 - val_loss: 0.1714 - val_accuracy: 0.9592\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 1.0366e-04 - accuracy: 1.0000 - val_loss: 0.1560 - val_accuracy: 0.9728\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 2.4676e-05 - accuracy: 1.0000 - val_loss: 0.1540 - val_accuracy: 0.9728\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 3.0374e-05 - accuracy: 1.0000 - val_loss: 0.1640 - val_accuracy: 0.9728\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 1.4051e-04 - accuracy: 1.0000 - val_loss: 0.1807 - val_accuracy: 0.9592\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 4.3329e-05 - accuracy: 1.0000 - val_loss: 0.1954 - val_accuracy: 0.9592\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 1.1137e-05 - accuracy: 1.0000 - val_loss: 0.2018 - val_accuracy: 0.9592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDVY6HbxMOlH",
        "outputId": "a216d25c-b273-4537-8a8b-6f2cfddc524a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# pred_test= model.predict_classes(X_test)\n",
        "pred_test = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict   0   1\n",
            "Actual         \n",
            "0        70   2\n",
            "1         4  71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7pT2q7traXg",
        "outputId": "fa2295a0-5792-4bba-8d79-4e7da11fe9c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(METRICS)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96        72\n",
            "           1       0.97      0.95      0.96        75\n",
            "\n",
            "    accuracy                           0.96       147\n",
            "   macro avg       0.96      0.96      0.96       147\n",
            "weighted avg       0.96      0.96      0.96       147\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElpxWbBnpgLX",
        "outputId": "20e1ec26-b427-45b3-bcb7-113c74675100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "'''\n",
        "#X =np.array(df.copy())/255.0 \n",
        "X =np.array(df.copy())\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)\n",
        "model = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh', solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  \n",
        "prediction = model.predict(X_test)  \n",
        "y =np.copy(y_test)\n",
        "data = {'y_true': y_test,'y_predict': prediction}  \n",
        "# este dado esta no formato de dicionario\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)\n",
        "print(METRICS)\n",
        "#X =np.array(df.copy())/255.0 X =np.array(df_all.copy())X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)model = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh',                       solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  prediction = model.predict(X_test)  y =np.copy(y_test)data = {'y_true': y_test,'y_predict': prediction}  # este dado esta no formato de dicionariodf = pd.DataFrame(data, columns=['y_true','y_predict'])confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])print(confusion_matrix)y_true = df['y_true']y_pred = df['y_predict']\n",
        "'''"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n#X =np.array(df.copy())/255.0 \\nX =np.array(df.copy())\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)\\nmodel = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh', solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  \\nprediction = model.predict(X_test)  \\ny =np.copy(y_test)\\ndata = {'y_true': y_test,'y_predict': prediction}  \\n# este dado esta no formato de dicionario\\ndf = pd.DataFrame(data, columns=['y_true','y_predict'])\\nconfusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\\nprint(confusion_matrix)\\ny_true = df['y_true']\\ny_pred = df['y_predict']  \\nMETRICS=sklearn.metrics.classification_report(y_true, y_pred)\\nprint(METRICS)\\n#X =np.array(df.copy())/255.0 X =np.array(df_all.copy())X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)model = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh',                       solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  prediction = model.predict(X_test)  y =np.copy(y_test)data = {'y_true': y_test,'y_predict': prediction}  # este dado esta no formato de dicionariodf = pd.DataFrame(data, columns=['y_true','y_predict'])confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])print(confusion_matrix)y_true = df['y_true']y_pred = df['y_predict']\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iFNNrlWV9tH",
        "outputId": "98421e37-4707-43ae-a585-66c8e75a9cb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pred_test"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
              "       1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv5I61yhPQmk",
        "outputId": "4e7454c0-1b07-417d-f95b-79c1186fa33c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[4] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  #prediction = model.predict_classes(result)\n",
        "  prediction= np.argmax(model.predict(result), axis=-1)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0   157.0  175.344482  177.915619  ...  177.862671  183.587906  189.601364\n",
            "1   133.0  124.376732  125.191139  ...  177.182831  166.695282  163.526306\n",
            "3   123.0  180.582352  180.984604  ...  191.248413  192.686493  203.787964\n",
            "6   102.0  178.883530  178.805099  ...  176.359863  168.362564  160.341797\n",
            "8   146.0  230.515106  216.956848  ...    0.995496    0.972978    1.007881\n",
            "17  106.0  137.002136  144.246002  ...  116.906372  125.475967  131.228912\n",
            "20  130.0  172.528061  169.815140  ...  159.675507  163.724747  162.381317\n",
            "22  194.0  148.494629  149.613113  ...    2.848230    1.284090    0.413859\n",
            "31  161.0  158.026459  169.304367  ...  170.436676  182.920609  175.400757\n",
            "32  159.0  118.642181  129.003555  ...  176.943787  173.552429  163.428772\n",
            "36  112.0    0.750000    1.500000  ...    0.812500    1.562500    1.562500\n",
            "37  143.0    1.498851    0.435718  ...  130.223434  130.882233  129.761307\n",
            "38  198.0  135.184875  148.469437  ...  182.118454  177.932953  175.044891\n",
            "42  109.0  161.113953  160.376312  ...  103.721657   97.503326  115.610893\n",
            "43  123.0  121.317078  115.633690  ...  144.101196  143.732697  130.084076\n",
            "45  186.0  131.579834  124.036545  ...  212.067200  226.306641  201.660095\n",
            "1   149.0  179.902527  174.739700  ...  211.659531  215.780396  215.947754\n",
            "13  150.0  153.028458  157.289764  ...  156.901337  149.930130  142.436447\n",
            "14  193.0  151.304688  144.617584  ...  199.284821  191.827179  180.701035\n",
            "18  104.0  179.628708  177.029602  ...  133.983734  142.713028  140.758896\n",
            "19  113.0  250.337540  251.204300  ...  142.492920  172.530807  216.551666\n",
            "30  133.0  186.229919  170.667587  ...  117.609421  116.066483  130.914124\n",
            "32  129.0  228.445999  184.313446  ...  137.173782  145.989777  176.193741\n",
            "33  101.0  131.764740  138.000000  ...  155.146759  153.582001  149.928436\n",
            "40  151.0  152.083237  177.229507  ...  165.453369  170.164307  179.889587\n",
            "42  161.0  193.287323  217.306244  ...  129.255203  131.678650  134.816635\n",
            "43  130.0  166.181778  170.499405  ...  228.469345  237.734924  238.391968\n",
            "44  128.0    1.246094    0.339844  ...   49.048828   43.777344   41.242188\n",
            "47  133.0  190.196671  182.285324  ...  185.301941  180.659271  174.393356\n",
            "49  131.0   91.860725   86.451492  ...    1.694540    0.763242    0.045685\n",
            "\n",
            "[30 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjRbWgmX_LFH",
        "outputId": "1ab608a7-49eb-4c3b-ddcf-867554ef523a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "#from GetBetterSegm import GetBetter"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'marquesgabi_paper_fev_2021' already exists and is not an empty directory.\n",
            "/content/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAG_I6FwCvFr",
        "outputId": "bfd1d48c-bbca-40b9-c172-d2da50c2b211",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "#!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd marquesgabi_out_2020\n",
        "#%cd Doutorado\n",
        "#PSD_imageJ = 'Amostra7.csv' \n",
        "#PSD_new = pd.read_csv(PSD_imageJ,sep=';')\n",
        "#encoding='utf8'\n",
        "\n",
        "PSD_imageJ = 'Areas_ImageJ.csv'\n",
        "PSD_new = pd.read_csv(PSD_imageJ)\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marquesgabi_out_2020'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 146 (delta 75), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (146/146), 1.00 MiB | 6.89 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "/content/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020\n",
            "   Juntas   Area\n",
            "0       1  2.001\n",
            "1       2  0.820\n",
            "2       3  1.270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tEPjIBnv_xM",
        "outputId": "a02d0df7-5311-4c29-983d-17b9a9afa3c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "PSD_new.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(95, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PekBHQOT_6CP",
        "outputId": "52dd6d46-6be6-46f5-cb09-dc366ef17800",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>157.0</td>\n",
              "      <td>175.344482</td>\n",
              "      <td>177.915619</td>\n",
              "      <td>170.963257</td>\n",
              "      <td>164.714630</td>\n",
              "      <td>171.085693</td>\n",
              "      <td>165.214142</td>\n",
              "      <td>139.734711</td>\n",
              "      <td>155.191406</td>\n",
              "      <td>176.499466</td>\n",
              "      <td>179.633224</td>\n",
              "      <td>178.529678</td>\n",
              "      <td>172.041870</td>\n",
              "      <td>172.663879</td>\n",
              "      <td>175.796997</td>\n",
              "      <td>178.129089</td>\n",
              "      <td>170.896027</td>\n",
              "      <td>161.869827</td>\n",
              "      <td>146.862396</td>\n",
              "      <td>145.305252</td>\n",
              "      <td>166.992218</td>\n",
              "      <td>184.215973</td>\n",
              "      <td>189.964188</td>\n",
              "      <td>190.954636</td>\n",
              "      <td>186.426758</td>\n",
              "      <td>178.154785</td>\n",
              "      <td>173.941071</td>\n",
              "      <td>161.570038</td>\n",
              "      <td>157.885757</td>\n",
              "      <td>184.488297</td>\n",
              "      <td>186.257462</td>\n",
              "      <td>176.524216</td>\n",
              "      <td>167.830231</td>\n",
              "      <td>169.812302</td>\n",
              "      <td>167.865753</td>\n",
              "      <td>158.502380</td>\n",
              "      <td>149.960541</td>\n",
              "      <td>166.336700</td>\n",
              "      <td>175.124725</td>\n",
              "      <td>182.149887</td>\n",
              "      <td>...</td>\n",
              "      <td>183.021866</td>\n",
              "      <td>178.970840</td>\n",
              "      <td>174.308929</td>\n",
              "      <td>177.395782</td>\n",
              "      <td>173.603470</td>\n",
              "      <td>169.884705</td>\n",
              "      <td>166.002609</td>\n",
              "      <td>164.962738</td>\n",
              "      <td>166.616714</td>\n",
              "      <td>166.231140</td>\n",
              "      <td>172.383865</td>\n",
              "      <td>186.319107</td>\n",
              "      <td>175.957199</td>\n",
              "      <td>178.063507</td>\n",
              "      <td>177.895737</td>\n",
              "      <td>171.464325</td>\n",
              "      <td>196.994171</td>\n",
              "      <td>246.752762</td>\n",
              "      <td>249.770065</td>\n",
              "      <td>231.748154</td>\n",
              "      <td>194.174072</td>\n",
              "      <td>187.462906</td>\n",
              "      <td>196.443146</td>\n",
              "      <td>196.556091</td>\n",
              "      <td>187.366791</td>\n",
              "      <td>184.571548</td>\n",
              "      <td>187.085159</td>\n",
              "      <td>191.395264</td>\n",
              "      <td>191.930954</td>\n",
              "      <td>186.039703</td>\n",
              "      <td>177.824219</td>\n",
              "      <td>174.439728</td>\n",
              "      <td>172.183609</td>\n",
              "      <td>173.104385</td>\n",
              "      <td>170.588196</td>\n",
              "      <td>162.182480</td>\n",
              "      <td>167.321869</td>\n",
              "      <td>177.862671</td>\n",
              "      <td>183.587906</td>\n",
              "      <td>189.601364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>133.0</td>\n",
              "      <td>124.376732</td>\n",
              "      <td>125.191139</td>\n",
              "      <td>125.321327</td>\n",
              "      <td>123.936295</td>\n",
              "      <td>124.842102</td>\n",
              "      <td>122.481987</td>\n",
              "      <td>115.667587</td>\n",
              "      <td>91.412743</td>\n",
              "      <td>93.166206</td>\n",
              "      <td>137.609421</td>\n",
              "      <td>146.947372</td>\n",
              "      <td>144.481995</td>\n",
              "      <td>146.623276</td>\n",
              "      <td>144.457062</td>\n",
              "      <td>133.221603</td>\n",
              "      <td>127.653740</td>\n",
              "      <td>122.653748</td>\n",
              "      <td>122.725761</td>\n",
              "      <td>121.828247</td>\n",
              "      <td>118.542931</td>\n",
              "      <td>117.019402</td>\n",
              "      <td>109.274239</td>\n",
              "      <td>105.573410</td>\n",
              "      <td>107.933517</td>\n",
              "      <td>147.412720</td>\n",
              "      <td>170.249298</td>\n",
              "      <td>173.986145</td>\n",
              "      <td>175.110794</td>\n",
              "      <td>120.914131</td>\n",
              "      <td>123.556793</td>\n",
              "      <td>127.797798</td>\n",
              "      <td>127.598343</td>\n",
              "      <td>132.476456</td>\n",
              "      <td>123.645432</td>\n",
              "      <td>121.512466</td>\n",
              "      <td>100.880890</td>\n",
              "      <td>96.473686</td>\n",
              "      <td>141.847641</td>\n",
              "      <td>148.581726</td>\n",
              "      <td>...</td>\n",
              "      <td>131.678680</td>\n",
              "      <td>130.099716</td>\n",
              "      <td>131.049866</td>\n",
              "      <td>156.722992</td>\n",
              "      <td>169.340714</td>\n",
              "      <td>180.437668</td>\n",
              "      <td>184.265945</td>\n",
              "      <td>193.656525</td>\n",
              "      <td>195.127426</td>\n",
              "      <td>196.734070</td>\n",
              "      <td>191.747925</td>\n",
              "      <td>184.235458</td>\n",
              "      <td>127.504150</td>\n",
              "      <td>121.310257</td>\n",
              "      <td>127.756241</td>\n",
              "      <td>140.260391</td>\n",
              "      <td>150.487534</td>\n",
              "      <td>156.066483</td>\n",
              "      <td>164.620483</td>\n",
              "      <td>165.833801</td>\n",
              "      <td>163.540161</td>\n",
              "      <td>160.631561</td>\n",
              "      <td>165.277023</td>\n",
              "      <td>172.756226</td>\n",
              "      <td>159.955673</td>\n",
              "      <td>130.049866</td>\n",
              "      <td>128.019394</td>\n",
              "      <td>124.601105</td>\n",
              "      <td>127.855957</td>\n",
              "      <td>129.368408</td>\n",
              "      <td>131.542938</td>\n",
              "      <td>155.576172</td>\n",
              "      <td>167.240982</td>\n",
              "      <td>178.548477</td>\n",
              "      <td>182.626038</td>\n",
              "      <td>182.952911</td>\n",
              "      <td>183.875336</td>\n",
              "      <td>177.182831</td>\n",
              "      <td>166.695282</td>\n",
              "      <td>163.526306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>123.0</td>\n",
              "      <td>180.582352</td>\n",
              "      <td>180.984604</td>\n",
              "      <td>185.103195</td>\n",
              "      <td>176.423569</td>\n",
              "      <td>182.031342</td>\n",
              "      <td>175.333603</td>\n",
              "      <td>169.244308</td>\n",
              "      <td>181.865631</td>\n",
              "      <td>193.934296</td>\n",
              "      <td>196.919693</td>\n",
              "      <td>203.053284</td>\n",
              "      <td>203.502502</td>\n",
              "      <td>199.893661</td>\n",
              "      <td>195.788940</td>\n",
              "      <td>192.316559</td>\n",
              "      <td>187.494553</td>\n",
              "      <td>187.371155</td>\n",
              "      <td>194.816849</td>\n",
              "      <td>202.361099</td>\n",
              "      <td>202.413727</td>\n",
              "      <td>198.452866</td>\n",
              "      <td>194.975800</td>\n",
              "      <td>191.393890</td>\n",
              "      <td>189.316681</td>\n",
              "      <td>190.519135</td>\n",
              "      <td>188.325623</td>\n",
              "      <td>187.720551</td>\n",
              "      <td>193.094727</td>\n",
              "      <td>184.585907</td>\n",
              "      <td>182.915588</td>\n",
              "      <td>181.994583</td>\n",
              "      <td>170.836807</td>\n",
              "      <td>157.015259</td>\n",
              "      <td>147.561249</td>\n",
              "      <td>164.484055</td>\n",
              "      <td>194.113770</td>\n",
              "      <td>199.999359</td>\n",
              "      <td>204.237167</td>\n",
              "      <td>214.239624</td>\n",
              "      <td>...</td>\n",
              "      <td>201.721741</td>\n",
              "      <td>203.191360</td>\n",
              "      <td>200.671646</td>\n",
              "      <td>197.271271</td>\n",
              "      <td>197.464554</td>\n",
              "      <td>191.191422</td>\n",
              "      <td>188.394409</td>\n",
              "      <td>188.897415</td>\n",
              "      <td>186.033661</td>\n",
              "      <td>183.265198</td>\n",
              "      <td>187.593246</td>\n",
              "      <td>198.494553</td>\n",
              "      <td>179.291962</td>\n",
              "      <td>167.124222</td>\n",
              "      <td>168.522461</td>\n",
              "      <td>173.389252</td>\n",
              "      <td>173.351837</td>\n",
              "      <td>177.656036</td>\n",
              "      <td>182.457886</td>\n",
              "      <td>184.476776</td>\n",
              "      <td>188.866959</td>\n",
              "      <td>181.607788</td>\n",
              "      <td>153.944427</td>\n",
              "      <td>157.605591</td>\n",
              "      <td>160.219452</td>\n",
              "      <td>160.928146</td>\n",
              "      <td>167.906281</td>\n",
              "      <td>182.455414</td>\n",
              "      <td>198.698807</td>\n",
              "      <td>203.030472</td>\n",
              "      <td>204.339813</td>\n",
              "      <td>204.661591</td>\n",
              "      <td>198.819031</td>\n",
              "      <td>188.750031</td>\n",
              "      <td>193.095062</td>\n",
              "      <td>193.280518</td>\n",
              "      <td>194.466324</td>\n",
              "      <td>191.248413</td>\n",
              "      <td>192.686493</td>\n",
              "      <td>203.787964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>102.0</td>\n",
              "      <td>178.883530</td>\n",
              "      <td>178.805099</td>\n",
              "      <td>174.592102</td>\n",
              "      <td>168.914276</td>\n",
              "      <td>163.611328</td>\n",
              "      <td>156.056534</td>\n",
              "      <td>139.745880</td>\n",
              "      <td>129.077667</td>\n",
              "      <td>145.577484</td>\n",
              "      <td>173.837372</td>\n",
              "      <td>197.981949</td>\n",
              "      <td>221.236084</td>\n",
              "      <td>232.962357</td>\n",
              "      <td>231.967346</td>\n",
              "      <td>232.243378</td>\n",
              "      <td>227.934265</td>\n",
              "      <td>218.788940</td>\n",
              "      <td>220.875839</td>\n",
              "      <td>214.074997</td>\n",
              "      <td>190.150360</td>\n",
              "      <td>171.600159</td>\n",
              "      <td>168.489059</td>\n",
              "      <td>167.716263</td>\n",
              "      <td>168.006927</td>\n",
              "      <td>170.072662</td>\n",
              "      <td>172.501740</td>\n",
              "      <td>172.628235</td>\n",
              "      <td>154.098434</td>\n",
              "      <td>188.378326</td>\n",
              "      <td>185.302597</td>\n",
              "      <td>175.152649</td>\n",
              "      <td>166.646698</td>\n",
              "      <td>141.374466</td>\n",
              "      <td>105.032303</td>\n",
              "      <td>80.671677</td>\n",
              "      <td>80.174942</td>\n",
              "      <td>109.582100</td>\n",
              "      <td>129.595947</td>\n",
              "      <td>148.545563</td>\n",
              "      <td>...</td>\n",
              "      <td>199.880844</td>\n",
              "      <td>200.955811</td>\n",
              "      <td>207.228394</td>\n",
              "      <td>206.936951</td>\n",
              "      <td>193.404465</td>\n",
              "      <td>188.636322</td>\n",
              "      <td>194.000397</td>\n",
              "      <td>193.733566</td>\n",
              "      <td>189.720139</td>\n",
              "      <td>183.252609</td>\n",
              "      <td>173.797394</td>\n",
              "      <td>167.106903</td>\n",
              "      <td>203.404877</td>\n",
              "      <td>195.936966</td>\n",
              "      <td>188.579773</td>\n",
              "      <td>176.210709</td>\n",
              "      <td>172.944656</td>\n",
              "      <td>176.609390</td>\n",
              "      <td>170.820465</td>\n",
              "      <td>161.282593</td>\n",
              "      <td>153.524429</td>\n",
              "      <td>136.087296</td>\n",
              "      <td>123.743179</td>\n",
              "      <td>124.667450</td>\n",
              "      <td>135.291061</td>\n",
              "      <td>156.734344</td>\n",
              "      <td>177.759338</td>\n",
              "      <td>188.728958</td>\n",
              "      <td>192.599792</td>\n",
              "      <td>198.466766</td>\n",
              "      <td>207.627472</td>\n",
              "      <td>209.455215</td>\n",
              "      <td>198.829315</td>\n",
              "      <td>189.081512</td>\n",
              "      <td>187.599792</td>\n",
              "      <td>182.570190</td>\n",
              "      <td>181.983109</td>\n",
              "      <td>176.359863</td>\n",
              "      <td>168.362564</td>\n",
              "      <td>160.341797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>146.0</td>\n",
              "      <td>230.515106</td>\n",
              "      <td>216.956848</td>\n",
              "      <td>177.661835</td>\n",
              "      <td>108.654915</td>\n",
              "      <td>107.531433</td>\n",
              "      <td>120.491089</td>\n",
              "      <td>142.791153</td>\n",
              "      <td>140.937515</td>\n",
              "      <td>151.349792</td>\n",
              "      <td>184.765045</td>\n",
              "      <td>198.501770</td>\n",
              "      <td>188.227448</td>\n",
              "      <td>164.183334</td>\n",
              "      <td>143.500656</td>\n",
              "      <td>93.989677</td>\n",
              "      <td>203.814789</td>\n",
              "      <td>248.258759</td>\n",
              "      <td>250.471191</td>\n",
              "      <td>252.355392</td>\n",
              "      <td>253.942184</td>\n",
              "      <td>253.988174</td>\n",
              "      <td>254.059113</td>\n",
              "      <td>254.598999</td>\n",
              "      <td>254.210159</td>\n",
              "      <td>254.462387</td>\n",
              "      <td>254.223312</td>\n",
              "      <td>254.220688</td>\n",
              "      <td>253.340591</td>\n",
              "      <td>243.006760</td>\n",
              "      <td>205.983688</td>\n",
              "      <td>118.425606</td>\n",
              "      <td>104.772377</td>\n",
              "      <td>104.745918</td>\n",
              "      <td>111.684189</td>\n",
              "      <td>120.127609</td>\n",
              "      <td>129.058731</td>\n",
              "      <td>145.578720</td>\n",
              "      <td>164.569519</td>\n",
              "      <td>172.257446</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.920060</td>\n",
              "      <td>0.985363</td>\n",
              "      <td>1.018390</td>\n",
              "      <td>1.291237</td>\n",
              "      <td>0.044661</td>\n",
              "      <td>1.409270</td>\n",
              "      <td>1.323138</td>\n",
              "      <td>0.764308</td>\n",
              "      <td>0.216363</td>\n",
              "      <td>1.212798</td>\n",
              "      <td>2.346031</td>\n",
              "      <td>0.809908</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.726778</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.384312</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.018390</td>\n",
              "      <td>1.291237</td>\n",
              "      <td>0.044661</td>\n",
              "      <td>1.409270</td>\n",
              "      <td>1.007881</td>\n",
              "      <td>0.995496</td>\n",
              "      <td>0.972978</td>\n",
              "      <td>1.007881</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Width           0           1  ...         781         782         783\n",
              "0  157.0  175.344482  177.915619  ...  177.862671  183.587906  189.601364\n",
              "1  133.0  124.376732  125.191139  ...  177.182831  166.695282  163.526306\n",
              "3  123.0  180.582352  180.984604  ...  191.248413  192.686493  203.787964\n",
              "6  102.0  178.883530  178.805099  ...  176.359863  168.362564  160.341797\n",
              "8  146.0  230.515106  216.956848  ...    0.995496    0.972978    1.007881\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaZPe_AxNBK9",
        "outputId": "5791f3d0-9bef-4f5e-ef82-88a017598291",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "PSD_new.head()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Juntas</th>\n",
              "      <th>Area</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1.162</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Juntas   Area\n",
              "0       1  2.001\n",
              "1       2  0.820\n",
              "2       3  1.270\n",
              "3       4  0.958\n",
              "4       5  1.162"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "#lost_value = float(PSD_new.columns[1])\n",
        "\n",
        "# Area = np.array(PSD_new.iloc[:,1])\n",
        "Area = PSD_new['Area'].values\n",
        "# Area = np.concatenate( (Area, [lost_value] ) )\n",
        "# Area = np.concatenate( (Area, [lost_value] ) )\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfk_fNXGDK5_"
      },
      "source": [
        "wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        "wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        "X = pd.DataFrame([Diam1,Diameter_All])\n",
        "wts = pd.DataFrame([wt1,wt2])\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OieAXw_by3nz",
        "outputId": "75151154-724d-49ab-ebcb-05a495bc31fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "A = plt.hist(X,weights=wts,bins=7)\n",
        "plt.legend(['True','CNN'])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb8c9926510>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpdrvEySy8Ij"
      },
      "source": [
        "B = A[0][0]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUhGZHT8y9Or",
        "outputId": "519b46de-add2-4174-c00b-5291cc3e69b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Novo = []\n",
        "k = 0\n",
        "soma = 0\n",
        "for i in B:\n",
        "  if(k<4):\n",
        "    Novo.append(i)\n",
        "  else:\n",
        "    soma = soma + i\n",
        "  k = k + 1\n",
        "Novo.append(soma)\n",
        "print(Novo)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14.736842105263156, 24.2105263157895, 42.1052631578948, 14.736842105263179, 4.21052631578948]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMK89w-fzCVe",
        "outputId": "35477c05-2ddf-4c43-99c6-68677b673628",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# Freq1 = [19.12043703, 29.22484843, 19.35872174, 20.82190224, 11.47409056] # avarage 4 samples\n",
        "Freq1 = [20.69301557, 28.55598044, 18.50768331, 22.7106327, 8.905907357] # avarage 10 samples\n",
        "#Freq2 = [16.93792791, 31.38008965, 24.93810752, 18.56158392, 6.233810752, 0.4]\n",
        "Freq2 = [16.93792791, 31.38008965, 24.93810752, 18.56158392, 6.633810752]\n",
        "Freq3 = Novo\n",
        "barWidth = 0.25\n",
        "\n",
        "br1 = range(len(Freq1))\n",
        "# Set position of bar on X axis\n",
        "br2 = [x + barWidth for x in br1]\n",
        "br3 = [x + barWidth for x in br2]\n",
        "# labels = [0.8, 1.0, 1.2, 1.4, 1.6, 1.8]\n",
        "labels = [0.8, 1.0, 1.2, 1.4, 1.6]\n",
        "\n",
        "xx=[]\n",
        "for a in labels:\n",
        "  xx.append(str(a))\n",
        "plt.bar(br1, Freq1 , color=\"green\", align=\"center\", width=0.3, tick_label= xx) \n",
        "plt.bar(br2, Freq2 , color=\"red\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.bar(br3, Freq3 , color=\"blue\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.legend(['CNN 1','CNN 2','True'])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb8c9694510>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}