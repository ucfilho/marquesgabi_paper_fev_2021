{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_final_amostra_03_nov_25_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/defesa/PSD_histogram_final_amostra_03_nov_25_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from sklearn.metrics import r2_score\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZEvJvfoibE4"
      },
      "source": [
        "#!pip install mahotas"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "445ca147-7e18-4fb4-b52c-a9cafe6092cc"
      },
      "source": [
        "'''\n",
        "import mahotas.features.texture as mht\n",
        "import mahotas.features\n",
        "'''"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nimport mahotas.features.texture as mht\\nimport mahotas.features\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "_VcTdaNVh9EE",
        "outputId": "025c0189-cdd0-4dd7-e38a-02180a021d01"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas\n",
        "'''"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\\n%cd marquesgabi_fev_2020\\nimport Go2BlackWhite\\nimport Go2Mahotas\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UZ30b1EVQhq"
      },
      "source": [
        "def BlackWhite(Transfere,Size):\n",
        "\n",
        "  img_name=[]\n",
        "  xw=[]\n",
        "  ww=[]\n",
        "\n",
        "  with zipfile.ZipFile(Transfere, \"r\") as f:\n",
        "    for name in f.namelist():\n",
        "      img_name.append(name)\n",
        "      #xw.append(cv2.imread(name))\n",
        "      xw.append(cv2.resize(cv2.imread(name),(Size,Size)))\n",
        "\n",
        "  nrow=len(img_name)\n",
        "  ncol=Size*Size\n",
        "  pw=np.zeros((nrow,ncol))\n",
        "  #pw=[]\n",
        "  for i in range(nrow):\n",
        "    ww.append(cv2.cvtColor(np.array(xw[i]), cv2.COLOR_BGR2GRAY))\n",
        "    pw[i,:]=ww[i].ravel()\n",
        "  return ww,img_name"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v7SRrc8mH2N",
        "outputId": "94c9dad1-0478-4019-f41c-d3f6c334a886"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip' \n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Doutorado' already exists and is not an empty directory.\n",
            "/content/Doutorado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqIYzUcnrdMp",
        "outputId": "b01e18e8-6956-4dc2-a666-4879538c9edd"
      },
      "source": [
        "labels =[]\n",
        "with zipfile.ZipFile(Transfere, \"r\") as f:\n",
        "  for f in f.namelist():\n",
        "    labels.append(f)\n",
        "print(labels)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Fotos_Grandes-3cdAmostra/Q6-8-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-4.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "# ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "ww,img_name=BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[4] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHgqAnaFyCjp",
        "outputId": "39ebe543-36a4-49e4-b808-4daeaedf5344"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MarquesGabi_Routines' already exists and is not an empty directory.\n",
            "/content/Doutorado/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDEGUiuubwuZ"
      },
      "source": [
        "FILE='SugarSample03.zip'\n",
        "img_name=[]\n",
        "x_original = [] \n",
        "\n",
        "data_file ='xls'\n",
        "\n",
        "\n",
        "file_name = zipfile.ZipFile(FILE, 'r')\n",
        "file_name.extractall()\n",
        "\n",
        "k = 0\n",
        "with zipfile.ZipFile(FILE, \"r\") as f:\n",
        "    for name in f.namelist():\n",
        "      if(name[-3:] == data_file):\n",
        "        #df =pd.read_csv(name)\n",
        "        if( k > 0):\n",
        "          df_old = df_ImgJ.copy()\n",
        "        df_ImgJ = pd.read_excel(name)\n",
        "        df_ImgJ = df_ImgJ.drop(labels=[0], axis=0)\n",
        "        if(k > 0):\n",
        "          df_ImgJ = pd.concat( [df_ImgJ, df_old], ignore_index = True)\n",
        "        k = k + 1"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbQ0tal0etXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c029199-f118-4914-a2fa-0a40ad12ade6"
      },
      "source": [
        "f.namelist()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Results_03_02.xls', 'Results_03_03.xls', 'Results_03_01.xls']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMBJ6C-YdF3q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "92916d08-ef71-4168-cb63-bdff8b7f0ce6"
      },
      "source": [
        "df_ImgJ.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Area</th>\n",
              "      <th>Mean</th>\n",
              "      <th>Min</th>\n",
              "      <th>Max</th>\n",
              "      <th>Major</th>\n",
              "      <th>Minor</th>\n",
              "      <th>Angle</th>\n",
              "      <th>Feret</th>\n",
              "      <th>FeretX</th>\n",
              "      <th>FeretY</th>\n",
              "      <th>FeretAngle</th>\n",
              "      <th>MinFeret</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>1.288</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>1.383</td>\n",
              "      <td>1.185</td>\n",
              "      <td>5.847</td>\n",
              "      <td>1.636</td>\n",
              "      <td>767</td>\n",
              "      <td>213</td>\n",
              "      <td>18.157</td>\n",
              "      <td>1.161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0.407</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>0.814</td>\n",
              "      <td>0.637</td>\n",
              "      <td>62.186</td>\n",
              "      <td>0.877</td>\n",
              "      <td>283</td>\n",
              "      <td>234</td>\n",
              "      <td>59.036</td>\n",
              "      <td>0.667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>0.592</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>0.925</td>\n",
              "      <td>0.815</td>\n",
              "      <td>117.923</td>\n",
              "      <td>1.078</td>\n",
              "      <td>633</td>\n",
              "      <td>154</td>\n",
              "      <td>122.335</td>\n",
              "      <td>0.802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>1.391</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>1.435</td>\n",
              "      <td>1.235</td>\n",
              "      <td>29.966</td>\n",
              "      <td>1.564</td>\n",
              "      <td>1321</td>\n",
              "      <td>333</td>\n",
              "      <td>53.253</td>\n",
              "      <td>1.165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>0.549</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>0.923</td>\n",
              "      <td>0.758</td>\n",
              "      <td>136.396</td>\n",
              "      <td>1.024</td>\n",
              "      <td>370</td>\n",
              "      <td>254</td>\n",
              "      <td>118.237</td>\n",
              "      <td>0.738</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Area  Mean  Min  Max  ...  Feret  FeretX  FeretY  FeretAngle  MinFeret\n",
              "0  2  1.288   255  255  255  ...  1.636     767     213      18.157     1.161\n",
              "1  3  0.407   255  255  255  ...  0.877     283     234      59.036     0.667\n",
              "2  4  0.592   255  255  255  ...  1.078     633     154     122.335     0.802\n",
              "3  5  1.391   255  255  255  ...  1.564    1321     333      53.253     1.165\n",
              "4  6  0.549   255  255  255  ...  1.024     370     254     118.237     0.738\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from Segment_Filter import Segmenta  # got image provided segmented"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5MN5a_v4np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f61a234-49a0-401d-daed-6008ebff52fd"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     143  128.641693  134.709366  ...  185.119705  185.375015  188.121582\n",
            "1     125  135.858765  134.134079  ...  190.103699  199.079315  200.879501\n",
            "2     187  206.851807  204.384018  ...  195.234711  164.001541  183.304428\n",
            "3     117  192.990585  175.238800  ...  168.930679  161.112000  123.847321\n",
            "4     130  144.270065  172.452332  ...  129.588882  167.569244  151.101059\n",
            "5     151  120.834038  137.649277  ...    0.623788    0.332398    1.405245\n",
            "6     108  173.115219  170.936890  ...  155.703690  156.274338  155.990387\n",
            "7     197  192.117905  139.849442  ...    1.462548    0.193383    1.300858\n",
            "8     176  148.141006  139.366730  ...    1.104339    0.153409    1.343492\n",
            "9     134   61.823120   63.769215  ...    0.327244    0.506349    1.461573\n",
            "10    186  167.432785  173.097031  ...  138.006943  143.771652  143.122101\n",
            "11    147  169.925156  176.868500  ...  146.083908  147.437653  131.419510\n",
            "12    178  247.531662  249.733383  ...  171.623306  183.478485  187.127258\n",
            "13    107  100.276443  112.514374  ...  149.498032  151.001923  151.044022\n",
            "14    126  118.308640   99.790123  ...  253.086411  247.432098  218.345673\n",
            "15    110  124.871063  138.246277  ...  169.301147  166.647934  164.628754\n",
            "16    110    1.000000    1.792066  ...   66.760658   63.984131   67.628426\n",
            "17    192  213.791656  210.453110  ...  152.586365  156.879761  159.603287\n",
            "18    134  211.238358  202.935410  ...  153.225220  154.319458  155.513031\n",
            "19    196   87.408165  105.265305  ...  215.265305  163.122452  163.632645\n",
            "20    190  230.211853  221.315460  ...  166.321655  193.435440  197.569519\n",
            "21    155  184.760941  172.979446  ...    0.701727    0.314631    1.411405\n",
            "22    122    0.134641    1.037624  ...    0.649557    2.711905    3.182746\n",
            "23    178  171.554871  163.874283  ...  133.907227  132.356415   96.365753\n",
            "24    145  183.140503  216.066620  ...  122.685272  112.209564  104.954010\n",
            "25    172  130.754471  130.741501  ...  177.870193  174.097885  173.716599\n",
            "26    153  131.370758  129.227905  ...  154.788422  153.596771  166.208847\n",
            "27    178  107.527222  187.493896  ...  158.461838  160.753204  163.948502\n",
            "28    131   77.640518   95.133789  ...    1.453587    0.962298    0.031001\n",
            "29    121   83.410286   80.407082  ...    1.000000    1.000000    1.000000\n",
            "30    160    0.155625    1.280625  ...    1.000000    0.781250    0.059375\n",
            "31    159  124.145714  121.862267  ...  180.177032  169.604767  173.384918\n",
            "32    171  152.278244  156.088089  ...    1.000000    1.000000    1.000000\n",
            "33    164   88.815590   86.223679  ...  163.224274  141.193329  110.584183\n",
            "34    159   48.773067   73.702621  ...  176.559967  190.396225  197.523315\n",
            "35    192  172.082886  159.128448  ...  172.086792  176.686630  196.807709\n",
            "36    177  108.173378  109.993668  ...    1.000000    1.000000    1.000000\n",
            "37    137   89.261330   80.335556  ...  105.543449   95.416382   83.953323\n",
            "38    195    1.307798    0.179698  ...  107.259773   94.597649   95.931351\n",
            "39    148   45.374001   40.619431  ...    0.576333    0.360117    1.414171\n",
            "40    199  186.721237  176.022903  ...  173.389847  169.738525  166.234253\n",
            "41    130  149.672668  161.920715  ...    1.415621    0.600237    0.261775\n",
            "42    135  150.981873  151.777161  ...  174.998947  194.211609  190.507721\n",
            "43    124  147.450546  144.251801  ...   81.363152   80.762733   82.331940\n",
            "44    101  176.170181  174.801498  ...  158.210968  138.530838  143.808945\n",
            "45    123    0.805076    0.458854  ...  136.002594  137.288193  141.944016\n",
            "46    133  169.595581  170.132965  ...    0.296399    0.506925    1.454294\n",
            "47    136  113.459343  130.782867  ...  158.582184  155.076126  155.208481\n",
            "48    131  191.226776  196.751297  ...  173.860428  182.286514  189.915329\n",
            "49    113  234.821594  232.900543  ...  150.452271  189.792297  167.983551\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR2emP4rNjQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92a176de-f958-4bcc-ebc3-a64bd4fdc1f8"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MarquesGabi_Routines' already exists and is not an empty directory.\n",
            "/content/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIFPGE_-vx3T"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "Description = '3 layers of Convolution: 64, 128, 256 '\n",
        "N1 = 20\n",
        "N2 = 20\n",
        "\n",
        "# make the CNN\n",
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=64, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=N1))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=N2))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpbPQ1FSRG6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d31655c-d39f-47e8-e23f-2914b8f3415e"
      },
      "source": [
        "\n",
        "# training the model\n",
        "r = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), \n",
        "              epochs=200, batch_size=32)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 3s 152ms/step - loss: 0.5134 - accuracy: 0.7930 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.2737 - accuracy: 0.8805 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.1630 - accuracy: 0.9300 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.1084 - accuracy: 0.9563 - val_loss: 0.6930 - val_accuracy: 0.4898\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0561 - accuracy: 0.9796 - val_loss: 0.6942 - val_accuracy: 0.4898\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0895 - accuracy: 0.9679 - val_loss: 0.6939 - val_accuracy: 0.4898\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0472 - accuracy: 0.9854 - val_loss: 0.6931 - val_accuracy: 0.4898\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0398 - accuracy: 0.9883 - val_loss: 0.6925 - val_accuracy: 0.7959\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0246 - accuracy: 0.9942 - val_loss: 0.6924 - val_accuracy: 0.8163\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0127 - accuracy: 0.9971 - val_loss: 0.6928 - val_accuracy: 0.4898\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.6923 - val_accuracy: 0.4898\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.6920 - val_accuracy: 0.4898\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.6912 - val_accuracy: 0.5102\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0101 - accuracy: 0.9971 - val_loss: 0.6909 - val_accuracy: 0.7279\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0334 - accuracy: 0.9942 - val_loss: 0.6899 - val_accuracy: 0.5102\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0111 - accuracy: 0.9971 - val_loss: 0.6913 - val_accuracy: 0.5102\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0280 - accuracy: 0.9942 - val_loss: 0.6912 - val_accuracy: 0.5102\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0353 - accuracy: 0.9854 - val_loss: 0.7118 - val_accuracy: 0.5102\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0249 - accuracy: 0.9942 - val_loss: 0.7091 - val_accuracy: 0.5102\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0129 - accuracy: 0.9971 - val_loss: 0.7333 - val_accuracy: 0.5102\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0089 - accuracy: 0.9971 - val_loss: 0.7798 - val_accuracy: 0.5102\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0094 - accuracy: 0.9971 - val_loss: 0.7839 - val_accuracy: 0.5102\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 0.7631 - val_accuracy: 0.5102\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.7798 - val_accuracy: 0.5102\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0044 - accuracy: 0.9971 - val_loss: 0.7936 - val_accuracy: 0.5102\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.8090 - val_accuracy: 0.5102\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0248 - accuracy: 0.9942 - val_loss: 0.8097 - val_accuracy: 0.5102\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 0.7737 - val_accuracy: 0.5102\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.7754 - val_accuracy: 0.5102\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0058 - accuracy: 0.9971 - val_loss: 0.7463 - val_accuracy: 0.5102\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0106 - accuracy: 0.9942 - val_loss: 0.8230 - val_accuracy: 0.5102\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 1.1308 - val_accuracy: 0.5102\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0090 - accuracy: 0.9942 - val_loss: 0.9232 - val_accuracy: 0.5102\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0033 - accuracy: 0.9971 - val_loss: 0.7918 - val_accuracy: 0.5102\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0043 - accuracy: 0.9971 - val_loss: 0.9838 - val_accuracy: 0.5102\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 8.8350e-04 - accuracy: 1.0000 - val_loss: 1.2068 - val_accuracy: 0.5102\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 1.4849 - val_accuracy: 0.5102\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0033 - accuracy: 0.9971 - val_loss: 1.5320 - val_accuracy: 0.5102\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0110 - accuracy: 0.9942 - val_loss: 3.0624 - val_accuracy: 0.5102\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0056 - accuracy: 0.9971 - val_loss: 4.0725 - val_accuracy: 0.5102\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0042 - accuracy: 0.9971 - val_loss: 4.7395 - val_accuracy: 0.5102\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 5.3857 - val_accuracy: 0.5102\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 3.8160e-04 - accuracy: 1.0000 - val_loss: 5.6024 - val_accuracy: 0.5102\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 6.0814 - val_accuracy: 0.5102\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 5.2186e-04 - accuracy: 1.0000 - val_loss: 6.2326 - val_accuracy: 0.5102\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 6.7849 - val_accuracy: 0.5102\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 4.0815e-04 - accuracy: 1.0000 - val_loss: 6.7187 - val_accuracy: 0.5102\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 4.0123e-04 - accuracy: 1.0000 - val_loss: 7.2210 - val_accuracy: 0.5102\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 9.5071e-04 - accuracy: 1.0000 - val_loss: 6.9910 - val_accuracy: 0.5102\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 3.6333e-04 - accuracy: 1.0000 - val_loss: 6.6351 - val_accuracy: 0.5102\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.6784e-04 - accuracy: 1.0000 - val_loss: 6.4728 - val_accuracy: 0.5102\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 3.6435e-04 - accuracy: 1.0000 - val_loss: 6.3748 - val_accuracy: 0.5102\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 3.8025e-04 - accuracy: 1.0000 - val_loss: 6.7639 - val_accuracy: 0.5102\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 2.5006e-04 - accuracy: 1.0000 - val_loss: 6.9138 - val_accuracy: 0.5102\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 3.9109e-04 - accuracy: 1.0000 - val_loss: 7.0817 - val_accuracy: 0.5102\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 2.2185e-04 - accuracy: 1.0000 - val_loss: 7.1957 - val_accuracy: 0.5102\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 5.6322e-04 - accuracy: 1.0000 - val_loss: 6.6433 - val_accuracy: 0.5102\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 2.1419e-04 - accuracy: 1.0000 - val_loss: 6.1998 - val_accuracy: 0.5102\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 2.5240e-04 - accuracy: 1.0000 - val_loss: 5.7907 - val_accuracy: 0.5102\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 3.7949e-04 - accuracy: 1.0000 - val_loss: 4.9501 - val_accuracy: 0.5102\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 3.7974e-04 - accuracy: 1.0000 - val_loss: 4.3369 - val_accuracy: 0.5102\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 2.1510e-04 - accuracy: 1.0000 - val_loss: 4.4705 - val_accuracy: 0.5102\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 1.2557e-04 - accuracy: 1.0000 - val_loss: 4.1985 - val_accuracy: 0.5102\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 1.4901e-04 - accuracy: 1.0000 - val_loss: 4.1115 - val_accuracy: 0.5102\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.2801e-04 - accuracy: 1.0000 - val_loss: 3.8846 - val_accuracy: 0.5102\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 5.0724e-04 - accuracy: 1.0000 - val_loss: 3.1387 - val_accuracy: 0.5102\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.4820e-04 - accuracy: 1.0000 - val_loss: 2.1080 - val_accuracy: 0.5238\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 4.3331e-04 - accuracy: 1.0000 - val_loss: 1.5476 - val_accuracy: 0.5986\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 3.6651e-04 - accuracy: 1.0000 - val_loss: 1.7096 - val_accuracy: 0.5646\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 1.1061e-04 - accuracy: 1.0000 - val_loss: 1.3840 - val_accuracy: 0.6122\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 5.5438e-05 - accuracy: 1.0000 - val_loss: 0.8370 - val_accuracy: 0.7007\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.7876 - val_accuracy: 0.7347\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 9.4822e-04 - accuracy: 1.0000 - val_loss: 6.4980 - val_accuracy: 0.5102\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 1.1190e-04 - accuracy: 1.0000 - val_loss: 8.1693 - val_accuracy: 0.5102\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 3.8141e-04 - accuracy: 1.0000 - val_loss: 8.0973 - val_accuracy: 0.5102\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 5.3723e-05 - accuracy: 1.0000 - val_loss: 7.1850 - val_accuracy: 0.5102\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 6.7474e-05 - accuracy: 1.0000 - val_loss: 6.4291 - val_accuracy: 0.5102\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 1.0350e-04 - accuracy: 1.0000 - val_loss: 5.9060 - val_accuracy: 0.5102\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 1.1539e-04 - accuracy: 1.0000 - val_loss: 6.3262 - val_accuracy: 0.5102\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 7.1132e-05 - accuracy: 1.0000 - val_loss: 6.7114 - val_accuracy: 0.5102\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.1613e-04 - accuracy: 1.0000 - val_loss: 6.7110 - val_accuracy: 0.5102\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 1.3007e-04 - accuracy: 1.0000 - val_loss: 5.8367 - val_accuracy: 0.5102\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 2.6445e-05 - accuracy: 1.0000 - val_loss: 5.1389 - val_accuracy: 0.5102\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 7.9455e-05 - accuracy: 1.0000 - val_loss: 5.1298 - val_accuracy: 0.5102\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 3.6123e-05 - accuracy: 1.0000 - val_loss: 5.1620 - val_accuracy: 0.5102\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 1.5664e-04 - accuracy: 1.0000 - val_loss: 3.8380 - val_accuracy: 0.5374\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 4.0395e-05 - accuracy: 1.0000 - val_loss: 2.1971 - val_accuracy: 0.5918\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 2.3774e-04 - accuracy: 1.0000 - val_loss: 0.5138 - val_accuracy: 0.8163\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 3.4422e-05 - accuracy: 1.0000 - val_loss: 0.3450 - val_accuracy: 0.8776\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 2.5149e-05 - accuracy: 1.0000 - val_loss: 0.2965 - val_accuracy: 0.9116\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 4.5183e-05 - accuracy: 1.0000 - val_loss: 0.2446 - val_accuracy: 0.9320\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 4.7990e-05 - accuracy: 1.0000 - val_loss: 0.2190 - val_accuracy: 0.9320\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 7.8827e-05 - accuracy: 1.0000 - val_loss: 0.2036 - val_accuracy: 0.9456\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 7.8707e-05 - accuracy: 1.0000 - val_loss: 0.1959 - val_accuracy: 0.9388\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 1.6564e-04 - accuracy: 1.0000 - val_loss: 0.3183 - val_accuracy: 0.9388\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 2.3081e-05 - accuracy: 1.0000 - val_loss: 0.3403 - val_accuracy: 0.9388\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 3.4709e-05 - accuracy: 1.0000 - val_loss: 0.3221 - val_accuracy: 0.9388\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 3.2830e-05 - accuracy: 1.0000 - val_loss: 0.3141 - val_accuracy: 0.9456\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 1.5161e-05 - accuracy: 1.0000 - val_loss: 0.3048 - val_accuracy: 0.9456\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 2.2200e-05 - accuracy: 1.0000 - val_loss: 0.2983 - val_accuracy: 0.9524\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 3.2395e-05 - accuracy: 1.0000 - val_loss: 0.2757 - val_accuracy: 0.9524\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 9.0472e-05 - accuracy: 1.0000 - val_loss: 0.2376 - val_accuracy: 0.9592\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 4.4151e-04 - accuracy: 1.0000 - val_loss: 0.1849 - val_accuracy: 0.9728\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.6414e-04 - accuracy: 1.0000 - val_loss: 0.2132 - val_accuracy: 0.9116\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 3.3620e-05 - accuracy: 1.0000 - val_loss: 0.2333 - val_accuracy: 0.8980\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 1.4621e-04 - accuracy: 1.0000 - val_loss: 0.2594 - val_accuracy: 0.8844\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 2.9397e-05 - accuracy: 1.0000 - val_loss: 0.3100 - val_accuracy: 0.8571\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 2.5097e-05 - accuracy: 1.0000 - val_loss: 0.3072 - val_accuracy: 0.8639\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 6.7654e-06 - accuracy: 1.0000 - val_loss: 0.2846 - val_accuracy: 0.8776\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 3.3256e-05 - accuracy: 1.0000 - val_loss: 0.2467 - val_accuracy: 0.8912\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 1.1643e-05 - accuracy: 1.0000 - val_loss: 0.2178 - val_accuracy: 0.9116\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 3.7854e-05 - accuracy: 1.0000 - val_loss: 0.2005 - val_accuracy: 0.9388\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 3.7349e-05 - accuracy: 1.0000 - val_loss: 0.2054 - val_accuracy: 0.9456\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 6.2527e-05 - accuracy: 1.0000 - val_loss: 0.2181 - val_accuracy: 0.9524\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 3.3379e-05 - accuracy: 1.0000 - val_loss: 0.2228 - val_accuracy: 0.9524\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 1.9967e-05 - accuracy: 1.0000 - val_loss: 0.2400 - val_accuracy: 0.9388\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0158 - accuracy: 0.9971 - val_loss: 21.9144 - val_accuracy: 0.5102\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.1643 - accuracy: 0.9621 - val_loss: 244.5945 - val_accuracy: 0.5102\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.1530 - accuracy: 0.9446 - val_loss: 216.7018 - val_accuracy: 0.5102\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0869 - accuracy: 0.9650 - val_loss: 205.5283 - val_accuracy: 0.5102\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0365 - accuracy: 0.9883 - val_loss: 17.9218 - val_accuracy: 0.4898\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0366 - accuracy: 0.9913 - val_loss: 130.2975 - val_accuracy: 0.5102\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0208 - accuracy: 0.9942 - val_loss: 161.2054 - val_accuracy: 0.5102\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0383 - accuracy: 0.9883 - val_loss: 1.1416 - val_accuracy: 0.7687\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0359 - accuracy: 0.9883 - val_loss: 36.6013 - val_accuracy: 0.4898\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0190 - accuracy: 0.9913 - val_loss: 124.5458 - val_accuracy: 0.5102\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0280 - accuracy: 0.9883 - val_loss: 133.3630 - val_accuracy: 0.5102\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 161.4788 - val_accuracy: 0.5102\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 159.5134 - val_accuracy: 0.5102\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 142.0164 - val_accuracy: 0.5102\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 5.8608e-04 - accuracy: 1.0000 - val_loss: 123.2961 - val_accuracy: 0.5102\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 5.5586e-04 - accuracy: 1.0000 - val_loss: 108.3738 - val_accuracy: 0.5102\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 4.9374e-04 - accuracy: 1.0000 - val_loss: 96.8588 - val_accuracy: 0.5102\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 4.3308e-04 - accuracy: 1.0000 - val_loss: 86.0163 - val_accuracy: 0.5102\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 3.7402e-04 - accuracy: 1.0000 - val_loss: 75.7123 - val_accuracy: 0.5102\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 3.9681e-04 - accuracy: 1.0000 - val_loss: 65.3406 - val_accuracy: 0.5102\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 7.1448e-04 - accuracy: 1.0000 - val_loss: 56.8457 - val_accuracy: 0.5102\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 3.5090e-04 - accuracy: 1.0000 - val_loss: 49.9526 - val_accuracy: 0.5102\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0027 - accuracy: 0.9971 - val_loss: 49.7639 - val_accuracy: 0.5102\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 4.4132e-04 - accuracy: 1.0000 - val_loss: 52.1745 - val_accuracy: 0.5102\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 5.7034e-04 - accuracy: 1.0000 - val_loss: 48.3616 - val_accuracy: 0.5102\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 4.6740e-04 - accuracy: 1.0000 - val_loss: 41.6941 - val_accuracy: 0.5102\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 2.6225e-04 - accuracy: 1.0000 - val_loss: 33.8704 - val_accuracy: 0.5102\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 4.9030e-04 - accuracy: 1.0000 - val_loss: 28.7187 - val_accuracy: 0.5102\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 4.9994e-04 - accuracy: 1.0000 - val_loss: 23.6216 - val_accuracy: 0.5102\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 1.8352e-04 - accuracy: 1.0000 - val_loss: 18.8648 - val_accuracy: 0.5102\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 1.6394e-04 - accuracy: 1.0000 - val_loss: 15.3031 - val_accuracy: 0.5102\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 2.2814e-04 - accuracy: 1.0000 - val_loss: 12.7023 - val_accuracy: 0.5102\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 1.2505e-04 - accuracy: 1.0000 - val_loss: 10.0799 - val_accuracy: 0.5102\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 3.6990e-04 - accuracy: 1.0000 - val_loss: 8.5631 - val_accuracy: 0.5306\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 4.1382e-04 - accuracy: 1.0000 - val_loss: 6.3957 - val_accuracy: 0.5442\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 1.5776e-04 - accuracy: 1.0000 - val_loss: 4.4830 - val_accuracy: 0.6122\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 8.0374e-05 - accuracy: 1.0000 - val_loss: 3.0265 - val_accuracy: 0.6531\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 1.0553e-04 - accuracy: 1.0000 - val_loss: 1.9881 - val_accuracy: 0.7143\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 1.5341e-04 - accuracy: 1.0000 - val_loss: 1.1928 - val_accuracy: 0.7823\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 1.7905e-04 - accuracy: 1.0000 - val_loss: 0.9089 - val_accuracy: 0.8095\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 5.2514e-04 - accuracy: 1.0000 - val_loss: 0.4438 - val_accuracy: 0.8980\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 1.1785e-04 - accuracy: 1.0000 - val_loss: 0.2599 - val_accuracy: 0.9320\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 3.9488e-04 - accuracy: 1.0000 - val_loss: 0.3703 - val_accuracy: 0.9116\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 1.9667e-04 - accuracy: 1.0000 - val_loss: 0.4480 - val_accuracy: 0.8776\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 3.3335e-04 - accuracy: 1.0000 - val_loss: 0.4543 - val_accuracy: 0.8776\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 2.7749e-04 - accuracy: 1.0000 - val_loss: 0.2551 - val_accuracy: 0.9252\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 1.9421e-04 - accuracy: 1.0000 - val_loss: 0.2545 - val_accuracy: 0.9116\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 1.9431e-04 - accuracy: 1.0000 - val_loss: 0.2318 - val_accuracy: 0.9116\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 1.0001e-04 - accuracy: 1.0000 - val_loss: 0.2063 - val_accuracy: 0.9388\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 6.0302e-04 - accuracy: 1.0000 - val_loss: 0.2261 - val_accuracy: 0.9252\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 1.4259e-04 - accuracy: 1.0000 - val_loss: 0.2567 - val_accuracy: 0.9252\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 2.1197e-04 - accuracy: 1.0000 - val_loss: 0.5930 - val_accuracy: 0.8912\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 2.6426e-04 - accuracy: 1.0000 - val_loss: 0.8741 - val_accuracy: 0.8299\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 5.4707e-05 - accuracy: 1.0000 - val_loss: 0.8044 - val_accuracy: 0.8299\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 1.1200e-04 - accuracy: 1.0000 - val_loss: 0.6869 - val_accuracy: 0.8776\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 3.1358e-04 - accuracy: 1.0000 - val_loss: 0.3337 - val_accuracy: 0.9320\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 1.7128e-04 - accuracy: 1.0000 - val_loss: 0.3299 - val_accuracy: 0.9320\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 3.2865e-05 - accuracy: 1.0000 - val_loss: 0.3348 - val_accuracy: 0.9320\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 6.4537e-05 - accuracy: 1.0000 - val_loss: 0.3267 - val_accuracy: 0.9252\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 4.6868e-05 - accuracy: 1.0000 - val_loss: 0.3740 - val_accuracy: 0.9252\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 2.6965e-05 - accuracy: 1.0000 - val_loss: 0.3752 - val_accuracy: 0.9252\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 1.1906e-04 - accuracy: 1.0000 - val_loss: 0.3058 - val_accuracy: 0.9388\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 8.6741e-05 - accuracy: 1.0000 - val_loss: 0.2812 - val_accuracy: 0.9524\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 1.5668e-04 - accuracy: 1.0000 - val_loss: 0.9089 - val_accuracy: 0.8299\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 6.8840e-05 - accuracy: 1.0000 - val_loss: 0.9033 - val_accuracy: 0.8231\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 1.1523e-04 - accuracy: 1.0000 - val_loss: 0.6142 - val_accuracy: 0.8776\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 2.0000e-05 - accuracy: 1.0000 - val_loss: 0.4733 - val_accuracy: 0.9048\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 3.4233e-05 - accuracy: 1.0000 - val_loss: 0.3623 - val_accuracy: 0.9388\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 1.2340e-04 - accuracy: 1.0000 - val_loss: 0.3722 - val_accuracy: 0.9252\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 6.7058e-04 - accuracy: 1.0000 - val_loss: 3.0158 - val_accuracy: 0.5578\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 1.9867e-04 - accuracy: 1.0000 - val_loss: 2.6274 - val_accuracy: 0.5646\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 7.3081e-05 - accuracy: 1.0000 - val_loss: 2.5332 - val_accuracy: 0.5646\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 9.8913e-05 - accuracy: 1.0000 - val_loss: 2.3800 - val_accuracy: 0.5714\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 9.5160e-05 - accuracy: 1.0000 - val_loss: 1.9155 - val_accuracy: 0.5986\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 1.7714e-04 - accuracy: 1.0000 - val_loss: 2.2334 - val_accuracy: 0.5918\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 4.6047e-04 - accuracy: 1.0000 - val_loss: 1.6108 - val_accuracy: 0.6599\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 6.7463e-05 - accuracy: 1.0000 - val_loss: 0.8591 - val_accuracy: 0.7415\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 6.2824e-05 - accuracy: 1.0000 - val_loss: 0.6917 - val_accuracy: 0.7619\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 1.0743e-04 - accuracy: 1.0000 - val_loss: 0.9211 - val_accuracy: 0.7279\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 2.0178e-04 - accuracy: 1.0000 - val_loss: 0.1982 - val_accuracy: 0.9388\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 5.7237e-05 - accuracy: 1.0000 - val_loss: 0.1694 - val_accuracy: 0.9456\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 5.5603e-05 - accuracy: 1.0000 - val_loss: 0.1589 - val_accuracy: 0.9592\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 3.5952e-05 - accuracy: 1.0000 - val_loss: 0.1574 - val_accuracy: 0.9592\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.6741e-04 - accuracy: 1.0000 - val_loss: 0.1991 - val_accuracy: 0.9456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDVY6HbxMOlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d42ff3-5fcc-49dd-a1c2-91aba6ae9d42"
      },
      "source": [
        "# pred_test= model.predict_classes(X_test)\n",
        "pred_test = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict   0   1\n",
            "Actual         \n",
            "0        65   7\n",
            "1         1  74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7pT2q7traXg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88836513-d2e2-4278-97e1-b9085889da0b"
      },
      "source": [
        "print(METRICS)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.90      0.94        72\n",
            "           1       0.91      0.99      0.95        75\n",
            "\n",
            "    accuracy                           0.95       147\n",
            "   macro avg       0.95      0.94      0.95       147\n",
            "weighted avg       0.95      0.95      0.95       147\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElpxWbBnpgLX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "c5319b86-8df5-4f9d-bbc9-1391747813b0"
      },
      "source": [
        "'''\n",
        "#X =np.array(df.copy())/255.0 \n",
        "X =np.array(df.copy())\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)\n",
        "model = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh', solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  \n",
        "prediction = model.predict(X_test)  \n",
        "y =np.copy(y_test)\n",
        "data = {'y_true': y_test,'y_predict': prediction}  \n",
        "# este dado esta no formato de dicionario\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)\n",
        "print(METRICS)\n",
        "#X =np.array(df.copy())/255.0 X =np.array(df_all.copy())X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)model = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh',                       solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  prediction = model.predict(X_test)  y =np.copy(y_test)data = {'y_true': y_test,'y_predict': prediction}  # este dado esta no formato de dicionariodf = pd.DataFrame(data, columns=['y_true','y_predict'])confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])print(confusion_matrix)y_true = df['y_true']y_pred = df['y_predict']\n",
        "'''"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n#X =np.array(df.copy())/255.0 \\nX =np.array(df.copy())\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)\\nmodel = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh', solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  \\nprediction = model.predict(X_test)  \\ny =np.copy(y_test)\\ndata = {'y_true': y_test,'y_predict': prediction}  \\n# este dado esta no formato de dicionario\\ndf = pd.DataFrame(data, columns=['y_true','y_predict'])\\nconfusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\\nprint(confusion_matrix)\\ny_true = df['y_true']\\ny_pred = df['y_predict']  \\nMETRICS=sklearn.metrics.classification_report(y_true, y_pred)\\nprint(METRICS)\\n#X =np.array(df.copy())/255.0 X =np.array(df_all.copy())X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)model = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh',                       solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  prediction = model.predict(X_test)  y =np.copy(y_test)data = {'y_true': y_test,'y_predict': prediction}  # este dado esta no formato de dicionariodf = pd.DataFrame(data, columns=['y_true','y_predict'])confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])print(confusion_matrix)y_true = df['y_true']y_pred = df['y_predict']\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iFNNrlWV9tH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "279b5208-6ab6-47d8-fe0c-7e962d160eb0"
      },
      "source": [
        "pred_test"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
              "       1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv5I61yhPQmk",
        "outputId": "d2e92ad5-92be-4755-d522-a2e4bf23e9cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[4] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  #prediction = model.predict_classes(result)\n",
        "  prediction= np.argmax(model.predict(result), axis=-1)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "2   181.0  151.053055  148.180374  ...  156.222534  124.669952  124.703087\n",
            "6   160.0    1.193125    1.358125  ...  161.975616  161.588104  151.586868\n",
            "28  107.0  109.773170  111.957642  ...  112.262207  111.302475  112.922005\n",
            "30  101.0   95.445358  103.301247  ...   94.286446  109.396439  129.550537\n",
            "34  145.0  115.758232  141.597183  ...   91.621979   87.914337   90.510773\n",
            "35  166.0  112.088394  106.791534  ...    0.109450    1.339382    0.886195\n",
            "46  191.0  169.430695  157.328049  ...   85.172287   75.887093   62.323761\n",
            "2   117.0  111.885460  109.507927  ...    0.030682    0.732486    1.535905\n",
            "12  144.0   99.996147  110.625778  ...  183.473770  180.206024  168.305573\n",
            "6   102.0   33.599770   38.278740  ...  144.427536  147.002319  143.444458\n",
            "29  136.0  121.526825  125.189453  ...  141.268173  136.467133  129.463669\n",
            "32  151.0    0.292663    1.365510  ...  180.324722  187.006332  189.646805\n",
            "46  123.0  139.906418  115.575920  ...  153.346893  148.071991  156.368500\n",
            "38  156.0   95.587776   89.383965  ...  143.594360  135.595673  119.752144\n",
            "47  103.0   60.519928   61.869545  ...  137.301819  136.780945  135.223770\n",
            "17  113.0  149.616104  151.903824  ...  128.715637  142.896088  191.407944\n",
            "24  168.0  150.777786   91.722221  ...  153.833328  146.916672  137.388885\n",
            "27  178.0   45.022598   45.067043  ...  180.025635  186.079041  205.565735\n",
            "33  136.0    0.454152    0.395329  ...   62.762112   66.250000   69.586510\n",
            "3   121.0    0.590738    1.433714  ...  104.053070   85.586578  129.810730\n",
            "18  194.0    1.102668    0.816984  ...  156.470490  108.624817  108.704529\n",
            "23  118.0    0.000000    0.778512  ...  139.542953  152.195618  150.670212\n",
            "40  148.0  165.910172  170.991974  ...  139.856110  137.360123  136.588028\n",
            "43  147.0   58.136059   59.487534  ...  151.138321  146.743774  157.278931\n",
            "47  110.0   58.657520   61.088593  ...  162.865448  158.795029  157.482651\n",
            "\n",
            "[25 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjRbWgmX_LFH",
        "outputId": "bc3ea7a9-3fe8-43cb-e460-5e926b9b6d18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "#from GetBetterSegm import GetBetter"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'marquesgabi_paper_fev_2021' already exists and is not an empty directory.\n",
            "/content/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAG_I6FwCvFr",
        "outputId": "1a05df87-e46a-4d4b-9b79-55895ec98255",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "\n",
        "!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "#!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd marquesgabi_out_2020\n",
        "#%cd Doutorado\n",
        "#PSD_imageJ = 'Amostra7.csv' \n",
        "#PSD_new = pd.read_csv(PSD_imageJ,sep=';')\n",
        "#encoding='utf8'\n",
        "\n",
        "PSD_imageJ = 'Areas_ImageJ.csv'\n",
        "PSD_new = pd.read_csv(PSD_imageJ)\n",
        "print(PSD_new.head(3))\n",
        "''''''"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'marquesgabi_out_2020' already exists and is not an empty directory.\n",
            "/content/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020\n",
            "   Juntas   Area\n",
            "0       1  2.001\n",
            "1       2  0.820\n",
            "2       3  1.270\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PekBHQOT_6CP",
        "outputId": "50e83003-dc4c-42bc-cfce-6bac035ec5fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>181.0</td>\n",
              "      <td>151.053055</td>\n",
              "      <td>148.180374</td>\n",
              "      <td>146.987579</td>\n",
              "      <td>152.125824</td>\n",
              "      <td>149.395905</td>\n",
              "      <td>149.388153</td>\n",
              "      <td>140.069107</td>\n",
              "      <td>117.252228</td>\n",
              "      <td>121.805229</td>\n",
              "      <td>112.787369</td>\n",
              "      <td>111.839790</td>\n",
              "      <td>113.273140</td>\n",
              "      <td>121.985840</td>\n",
              "      <td>141.209015</td>\n",
              "      <td>158.084991</td>\n",
              "      <td>152.915131</td>\n",
              "      <td>141.774216</td>\n",
              "      <td>153.221191</td>\n",
              "      <td>222.186310</td>\n",
              "      <td>247.242264</td>\n",
              "      <td>249.419754</td>\n",
              "      <td>252.933975</td>\n",
              "      <td>253.049561</td>\n",
              "      <td>249.625137</td>\n",
              "      <td>246.282974</td>\n",
              "      <td>211.203766</td>\n",
              "      <td>205.200424</td>\n",
              "      <td>201.493225</td>\n",
              "      <td>161.344131</td>\n",
              "      <td>157.681625</td>\n",
              "      <td>154.276062</td>\n",
              "      <td>161.077881</td>\n",
              "      <td>166.829147</td>\n",
              "      <td>167.378860</td>\n",
              "      <td>149.029083</td>\n",
              "      <td>119.953056</td>\n",
              "      <td>132.890305</td>\n",
              "      <td>128.859482</td>\n",
              "      <td>129.144699</td>\n",
              "      <td>...</td>\n",
              "      <td>154.056839</td>\n",
              "      <td>154.550598</td>\n",
              "      <td>166.173813</td>\n",
              "      <td>177.569565</td>\n",
              "      <td>163.776886</td>\n",
              "      <td>166.646652</td>\n",
              "      <td>173.101166</td>\n",
              "      <td>165.498825</td>\n",
              "      <td>154.131073</td>\n",
              "      <td>153.526886</td>\n",
              "      <td>125.601181</td>\n",
              "      <td>122.146271</td>\n",
              "      <td>169.088150</td>\n",
              "      <td>177.661591</td>\n",
              "      <td>182.182983</td>\n",
              "      <td>190.529602</td>\n",
              "      <td>198.074265</td>\n",
              "      <td>202.452225</td>\n",
              "      <td>201.526031</td>\n",
              "      <td>203.711685</td>\n",
              "      <td>206.436920</td>\n",
              "      <td>244.247345</td>\n",
              "      <td>241.960571</td>\n",
              "      <td>247.547668</td>\n",
              "      <td>248.196198</td>\n",
              "      <td>249.852600</td>\n",
              "      <td>248.497284</td>\n",
              "      <td>240.535675</td>\n",
              "      <td>219.609604</td>\n",
              "      <td>207.685303</td>\n",
              "      <td>203.244598</td>\n",
              "      <td>192.638336</td>\n",
              "      <td>180.233246</td>\n",
              "      <td>184.377670</td>\n",
              "      <td>184.477982</td>\n",
              "      <td>177.994415</td>\n",
              "      <td>165.188873</td>\n",
              "      <td>156.222534</td>\n",
              "      <td>124.669952</td>\n",
              "      <td>124.703087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>160.0</td>\n",
              "      <td>1.193125</td>\n",
              "      <td>1.358125</td>\n",
              "      <td>3.280000</td>\n",
              "      <td>5.933750</td>\n",
              "      <td>19.915625</td>\n",
              "      <td>38.997498</td>\n",
              "      <td>52.561249</td>\n",
              "      <td>57.733749</td>\n",
              "      <td>60.241245</td>\n",
              "      <td>62.998123</td>\n",
              "      <td>61.907501</td>\n",
              "      <td>53.994373</td>\n",
              "      <td>43.566250</td>\n",
              "      <td>36.771248</td>\n",
              "      <td>38.556873</td>\n",
              "      <td>43.891247</td>\n",
              "      <td>41.603123</td>\n",
              "      <td>31.888123</td>\n",
              "      <td>45.732494</td>\n",
              "      <td>83.561249</td>\n",
              "      <td>94.223740</td>\n",
              "      <td>91.248123</td>\n",
              "      <td>88.706245</td>\n",
              "      <td>92.878746</td>\n",
              "      <td>109.194374</td>\n",
              "      <td>129.561859</td>\n",
              "      <td>131.263748</td>\n",
              "      <td>124.215622</td>\n",
              "      <td>0.501250</td>\n",
              "      <td>0.572500</td>\n",
              "      <td>4.588750</td>\n",
              "      <td>30.113127</td>\n",
              "      <td>44.739376</td>\n",
              "      <td>48.755623</td>\n",
              "      <td>52.794998</td>\n",
              "      <td>57.631248</td>\n",
              "      <td>58.703747</td>\n",
              "      <td>60.744991</td>\n",
              "      <td>60.474998</td>\n",
              "      <td>...</td>\n",
              "      <td>85.681870</td>\n",
              "      <td>81.278748</td>\n",
              "      <td>106.762497</td>\n",
              "      <td>160.072495</td>\n",
              "      <td>148.992493</td>\n",
              "      <td>138.948746</td>\n",
              "      <td>147.336884</td>\n",
              "      <td>145.480621</td>\n",
              "      <td>143.207489</td>\n",
              "      <td>135.504364</td>\n",
              "      <td>124.418129</td>\n",
              "      <td>122.200623</td>\n",
              "      <td>16.826874</td>\n",
              "      <td>48.371250</td>\n",
              "      <td>40.938122</td>\n",
              "      <td>100.346870</td>\n",
              "      <td>164.091248</td>\n",
              "      <td>120.924377</td>\n",
              "      <td>96.033119</td>\n",
              "      <td>94.229370</td>\n",
              "      <td>93.059998</td>\n",
              "      <td>91.366249</td>\n",
              "      <td>95.486870</td>\n",
              "      <td>96.880623</td>\n",
              "      <td>108.238739</td>\n",
              "      <td>133.534988</td>\n",
              "      <td>140.444992</td>\n",
              "      <td>121.577499</td>\n",
              "      <td>84.043129</td>\n",
              "      <td>93.175003</td>\n",
              "      <td>122.862495</td>\n",
              "      <td>146.684357</td>\n",
              "      <td>144.901871</td>\n",
              "      <td>147.405624</td>\n",
              "      <td>151.150620</td>\n",
              "      <td>153.641861</td>\n",
              "      <td>157.887497</td>\n",
              "      <td>161.975616</td>\n",
              "      <td>161.588104</td>\n",
              "      <td>151.586868</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>107.0</td>\n",
              "      <td>109.773170</td>\n",
              "      <td>111.957642</td>\n",
              "      <td>115.236176</td>\n",
              "      <td>116.491302</td>\n",
              "      <td>111.420914</td>\n",
              "      <td>104.324219</td>\n",
              "      <td>101.345009</td>\n",
              "      <td>101.410690</td>\n",
              "      <td>101.206573</td>\n",
              "      <td>98.373650</td>\n",
              "      <td>96.906456</td>\n",
              "      <td>99.540222</td>\n",
              "      <td>101.551132</td>\n",
              "      <td>111.844788</td>\n",
              "      <td>115.748802</td>\n",
              "      <td>119.361862</td>\n",
              "      <td>142.909775</td>\n",
              "      <td>153.267532</td>\n",
              "      <td>121.770554</td>\n",
              "      <td>86.206909</td>\n",
              "      <td>63.220459</td>\n",
              "      <td>56.219143</td>\n",
              "      <td>67.140182</td>\n",
              "      <td>84.021317</td>\n",
              "      <td>112.691589</td>\n",
              "      <td>128.223602</td>\n",
              "      <td>153.319153</td>\n",
              "      <td>163.560394</td>\n",
              "      <td>122.998260</td>\n",
              "      <td>121.361954</td>\n",
              "      <td>116.889420</td>\n",
              "      <td>116.014412</td>\n",
              "      <td>116.663109</td>\n",
              "      <td>112.309029</td>\n",
              "      <td>108.534103</td>\n",
              "      <td>107.058693</td>\n",
              "      <td>106.066902</td>\n",
              "      <td>103.036247</td>\n",
              "      <td>104.608429</td>\n",
              "      <td>...</td>\n",
              "      <td>146.495682</td>\n",
              "      <td>135.473068</td>\n",
              "      <td>125.429909</td>\n",
              "      <td>112.749496</td>\n",
              "      <td>101.318718</td>\n",
              "      <td>105.150490</td>\n",
              "      <td>104.009262</td>\n",
              "      <td>98.417328</td>\n",
              "      <td>106.425888</td>\n",
              "      <td>113.456894</td>\n",
              "      <td>113.784256</td>\n",
              "      <td>115.525024</td>\n",
              "      <td>89.992928</td>\n",
              "      <td>96.366585</td>\n",
              "      <td>93.903046</td>\n",
              "      <td>85.103149</td>\n",
              "      <td>76.946716</td>\n",
              "      <td>98.078354</td>\n",
              "      <td>101.295654</td>\n",
              "      <td>125.098785</td>\n",
              "      <td>149.049698</td>\n",
              "      <td>140.959549</td>\n",
              "      <td>126.022095</td>\n",
              "      <td>138.482742</td>\n",
              "      <td>138.791168</td>\n",
              "      <td>135.904617</td>\n",
              "      <td>139.636566</td>\n",
              "      <td>142.090393</td>\n",
              "      <td>132.165695</td>\n",
              "      <td>117.656303</td>\n",
              "      <td>115.610001</td>\n",
              "      <td>116.202377</td>\n",
              "      <td>112.396194</td>\n",
              "      <td>112.354797</td>\n",
              "      <td>108.017471</td>\n",
              "      <td>101.473145</td>\n",
              "      <td>106.278976</td>\n",
              "      <td>112.262207</td>\n",
              "      <td>111.302475</td>\n",
              "      <td>112.922005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>101.0</td>\n",
              "      <td>95.445358</td>\n",
              "      <td>103.301247</td>\n",
              "      <td>94.450249</td>\n",
              "      <td>87.181946</td>\n",
              "      <td>92.807968</td>\n",
              "      <td>96.782280</td>\n",
              "      <td>100.882965</td>\n",
              "      <td>99.191353</td>\n",
              "      <td>87.556816</td>\n",
              "      <td>86.769829</td>\n",
              "      <td>93.527504</td>\n",
              "      <td>80.963448</td>\n",
              "      <td>69.462311</td>\n",
              "      <td>64.403008</td>\n",
              "      <td>59.697479</td>\n",
              "      <td>57.794926</td>\n",
              "      <td>56.850403</td>\n",
              "      <td>51.153912</td>\n",
              "      <td>54.891876</td>\n",
              "      <td>49.381824</td>\n",
              "      <td>60.868153</td>\n",
              "      <td>77.863541</td>\n",
              "      <td>111.847656</td>\n",
              "      <td>138.457016</td>\n",
              "      <td>148.713272</td>\n",
              "      <td>150.743073</td>\n",
              "      <td>150.784439</td>\n",
              "      <td>152.716599</td>\n",
              "      <td>93.529266</td>\n",
              "      <td>98.756790</td>\n",
              "      <td>90.689842</td>\n",
              "      <td>83.973625</td>\n",
              "      <td>90.469170</td>\n",
              "      <td>94.655624</td>\n",
              "      <td>96.835510</td>\n",
              "      <td>97.379768</td>\n",
              "      <td>88.484962</td>\n",
              "      <td>88.339874</td>\n",
              "      <td>91.301445</td>\n",
              "      <td>...</td>\n",
              "      <td>60.531223</td>\n",
              "      <td>63.505836</td>\n",
              "      <td>62.874031</td>\n",
              "      <td>64.227036</td>\n",
              "      <td>63.899818</td>\n",
              "      <td>60.940109</td>\n",
              "      <td>68.170677</td>\n",
              "      <td>92.764931</td>\n",
              "      <td>115.621223</td>\n",
              "      <td>126.858749</td>\n",
              "      <td>137.709930</td>\n",
              "      <td>146.873352</td>\n",
              "      <td>107.287231</td>\n",
              "      <td>112.713356</td>\n",
              "      <td>116.701706</td>\n",
              "      <td>113.357132</td>\n",
              "      <td>111.608086</td>\n",
              "      <td>89.146553</td>\n",
              "      <td>56.472794</td>\n",
              "      <td>50.836884</td>\n",
              "      <td>50.284489</td>\n",
              "      <td>50.467796</td>\n",
              "      <td>50.944714</td>\n",
              "      <td>46.351044</td>\n",
              "      <td>45.870308</td>\n",
              "      <td>49.419567</td>\n",
              "      <td>49.014610</td>\n",
              "      <td>51.851589</td>\n",
              "      <td>56.995888</td>\n",
              "      <td>56.761005</td>\n",
              "      <td>62.377022</td>\n",
              "      <td>65.160477</td>\n",
              "      <td>63.187538</td>\n",
              "      <td>65.067741</td>\n",
              "      <td>61.927166</td>\n",
              "      <td>58.411629</td>\n",
              "      <td>81.138123</td>\n",
              "      <td>94.286446</td>\n",
              "      <td>109.396439</td>\n",
              "      <td>129.550537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>145.0</td>\n",
              "      <td>115.758232</td>\n",
              "      <td>141.597183</td>\n",
              "      <td>169.541260</td>\n",
              "      <td>176.632828</td>\n",
              "      <td>240.813705</td>\n",
              "      <td>251.103806</td>\n",
              "      <td>250.619827</td>\n",
              "      <td>250.766602</td>\n",
              "      <td>249.752197</td>\n",
              "      <td>252.162766</td>\n",
              "      <td>252.052963</td>\n",
              "      <td>250.806366</td>\n",
              "      <td>232.392960</td>\n",
              "      <td>98.922997</td>\n",
              "      <td>101.185204</td>\n",
              "      <td>113.531845</td>\n",
              "      <td>123.506828</td>\n",
              "      <td>132.735001</td>\n",
              "      <td>143.309921</td>\n",
              "      <td>151.040802</td>\n",
              "      <td>151.681427</td>\n",
              "      <td>132.491226</td>\n",
              "      <td>118.879333</td>\n",
              "      <td>113.962234</td>\n",
              "      <td>160.366791</td>\n",
              "      <td>177.219025</td>\n",
              "      <td>173.396286</td>\n",
              "      <td>162.049606</td>\n",
              "      <td>126.093452</td>\n",
              "      <td>132.514374</td>\n",
              "      <td>130.286652</td>\n",
              "      <td>139.479523</td>\n",
              "      <td>157.479370</td>\n",
              "      <td>183.454071</td>\n",
              "      <td>223.841827</td>\n",
              "      <td>239.908279</td>\n",
              "      <td>243.975266</td>\n",
              "      <td>248.446976</td>\n",
              "      <td>249.762558</td>\n",
              "      <td>...</td>\n",
              "      <td>136.806549</td>\n",
              "      <td>132.916901</td>\n",
              "      <td>153.130554</td>\n",
              "      <td>170.149155</td>\n",
              "      <td>166.427734</td>\n",
              "      <td>159.453934</td>\n",
              "      <td>148.085281</td>\n",
              "      <td>128.303864</td>\n",
              "      <td>119.790253</td>\n",
              "      <td>108.685951</td>\n",
              "      <td>112.609985</td>\n",
              "      <td>128.103638</td>\n",
              "      <td>112.656456</td>\n",
              "      <td>123.933502</td>\n",
              "      <td>112.772125</td>\n",
              "      <td>115.806244</td>\n",
              "      <td>126.841667</td>\n",
              "      <td>152.677383</td>\n",
              "      <td>137.145157</td>\n",
              "      <td>141.170944</td>\n",
              "      <td>160.215607</td>\n",
              "      <td>160.146271</td>\n",
              "      <td>157.037766</td>\n",
              "      <td>149.623825</td>\n",
              "      <td>139.906357</td>\n",
              "      <td>135.294113</td>\n",
              "      <td>131.489899</td>\n",
              "      <td>124.968506</td>\n",
              "      <td>137.354767</td>\n",
              "      <td>153.444183</td>\n",
              "      <td>165.222824</td>\n",
              "      <td>167.843338</td>\n",
              "      <td>159.167328</td>\n",
              "      <td>145.982254</td>\n",
              "      <td>143.754913</td>\n",
              "      <td>136.040283</td>\n",
              "      <td>112.458878</td>\n",
              "      <td>91.621979</td>\n",
              "      <td>87.914337</td>\n",
              "      <td>90.510773</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Width           0           1  ...         781         782         783\n",
              "2   181.0  151.053055  148.180374  ...  156.222534  124.669952  124.703087\n",
              "6   160.0    1.193125    1.358125  ...  161.975616  161.588104  151.586868\n",
              "28  107.0  109.773170  111.957642  ...  112.262207  111.302475  112.922005\n",
              "30  101.0   95.445358  103.301247  ...   94.286446  109.396439  129.550537\n",
              "34  145.0  115.758232  141.597183  ...   91.621979   87.914337   90.510773\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaZPe_AxNBK9",
        "outputId": "85066524-bc5b-4b8d-9343-0e863743950c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "PSD_new.head()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Juntas</th>\n",
              "      <th>Area</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1.162</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Juntas   Area\n",
              "0       1  2.001\n",
              "1       2  0.820\n",
              "2       3  1.270\n",
              "3       4  0.958\n",
              "4       5  1.162"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "\n",
        "\n",
        "# \n",
        "Area = df_ImgJ['Area'].values\n",
        "# Area = np.concatenate( (Area, [lost_value] ) )\n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79MY9ZHxBW37",
        "outputId": "1f1e7178-94fa-4665-a5b0-99664af2fc19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "len(Diameter_All)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KooHVpH5k2mZ",
        "outputId": "071b78d9-06e8-4084-fc79-4f113a00b350",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "PSD_new['Area'].shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(95,)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPWCPPf7bzsf"
      },
      "source": [
        "#lost_value = float(PSD_new.columns[1])\n",
        "#Area2 = np.array(PSD_new.iloc[:,1])\n",
        "#Area2 = np.concatenate( (Area2, [lost_value] ) )\n",
        "Area2 = PSD_new['Area'].shape\n",
        "for A in Area2:\n",
        "  Diam1.append((4*A/np.pi)**0.5) \n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfk_fNXGDK5_"
      },
      "source": [
        "wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        "wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        "X = pd.DataFrame([Diam1,Diameter_All])\n",
        "wts = pd.DataFrame([wt1,wt2])\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWf2nmnEp6yX",
        "outputId": "dfde58c3-710e-4243-ad25-5a59a60396fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "X"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.280599</td>\n",
              "      <td>0.719867</td>\n",
              "      <td>0.868192</td>\n",
              "      <td>1.330818</td>\n",
              "      <td>0.836067</td>\n",
              "      <td>0.923618</td>\n",
              "      <td>0.955465</td>\n",
              "      <td>0.904113</td>\n",
              "      <td>0.697408</td>\n",
              "      <td>0.911127</td>\n",
              "      <td>1.045198</td>\n",
              "      <td>0.970012</td>\n",
              "      <td>0.579772</td>\n",
              "      <td>0.628255</td>\n",
              "      <td>0.600272</td>\n",
              "      <td>0.659885</td>\n",
              "      <td>0.934581</td>\n",
              "      <td>0.731273</td>\n",
              "      <td>0.777682</td>\n",
              "      <td>1.113042</td>\n",
              "      <td>0.753568</td>\n",
              "      <td>0.874039</td>\n",
              "      <td>0.918781</td>\n",
              "      <td>0.970012</td>\n",
              "      <td>0.623168</td>\n",
              "      <td>1.355934</td>\n",
              "      <td>0.932536</td>\n",
              "      <td>0.906925</td>\n",
              "      <td>0.858607</td>\n",
              "      <td>0.660849</td>\n",
              "      <td>0.952796</td>\n",
              "      <td>1.028621</td>\n",
              "      <td>0.891348</td>\n",
              "      <td>0.85638</td>\n",
              "      <td>0.575363</td>\n",
              "      <td>1.260557</td>\n",
              "      <td>1.048239</td>\n",
              "      <td>0.606602</td>\n",
              "      <td>0.860088</td>\n",
              "      <td>0.769452</td>\n",
              "      <td>...</td>\n",
              "      <td>1.126685</td>\n",
              "      <td>0.925684</td>\n",
              "      <td>1.072257</td>\n",
              "      <td>0.87112</td>\n",
              "      <td>0.850412</td>\n",
              "      <td>1.110752</td>\n",
              "      <td>1.126685</td>\n",
              "      <td>1.700823</td>\n",
              "      <td>1.288034</td>\n",
              "      <td>0.927745</td>\n",
              "      <td>1.064511</td>\n",
              "      <td>1.028621</td>\n",
              "      <td>1.412053</td>\n",
              "      <td>1.253974</td>\n",
              "      <td>0.932536</td>\n",
              "      <td>1.027383</td>\n",
              "      <td>1.078768</td>\n",
              "      <td>1.299842</td>\n",
              "      <td>1.120453</td>\n",
              "      <td>1.232983</td>\n",
              "      <td>0.869658</td>\n",
              "      <td>1.27312</td>\n",
              "      <td>1.016794</td>\n",
              "      <td>0.990149</td>\n",
              "      <td>1.220529</td>\n",
              "      <td>1.315905</td>\n",
              "      <td>1.401645</td>\n",
              "      <td>1.317355</td>\n",
              "      <td>1.642945</td>\n",
              "      <td>1.173189</td>\n",
              "      <td>1.139607</td>\n",
              "      <td>1.340826</td>\n",
              "      <td>1.232467</td>\n",
              "      <td>1.183454</td>\n",
              "      <td>0.833016</td>\n",
              "      <td>0.864518</td>\n",
              "      <td>0.885616</td>\n",
              "      <td>0.927058</td>\n",
              "      <td>1.106157</td>\n",
              "      <td>10.99808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.625137</td>\n",
              "      <td>1.194027</td>\n",
              "      <td>0.899597</td>\n",
              "      <td>0.792275</td>\n",
              "      <td>1.290852</td>\n",
              "      <td>1.517218</td>\n",
              "      <td>1.658059</td>\n",
              "      <td>0.970028</td>\n",
              "      <td>1.262909</td>\n",
              "      <td>0.732714</td>\n",
              "      <td>1.224043</td>\n",
              "      <td>1.248365</td>\n",
              "      <td>1.142400</td>\n",
              "      <td>1.289702</td>\n",
              "      <td>0.778005</td>\n",
              "      <td>0.915387</td>\n",
              "      <td>1.307376</td>\n",
              "      <td>1.500644</td>\n",
              "      <td>0.857166</td>\n",
              "      <td>1.179523</td>\n",
              "      <td>1.424796</td>\n",
              "      <td>0.880595</td>\n",
              "      <td>1.283118</td>\n",
              "      <td>1.100841</td>\n",
              "      <td>0.889761</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 175 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       172       173       174\n",
              "0  1.280599  0.719867  0.868192  ...  0.927058  1.106157  10.99808\n",
              "1  1.625137  1.194027  0.899597  ...       NaN       NaN       NaN\n",
              "\n",
              "[2 rows x 175 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OieAXw_by3nz",
        "outputId": "725ea5f9-296f-47c9-bbb7-1280517580ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "A = plt.hist(X,weights=wts,bins=7)\n",
        "plt.legend(['True','CNN'])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f0bf586ee90>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WryLryB9ovi3",
        "outputId": "0bfd7f48-dd55-481f-dc68-8e6f0e5f4940",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('ImgJ:','media=',np.mean(np.array(Diam1)),'desvio=',np.std(np.array(Diam1)),'pontos=',len(Diam1))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ImgJ: media= 1.0488562027957447 desvio= 0.7922576454370286 pontos= 175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Nyv-Nopbkc",
        "outputId": "f5ea8e6d-2e79-4bd9-ba1f-b937a8c7b155",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Software:','media=',np.mean(np.array(Diameter_All)),'desvio=',np.std(np.array(Diameter_All)),'pontos=',len(Diameter_All))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Software: media= 1.1585815206911052 desvio= 0.2643520413912642 pontos= 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpdrvEySy8Ij",
        "outputId": "be23629b-e33c-40ed-bf42-0dbafedbbb2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.mean(np.array(Diameter_All))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.1585815206911052"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMK89w-fzCVe",
        "outputId": "21622ac5-34ff-4f01-d44d-4dfc83b984f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# Freq1 = [19.12043703, 29.22484843, 19.35872174, 20.82190224, 11.47409056] # avarage 4 samples\n",
        "Freq1 = [20.69301557, 28.55598044, 18.50768331, 22.7106327, 8.905907357] # avarage 10 samples\n",
        "#Freq2 = [16.93792791, 31.38008965, 24.93810752, 18.56158392, 6.233810752, 0.4]\n",
        "Freq2 = [16.93792791, 31.38008965, 24.93810752, 18.56158392, 6.633810752]\n",
        "Freq3 = [22.22489, 30.15078, 25.10463, 19.30926, 2.810434]\n",
        "barWidth = 0.25\n",
        "\n",
        "br1 = range(len(Freq1))\n",
        "# Set position of bar on X axis\n",
        "br2 = [x + barWidth for x in br1]\n",
        "br3 = [x + barWidth for x in br2]\n",
        "# labels = [0.8, 1.0, 1.2, 1.4, 1.6, 1.8]\n",
        "labels = [0.8, 1.0, 1.2, 1.4, 1.6]\n",
        "\n",
        "xx=[]\n",
        "for a in labels:\n",
        "  xx.append(str(a))\n",
        "plt.bar(br1, Freq1 , color=\"green\", align=\"center\", width=0.3, tick_label= xx) \n",
        "plt.bar(br2, Freq2 , color=\"red\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.bar(br3, Freq3 , color=\"blue\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.legend(['CNN 1','CNN 2','True'])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f0bfd28e950>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAS1klEQVR4nO3df5BdZX3H8fe3SXApP4qETcwk0o0UShIKISxEB7RACgNxpoiRQmqV1MxEp8YxVWcQ0ilB7VBGfgRUdKIwSRFFBqEgQymIKAqiZkOKMRlRMOjShGwSbcUaIOTbP/YGQ7K79+7eH7tP9v2aubP3nvPcc77P7uaTs8895zmRmUiSyvNHw12AJGloDHBJKpQBLkmFMsAlqVAGuCQVamwrd3bEEUdkR0dHK3cpScXr6urampntey9vaYB3dHSwevXqVu5SkooXEc/2tdwhFEkqlAEuSYUywCWpUC0dA5ekvb388st0d3ezY8eO4S5l2LW1tTFlyhTGjRtXU3sDXNKw6u7u5pBDDqGjo4OIGO5yhk1msm3bNrq7u5k6dWpN73EIRdKw2rFjB+PHjx/V4Q0QEYwfP35Qf4kY4JKG3WgP790G+30wwCWpUI6BSxpR4orGHo3n5dXvebB582aWLFnCj370Iw477DAmTpzI8uXLOeCAA5g6dSo33HADH/rQhwBYvHgxnZ2dLFiwgAULFvDggw/yzDPP8LrXvY6tW7fS2dnJxo0b99nH+973Pu69914mTJjAunXrGtI3j8BHg4jGPaT9TGZy/vnnc/rpp/P000/T1dXFlVdeyfPPPw/AhAkTuP7663nppZf6fP+YMWO4+eabq+5nwYIF3H///Q2t3QCXNKo9/PDDjBs3jg984AOvLjvhhBN461vfCkB7eztz5sxh1apVfb5/yZIlXHfddezcuXPA/bztbW/j8MMPb1zhGOCSRrl169Zx0kknDdjmkksu4eqrr+aVV17ZZ92RRx7Jaaedxi233NKsEvtlgEtSFW9605uYPXs2X/nKV/pcf+mll/LpT3+aXbt2tbQuA1zSqDZjxgy6urqqtrvsssu46qqr6OtG8EcffTQzZ87k9ttvb0aJ/TLAJY1qZ555Ji+++CIrVqx4ddmTTz7Jd7/73de0O/bYY5k+fTrf+MY3+tzO0qVLufrqq5ta696qnkYYEW3AI8DrKu3vyMzLI2IqcBswHugC3pOZfX9MK0k1quW0v0aKCO666y6WLFnCVVddRVtbGx0dHSxfvnyftkuXLuXEE0/sczszZsxg1qxZrFmzps/18+fP59vf/jZbt25lypQpXHHFFSxcuLC+2vv6c+A1DXovDTooM1+IiHHA94APAx8B7szM2yLiC8B/ZebnB9pWZ2dnekOHYdDI0/+q/L5Ig7VhwwamTZs23GWMGH19PyKiKzM7925bdQgle71QeTmu8kjgTOCOyvJVwDvqKVojV5B/eHgauTRi1DQGHhFjImItsAV4EHga+E1m7j7xsRuY3M97F0XE6ohY3dPT04iaJUnUGOCZ+UpmzgSmAKcAx9a6g8xckZmdmdnZ3r7PPTklSUM0qLNQMvM3wMPAW4DDImL3h6BTgOcaXJskaQBVAzwi2iPisMrzA4GzgA30Bvm7Ks0uBu5uVpGSpH3VMhvhJGBVRIyhN/Bvz8x7I2I9cFtEfAp4AripiXVKkvZSy1koT2bmiZl5fGYel5mfqCx/JjNPycw/y8wLMvPF5pcrab/XyNkzazz1afPmzVx00UUcddRRnHTSScydO5ennnqKjRs3EhF85jOfebXt4sWLWblyJdA7w+DkyZN58cXe+Nu6dSsdHR37bP9Xv/oVZ5xxBtOnT2fGjBlcf/31dX+bwCsxJY1yrZhOduzYsVxzzTWsX7+exx9/nM997nOsX7++7toNcEmjWiumk500aRKzZs0C4JBDDmHatGk891z9530Y4JJGtVZPJ7tx40aeeOIJZs+ePaR69+Qt1QpRz22mvPhdqk8t08med955vP3tbx9wOy+88ALz5s1j+fLlHHrooXXX5RG4pFGtVdPJvvzyy8ybN493v/vdvPOd76yr5t0McEmjWiumk81MFi5cyLRp0/jIRz7SsNoNcEkjS2ZjH1Xsnk72m9/8JkcddRQzZszg0ksv5Q1veMM+bZcuXUp3d3ef29k9nWxfHn30UW655Ra+9a1vMXPmTGbOnMl99903uO9LX7VXm062kZxOdujqGgNfVue+GziK7my02pvTyb5WQ6eTlSSNTAa4JBXKAJekQhngklQoA1ySCmWAS1KhvJRe0ojS6JtfVzt1ddu2bcyZMwfonVZ2zJgx7L794w9/+EMOOOCAxhbUQAa4pFFt/PjxrF27FoBly5Zx8MEH87GPfezV9Tt37mTs2JEZlSOzKkkaRgsWLKCtrY0nnniCU089lUMPPfQ1wX7cccdx77330tHRwZe//GVuuOEGXnrpJWbPns2NN97ImDFjWlKnY+CS1Ifu7m4ee+wxrr322n7bbNiwga997Ws8+uijrF27ljFjxnDrrbe2rEaPwCWpDxdccEHVI+mHHnqIrq4uTj75ZAB+//vfM2HChFaUBxjgktSngw466NXnY8eOZdeuXa++3rFjB9A7y+DFF1/MlVde2fL6wCEUSaqqo6ODNWvWALBmzRp+8YtfADBnzhzuuOMOtmzZAsD27dt59tlnW1aXAS5pRGnxbLI1mTdvHtu3b2fGjBl89rOf5ZhjjgFg+vTpfOpTn+Lss8/m+OOP56yzzmLTpk2N2WkNHEKRpIply5b1ufzAAw/kgQce6HPdhRdeyIUXXtjEqvrnEbgkFapqgEfEGyPi4YhYHxE/iYgPV5Yvi4jnImJt5TG3+eVKknarZQhlJ/DRzFwTEYcAXRHxYGXddZnZ903gJKlGmUk0+hr6Ag32DmlVAzwzNwGbKs9/GxEbgMlDqk4qUCNzxVvK7autrY1t27Yxfvz4UR3imcm2bdtoa2ur+T2D+hAzIjqAE4EfAKcCiyPivcBqeo/Sf93HexYBiwCOPPLIwexOaoy6Q8HUbaYpU6bQ3d1NT0/PcJcy7Nra2pgyZUrN7Wu+qXFEHAx8B/iXzLwzIiYCW+n97f4kMCkz3zfQNryp8dB5U+M61BngxfdfxavrpsYRMQ74OnBrZt4JkJnPZ+YrmbkL+CJwSiMLliQNrJazUAK4CdiQmdfusXzSHs3OB9Y1vjxJUn9qGQM/FXgP8OOIWFtZdhkwPyJm0juEshF4f1MqlCT1qZazUL4H9DWIeF/jy5Ek1corMSWpUAa4JBXKAJekQhngklQoA1ySCmWAS1KhDHBJKpQBLkmFMsAlqVDeE1MaweqZhXJveblTIe5vPAKXpEIZ4JJUKIdQauRttSSNNB6BS1KhDHBJKpQBLkmFMsAlqVAGuCQVygCXpEIZ4JJUKANckgplgEtSoYq5EtNJfSTptTwCl6RCVQ3wiHhjRDwcEesj4icR8eHK8sMj4sGI+Fnl6+ubX64kabdajsB3Ah/NzOnAm4EPRsR04OPAQ5l5NPBQ5bUkqUWqBnhmbsrMNZXnvwU2AJOB84BVlWargHc0q0hJ0r4GNQYeER3AicAPgImZuamyajMwsZ/3LIqI1RGxuqenp45SJUl7qjnAI+Jg4OvAksz83z3XZWYCfZ7akZkrMrMzMzvb29vrKlaS9Ac1BXhEjKM3vG/NzDsri5+PiEmV9ZOALc0pUZLUl1rOQgngJmBDZl67x6p7gIsrzy8G7m58eZKk/tRyIc+pwHuAH0fE2sqyy4B/BW6PiIXAs8DfNKdEScPFWwmObFUDPDO/B/T3Y5zT2HIkSbXySkxJKpQBLkmFMsAlqVDFzEYoafBy2R4vlg3lE0k/eRzJDHCNePVOJWwEaX/lEIokFcoAl6RCGeCSVCgDXJIKZYBLUqEMcEkqlAEuSYUywCWpUAa4JBVq1FyJ6SXFkvY3HoFLUqEMcEkqlAEuSYUywCWpUAa4JBXKAJekQhngklQoA1ySCmWAS1KhqgZ4RNwcEVsiYt0ey5ZFxHMRsbbymNvcMiVJe6vlCHwlcE4fy6/LzJmVx32NLUuSVE3VAM/MR4DtLahFkjQI9YyBL46IJytDLK/vr1FELIqI1RGxuqenp47dSZL2NNQA/zxwFDAT2ARc01/DzFyRmZ2Z2dne3j7E3UmS9jakAM/M5zPzlczcBXwROKWxZUmSqhlSgEfEpD1eng+s66+tJKk5qt7QISK+CpwOHBER3cDlwOkRMZPeuxxsBN7fxBolSX2oGuCZOb+PxTc1oRZJ0iB4JaYkFcoAl6RCGeCSVCgDXJIKZYBLUqEMcEkqlAEuSYUywCWpUAa4JBXKAJekQhngklQoA1ySCmWAS1KhDHBJKpQBLkmFMsAlqVAGuCQVygCXpEIZ4JJUKANckgplgEtSoQxwSSqUAS5JhTLAJalQVQM8Im6OiC0RsW6PZYdHxIMR8bPK19c3t0xJ0t5qOQJfCZyz17KPAw9l5tHAQ5XXkqQWqhrgmfkIsH2vxecBqyrPVwHvaHBdkqQqhjoGPjEzN1WebwYm9tcwIhZFxOqIWN3T0zPE3UmS9lb3h5iZmUAOsH5FZnZmZmd7e3u9u5MkVQw1wJ+PiEkAla9bGleSJKkWQw3we4CLK88vBu5uTDmSpFrVchrhV4HvA38eEd0RsRD4V+CsiPgZ8FeV15KkFhpbrUFmzu9n1ZwG1yJJGgSvxJSkQlU9Apek4RJXRMO2lZf3e7JcsTwCl6RCGeCSVCgDXJIKZYBLUqEMcEkqlAEuSYUywCWpUAa4JBXKAJekQhngklQoA1ySCmWAS1KhDHBJKpQBLkmFcjpZSfutXLbHi2V1Tk2bI286Wo/AJalQBrgkFcoAl6RCGeCSVCgDXJIKZYBLUqEMcEkqVF3ngUfERuC3wCvAzszsbERRkqTqGnEhzxmZubUB25EkDYJDKJJUqHoDPIEHIqIrIhb11SAiFkXE6ohY3dPTU+fuJEm71Rvgp2XmLOBc4IMR8ba9G2TmiszszMzO9vb2OncnSdqtrgDPzOcqX7cAdwGnNKIoSVJ1Qw7wiDgoIg7Z/Rw4G1jXqMIkSQOr5yyUicBdEbF7O1/JzPsbUpUkqaohB3hmPgOc0MBaJEmD4GmEklQoA1ySCmWAS1KhDHBJKpQBLkmFMsAlqVAGuCQVygCXpEI1Yj5wSdovBbnni7pkVm8zWB6BS1KhDHBJKpQBLkmFMsAlqVAGuCQVygCXpEIZ4JJUKANckgplgEtSoQxwSSqUAS5JhTLAJalQBrgkFcoAl6RCGeCSVKi6AjwizomIn0bEzyPi440qSpJU3ZADPCLGAJ8DzgWmA/MjYnqjCpMkDayeI/BTgJ9n5jOZ+RJwG3BeY8qSJFVTzy3VJgO/2uN1NzB770YRsQhYVHn5QkT8tI59Dlmdd0OqdQtHAFurbqn+YgalRX2HGvrf6r6DP/sWbcGffbUt1VfMn/a1sOn3xMzMFcCKZu9nJIiI1ZnZOdx1DJfR3P/R3HcY3f0fzr7XM4TyHPDGPV5PqSyTJLVAPQH+I+DoiJgaEQcAFwH3NKYsSVI1Qx5CycydEbEY+E9gDHBzZv6kYZWVaVQMFQ1gNPd/NPcdRnf/h63vkZnDtW9JUh28ElOSCmWAS1KhDPAhqDaFQEQcGREPR8QTEfFkRMwdjjqbISJujogtEbGun/URETdUvjdPRsSsVtfYLDX0/d2VPv84Ih6LiBNaXWMzVev/Hu1OjoidEfGuVtXWbLX0PSJOj4i1EfGTiPhOK+oywAepxikE/gm4PTNPpPfsnBtbW2VTrQTOGWD9ucDRlcci4PMtqKlVVjJw338B/GVm/gXwSfa/D/ZWMnD/d//7uAp4oBUFtdBKBuh7RBxG77/zv87MGcAFrSjKAB+8WqYQSODQyvM/Af67hfU1VWY+AmwfoMl5wL9lr8eBwyJiUmuqa65qfc/MxzLz15WXj9N7bcR+o4afPcCHgK8DW5pfUevU0Pe/Be7MzF9W2rek/wb44PU1hcDkvdosA/4uIrqB++j9pR4tavn+jAYLgf8Y7iJaKSImA+ezf/3VVatjgNdHxLcjoisi3tuKnTb9UvpRaj6wMjOviYi3ALdExHGZuWu4C1PzRcQZ9Ab4acNdS4stBy7JzF0xHBOfDK+xwEnAHOBA4PsR8XhmPtXsnWpwaplCYCGV8bLM/H5EtNE74c1+9WdlP0b1FAsRcTzwJeDczNw23PW0WCdwWyW8jwDmRsTOzPz34S2rJbqBbZn5O+B3EfEIcALQ1AB3CGXwaplC4Jf0/k9MREwD2oCellY5fO4B3ls5G+XNwP9k5qbhLqoVIuJI4E7gPc0+8hqJMnNqZnZkZgdwB/APoyS8Ae4GTouIsRHxx/TOzLqh2Tv1CHyQ+ptCICI+AazOzHuAjwJfjIh/pPcDzQW5n1zyGhFfBU4HjqiM8V8OjAPIzC/QO+Y/F/g58H/A3w9PpY1XQ9//GRgP3Fg5Ct25P83QV0P/91vV+p6ZGyLifuBJYBfwpcwc8HTLhtS1n+SKJI06DqFIUqEMcEkqlAEuSYUywCWpUAa4JBXKAJekQhngklSo/wdLeb2I69svzAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}