{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_final_amostra_03_nov_25_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/defesa/PSD_histogram_final_amostra_03_nov_25_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from sklearn.metrics import r2_score\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZEvJvfoibE4"
      },
      "source": [
        "#!pip install mahotas"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UZ30b1EVQhq"
      },
      "source": [
        "def BlackWhite(Transfere,Size):\n",
        "\n",
        "  img_name=[]\n",
        "  xw=[]\n",
        "  ww=[]\n",
        "\n",
        "  with zipfile.ZipFile(Transfere, \"r\") as f:\n",
        "    for name in f.namelist():\n",
        "      img_name.append(name)\n",
        "      #xw.append(cv2.imread(name))\n",
        "      xw.append(cv2.resize(cv2.imread(name),(Size,Size)))\n",
        "\n",
        "  nrow=len(img_name)\n",
        "  ncol=Size*Size\n",
        "  pw=np.zeros((nrow,ncol))\n",
        "  #pw=[]\n",
        "  for i in range(nrow):\n",
        "    ww.append(cv2.cvtColor(np.array(xw[i]), cv2.COLOR_BGR2GRAY))\n",
        "    pw[i,:]=ww[i].ravel()\n",
        "  return ww,img_name"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v7SRrc8mH2N",
        "outputId": "7a7bd0a9-29d7-4be3-aac2-389fd1fafeb5"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip' \n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Doutorado' already exists and is not an empty directory.\n",
            "/content/Doutorado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqIYzUcnrdMp",
        "outputId": "8002f0fa-3e4a-49ed-9d52-21d284f7061f"
      },
      "source": [
        "labels =[]\n",
        "with zipfile.ZipFile(Transfere, \"r\") as f:\n",
        "  for f in f.namelist():\n",
        "    labels.append(f)\n",
        "print(labels)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Fotos_Grandes-3cdAmostra/Q6-8-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-4.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "# ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "ww,img_name=BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "\n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHgqAnaFyCjp",
        "outputId": "998899e4-051f-49a0-ae59-6059d25f6bf5"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MarquesGabi_Routines' already exists and is not an empty directory.\n",
            "/content/Doutorado/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDEGUiuubwuZ"
      },
      "source": [
        "FILE='SugarSample03.zip'\n",
        "img_name=[]\n",
        "x_original = [] \n",
        "\n",
        "data_file ='xls'\n",
        "\n",
        "\n",
        "file_name = zipfile.ZipFile(FILE, 'r')\n",
        "file_name.extractall()\n",
        "\n",
        "k = 0\n",
        "with zipfile.ZipFile(FILE, \"r\") as f:\n",
        "    for name in f.namelist():\n",
        "      if(name[-3:] == data_file):\n",
        "        #df =pd.read_csv(name)\n",
        "        if( k > 0):\n",
        "          df_old = df_ImgJ.copy()\n",
        "        df_ImgJ = pd.read_excel(name)\n",
        "        df_ImgJ = df_ImgJ.drop(labels=[0], axis=0)\n",
        "        if(k > 0):\n",
        "          df_ImgJ = pd.concat( [df_ImgJ, df_old], ignore_index = True)\n",
        "        k = k + 1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbQ0tal0etXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c0fdba9-7af8-4d35-c895-5310b518e177"
      },
      "source": [
        "f.namelist()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Results_03_02.xls', 'Results_03_03.xls', 'Results_03_01.xls']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSYrykxxGJ5d",
        "outputId": "c42dc1b2-a27c-4145-c885-d619d5b5724d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df_ImgJ.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(174, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KDJn09lGTBD",
        "outputId": "8af1c05b-34bd-4248-a150-031b7d094192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "df_ImgJ.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Area</th>\n",
              "      <th>Mean</th>\n",
              "      <th>Min</th>\n",
              "      <th>Max</th>\n",
              "      <th>Major</th>\n",
              "      <th>Minor</th>\n",
              "      <th>Angle</th>\n",
              "      <th>Feret</th>\n",
              "      <th>FeretX</th>\n",
              "      <th>FeretY</th>\n",
              "      <th>FeretAngle</th>\n",
              "      <th>MinFeret</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>1.288</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>1.383</td>\n",
              "      <td>1.185</td>\n",
              "      <td>5.847</td>\n",
              "      <td>1.636</td>\n",
              "      <td>767</td>\n",
              "      <td>213</td>\n",
              "      <td>18.157</td>\n",
              "      <td>1.161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0.407</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>0.814</td>\n",
              "      <td>0.637</td>\n",
              "      <td>62.186</td>\n",
              "      <td>0.877</td>\n",
              "      <td>283</td>\n",
              "      <td>234</td>\n",
              "      <td>59.036</td>\n",
              "      <td>0.667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>0.592</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>0.925</td>\n",
              "      <td>0.815</td>\n",
              "      <td>117.923</td>\n",
              "      <td>1.078</td>\n",
              "      <td>633</td>\n",
              "      <td>154</td>\n",
              "      <td>122.335</td>\n",
              "      <td>0.802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>1.391</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>1.435</td>\n",
              "      <td>1.235</td>\n",
              "      <td>29.966</td>\n",
              "      <td>1.564</td>\n",
              "      <td>1321</td>\n",
              "      <td>333</td>\n",
              "      <td>53.253</td>\n",
              "      <td>1.165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>0.549</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>0.923</td>\n",
              "      <td>0.758</td>\n",
              "      <td>136.396</td>\n",
              "      <td>1.024</td>\n",
              "      <td>370</td>\n",
              "      <td>254</td>\n",
              "      <td>118.237</td>\n",
              "      <td>0.738</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Area  Mean  Min  Max  ...  Feret  FeretX  FeretY  FeretAngle  MinFeret\n",
              "0  2  1.288   255  255  255  ...  1.636     767     213      18.157     1.161\n",
              "1  3  0.407   255  255  255  ...  0.877     283     234      59.036     0.667\n",
              "2  4  0.592   255  255  255  ...  1.078     633     154     122.335     0.802\n",
              "3  5  1.391   255  255  255  ...  1.564    1321     333      53.253     1.165\n",
              "4  6  0.549   255  255  255  ...  1.024     370     254     118.237     0.738\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMBJ6C-YdF3q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "d01f3db3-4129-42d2-a587-ec115fb9bf25"
      },
      "source": [
        "df_ImgJ.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Area</th>\n",
              "      <th>Mean</th>\n",
              "      <th>Min</th>\n",
              "      <th>Max</th>\n",
              "      <th>Major</th>\n",
              "      <th>Minor</th>\n",
              "      <th>Angle</th>\n",
              "      <th>Feret</th>\n",
              "      <th>FeretX</th>\n",
              "      <th>FeretY</th>\n",
              "      <th>FeretAngle</th>\n",
              "      <th>MinFeret</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>1.288</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>1.383</td>\n",
              "      <td>1.185</td>\n",
              "      <td>5.847</td>\n",
              "      <td>1.636</td>\n",
              "      <td>767</td>\n",
              "      <td>213</td>\n",
              "      <td>18.157</td>\n",
              "      <td>1.161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0.407</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>0.814</td>\n",
              "      <td>0.637</td>\n",
              "      <td>62.186</td>\n",
              "      <td>0.877</td>\n",
              "      <td>283</td>\n",
              "      <td>234</td>\n",
              "      <td>59.036</td>\n",
              "      <td>0.667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>0.592</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>0.925</td>\n",
              "      <td>0.815</td>\n",
              "      <td>117.923</td>\n",
              "      <td>1.078</td>\n",
              "      <td>633</td>\n",
              "      <td>154</td>\n",
              "      <td>122.335</td>\n",
              "      <td>0.802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>1.391</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>1.435</td>\n",
              "      <td>1.235</td>\n",
              "      <td>29.966</td>\n",
              "      <td>1.564</td>\n",
              "      <td>1321</td>\n",
              "      <td>333</td>\n",
              "      <td>53.253</td>\n",
              "      <td>1.165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>0.549</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>0.923</td>\n",
              "      <td>0.758</td>\n",
              "      <td>136.396</td>\n",
              "      <td>1.024</td>\n",
              "      <td>370</td>\n",
              "      <td>254</td>\n",
              "      <td>118.237</td>\n",
              "      <td>0.738</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Area  Mean  Min  Max  ...  Feret  FeretX  FeretY  FeretAngle  MinFeret\n",
              "0  2  1.288   255  255  255  ...  1.636     767     213      18.157     1.161\n",
              "1  3  0.407   255  255  255  ...  0.877     283     234      59.036     0.667\n",
              "2  4  0.592   255  255  255  ...  1.078     633     154     122.335     0.802\n",
              "3  5  1.391   255  255  255  ...  1.564    1321     333      53.253     1.165\n",
              "4  6  0.549   255  255  255  ...  1.024     370     254     118.237     0.738\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from Segment_Filter import Segmenta  # got image provided segmented"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863",
        "outputId": "c4ad3e8f-d652-449c-d440-fd4c0d64aebc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "'''\n",
        "Sample3 =[4,13,25]\n",
        "k = 0\n",
        "\n",
        "for i in Sample3:\n",
        "  img=ww[i]\n",
        "  if( k > 0):\n",
        "    df_old = df.copy()\n",
        "  df=Segmenta(img)\n",
        "  if(k > 0):\n",
        "    df = pd.concat( [df, df_old], ignore_index = True)\n",
        "  k = k + 1\n",
        "df_ann = df.copy\n",
        "Img_Size = 28\n",
        "'''"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nSample3 =[4,13,25]\\nk = 0\\n\\nfor i in Sample3:\\n  img=ww[i]\\n  if( k > 0):\\n    df_old = df.copy()\\n  df=Segmenta(img)\\n  if(k > 0):\\n    df = pd.concat( [df, df_old], ignore_index = True)\\n  k = k + 1\\ndf_ann = df.copy\\nImg_Size = 28\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5MN5a_v4np"
      },
      "source": [
        "#print(df)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-zmz6IQGlQS"
      },
      "source": [
        "#df.shape"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR2emP4rNjQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45a885bb-4406-4b40-fabc-cae59a21fa56"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MarquesGabi_Routines' already exists and is not an empty directory.\n",
            "/content/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIFPGE_-vx3T"
      },
      "source": [
        "Img_Size = 28"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "Description = '3 layers of Convolution: 64, 128, 256 '\n",
        "N1 = 20\n",
        "N2 = 20\n",
        "\n",
        "# make the CNN\n",
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=64, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=N1))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=N2))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpbPQ1FSRG6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a09eb0a-ba86-43e3-909f-798e0792ac96"
      },
      "source": [
        "\n",
        "# training the model\n",
        "r = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), \n",
        "              epochs=200, batch_size=32)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 3s 160ms/step - loss: 0.5331 - accuracy: 0.7638 - val_loss: 0.6931 - val_accuracy: 0.4898\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.3340 - accuracy: 0.8659 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.1734 - accuracy: 0.9475 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.1437 - accuracy: 0.9329 - val_loss: 0.6928 - val_accuracy: 0.5102\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.1253 - accuracy: 0.9563 - val_loss: 0.6934 - val_accuracy: 0.5102\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0727 - accuracy: 0.9708 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0555 - accuracy: 0.9825 - val_loss: 0.6926 - val_accuracy: 0.5102\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 0.0465 - accuracy: 0.9883 - val_loss: 0.6926 - val_accuracy: 0.5102\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0219 - accuracy: 0.9971 - val_loss: 0.6924 - val_accuracy: 0.5102\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0210 - accuracy: 0.9913 - val_loss: 0.6928 - val_accuracy: 0.5102\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.6933 - val_accuracy: 0.5102\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 0.0146 - accuracy: 0.9942 - val_loss: 0.6934 - val_accuracy: 0.5102\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 0.0142 - accuracy: 0.9971 - val_loss: 0.6951 - val_accuracy: 0.5102\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.6992 - val_accuracy: 0.5102\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.7011 - val_accuracy: 0.5102\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.7034 - val_accuracy: 0.5102\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.7072 - val_accuracy: 0.5102\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.7087 - val_accuracy: 0.5102\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.7156 - val_accuracy: 0.5102\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.7314 - val_accuracy: 0.5102\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7417 - val_accuracy: 0.5102\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.7448 - val_accuracy: 0.5102\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.7498 - val_accuracy: 0.5102\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7484 - val_accuracy: 0.5102\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 0.0054 - accuracy: 0.9971 - val_loss: 0.7711 - val_accuracy: 0.5102\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.8037 - val_accuracy: 0.5102\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 0.0132 - accuracy: 0.9942 - val_loss: 0.7421 - val_accuracy: 0.5102\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 2s 155ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.7851 - val_accuracy: 0.5102\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 2s 153ms/step - loss: 0.0086 - accuracy: 0.9971 - val_loss: 1.3321 - val_accuracy: 0.5102\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 1.8578 - val_accuracy: 0.5102\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.7985 - val_accuracy: 0.5102\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 2.3126 - val_accuracy: 0.5102\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.6621 - val_accuracy: 0.5102\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 0.0143 - accuracy: 0.9913 - val_loss: 2.2346 - val_accuracy: 0.5102\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 3.6828 - val_accuracy: 0.5102\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0449 - accuracy: 0.9854 - val_loss: 3.9425 - val_accuracy: 0.5102\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0147 - accuracy: 0.9942 - val_loss: 5.3997 - val_accuracy: 0.5102\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0567 - accuracy: 0.9825 - val_loss: 7.8161 - val_accuracy: 0.5102\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0250 - accuracy: 0.9883 - val_loss: 12.9382 - val_accuracy: 0.5102\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 0.0239 - accuracy: 0.9913 - val_loss: 14.9532 - val_accuracy: 0.5102\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 0.0326 - accuracy: 0.9913 - val_loss: 13.9369 - val_accuracy: 0.5102\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0158 - accuracy: 0.9971 - val_loss: 16.8083 - val_accuracy: 0.5102\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0213 - accuracy: 0.9971 - val_loss: 24.7606 - val_accuracy: 0.5102\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0084 - accuracy: 0.9971 - val_loss: 22.7589 - val_accuracy: 0.5102\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 25.9182 - val_accuracy: 0.5102\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 8.9770e-04 - accuracy: 1.0000 - val_loss: 29.4249 - val_accuracy: 0.5102\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 29.6569 - val_accuracy: 0.5102\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 23.7726 - val_accuracy: 0.5102\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 0.0051 - accuracy: 0.9971 - val_loss: 17.6141 - val_accuracy: 0.5102\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 8.4470e-04 - accuracy: 1.0000 - val_loss: 13.3777 - val_accuracy: 0.5102\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 12.1729 - val_accuracy: 0.5102\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 0.0051 - accuracy: 0.9971 - val_loss: 16.4598 - val_accuracy: 0.5102\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 2s 155ms/step - loss: 0.0034 - accuracy: 0.9971 - val_loss: 16.7870 - val_accuracy: 0.5102\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 9.0448e-04 - accuracy: 1.0000 - val_loss: 14.2678 - val_accuracy: 0.5102\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 3.0368e-04 - accuracy: 1.0000 - val_loss: 13.3940 - val_accuracy: 0.5102\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 2s 152ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 14.2303 - val_accuracy: 0.5102\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 5.2437e-04 - accuracy: 1.0000 - val_loss: 14.0288 - val_accuracy: 0.5102\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 2s 153ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 13.4710 - val_accuracy: 0.5102\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 3.5640e-04 - accuracy: 1.0000 - val_loss: 13.1050 - val_accuracy: 0.5102\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 2s 159ms/step - loss: 4.5147e-04 - accuracy: 1.0000 - val_loss: 12.5150 - val_accuracy: 0.5102\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 2.5881e-04 - accuracy: 1.0000 - val_loss: 11.8214 - val_accuracy: 0.5102\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 2s 151ms/step - loss: 1.0062e-04 - accuracy: 1.0000 - val_loss: 11.2583 - val_accuracy: 0.5102\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 3.6687e-04 - accuracy: 1.0000 - val_loss: 10.3150 - val_accuracy: 0.5102\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 2s 152ms/step - loss: 3.3014e-04 - accuracy: 1.0000 - val_loss: 9.2654 - val_accuracy: 0.5102\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 2s 155ms/step - loss: 6.0867e-04 - accuracy: 1.0000 - val_loss: 10.5011 - val_accuracy: 0.5102\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 2s 157ms/step - loss: 3.9257e-04 - accuracy: 1.0000 - val_loss: 10.5501 - val_accuracy: 0.5102\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 2s 155ms/step - loss: 1.3732e-04 - accuracy: 1.0000 - val_loss: 10.1425 - val_accuracy: 0.5102\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 2s 156ms/step - loss: 7.0228e-05 - accuracy: 1.0000 - val_loss: 9.5025 - val_accuracy: 0.5102\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 2s 157ms/step - loss: 4.7047e-04 - accuracy: 1.0000 - val_loss: 8.8305 - val_accuracy: 0.5102\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 1.2792e-04 - accuracy: 1.0000 - val_loss: 8.0181 - val_accuracy: 0.5102\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 2s 155ms/step - loss: 2.5610e-04 - accuracy: 1.0000 - val_loss: 7.2653 - val_accuracy: 0.5102\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 2s 159ms/step - loss: 1.4968e-04 - accuracy: 1.0000 - val_loss: 6.0871 - val_accuracy: 0.5102\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 4.7461e-04 - accuracy: 1.0000 - val_loss: 5.8628 - val_accuracy: 0.5102\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 3.8990e-05 - accuracy: 1.0000 - val_loss: 5.8000 - val_accuracy: 0.5102\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 1.3833e-04 - accuracy: 1.0000 - val_loss: 5.1208 - val_accuracy: 0.5102\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 8.3876e-05 - accuracy: 1.0000 - val_loss: 4.1702 - val_accuracy: 0.5306\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 4.3494e-05 - accuracy: 1.0000 - val_loss: 3.4865 - val_accuracy: 0.5578\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 9.1660e-05 - accuracy: 1.0000 - val_loss: 2.5196 - val_accuracy: 0.6122\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 3.8407e-05 - accuracy: 1.0000 - val_loss: 1.7986 - val_accuracy: 0.6735\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 5.6687e-05 - accuracy: 1.0000 - val_loss: 1.3644 - val_accuracy: 0.7007\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 2.9292e-05 - accuracy: 1.0000 - val_loss: 1.0141 - val_accuracy: 0.7959\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 2s 156ms/step - loss: 1.1995e-04 - accuracy: 1.0000 - val_loss: 0.8572 - val_accuracy: 0.8299\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 2s 155ms/step - loss: 1.8924e-05 - accuracy: 1.0000 - val_loss: 0.7283 - val_accuracy: 0.8435\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 4.3051e-05 - accuracy: 1.0000 - val_loss: 0.5983 - val_accuracy: 0.8503\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 2s 157ms/step - loss: 1.4607e-04 - accuracy: 1.0000 - val_loss: 0.5397 - val_accuracy: 0.8707\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 8.9706e-05 - accuracy: 1.0000 - val_loss: 0.6107 - val_accuracy: 0.8707\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 2.0994e-04 - accuracy: 1.0000 - val_loss: 0.6010 - val_accuracy: 0.8639\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 1.7452e-04 - accuracy: 1.0000 - val_loss: 0.3972 - val_accuracy: 0.9116\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 4.9777e-05 - accuracy: 1.0000 - val_loss: 0.3052 - val_accuracy: 0.9456\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 4.9536e-05 - accuracy: 1.0000 - val_loss: 0.2593 - val_accuracy: 0.9524\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 1.4271e-04 - accuracy: 1.0000 - val_loss: 0.2119 - val_accuracy: 0.9660\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 2s 155ms/step - loss: 7.9335e-05 - accuracy: 1.0000 - val_loss: 0.1793 - val_accuracy: 0.9728\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 2.8698e-05 - accuracy: 1.0000 - val_loss: 0.1771 - val_accuracy: 0.9728\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 4.6563e-05 - accuracy: 1.0000 - val_loss: 0.1745 - val_accuracy: 0.9728\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 2s 155ms/step - loss: 1.0344e-04 - accuracy: 1.0000 - val_loss: 0.1705 - val_accuracy: 0.9796\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 2s 153ms/step - loss: 1.0059e-04 - accuracy: 1.0000 - val_loss: 0.1977 - val_accuracy: 0.9660\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 3.3343e-05 - accuracy: 1.0000 - val_loss: 0.2136 - val_accuracy: 0.9592\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 2s 155ms/step - loss: 4.9372e-05 - accuracy: 1.0000 - val_loss: 0.1912 - val_accuracy: 0.9728\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 2s 156ms/step - loss: 1.8309e-04 - accuracy: 1.0000 - val_loss: 0.1790 - val_accuracy: 0.9864\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 2s 156ms/step - loss: 4.9759e-05 - accuracy: 1.0000 - val_loss: 0.1935 - val_accuracy: 0.9796\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 6.5465e-05 - accuracy: 1.0000 - val_loss: 0.2086 - val_accuracy: 0.9796\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 2s 159ms/step - loss: 4.6904e-05 - accuracy: 1.0000 - val_loss: 0.2146 - val_accuracy: 0.9796\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 2s 157ms/step - loss: 8.5377e-05 - accuracy: 1.0000 - val_loss: 0.2048 - val_accuracy: 0.9796\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 2s 156ms/step - loss: 9.1949e-05 - accuracy: 1.0000 - val_loss: 0.1861 - val_accuracy: 0.9796\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 2s 155ms/step - loss: 9.6778e-05 - accuracy: 1.0000 - val_loss: 0.1765 - val_accuracy: 0.9796\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 2s 156ms/step - loss: 1.3026e-04 - accuracy: 1.0000 - val_loss: 0.1727 - val_accuracy: 0.9864\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 2s 153ms/step - loss: 1.0768e-04 - accuracy: 1.0000 - val_loss: 0.1685 - val_accuracy: 0.9728\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 2s 158ms/step - loss: 8.1213e-05 - accuracy: 1.0000 - val_loss: 0.1648 - val_accuracy: 0.9728\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 3.9200e-05 - accuracy: 1.0000 - val_loss: 0.1660 - val_accuracy: 0.9728\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 2s 155ms/step - loss: 1.9852e-05 - accuracy: 1.0000 - val_loss: 0.1706 - val_accuracy: 0.9796\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 2s 156ms/step - loss: 3.4536e-05 - accuracy: 1.0000 - val_loss: 0.1661 - val_accuracy: 0.9728\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 2s 152ms/step - loss: 7.7304e-05 - accuracy: 1.0000 - val_loss: 0.2024 - val_accuracy: 0.9660\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 2.9380e-04 - accuracy: 1.0000 - val_loss: 0.2008 - val_accuracy: 0.9728\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 6.2927e-05 - accuracy: 1.0000 - val_loss: 0.1551 - val_accuracy: 0.9660\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 2.7829e-05 - accuracy: 1.0000 - val_loss: 0.1584 - val_accuracy: 0.9660\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 2s 159ms/step - loss: 1.9282e-04 - accuracy: 1.0000 - val_loss: 0.7638 - val_accuracy: 0.8503\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 2s 151ms/step - loss: 3.6594e-05 - accuracy: 1.0000 - val_loss: 4.3699 - val_accuracy: 0.5714\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 3.5252e-05 - accuracy: 1.0000 - val_loss: 5.0465 - val_accuracy: 0.5374\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 2s 159ms/step - loss: 4.1375e-05 - accuracy: 1.0000 - val_loss: 5.2547 - val_accuracy: 0.5374\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 4.7571e-05 - accuracy: 1.0000 - val_loss: 4.1794 - val_accuracy: 0.5986\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 2.0325e-05 - accuracy: 1.0000 - val_loss: 2.0239 - val_accuracy: 0.6939\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 3.4006e-05 - accuracy: 1.0000 - val_loss: 1.0921 - val_accuracy: 0.7891\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 1.7350e-05 - accuracy: 1.0000 - val_loss: 0.8075 - val_accuracy: 0.8367\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 2s 151ms/step - loss: 1.8834e-05 - accuracy: 1.0000 - val_loss: 0.5968 - val_accuracy: 0.8844\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 2s 153ms/step - loss: 3.0908e-05 - accuracy: 1.0000 - val_loss: 0.5357 - val_accuracy: 0.9048\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 3.4967e-05 - accuracy: 1.0000 - val_loss: 0.4912 - val_accuracy: 0.9048\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 1s 138ms/step - loss: 5.9803e-06 - accuracy: 1.0000 - val_loss: 0.4488 - val_accuracy: 0.9116\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 3.3827e-05 - accuracy: 1.0000 - val_loss: 0.4020 - val_accuracy: 0.9184\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 3.6489e-05 - accuracy: 1.0000 - val_loss: 0.3431 - val_accuracy: 0.9388\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 2.5094e-05 - accuracy: 1.0000 - val_loss: 0.3073 - val_accuracy: 0.9388\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 9.4361e-06 - accuracy: 1.0000 - val_loss: 0.2617 - val_accuracy: 0.9456\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 2s 153ms/step - loss: 5.9578e-05 - accuracy: 1.0000 - val_loss: 0.2153 - val_accuracy: 0.9728\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 1.9829e-05 - accuracy: 1.0000 - val_loss: 0.2099 - val_accuracy: 0.9728\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 3.2479e-05 - accuracy: 1.0000 - val_loss: 0.2308 - val_accuracy: 0.9660\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 2s 158ms/step - loss: 1.6553e-05 - accuracy: 1.0000 - val_loss: 0.2424 - val_accuracy: 0.9660\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 2s 160ms/step - loss: 2.0560e-05 - accuracy: 1.0000 - val_loss: 0.2341 - val_accuracy: 0.9660\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 2s 157ms/step - loss: 2.8895e-05 - accuracy: 1.0000 - val_loss: 0.2089 - val_accuracy: 0.9728\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 1.7340e-05 - accuracy: 1.0000 - val_loss: 0.1993 - val_accuracy: 0.9728\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 2s 157ms/step - loss: 1.8547e-04 - accuracy: 1.0000 - val_loss: 0.2006 - val_accuracy: 0.9728\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 7.0329e-05 - accuracy: 1.0000 - val_loss: 0.2212 - val_accuracy: 0.9592\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 1.9680e-05 - accuracy: 1.0000 - val_loss: 0.2316 - val_accuracy: 0.9592\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 7.5580e-05 - accuracy: 1.0000 - val_loss: 0.2049 - val_accuracy: 0.9728\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 3.0149e-05 - accuracy: 1.0000 - val_loss: 0.1937 - val_accuracy: 0.9728\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 6.7217e-05 - accuracy: 1.0000 - val_loss: 0.1706 - val_accuracy: 0.9796\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 2s 158ms/step - loss: 3.1045e-05 - accuracy: 1.0000 - val_loss: 0.1698 - val_accuracy: 0.9796\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 3.5260e-05 - accuracy: 1.0000 - val_loss: 0.1749 - val_accuracy: 0.9796\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 2s 157ms/step - loss: 2.3667e-05 - accuracy: 1.0000 - val_loss: 0.1768 - val_accuracy: 0.9796\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 2s 158ms/step - loss: 7.7528e-05 - accuracy: 1.0000 - val_loss: 0.2144 - val_accuracy: 0.9728\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 3.1400e-05 - accuracy: 1.0000 - val_loss: 0.2341 - val_accuracy: 0.9660\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 2s 156ms/step - loss: 2.9775e-05 - accuracy: 1.0000 - val_loss: 0.2357 - val_accuracy: 0.9660\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 1.3579e-05 - accuracy: 1.0000 - val_loss: 0.2358 - val_accuracy: 0.9660\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 2s 155ms/step - loss: 4.0616e-05 - accuracy: 1.0000 - val_loss: 0.2420 - val_accuracy: 0.9660\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 2s 159ms/step - loss: 1.7794e-05 - accuracy: 1.0000 - val_loss: 0.2434 - val_accuracy: 0.9660\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 6.6814e-06 - accuracy: 1.0000 - val_loss: 0.2451 - val_accuracy: 0.9660\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 2s 156ms/step - loss: 4.8887e-06 - accuracy: 1.0000 - val_loss: 0.2449 - val_accuracy: 0.9660\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 2s 157ms/step - loss: 5.4066e-05 - accuracy: 1.0000 - val_loss: 0.1878 - val_accuracy: 0.9796\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 2s 157ms/step - loss: 8.6810e-06 - accuracy: 1.0000 - val_loss: 0.1760 - val_accuracy: 0.9796\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 2s 160ms/step - loss: 5.8651e-05 - accuracy: 1.0000 - val_loss: 0.1855 - val_accuracy: 0.9796\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 2s 158ms/step - loss: 6.3752e-06 - accuracy: 1.0000 - val_loss: 0.1926 - val_accuracy: 0.9796\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 2s 155ms/step - loss: 1.1263e-05 - accuracy: 1.0000 - val_loss: 0.1985 - val_accuracy: 0.9796\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 4.5749e-05 - accuracy: 1.0000 - val_loss: 0.1950 - val_accuracy: 0.9796\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 1.6299e-05 - accuracy: 1.0000 - val_loss: 0.1997 - val_accuracy: 0.9728\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 2s 158ms/step - loss: 3.8459e-05 - accuracy: 1.0000 - val_loss: 0.1980 - val_accuracy: 0.9728\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 2s 158ms/step - loss: 6.8207e-06 - accuracy: 1.0000 - val_loss: 0.1934 - val_accuracy: 0.9728\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 2s 162ms/step - loss: 3.2549e-05 - accuracy: 1.0000 - val_loss: 0.1972 - val_accuracy: 0.9796\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 5.5819e-06 - accuracy: 1.0000 - val_loss: 0.1945 - val_accuracy: 0.9796\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 2s 160ms/step - loss: 1.0257e-04 - accuracy: 1.0000 - val_loss: 0.2053 - val_accuracy: 0.9660\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 2s 156ms/step - loss: 4.2679e-05 - accuracy: 1.0000 - val_loss: 0.2180 - val_accuracy: 0.9660\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 2s 160ms/step - loss: 1.3323e-04 - accuracy: 1.0000 - val_loss: 0.2355 - val_accuracy: 0.9728\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 3.5680e-05 - accuracy: 1.0000 - val_loss: 0.3751 - val_accuracy: 0.9456\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 1.4806e-05 - accuracy: 1.0000 - val_loss: 0.4134 - val_accuracy: 0.9388\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 1.2422e-05 - accuracy: 1.0000 - val_loss: 0.4206 - val_accuracy: 0.9388\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 1.1665e-05 - accuracy: 1.0000 - val_loss: 0.3581 - val_accuracy: 0.9456\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 1.5507e-05 - accuracy: 1.0000 - val_loss: 0.3179 - val_accuracy: 0.9660\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 2s 162ms/step - loss: 1.5551e-05 - accuracy: 1.0000 - val_loss: 0.2913 - val_accuracy: 0.9660\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 1.5013e-05 - accuracy: 1.0000 - val_loss: 0.2788 - val_accuracy: 0.9660\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 2s 160ms/step - loss: 7.4845e-05 - accuracy: 1.0000 - val_loss: 0.2790 - val_accuracy: 0.9660\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 2.3663e-05 - accuracy: 1.0000 - val_loss: 0.2659 - val_accuracy: 0.9660\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 1.6163e-05 - accuracy: 1.0000 - val_loss: 0.2669 - val_accuracy: 0.9660\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 7.7254e-06 - accuracy: 1.0000 - val_loss: 0.2653 - val_accuracy: 0.9660\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 2s 163ms/step - loss: 2.2045e-05 - accuracy: 1.0000 - val_loss: 0.2508 - val_accuracy: 0.9660\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 2s 158ms/step - loss: 9.1772e-06 - accuracy: 1.0000 - val_loss: 0.2451 - val_accuracy: 0.9660\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 2s 167ms/step - loss: 2.9281e-05 - accuracy: 1.0000 - val_loss: 0.2289 - val_accuracy: 0.9796\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 6.5699e-06 - accuracy: 1.0000 - val_loss: 0.2054 - val_accuracy: 0.9796\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 1.7158e-05 - accuracy: 1.0000 - val_loss: 0.1971 - val_accuracy: 0.9796\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 2.1706e-04 - accuracy: 1.0000 - val_loss: 0.1969 - val_accuracy: 0.9796\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 2s 151ms/step - loss: 4.7994e-06 - accuracy: 1.0000 - val_loss: 0.1971 - val_accuracy: 0.9796\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 1.3577e-05 - accuracy: 1.0000 - val_loss: 0.2033 - val_accuracy: 0.9660\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 1.7540e-05 - accuracy: 1.0000 - val_loss: 0.1942 - val_accuracy: 0.9660\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 2s 165ms/step - loss: 3.7349e-05 - accuracy: 1.0000 - val_loss: 0.2068 - val_accuracy: 0.9796\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 1.2015e-04 - accuracy: 1.0000 - val_loss: 0.9697 - val_accuracy: 0.8095\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 2.9165e-05 - accuracy: 1.0000 - val_loss: 2.6105 - val_accuracy: 0.6463\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 6.3910e-06 - accuracy: 1.0000 - val_loss: 2.6220 - val_accuracy: 0.6463\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 8.4363e-06 - accuracy: 1.0000 - val_loss: 2.0696 - val_accuracy: 0.7007\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 6.8097e-06 - accuracy: 1.0000 - val_loss: 1.3971 - val_accuracy: 0.7551\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 2s 162ms/step - loss: 2.1773e-05 - accuracy: 1.0000 - val_loss: 0.9246 - val_accuracy: 0.8299\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 4.1752e-05 - accuracy: 1.0000 - val_loss: 0.6063 - val_accuracy: 0.8844\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 2s 153ms/step - loss: 6.3347e-05 - accuracy: 1.0000 - val_loss: 0.4428 - val_accuracy: 0.9320\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 2.3535e-05 - accuracy: 1.0000 - val_loss: 0.3660 - val_accuracy: 0.9388\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 2s 164ms/step - loss: 8.9654e-06 - accuracy: 1.0000 - val_loss: 0.3330 - val_accuracy: 0.9456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDVY6HbxMOlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d7feee5-2277-429d-9672-679cef4a4ec9"
      },
      "source": [
        "# pred_test= model.predict_classes(X_test)\n",
        "pred_test = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict   0   1\n",
            "Actual         \n",
            "0        65   7\n",
            "1         1  74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7pT2q7traXg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "799ee7b5-b974-468b-e310-1c484c4afd7f"
      },
      "source": [
        "print(METRICS)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.90      0.94        72\n",
            "           1       0.91      0.99      0.95        75\n",
            "\n",
            "    accuracy                           0.95       147\n",
            "   macro avg       0.95      0.94      0.95       147\n",
            "weighted avg       0.95      0.95      0.95       147\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElpxWbBnpgLX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "fc4a997f-42a9-4ed5-b61f-fad5d68c9af7"
      },
      "source": [
        "'''\n",
        "#X =np.array(df.copy())/255.0 \n",
        "X =np.array(df.copy())\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)\n",
        "model = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh', solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  \n",
        "prediction = model.predict(X_test)  \n",
        "y =np.copy(y_test)\n",
        "data = {'y_true': y_test,'y_predict': prediction}  \n",
        "# este dado esta no formato de dicionario\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)\n",
        "print(METRICS)\n",
        "#X =np.array(df.copy())/255.0 X =np.array(df_all.copy())X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)model = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh',                       solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  prediction = model.predict(X_test)  y =np.copy(y_test)data = {'y_true': y_test,'y_predict': prediction}  # este dado esta no formato de dicionariodf = pd.DataFrame(data, columns=['y_true','y_predict'])confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])print(confusion_matrix)y_true = df['y_true']y_pred = df['y_predict']\n",
        "'''"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n#X =np.array(df.copy())/255.0 \\nX =np.array(df.copy())\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)\\nmodel = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh', solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  \\nprediction = model.predict(X_test)  \\ny =np.copy(y_test)\\ndata = {'y_true': y_test,'y_predict': prediction}  \\n# este dado esta no formato de dicionario\\ndf = pd.DataFrame(data, columns=['y_true','y_predict'])\\nconfusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\\nprint(confusion_matrix)\\ny_true = df['y_true']\\ny_pred = df['y_predict']  \\nMETRICS=sklearn.metrics.classification_report(y_true, y_pred)\\nprint(METRICS)\\n#X =np.array(df.copy())/255.0 X =np.array(df_all.copy())X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)model = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh',                       solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  prediction = model.predict(X_test)  y =np.copy(y_test)data = {'y_true': y_test,'y_predict': prediction}  # este dado esta no formato de dicionariodf = pd.DataFrame(data, columns=['y_true','y_predict'])confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])print(confusion_matrix)y_true = df['y_true']y_pred = df['y_predict']\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iFNNrlWV9tH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba1a9609-bb70-4ad0-8f86-46a2d519a3b5"
      },
      "source": [
        "pred_test"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5pq5z8DHeJ0",
        "outputId": "7eb71dd1-0041-4b0a-e25b-1e9cf52bbee5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "img=ww[4] \n",
        "df=Segmenta(img)\n",
        "df.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 785)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8ofkAXlS-0F"
      },
      "source": [
        "Sample3 =[4,13,25]\n",
        "k = 0\n",
        "\n",
        "for i in Sample3:\n",
        "  img=ww[i]\n",
        "  if( k > 0):\n",
        "    df_old = df.copy()\n",
        "  df_ann=Segmenta(img)\n",
        "  if(k > 0):\n",
        "    df_ann = pd.concat( [df_ann, df_old], ignore_index = True)\n",
        "  k = k + 1\n",
        "#df_ann = df.copy\n",
        "\n",
        "\n",
        "Width = df_ann['Width']\n",
        "del df_ann['Width']\n",
        "\n",
        "result = np.array(df_ann)\n",
        "result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "prediction= np.argmax(model.predict(result), axis=-1)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31usb3UnY7lD",
        "outputId": "1c9315cc-9ab7-426c-cddd-faef612e4bd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#df_ann.shape\n",
        "df.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 785)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVHUUaL8XXs-",
        "outputId": "694ec924-e114-43a1-b3e0-313e18a1fab9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "source": [
        "df_ann"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>127.186852</td>\n",
              "      <td>109.609833</td>\n",
              "      <td>119.858261</td>\n",
              "      <td>121.125267</td>\n",
              "      <td>119.041389</td>\n",
              "      <td>107.515015</td>\n",
              "      <td>102.478073</td>\n",
              "      <td>84.992668</td>\n",
              "      <td>72.818680</td>\n",
              "      <td>67.863678</td>\n",
              "      <td>90.043045</td>\n",
              "      <td>128.383255</td>\n",
              "      <td>133.359314</td>\n",
              "      <td>119.685120</td>\n",
              "      <td>80.143806</td>\n",
              "      <td>64.418411</td>\n",
              "      <td>72.123878</td>\n",
              "      <td>81.225052</td>\n",
              "      <td>103.510872</td>\n",
              "      <td>115.339249</td>\n",
              "      <td>149.115585</td>\n",
              "      <td>204.181885</td>\n",
              "      <td>144.215515</td>\n",
              "      <td>9.757924</td>\n",
              "      <td>2.178132</td>\n",
              "      <td>0.556540</td>\n",
              "      <td>0.509481</td>\n",
              "      <td>1.685952</td>\n",
              "      <td>148.841110</td>\n",
              "      <td>123.388664</td>\n",
              "      <td>127.262009</td>\n",
              "      <td>129.231567</td>\n",
              "      <td>113.072670</td>\n",
              "      <td>101.976471</td>\n",
              "      <td>106.687340</td>\n",
              "      <td>97.198624</td>\n",
              "      <td>69.054543</td>\n",
              "      <td>63.667137</td>\n",
              "      <td>87.143669</td>\n",
              "      <td>109.023254</td>\n",
              "      <td>...</td>\n",
              "      <td>0.395709</td>\n",
              "      <td>1.572180</td>\n",
              "      <td>1.100900</td>\n",
              "      <td>0.121523</td>\n",
              "      <td>1.278062</td>\n",
              "      <td>1.442768</td>\n",
              "      <td>0.266298</td>\n",
              "      <td>0.829619</td>\n",
              "      <td>1.698685</td>\n",
              "      <td>0.560415</td>\n",
              "      <td>0.513356</td>\n",
              "      <td>1.689827</td>\n",
              "      <td>2.311696</td>\n",
              "      <td>2.654395</td>\n",
              "      <td>1.357094</td>\n",
              "      <td>3.040139</td>\n",
              "      <td>3.065467</td>\n",
              "      <td>4.720000</td>\n",
              "      <td>5.220208</td>\n",
              "      <td>3.107267</td>\n",
              "      <td>1.226713</td>\n",
              "      <td>0.105329</td>\n",
              "      <td>1.117232</td>\n",
              "      <td>1.556540</td>\n",
              "      <td>0.380069</td>\n",
              "      <td>0.685952</td>\n",
              "      <td>1.713080</td>\n",
              "      <td>0.674187</td>\n",
              "      <td>0.391834</td>\n",
              "      <td>1.568305</td>\n",
              "      <td>1.095640</td>\n",
              "      <td>0.115156</td>\n",
              "      <td>1.274187</td>\n",
              "      <td>1.438893</td>\n",
              "      <td>0.262422</td>\n",
              "      <td>0.822976</td>\n",
              "      <td>1.693703</td>\n",
              "      <td>0.556540</td>\n",
              "      <td>0.509481</td>\n",
              "      <td>1.685952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>191.587921</td>\n",
              "      <td>189.490097</td>\n",
              "      <td>176.532776</td>\n",
              "      <td>165.786682</td>\n",
              "      <td>163.491165</td>\n",
              "      <td>166.914658</td>\n",
              "      <td>166.665970</td>\n",
              "      <td>168.350662</td>\n",
              "      <td>169.442230</td>\n",
              "      <td>166.022873</td>\n",
              "      <td>164.562958</td>\n",
              "      <td>166.147766</td>\n",
              "      <td>168.252838</td>\n",
              "      <td>178.137344</td>\n",
              "      <td>188.790817</td>\n",
              "      <td>192.252853</td>\n",
              "      <td>192.738800</td>\n",
              "      <td>193.600403</td>\n",
              "      <td>189.521317</td>\n",
              "      <td>182.237244</td>\n",
              "      <td>181.243484</td>\n",
              "      <td>178.616028</td>\n",
              "      <td>168.965637</td>\n",
              "      <td>165.789795</td>\n",
              "      <td>163.706543</td>\n",
              "      <td>170.875122</td>\n",
              "      <td>191.370422</td>\n",
              "      <td>203.216415</td>\n",
              "      <td>186.023926</td>\n",
              "      <td>183.208099</td>\n",
              "      <td>159.839737</td>\n",
              "      <td>153.584793</td>\n",
              "      <td>164.399582</td>\n",
              "      <td>168.651413</td>\n",
              "      <td>171.793961</td>\n",
              "      <td>173.876144</td>\n",
              "      <td>175.043701</td>\n",
              "      <td>168.279907</td>\n",
              "      <td>168.427658</td>\n",
              "      <td>169.164398</td>\n",
              "      <td>...</td>\n",
              "      <td>217.807480</td>\n",
              "      <td>218.295502</td>\n",
              "      <td>220.164398</td>\n",
              "      <td>221.306946</td>\n",
              "      <td>221.806442</td>\n",
              "      <td>227.683670</td>\n",
              "      <td>229.475525</td>\n",
              "      <td>226.976044</td>\n",
              "      <td>216.129028</td>\n",
              "      <td>183.295517</td>\n",
              "      <td>162.941727</td>\n",
              "      <td>141.720078</td>\n",
              "      <td>134.442245</td>\n",
              "      <td>140.433914</td>\n",
              "      <td>146.809555</td>\n",
              "      <td>169.185211</td>\n",
              "      <td>166.734634</td>\n",
              "      <td>181.806442</td>\n",
              "      <td>215.172729</td>\n",
              "      <td>235.419342</td>\n",
              "      <td>238.218521</td>\n",
              "      <td>207.824127</td>\n",
              "      <td>217.060349</td>\n",
              "      <td>212.127975</td>\n",
              "      <td>208.786667</td>\n",
              "      <td>210.678467</td>\n",
              "      <td>213.729431</td>\n",
              "      <td>217.153992</td>\n",
              "      <td>219.815811</td>\n",
              "      <td>220.276764</td>\n",
              "      <td>224.460968</td>\n",
              "      <td>229.252838</td>\n",
              "      <td>230.358994</td>\n",
              "      <td>233.447449</td>\n",
              "      <td>235.516129</td>\n",
              "      <td>234.252838</td>\n",
              "      <td>217.985428</td>\n",
              "      <td>181.266373</td>\n",
              "      <td>157.354828</td>\n",
              "      <td>146.783539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>76.329605</td>\n",
              "      <td>76.205719</td>\n",
              "      <td>76.074371</td>\n",
              "      <td>75.973267</td>\n",
              "      <td>70.058441</td>\n",
              "      <td>51.128349</td>\n",
              "      <td>46.416534</td>\n",
              "      <td>50.046169</td>\n",
              "      <td>55.652859</td>\n",
              "      <td>49.906788</td>\n",
              "      <td>16.732487</td>\n",
              "      <td>1.756958</td>\n",
              "      <td>0.761268</td>\n",
              "      <td>1.320111</td>\n",
              "      <td>1.129885</td>\n",
              "      <td>0.326467</td>\n",
              "      <td>0.258090</td>\n",
              "      <td>1.061509</td>\n",
              "      <td>1.910001</td>\n",
              "      <td>1.006209</td>\n",
              "      <td>0.112791</td>\n",
              "      <td>0.471766</td>\n",
              "      <td>1.275185</td>\n",
              "      <td>1.702535</td>\n",
              "      <td>0.893637</td>\n",
              "      <td>0.005479</td>\n",
              "      <td>0.685441</td>\n",
              "      <td>1.488860</td>\n",
              "      <td>90.722771</td>\n",
              "      <td>85.504280</td>\n",
              "      <td>83.879181</td>\n",
              "      <td>84.978745</td>\n",
              "      <td>78.184090</td>\n",
              "      <td>60.031265</td>\n",
              "      <td>46.729927</td>\n",
              "      <td>49.811089</td>\n",
              "      <td>57.640148</td>\n",
              "      <td>51.404049</td>\n",
              "      <td>16.084522</td>\n",
              "      <td>2.052816</td>\n",
              "      <td>...</td>\n",
              "      <td>0.305136</td>\n",
              "      <td>1.108554</td>\n",
              "      <td>1.928483</td>\n",
              "      <td>1.034773</td>\n",
              "      <td>0.159836</td>\n",
              "      <td>0.518811</td>\n",
              "      <td>1.322230</td>\n",
              "      <td>1.749580</td>\n",
              "      <td>0.915480</td>\n",
              "      <td>0.030682</td>\n",
              "      <td>0.732486</td>\n",
              "      <td>1.535905</td>\n",
              "      <td>0.967492</td>\n",
              "      <td>0.047045</td>\n",
              "      <td>0.664110</td>\n",
              "      <td>1.467529</td>\n",
              "      <td>1.604281</td>\n",
              "      <td>0.800862</td>\n",
              "      <td>0.014318</td>\n",
              "      <td>0.863467</td>\n",
              "      <td>1.681204</td>\n",
              "      <td>1.390606</td>\n",
              "      <td>0.587187</td>\n",
              "      <td>0.091460</td>\n",
              "      <td>1.018409</td>\n",
              "      <td>1.876470</td>\n",
              "      <td>1.176930</td>\n",
              "      <td>0.373512</td>\n",
              "      <td>0.305136</td>\n",
              "      <td>1.108554</td>\n",
              "      <td>1.928483</td>\n",
              "      <td>1.034773</td>\n",
              "      <td>0.159836</td>\n",
              "      <td>0.518811</td>\n",
              "      <td>1.322230</td>\n",
              "      <td>1.749580</td>\n",
              "      <td>0.915480</td>\n",
              "      <td>0.030682</td>\n",
              "      <td>0.732486</td>\n",
              "      <td>1.535905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>139.334717</td>\n",
              "      <td>127.101097</td>\n",
              "      <td>116.266037</td>\n",
              "      <td>112.164604</td>\n",
              "      <td>99.742767</td>\n",
              "      <td>93.942146</td>\n",
              "      <td>93.233574</td>\n",
              "      <td>89.178780</td>\n",
              "      <td>83.933937</td>\n",
              "      <td>87.238319</td>\n",
              "      <td>100.358635</td>\n",
              "      <td>99.767639</td>\n",
              "      <td>100.687653</td>\n",
              "      <td>95.447121</td>\n",
              "      <td>75.639404</td>\n",
              "      <td>25.614931</td>\n",
              "      <td>19.730534</td>\n",
              "      <td>22.338879</td>\n",
              "      <td>25.326488</td>\n",
              "      <td>22.763584</td>\n",
              "      <td>16.734434</td>\n",
              "      <td>5.334166</td>\n",
              "      <td>1.671203</td>\n",
              "      <td>0.644580</td>\n",
              "      <td>0.465809</td>\n",
              "      <td>1.655410</td>\n",
              "      <td>0.792297</td>\n",
              "      <td>0.337318</td>\n",
              "      <td>136.015045</td>\n",
              "      <td>123.894020</td>\n",
              "      <td>116.593361</td>\n",
              "      <td>111.947250</td>\n",
              "      <td>102.312782</td>\n",
              "      <td>97.239975</td>\n",
              "      <td>96.657997</td>\n",
              "      <td>103.386124</td>\n",
              "      <td>105.842361</td>\n",
              "      <td>105.797668</td>\n",
              "      <td>102.043854</td>\n",
              "      <td>86.863419</td>\n",
              "      <td>...</td>\n",
              "      <td>138.405182</td>\n",
              "      <td>142.567383</td>\n",
              "      <td>139.174194</td>\n",
              "      <td>122.627075</td>\n",
              "      <td>125.498634</td>\n",
              "      <td>151.869232</td>\n",
              "      <td>127.762306</td>\n",
              "      <td>109.981712</td>\n",
              "      <td>125.004501</td>\n",
              "      <td>135.135635</td>\n",
              "      <td>134.426163</td>\n",
              "      <td>140.447464</td>\n",
              "      <td>143.115540</td>\n",
              "      <td>129.657410</td>\n",
              "      <td>154.585144</td>\n",
              "      <td>175.455673</td>\n",
              "      <td>179.245056</td>\n",
              "      <td>180.150406</td>\n",
              "      <td>168.999588</td>\n",
              "      <td>144.414658</td>\n",
              "      <td>132.646103</td>\n",
              "      <td>123.573112</td>\n",
              "      <td>111.821571</td>\n",
              "      <td>109.290001</td>\n",
              "      <td>111.416031</td>\n",
              "      <td>118.131668</td>\n",
              "      <td>129.698013</td>\n",
              "      <td>140.845917</td>\n",
              "      <td>140.799973</td>\n",
              "      <td>143.439529</td>\n",
              "      <td>143.863358</td>\n",
              "      <td>143.442841</td>\n",
              "      <td>153.647202</td>\n",
              "      <td>132.318405</td>\n",
              "      <td>126.965454</td>\n",
              "      <td>131.834595</td>\n",
              "      <td>136.992264</td>\n",
              "      <td>141.495239</td>\n",
              "      <td>136.131058</td>\n",
              "      <td>140.828781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>141.983994</td>\n",
              "      <td>139.539200</td>\n",
              "      <td>130.657593</td>\n",
              "      <td>122.679993</td>\n",
              "      <td>117.724792</td>\n",
              "      <td>113.006401</td>\n",
              "      <td>111.166397</td>\n",
              "      <td>111.553596</td>\n",
              "      <td>108.587204</td>\n",
              "      <td>105.473602</td>\n",
              "      <td>101.619194</td>\n",
              "      <td>97.716805</td>\n",
              "      <td>88.919998</td>\n",
              "      <td>100.823997</td>\n",
              "      <td>124.004807</td>\n",
              "      <td>122.307198</td>\n",
              "      <td>21.366402</td>\n",
              "      <td>1.339200</td>\n",
              "      <td>1.108800</td>\n",
              "      <td>1.646400</td>\n",
              "      <td>1.160000</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.760000</td>\n",
              "      <td>1.360000</td>\n",
              "      <td>1.760000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>142.617599</td>\n",
              "      <td>136.236816</td>\n",
              "      <td>128.024002</td>\n",
              "      <td>129.724792</td>\n",
              "      <td>124.411194</td>\n",
              "      <td>118.766411</td>\n",
              "      <td>115.995201</td>\n",
              "      <td>115.912003</td>\n",
              "      <td>116.134399</td>\n",
              "      <td>113.332794</td>\n",
              "      <td>108.984001</td>\n",
              "      <td>106.035202</td>\n",
              "      <td>...</td>\n",
              "      <td>9.688000</td>\n",
              "      <td>3.248000</td>\n",
              "      <td>5.147200</td>\n",
              "      <td>4.942400</td>\n",
              "      <td>2.460800</td>\n",
              "      <td>3.395200</td>\n",
              "      <td>1.108800</td>\n",
              "      <td>0.964800</td>\n",
              "      <td>1.966400</td>\n",
              "      <td>1.451200</td>\n",
              "      <td>1.011200</td>\n",
              "      <td>0.238400</td>\n",
              "      <td>126.623993</td>\n",
              "      <td>125.195198</td>\n",
              "      <td>125.507202</td>\n",
              "      <td>127.270386</td>\n",
              "      <td>131.931198</td>\n",
              "      <td>138.204803</td>\n",
              "      <td>134.427200</td>\n",
              "      <td>130.484802</td>\n",
              "      <td>126.388802</td>\n",
              "      <td>122.051201</td>\n",
              "      <td>105.894394</td>\n",
              "      <td>84.856003</td>\n",
              "      <td>92.611198</td>\n",
              "      <td>95.136002</td>\n",
              "      <td>83.449600</td>\n",
              "      <td>88.427200</td>\n",
              "      <td>70.624001</td>\n",
              "      <td>30.425598</td>\n",
              "      <td>13.164800</td>\n",
              "      <td>5.020800</td>\n",
              "      <td>1.320000</td>\n",
              "      <td>5.862400</td>\n",
              "      <td>2.302400</td>\n",
              "      <td>1.718400</td>\n",
              "      <td>2.942400</td>\n",
              "      <td>1.548800</td>\n",
              "      <td>1.011200</td>\n",
              "      <td>0.238400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>155.271622</td>\n",
              "      <td>157.444458</td>\n",
              "      <td>158.185181</td>\n",
              "      <td>142.654327</td>\n",
              "      <td>115.197533</td>\n",
              "      <td>108.012344</td>\n",
              "      <td>109.567902</td>\n",
              "      <td>104.333328</td>\n",
              "      <td>87.827164</td>\n",
              "      <td>89.580254</td>\n",
              "      <td>82.604935</td>\n",
              "      <td>72.456795</td>\n",
              "      <td>73.604935</td>\n",
              "      <td>74.049377</td>\n",
              "      <td>73.024689</td>\n",
              "      <td>75.271606</td>\n",
              "      <td>75.308640</td>\n",
              "      <td>76.790131</td>\n",
              "      <td>42.938274</td>\n",
              "      <td>1.851852</td>\n",
              "      <td>0.716049</td>\n",
              "      <td>0.049383</td>\n",
              "      <td>1.024691</td>\n",
              "      <td>1.913580</td>\n",
              "      <td>1.049383</td>\n",
              "      <td>0.160494</td>\n",
              "      <td>0.604938</td>\n",
              "      <td>1.493827</td>\n",
              "      <td>139.296295</td>\n",
              "      <td>118.209869</td>\n",
              "      <td>93.641968</td>\n",
              "      <td>92.135803</td>\n",
              "      <td>99.419754</td>\n",
              "      <td>104.827164</td>\n",
              "      <td>108.506172</td>\n",
              "      <td>107.716049</td>\n",
              "      <td>104.320984</td>\n",
              "      <td>97.530869</td>\n",
              "      <td>87.888885</td>\n",
              "      <td>79.716057</td>\n",
              "      <td>...</td>\n",
              "      <td>102.395065</td>\n",
              "      <td>46.074074</td>\n",
              "      <td>2.716050</td>\n",
              "      <td>1.407407</td>\n",
              "      <td>0.716049</td>\n",
              "      <td>0.049383</td>\n",
              "      <td>1.024691</td>\n",
              "      <td>1.913580</td>\n",
              "      <td>1.049383</td>\n",
              "      <td>0.160494</td>\n",
              "      <td>0.604938</td>\n",
              "      <td>1.493827</td>\n",
              "      <td>126.901237</td>\n",
              "      <td>124.358032</td>\n",
              "      <td>108.975311</td>\n",
              "      <td>102.432098</td>\n",
              "      <td>98.012344</td>\n",
              "      <td>92.950615</td>\n",
              "      <td>93.024689</td>\n",
              "      <td>90.333336</td>\n",
              "      <td>88.296303</td>\n",
              "      <td>87.604942</td>\n",
              "      <td>87.333336</td>\n",
              "      <td>90.246910</td>\n",
              "      <td>94.851852</td>\n",
              "      <td>96.679016</td>\n",
              "      <td>99.209877</td>\n",
              "      <td>105.901230</td>\n",
              "      <td>106.864197</td>\n",
              "      <td>51.432098</td>\n",
              "      <td>1.555556</td>\n",
              "      <td>1.629630</td>\n",
              "      <td>0.716049</td>\n",
              "      <td>0.049383</td>\n",
              "      <td>1.024691</td>\n",
              "      <td>1.913580</td>\n",
              "      <td>1.049383</td>\n",
              "      <td>0.160494</td>\n",
              "      <td>0.604938</td>\n",
              "      <td>1.493827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>251.530670</td>\n",
              "      <td>235.549911</td>\n",
              "      <td>133.506805</td>\n",
              "      <td>116.611748</td>\n",
              "      <td>127.652458</td>\n",
              "      <td>124.286964</td>\n",
              "      <td>106.144363</td>\n",
              "      <td>83.003525</td>\n",
              "      <td>111.806755</td>\n",
              "      <td>127.545898</td>\n",
              "      <td>136.037338</td>\n",
              "      <td>137.143570</td>\n",
              "      <td>137.347366</td>\n",
              "      <td>133.117935</td>\n",
              "      <td>115.637398</td>\n",
              "      <td>99.149643</td>\n",
              "      <td>81.408424</td>\n",
              "      <td>68.157028</td>\n",
              "      <td>76.877106</td>\n",
              "      <td>137.450729</td>\n",
              "      <td>174.661423</td>\n",
              "      <td>186.258606</td>\n",
              "      <td>194.042130</td>\n",
              "      <td>190.923721</td>\n",
              "      <td>197.036850</td>\n",
              "      <td>201.170181</td>\n",
              "      <td>213.987167</td>\n",
              "      <td>205.021927</td>\n",
              "      <td>235.795700</td>\n",
              "      <td>166.840729</td>\n",
              "      <td>95.781448</td>\n",
              "      <td>108.964897</td>\n",
              "      <td>109.032036</td>\n",
              "      <td>116.873741</td>\n",
              "      <td>117.012177</td>\n",
              "      <td>119.429413</td>\n",
              "      <td>122.890564</td>\n",
              "      <td>128.501846</td>\n",
              "      <td>132.005600</td>\n",
              "      <td>131.120483</td>\n",
              "      <td>...</td>\n",
              "      <td>167.018433</td>\n",
              "      <td>143.398178</td>\n",
              "      <td>192.917160</td>\n",
              "      <td>201.817337</td>\n",
              "      <td>181.840729</td>\n",
              "      <td>164.636139</td>\n",
              "      <td>161.420288</td>\n",
              "      <td>158.338409</td>\n",
              "      <td>149.350891</td>\n",
              "      <td>135.845703</td>\n",
              "      <td>114.334244</td>\n",
              "      <td>103.979492</td>\n",
              "      <td>130.058960</td>\n",
              "      <td>125.148544</td>\n",
              "      <td>113.235535</td>\n",
              "      <td>111.117599</td>\n",
              "      <td>118.582443</td>\n",
              "      <td>105.597977</td>\n",
              "      <td>111.474602</td>\n",
              "      <td>146.281860</td>\n",
              "      <td>161.233444</td>\n",
              "      <td>171.049500</td>\n",
              "      <td>173.812531</td>\n",
              "      <td>166.777435</td>\n",
              "      <td>169.126587</td>\n",
              "      <td>173.416916</td>\n",
              "      <td>178.724884</td>\n",
              "      <td>171.696518</td>\n",
              "      <td>164.071136</td>\n",
              "      <td>151.624725</td>\n",
              "      <td>183.268051</td>\n",
              "      <td>176.004807</td>\n",
              "      <td>164.001923</td>\n",
              "      <td>156.401535</td>\n",
              "      <td>158.008652</td>\n",
              "      <td>155.276077</td>\n",
              "      <td>151.323013</td>\n",
              "      <td>153.649246</td>\n",
              "      <td>147.953857</td>\n",
              "      <td>141.270950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>155.757416</td>\n",
              "      <td>160.831375</td>\n",
              "      <td>167.267776</td>\n",
              "      <td>169.389069</td>\n",
              "      <td>172.486694</td>\n",
              "      <td>172.763336</td>\n",
              "      <td>172.140533</td>\n",
              "      <td>171.337280</td>\n",
              "      <td>170.517761</td>\n",
              "      <td>169.224884</td>\n",
              "      <td>167.474884</td>\n",
              "      <td>164.517761</td>\n",
              "      <td>153.642029</td>\n",
              "      <td>126.112442</td>\n",
              "      <td>91.347649</td>\n",
              "      <td>91.405334</td>\n",
              "      <td>125.071022</td>\n",
              "      <td>145.180481</td>\n",
              "      <td>147.911255</td>\n",
              "      <td>143.338776</td>\n",
              "      <td>157.502975</td>\n",
              "      <td>164.733749</td>\n",
              "      <td>162.918655</td>\n",
              "      <td>163.241150</td>\n",
              "      <td>159.423080</td>\n",
              "      <td>154.599136</td>\n",
              "      <td>147.809189</td>\n",
              "      <td>142.588776</td>\n",
              "      <td>156.556244</td>\n",
              "      <td>159.931976</td>\n",
              "      <td>166.948242</td>\n",
              "      <td>171.115402</td>\n",
              "      <td>173.844696</td>\n",
              "      <td>174.384628</td>\n",
              "      <td>173.560669</td>\n",
              "      <td>173.255936</td>\n",
              "      <td>173.016296</td>\n",
              "      <td>171.563629</td>\n",
              "      <td>169.106506</td>\n",
              "      <td>166.343216</td>\n",
              "      <td>...</td>\n",
              "      <td>168.628723</td>\n",
              "      <td>175.038483</td>\n",
              "      <td>172.789948</td>\n",
              "      <td>169.374283</td>\n",
              "      <td>172.784042</td>\n",
              "      <td>174.031082</td>\n",
              "      <td>162.069534</td>\n",
              "      <td>115.794395</td>\n",
              "      <td>103.810654</td>\n",
              "      <td>105.855042</td>\n",
              "      <td>99.727814</td>\n",
              "      <td>102.828415</td>\n",
              "      <td>122.270721</td>\n",
              "      <td>126.346169</td>\n",
              "      <td>130.439362</td>\n",
              "      <td>134.365387</td>\n",
              "      <td>137.971909</td>\n",
              "      <td>141.903854</td>\n",
              "      <td>143.646469</td>\n",
              "      <td>137.902390</td>\n",
              "      <td>136.415680</td>\n",
              "      <td>135.750015</td>\n",
              "      <td>138.371323</td>\n",
              "      <td>149.933441</td>\n",
              "      <td>158.729309</td>\n",
              "      <td>162.050323</td>\n",
              "      <td>165.023682</td>\n",
              "      <td>166.457123</td>\n",
              "      <td>165.599121</td>\n",
              "      <td>163.323975</td>\n",
              "      <td>165.573975</td>\n",
              "      <td>164.402390</td>\n",
              "      <td>166.079895</td>\n",
              "      <td>165.439362</td>\n",
              "      <td>147.618362</td>\n",
              "      <td>112.621315</td>\n",
              "      <td>113.218948</td>\n",
              "      <td>117.439362</td>\n",
              "      <td>111.976341</td>\n",
              "      <td>104.429001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>217.366211</td>\n",
              "      <td>201.043991</td>\n",
              "      <td>182.412628</td>\n",
              "      <td>133.810928</td>\n",
              "      <td>106.166466</td>\n",
              "      <td>110.829964</td>\n",
              "      <td>106.326981</td>\n",
              "      <td>108.875153</td>\n",
              "      <td>107.009506</td>\n",
              "      <td>104.160522</td>\n",
              "      <td>99.340073</td>\n",
              "      <td>87.593338</td>\n",
              "      <td>78.488701</td>\n",
              "      <td>78.117706</td>\n",
              "      <td>77.650421</td>\n",
              "      <td>124.300827</td>\n",
              "      <td>154.051117</td>\n",
              "      <td>155.152191</td>\n",
              "      <td>150.728897</td>\n",
              "      <td>159.644455</td>\n",
              "      <td>162.575485</td>\n",
              "      <td>160.407852</td>\n",
              "      <td>156.611176</td>\n",
              "      <td>157.492264</td>\n",
              "      <td>154.466095</td>\n",
              "      <td>149.913193</td>\n",
              "      <td>157.955994</td>\n",
              "      <td>162.353149</td>\n",
              "      <td>207.234238</td>\n",
              "      <td>191.995255</td>\n",
              "      <td>155.711060</td>\n",
              "      <td>116.099876</td>\n",
              "      <td>109.714622</td>\n",
              "      <td>110.563614</td>\n",
              "      <td>106.609985</td>\n",
              "      <td>108.856125</td>\n",
              "      <td>106.812126</td>\n",
              "      <td>105.109390</td>\n",
              "      <td>104.253265</td>\n",
              "      <td>93.889420</td>\n",
              "      <td>...</td>\n",
              "      <td>107.302017</td>\n",
              "      <td>116.979782</td>\n",
              "      <td>129.732468</td>\n",
              "      <td>133.136734</td>\n",
              "      <td>138.073715</td>\n",
              "      <td>147.160522</td>\n",
              "      <td>169.118896</td>\n",
              "      <td>185.853760</td>\n",
              "      <td>187.969086</td>\n",
              "      <td>191.378113</td>\n",
              "      <td>199.268738</td>\n",
              "      <td>203.829956</td>\n",
              "      <td>130.158142</td>\n",
              "      <td>133.128418</td>\n",
              "      <td>140.217590</td>\n",
              "      <td>146.449463</td>\n",
              "      <td>146.844223</td>\n",
              "      <td>143.400726</td>\n",
              "      <td>134.407837</td>\n",
              "      <td>131.712234</td>\n",
              "      <td>115.725311</td>\n",
              "      <td>114.038048</td>\n",
              "      <td>108.838287</td>\n",
              "      <td>110.868011</td>\n",
              "      <td>110.513672</td>\n",
              "      <td>110.690842</td>\n",
              "      <td>106.155762</td>\n",
              "      <td>93.631386</td>\n",
              "      <td>104.060638</td>\n",
              "      <td>130.785965</td>\n",
              "      <td>137.920334</td>\n",
              "      <td>132.114151</td>\n",
              "      <td>131.103455</td>\n",
              "      <td>112.060631</td>\n",
              "      <td>125.186676</td>\n",
              "      <td>168.049927</td>\n",
              "      <td>183.313904</td>\n",
              "      <td>185.033295</td>\n",
              "      <td>172.542191</td>\n",
              "      <td>140.494644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>244.699493</td>\n",
              "      <td>243.715530</td>\n",
              "      <td>236.323822</td>\n",
              "      <td>211.522537</td>\n",
              "      <td>184.466354</td>\n",
              "      <td>162.982956</td>\n",
              "      <td>140.733963</td>\n",
              "      <td>113.179977</td>\n",
              "      <td>88.144402</td>\n",
              "      <td>90.697937</td>\n",
              "      <td>105.063919</td>\n",
              "      <td>117.433197</td>\n",
              "      <td>140.134842</td>\n",
              "      <td>156.306717</td>\n",
              "      <td>147.703690</td>\n",
              "      <td>123.383530</td>\n",
              "      <td>126.432701</td>\n",
              "      <td>146.466660</td>\n",
              "      <td>147.480011</td>\n",
              "      <td>105.818649</td>\n",
              "      <td>83.358047</td>\n",
              "      <td>120.374474</td>\n",
              "      <td>132.514847</td>\n",
              "      <td>128.223083</td>\n",
              "      <td>125.816597</td>\n",
              "      <td>115.977806</td>\n",
              "      <td>105.932915</td>\n",
              "      <td>107.555466</td>\n",
              "      <td>141.743561</td>\n",
              "      <td>133.281158</td>\n",
              "      <td>119.392700</td>\n",
              "      <td>109.802780</td>\n",
              "      <td>108.084816</td>\n",
              "      <td>114.691422</td>\n",
              "      <td>120.178436</td>\n",
              "      <td>122.279129</td>\n",
              "      <td>125.826202</td>\n",
              "      <td>127.111969</td>\n",
              "      <td>122.242172</td>\n",
              "      <td>117.174278</td>\n",
              "      <td>...</td>\n",
              "      <td>0.509703</td>\n",
              "      <td>1.022568</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.977122</td>\n",
              "      <td>0.101308</td>\n",
              "      <td>0.001364</td>\n",
              "      <td>0.302065</td>\n",
              "      <td>0.809288</td>\n",
              "      <td>0.666687</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.032984</td>\n",
              "      <td>1.419741</td>\n",
              "      <td>0.038192</td>\n",
              "      <td>1.056482</td>\n",
              "      <td>1.325873</td>\n",
              "      <td>1.295121</td>\n",
              "      <td>0.459421</td>\n",
              "      <td>0.412177</td>\n",
              "      <td>1.261207</td>\n",
              "      <td>1.403683</td>\n",
              "      <td>0.480005</td>\n",
              "      <td>0.027776</td>\n",
              "      <td>1.136400</td>\n",
              "      <td>1.545105</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.976006</td>\n",
              "      <td>0.098952</td>\n",
              "      <td>0.038192</td>\n",
              "      <td>0.824478</td>\n",
              "      <td>1.123256</td>\n",
              "      <td>0.335111</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 784 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0           1           2    ...         781         782         783\n",
              "0   127.186852  109.609833  119.858261  ...    0.556540    0.509481    1.685952\n",
              "1   191.587921  189.490097  176.532776  ...  181.266373  157.354828  146.783539\n",
              "2    76.329605   76.205719   76.074371  ...    0.030682    0.732486    1.535905\n",
              "3   139.334717  127.101097  116.266037  ...  141.495239  136.131058  140.828781\n",
              "4   141.983994  139.539200  130.657593  ...    1.548800    1.011200    0.238400\n",
              "..         ...         ...         ...  ...         ...         ...         ...\n",
              "95  155.271622  157.444458  158.185181  ...    0.160494    0.604938    1.493827\n",
              "96  251.530670  235.549911  133.506805  ...  153.649246  147.953857  141.270950\n",
              "97  155.757416  160.831375  167.267776  ...  117.439362  111.976341  104.429001\n",
              "98  217.366211  201.043991  182.412628  ...  185.033295  172.542191  140.494644\n",
              "99  244.699493  243.715530  236.323822  ...    0.824478    1.123256    0.335111\n",
              "\n",
              "[100 rows x 784 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QBV18nYTaNE"
      },
      "source": [
        "img_graos = []\n",
        "Width_new = []\n",
        "k = 0\n",
        "for i in prediction:\n",
        "  if( i == 0):\n",
        "    img_graos.append(df_ann.iloc[k,:])\n",
        "    Width_new.append(Width.iloc[k])\n",
        "\n",
        "  k = k +1\n",
        "\n",
        "img_graos = pd.DataFrame(img_graos)\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMCLCNQobH-d",
        "outputId": "7789c268-4d0c-424c-fc5a-b2808ad55d38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Width.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "touLevDmbBx5",
        "outputId": "08f6373e-18e6-4ccd-ddab-3e6fdbbad855",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "k"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4RSVgX4UhbC",
        "outputId": "e4e465e7-a217-40ea-af37-817a74772ba1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "img_graos.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "hJI2wQUET0gW",
        "outputId": "697645e7-351d-4231-a557-b197e5512478"
      },
      "source": [
        "'''\n",
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[4] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  #prediction = model.predict_classes(result)\n",
        "  prediction= np.argmax(model.predict(result), axis=-1)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)\n",
        "'''"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ncont = 0; num =25\\nimg_graos = []\\nWidth_new = []\\nimg=ww[4] \\nwhile( cont < num):\\n  df=Segmenta(img)\\n  df_ann =df.copy()\\n  Width = df['Width']\\n  del df_ann['Width']\\n  result = np.array(df_ann)\\n  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\\n  #prediction = model.predict_classes(result)\\n  prediction= np.argmax(model.predict(result), axis=-1)\\n  loc_grao =[];k=0\\n  for i in prediction:\\n    if( i == 0):\\n      img_graos.append(df.iloc[k,:])\\n      Width_new.append(Width.iloc[k])\\n      cont = cont + 1\\n    k = k +1\\nimg_graos = pd.DataFrame(img_graos)\\nprint(img_graos)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv5I61yhPQmk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "746bdf47-8903-4e87-dbd7-ffe67c1d389e"
      },
      "source": [
        "'''\n",
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[4] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  #prediction = model.predict_classes(result)\n",
        "  prediction= np.argmax(model.predict(result), axis=-1)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)\n",
        "'''"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ncont = 0; num =25\\nimg_graos = []\\nWidth_new = []\\nimg=ww[4] \\nwhile( cont < num):\\n  df=Segmenta(img)\\n  df_ann =df.copy()\\n  Width = df['Width']\\n  del df_ann['Width']\\n  result = np.array(df_ann)\\n  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\\n  #prediction = model.predict_classes(result)\\n  prediction= np.argmax(model.predict(result), axis=-1)\\n  loc_grao =[];k=0\\n  for i in prediction:\\n    if( i == 0):\\n      img_graos.append(df.iloc[k,:])\\n      Width_new.append(Width.iloc[k])\\n      cont = cont + 1\\n    k = k +1\\nimg_graos = pd.DataFrame(img_graos)\\nprint(img_graos)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjRbWgmX_LFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "983e2171-77d8-4555-b8e7-da35796eb025"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "#from GetBetterSegm import GetBetter"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'marquesgabi_paper_fev_2021' already exists and is not an empty directory.\n",
            "/content/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAG_I6FwCvFr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "e430c748-4fdc-4aa8-e3e5-975e5b4f63de"
      },
      "source": [
        "\n",
        "!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "#!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd marquesgabi_out_2020\n",
        "#%cd Doutorado\n",
        "#PSD_imageJ = 'Amostra7.csv' \n",
        "#PSD_new = pd.read_csv(PSD_imageJ,sep=';')\n",
        "#encoding='utf8'\n",
        "\n",
        "PSD_imageJ = 'Areas_ImageJ.csv'\n",
        "PSD_new = pd.read_csv(PSD_imageJ)\n",
        "print(PSD_new.head(3))\n",
        "''''''"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'marquesgabi_out_2020' already exists and is not an empty directory.\n",
            "/content/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020\n",
            "   Juntas   Area\n",
            "0       1  2.001\n",
            "1       2  0.820\n",
            "2       3  1.270\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO",
        "outputId": "4b674576-22f6-4571-ab94-423bfee20ea0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        }
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2898\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Width'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-8942ee48d0f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mArea_All\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDiameter_All\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPSDArea\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_graos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/Get_PSDArea_New.py\u001b[0m in \u001b[0;36mPSDArea\u001b[0;34m(df_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mSize_foto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1200\u001b[0m \u001b[0;31m# tamanho da foto (se mudar no big_segment.py tem q alterar aqui)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrame\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mSize_foto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mWidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Width'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mNx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2905\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2906\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2902\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Width'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nO6cSz2dIqb",
        "outputId": "bd42f0e7-e8a9-40a6-c1c8-87e328308b2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "img_graos.shape"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PekBHQOT_6CP"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaZPe_AxNBK9"
      },
      "source": [
        "PSD_new.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "\n",
        "\n",
        "# \n",
        "Area = df_ImgJ['Area'].values\n",
        "# Area = np.concatenate( (Area, [lost_value] ) )\n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79MY9ZHxBW37"
      },
      "source": [
        "len(Diameter_All)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KooHVpH5k2mZ"
      },
      "source": [
        "PSD_new['Area'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPWCPPf7bzsf"
      },
      "source": [
        "#lost_value = float(PSD_new.columns[1])\n",
        "#Area2 = np.array(PSD_new.iloc[:,1])\n",
        "#Area2 = np.concatenate( (Area2, [lost_value] ) )\n",
        "Area2 = PSD_new['Area'].shape\n",
        "for A in Area2:\n",
        "  Diam1.append((4*A/np.pi)**0.5) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfk_fNXGDK5_"
      },
      "source": [
        "wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        "wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        "X = pd.DataFrame([Diam1,Diameter_All])\n",
        "wts = pd.DataFrame([wt1,wt2])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWf2nmnEp6yX"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OieAXw_by3nz"
      },
      "source": [
        "A = plt.hist(X,weights=wts,bins=7)\n",
        "plt.legend(['True','CNN'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WryLryB9ovi3"
      },
      "source": [
        "print('ImgJ:','media=',np.mean(np.array(Diam1)),'desvio=',np.std(np.array(Diam1)),'pontos=',len(Diam1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Nyv-Nopbkc"
      },
      "source": [
        "print('Software:','media=',np.mean(np.array(Diameter_All)),'desvio=',np.std(np.array(Diameter_All)),'pontos=',len(Diameter_All))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpdrvEySy8Ij"
      },
      "source": [
        "np.mean(np.array(Diameter_All))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMK89w-fzCVe"
      },
      "source": [
        "# Freq1 = [19.12043703, 29.22484843, 19.35872174, 20.82190224, 11.47409056] # avarage 4 samples\n",
        "Freq1 = [20.69301557, 28.55598044, 18.50768331, 22.7106327, 8.905907357] # avarage 10 samples\n",
        "#Freq2 = [16.93792791, 31.38008965, 24.93810752, 18.56158392, 6.233810752, 0.4]\n",
        "Freq2 = [16.93792791, 31.38008965, 24.93810752, 18.56158392, 6.633810752]\n",
        "Freq3 = [22.22489, 30.15078, 25.10463, 19.30926, 2.810434]\n",
        "barWidth = 0.25\n",
        "\n",
        "br1 = range(len(Freq1))\n",
        "# Set position of bar on X axis\n",
        "br2 = [x + barWidth for x in br1]\n",
        "br3 = [x + barWidth for x in br2]\n",
        "# labels = [0.8, 1.0, 1.2, 1.4, 1.6, 1.8]\n",
        "labels = [0.8, 1.0, 1.2, 1.4, 1.6]\n",
        "\n",
        "xx=[]\n",
        "for a in labels:\n",
        "  xx.append(str(a))\n",
        "plt.bar(br1, Freq1 , color=\"green\", align=\"center\", width=0.3, tick_label= xx) \n",
        "plt.bar(br2, Freq2 , color=\"red\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.bar(br3, Freq3 , color=\"blue\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.legend(['CNN 1','CNN 2','True'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}