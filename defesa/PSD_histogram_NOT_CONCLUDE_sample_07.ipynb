{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_NOT_CONCLUDES_sample_07.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/defesa/PSD_histogram_NOT_CONCLUDE_sample_07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from sklearn.metrics import r2_score\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsqQZIu-mEfi"
      },
      "source": [
        "Repetir = 40"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwc7AZrXRyqk"
      },
      "source": [
        "# New version change routine inside MarquesGabi_Routines\n",
        "# Try to improve segmentation \n",
        "# New routine is called Segment_Filter_revisited_One... Two,Three, etc\n",
        "# this exemple threshold 0.4\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZEvJvfoibE4"
      },
      "source": [
        "#!pip install mahotas"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UZ30b1EVQhq"
      },
      "source": [
        "def BlackWhite(Transfere,Size):\n",
        "\n",
        "  img_name=[]\n",
        "  xw=[]\n",
        "  ww=[]\n",
        "\n",
        "  with zipfile.ZipFile(Transfere, \"r\") as f:\n",
        "    for name in f.namelist():\n",
        "      img_name.append(name)\n",
        "      #xw.append(cv2.imread(name))\n",
        "      xw.append(cv2.resize(cv2.imread(name),(Size,Size)))\n",
        "\n",
        "  nrow=len(img_name)\n",
        "  ncol=Size*Size\n",
        "  pw=np.zeros((nrow,ncol))\n",
        "  #pw=[]\n",
        "  for i in range(nrow):\n",
        "    ww.append(cv2.cvtColor(np.array(xw[i]), cv2.COLOR_BGR2GRAY))\n",
        "    pw[i,:]=ww[i].ravel()\n",
        "  return ww,img_name"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v7SRrc8mH2N",
        "outputId": "0c4288e0-bf31-4e46-ca3e-b537e93960d0"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip' \n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Doutorado' already exists and is not an empty directory.\n",
            "/content/Doutorado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqIYzUcnrdMp",
        "outputId": "14e596aa-6e4d-48d3-c267-ebc61bc77f2c"
      },
      "source": [
        "labels =[]\n",
        "with zipfile.ZipFile(Transfere, \"r\") as f:\n",
        "  for f in f.namelist():\n",
        "    labels.append(f)\n",
        "print(labels)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Fotos_Grandes-3cdAmostra/Q6-8-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-4.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "ww,img_name=BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHgqAnaFyCjp",
        "outputId": "c2b20304-41d9-4c83-918d-637c2800eb5e"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'MarquesGabi_Routines' already exists and is not an empty directory.\n",
            "/content/Doutorado/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample7 =[2, 5, 17] # \n",
        "# [2, 5, 17] sample 7---  [4,13,25] sample 3----[0, 3, 9] sample 8"
      ],
      "metadata": {
        "id": "yTmAAni65HgX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDEGUiuubwuZ"
      },
      "source": [
        "FILE='SugarSample07.zip'\n",
        "img_name=[]\n",
        "x_original = [] \n",
        "\n",
        "data_file ='xls'\n",
        "\n",
        "\n",
        "file_name = zipfile.ZipFile(FILE, 'r')\n",
        "file_name.extractall()\n",
        "\n",
        "k = 0\n",
        "with zipfile.ZipFile(FILE, \"r\") as f:\n",
        "    for name in f.namelist():\n",
        "      if(name[-3:] == data_file):\n",
        "        #df =pd.read_csv(name)\n",
        "        if( k > 0):\n",
        "          df_old = df_ImgJ.copy()\n",
        "        df_ImgJ = pd.read_excel(name)\n",
        "        df_ImgJ = df_ImgJ.drop(labels=[0], axis=0)\n",
        "        if(k > 0):\n",
        "          df_ImgJ = pd.concat( [df_ImgJ, df_old], ignore_index = True)\n",
        "        k = k + 1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbQ0tal0etXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fb89460-0c25-4afe-b3e3-e71edf3222e3"
      },
      "source": [
        "f.namelist()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Amostra 07 foto 02.csv', 'Amostra 07 foto 03.csv', 'Amostra 07 foto 04.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KDJn09lGTBD"
      },
      "source": [
        "#df_ImgJ.head()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMBJ6C-YdF3q"
      },
      "source": [
        "#df_ImgJ.head()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from Segment_Filter_Revival import Segmenta  # got image provided segmented"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR2emP4rNjQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bac8bc79-f6b9-4389-d8a8-b42e4607b48c"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 204, done.\u001b[K\n",
            "remote: Counting objects: 100% (204/204), done.\u001b[K\n",
            "remote: Compressing objects: 100% (202/202), done.\u001b[K\n",
            "remote: Total 204 (delta 87), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (204/204), 211.76 MiB | 19.84 MiB/s, done.\n",
            "Resolving deltas: 100% (87/87), done.\n",
            "Checking out files: 100% (53/53), done.\n",
            "/content/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIFPGE_-vx3T"
      },
      "source": [
        "Img_Size = 28"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "Description = '3 layers of Convolution: 64, 128, 256 '\n",
        "N1 = 20\n",
        "N2 = 20\n",
        "\n",
        "# make the CNN\n",
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=64, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=N1))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=N2))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpbPQ1FSRG6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36e46046-c401-40ad-ccaa-628e8c6428b1"
      },
      "source": [
        "# training the model\n",
        "r = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), \n",
        "              epochs=200, batch_size=32)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 2s 109ms/step - loss: 0.5780 - accuracy: 0.7464 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 0.3271 - accuracy: 0.8601 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 0.1936 - accuracy: 0.9271 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 0.1279 - accuracy: 0.9534 - val_loss: 0.6936 - val_accuracy: 0.5102\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 0.0922 - accuracy: 0.9679 - val_loss: 0.6945 - val_accuracy: 0.5102\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 0.0469 - accuracy: 0.9825 - val_loss: 0.6924 - val_accuracy: 0.5102\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 0.0657 - accuracy: 0.9767 - val_loss: 0.6963 - val_accuracy: 0.5102\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 1s 86ms/step - loss: 0.0576 - accuracy: 0.9767 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 0.0464 - accuracy: 0.9883 - val_loss: 0.6965 - val_accuracy: 0.5102\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 0.0207 - accuracy: 0.9942 - val_loss: 0.6971 - val_accuracy: 0.5102\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 0.0175 - accuracy: 0.9942 - val_loss: 0.6956 - val_accuracy: 0.5102\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 0.0159 - accuracy: 0.9942 - val_loss: 0.7015 - val_accuracy: 0.5102\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 0.0176 - accuracy: 0.9942 - val_loss: 0.7085 - val_accuracy: 0.5102\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 0.0118 - accuracy: 0.9971 - val_loss: 0.7088 - val_accuracy: 0.5102\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.7068 - val_accuracy: 0.5102\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.7070 - val_accuracy: 0.5102\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 1s 86ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.7019 - val_accuracy: 0.5102\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.7032 - val_accuracy: 0.5102\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.7031 - val_accuracy: 0.5102\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.6972 - val_accuracy: 0.5102\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6917 - val_accuracy: 0.5102\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6981 - val_accuracy: 0.5102\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.7148 - val_accuracy: 0.5102\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.7243 - val_accuracy: 0.5102\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.7132 - val_accuracy: 0.5102\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 6.6735e-04 - accuracy: 1.0000 - val_loss: 0.7026 - val_accuracy: 0.5102\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 6.2319e-04 - accuracy: 1.0000 - val_loss: 0.7012 - val_accuracy: 0.5102\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.6953 - val_accuracy: 0.5102\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7017 - val_accuracy: 0.5102\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7128 - val_accuracy: 0.5102\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 5.8417e-04 - accuracy: 1.0000 - val_loss: 0.7220 - val_accuracy: 0.5102\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 9.2739e-04 - accuracy: 1.0000 - val_loss: 0.7074 - val_accuracy: 0.5102\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 3.9633e-04 - accuracy: 1.0000 - val_loss: 0.6952 - val_accuracy: 0.5102\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.7236 - val_accuracy: 0.5102\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 1s 91ms/step - loss: 5.6195e-04 - accuracy: 1.0000 - val_loss: 0.8576 - val_accuracy: 0.5102\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.5102\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.7380 - val_accuracy: 0.5102\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.8897 - val_accuracy: 0.5102\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 1s 86ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.8457 - val_accuracy: 0.5102\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 8.2749e-04 - accuracy: 1.0000 - val_loss: 0.8550 - val_accuracy: 0.5102\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 4.4915e-04 - accuracy: 1.0000 - val_loss: 0.8373 - val_accuracy: 0.5102\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 1s 91ms/step - loss: 4.7078e-04 - accuracy: 1.0000 - val_loss: 0.8109 - val_accuracy: 0.5102\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 3.1216e-04 - accuracy: 1.0000 - val_loss: 0.8051 - val_accuracy: 0.5102\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 5.8436e-04 - accuracy: 1.0000 - val_loss: 0.8439 - val_accuracy: 0.5102\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 8.1239e-04 - accuracy: 1.0000 - val_loss: 0.8905 - val_accuracy: 0.5102\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 1.6698e-04 - accuracy: 1.0000 - val_loss: 0.6945 - val_accuracy: 0.5102\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 8.0510e-04 - accuracy: 1.0000 - val_loss: 1.0670 - val_accuracy: 0.5102\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 8.0095e-04 - accuracy: 1.0000 - val_loss: 1.6082 - val_accuracy: 0.5102\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 8.1602e-04 - accuracy: 1.0000 - val_loss: 1.9088 - val_accuracy: 0.5102\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 1s 86ms/step - loss: 3.9278e-04 - accuracy: 1.0000 - val_loss: 2.2974 - val_accuracy: 0.5102\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 7.9085e-04 - accuracy: 1.0000 - val_loss: 2.2603 - val_accuracy: 0.5102\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 1.0486e-04 - accuracy: 1.0000 - val_loss: 2.1701 - val_accuracy: 0.5102\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 1.5142e-04 - accuracy: 1.0000 - val_loss: 2.2021 - val_accuracy: 0.5102\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 5.6670e-04 - accuracy: 1.0000 - val_loss: 2.2199 - val_accuracy: 0.5102\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 3.2851e-04 - accuracy: 1.0000 - val_loss: 1.9775 - val_accuracy: 0.5102\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 7.2406e-04 - accuracy: 1.0000 - val_loss: 2.0969 - val_accuracy: 0.5102\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 2.0419e-04 - accuracy: 1.0000 - val_loss: 2.0894 - val_accuracy: 0.5102\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 1.8354e-04 - accuracy: 1.0000 - val_loss: 2.1127 - val_accuracy: 0.5102\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 1.7802e-04 - accuracy: 1.0000 - val_loss: 2.2371 - val_accuracy: 0.5102\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 2.3289e-04 - accuracy: 1.0000 - val_loss: 2.3475 - val_accuracy: 0.5102\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 1.7456e-04 - accuracy: 1.0000 - val_loss: 2.3597 - val_accuracy: 0.5102\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 1.9015e-04 - accuracy: 1.0000 - val_loss: 2.2503 - val_accuracy: 0.5102\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 2.3037e-04 - accuracy: 1.0000 - val_loss: 2.1340 - val_accuracy: 0.5102\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 2.2282e-04 - accuracy: 1.0000 - val_loss: 1.7968 - val_accuracy: 0.5170\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 2.9692e-04 - accuracy: 1.0000 - val_loss: 2.2055 - val_accuracy: 0.5102\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 1.8221e-04 - accuracy: 1.0000 - val_loss: 2.4846 - val_accuracy: 0.5102\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 1.5459e-04 - accuracy: 1.0000 - val_loss: 2.6863 - val_accuracy: 0.5102\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 5.7274e-05 - accuracy: 1.0000 - val_loss: 2.7053 - val_accuracy: 0.5102\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 4.5441e-04 - accuracy: 1.0000 - val_loss: 1.9647 - val_accuracy: 0.5442\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 2.3711e-04 - accuracy: 1.0000 - val_loss: 2.5387 - val_accuracy: 0.5102\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 5.6884e-04 - accuracy: 1.0000 - val_loss: 4.0588 - val_accuracy: 0.5102\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 8.9264e-04 - accuracy: 1.0000 - val_loss: 6.3324 - val_accuracy: 0.5102\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 2.3628e-04 - accuracy: 1.0000 - val_loss: 6.1285 - val_accuracy: 0.5102\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 1.2259e-04 - accuracy: 1.0000 - val_loss: 5.6226 - val_accuracy: 0.5102\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 6.3843e-05 - accuracy: 1.0000 - val_loss: 5.3632 - val_accuracy: 0.5102\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 6.9986e-05 - accuracy: 1.0000 - val_loss: 5.0258 - val_accuracy: 0.5102\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 3.6839e-05 - accuracy: 1.0000 - val_loss: 4.6444 - val_accuracy: 0.5102\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 1s 91ms/step - loss: 1.0539e-04 - accuracy: 1.0000 - val_loss: 4.9105 - val_accuracy: 0.5102\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 4.2406e-04 - accuracy: 1.0000 - val_loss: 4.2737 - val_accuracy: 0.5102\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 4.7945e-04 - accuracy: 1.0000 - val_loss: 2.9757 - val_accuracy: 0.5102\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 3.0868e-05 - accuracy: 1.0000 - val_loss: 2.7255 - val_accuracy: 0.5306\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 1.9253e-04 - accuracy: 1.0000 - val_loss: 2.4925 - val_accuracy: 0.5510\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 1s 91ms/step - loss: 1.4728e-04 - accuracy: 1.0000 - val_loss: 2.2472 - val_accuracy: 0.5782\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 4.1381e-05 - accuracy: 1.0000 - val_loss: 1.8731 - val_accuracy: 0.6122\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 1.8636e-04 - accuracy: 1.0000 - val_loss: 2.5657 - val_accuracy: 0.5646\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 4.3018e-05 - accuracy: 1.0000 - val_loss: 2.8092 - val_accuracy: 0.5170\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 7.7297e-05 - accuracy: 1.0000 - val_loss: 2.5911 - val_accuracy: 0.5442\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 2.0199e-04 - accuracy: 1.0000 - val_loss: 1.4578 - val_accuracy: 0.6871\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 2.7881e-04 - accuracy: 1.0000 - val_loss: 1.4147 - val_accuracy: 0.6803\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 1s 86ms/step - loss: 5.6726e-05 - accuracy: 1.0000 - val_loss: 1.5919 - val_accuracy: 0.6395\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 3.3105e-04 - accuracy: 1.0000 - val_loss: 2.4600 - val_accuracy: 0.5442\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 2.9589e-04 - accuracy: 1.0000 - val_loss: 4.2044 - val_accuracy: 0.5102\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 2.8259e-05 - accuracy: 1.0000 - val_loss: 4.8139 - val_accuracy: 0.5102\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 6.5473e-05 - accuracy: 1.0000 - val_loss: 4.5369 - val_accuracy: 0.5102\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 1.1712e-04 - accuracy: 1.0000 - val_loss: 4.1599 - val_accuracy: 0.5102\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 1.0006e-04 - accuracy: 1.0000 - val_loss: 3.3959 - val_accuracy: 0.5102\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 1.1968e-04 - accuracy: 1.0000 - val_loss: 4.1564 - val_accuracy: 0.5102\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 1.1468e-04 - accuracy: 1.0000 - val_loss: 5.0421 - val_accuracy: 0.5102\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 6.9909e-05 - accuracy: 1.0000 - val_loss: 5.3964 - val_accuracy: 0.5102\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 1.4657e-04 - accuracy: 1.0000 - val_loss: 5.7705 - val_accuracy: 0.5102\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 5.0631e-05 - accuracy: 1.0000 - val_loss: 5.6333 - val_accuracy: 0.5102\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 2.9730e-05 - accuracy: 1.0000 - val_loss: 5.3377 - val_accuracy: 0.5102\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 5.5388e-05 - accuracy: 1.0000 - val_loss: 4.9368 - val_accuracy: 0.5102\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 4.2459e-05 - accuracy: 1.0000 - val_loss: 4.2433 - val_accuracy: 0.5102\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 3.0562e-04 - accuracy: 1.0000 - val_loss: 3.8707 - val_accuracy: 0.5170\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 2.7652e-04 - accuracy: 1.0000 - val_loss: 6.0034 - val_accuracy: 0.5102\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 5.3715e-05 - accuracy: 1.0000 - val_loss: 6.0145 - val_accuracy: 0.5102\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 5.6087e-05 - accuracy: 1.0000 - val_loss: 5.5156 - val_accuracy: 0.5102\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 9.3962e-05 - accuracy: 1.0000 - val_loss: 5.2919 - val_accuracy: 0.5102\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 2.7011e-05 - accuracy: 1.0000 - val_loss: 5.1677 - val_accuracy: 0.5102\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 1.0750e-04 - accuracy: 1.0000 - val_loss: 4.4687 - val_accuracy: 0.5102\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 4.6884e-05 - accuracy: 1.0000 - val_loss: 3.8661 - val_accuracy: 0.5170\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 3.5630e-05 - accuracy: 1.0000 - val_loss: 3.1903 - val_accuracy: 0.5238\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 4.9355e-05 - accuracy: 1.0000 - val_loss: 2.3709 - val_accuracy: 0.5510\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 6.7437e-05 - accuracy: 1.0000 - val_loss: 0.9982 - val_accuracy: 0.7551\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 2.1980e-05 - accuracy: 1.0000 - val_loss: 0.3391 - val_accuracy: 0.8980\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 2.2826e-05 - accuracy: 1.0000 - val_loss: 0.1720 - val_accuracy: 0.9388\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 6.5886e-05 - accuracy: 1.0000 - val_loss: 0.5152 - val_accuracy: 0.8571\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 3.1757e-05 - accuracy: 1.0000 - val_loss: 0.9704 - val_accuracy: 0.7483\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 4.9303e-05 - accuracy: 1.0000 - val_loss: 1.0369 - val_accuracy: 0.7415\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 1s 92ms/step - loss: 3.1239e-05 - accuracy: 1.0000 - val_loss: 1.0817 - val_accuracy: 0.7483\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 2.5417e-04 - accuracy: 1.0000 - val_loss: 0.6427 - val_accuracy: 0.8231\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 4.3177e-05 - accuracy: 1.0000 - val_loss: 0.8568 - val_accuracy: 0.7687\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 5.3084e-05 - accuracy: 1.0000 - val_loss: 0.7885 - val_accuracy: 0.7619\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 1s 86ms/step - loss: 4.9061e-05 - accuracy: 1.0000 - val_loss: 0.8564 - val_accuracy: 0.7483\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 3.4954e-05 - accuracy: 1.0000 - val_loss: 0.8204 - val_accuracy: 0.7755\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 1.6289e-05 - accuracy: 1.0000 - val_loss: 0.6327 - val_accuracy: 0.8163\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 2.6059e-04 - accuracy: 1.0000 - val_loss: 1.2990 - val_accuracy: 0.7551\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 3.3402e-05 - accuracy: 1.0000 - val_loss: 2.8579 - val_accuracy: 0.6599\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 5.3977e-05 - accuracy: 1.0000 - val_loss: 2.4415 - val_accuracy: 0.6871\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 1.1687e-04 - accuracy: 1.0000 - val_loss: 2.5054 - val_accuracy: 0.6939\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 1.4425e-05 - accuracy: 1.0000 - val_loss: 2.8639 - val_accuracy: 0.6667\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 9.9399e-05 - accuracy: 1.0000 - val_loss: 2.1847 - val_accuracy: 0.7211\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 1.0838e-04 - accuracy: 1.0000 - val_loss: 0.6675 - val_accuracy: 0.8571\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 1s 91ms/step - loss: 3.4999e-05 - accuracy: 1.0000 - val_loss: 0.3147 - val_accuracy: 0.9184\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 6.4628e-05 - accuracy: 1.0000 - val_loss: 0.2171 - val_accuracy: 0.9456\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 1.5557e-05 - accuracy: 1.0000 - val_loss: 0.2229 - val_accuracy: 0.9592\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 1s 91ms/step - loss: 2.9275e-05 - accuracy: 1.0000 - val_loss: 0.2503 - val_accuracy: 0.9388\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 1.5496e-05 - accuracy: 1.0000 - val_loss: 0.2890 - val_accuracy: 0.9456\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 3.4488e-05 - accuracy: 1.0000 - val_loss: 0.3124 - val_accuracy: 0.9320\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 1.6604e-05 - accuracy: 1.0000 - val_loss: 0.3211 - val_accuracy: 0.9252\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 1.5452e-05 - accuracy: 1.0000 - val_loss: 0.3226 - val_accuracy: 0.9252\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 1s 91ms/step - loss: 5.7544e-05 - accuracy: 1.0000 - val_loss: 0.2706 - val_accuracy: 0.9524\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 1.0298e-05 - accuracy: 1.0000 - val_loss: 0.2405 - val_accuracy: 0.9660\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 1s 91ms/step - loss: 3.6277e-05 - accuracy: 1.0000 - val_loss: 0.6950 - val_accuracy: 0.8707\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 7.5423e-06 - accuracy: 1.0000 - val_loss: 0.7732 - val_accuracy: 0.8503\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 1s 91ms/step - loss: 3.7951e-05 - accuracy: 1.0000 - val_loss: 0.5998 - val_accuracy: 0.8707\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 3.9380e-05 - accuracy: 1.0000 - val_loss: 0.4665 - val_accuracy: 0.8844\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 1s 91ms/step - loss: 1.0864e-04 - accuracy: 1.0000 - val_loss: 0.5241 - val_accuracy: 0.8844\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 1.3465e-05 - accuracy: 1.0000 - val_loss: 0.2926 - val_accuracy: 0.8980\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 1s 91ms/step - loss: 8.6713e-04 - accuracy: 1.0000 - val_loss: 2.3622 - val_accuracy: 0.6122\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 0.1841 - accuracy: 0.9504 - val_loss: 822.2769 - val_accuracy: 0.5102\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 0.2563 - accuracy: 0.9213 - val_loss: 830.7523 - val_accuracy: 0.5102\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 0.1201 - accuracy: 0.9446 - val_loss: 865.3509 - val_accuracy: 0.5102\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 0.0879 - accuracy: 0.9475 - val_loss: 184.1036 - val_accuracy: 0.5102\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 0.1572 - accuracy: 0.9417 - val_loss: 1419.3301 - val_accuracy: 0.5102\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 0.0837 - accuracy: 0.9854 - val_loss: 1071.7068 - val_accuracy: 0.5102\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 0.1022 - accuracy: 0.9738 - val_loss: 845.0439 - val_accuracy: 0.5102\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 0.0223 - accuracy: 0.9883 - val_loss: 813.5744 - val_accuracy: 0.5102\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 0.0395 - accuracy: 0.9854 - val_loss: 575.7769 - val_accuracy: 0.5102\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 0.0100 - accuracy: 0.9971 - val_loss: 432.2245 - val_accuracy: 0.5102\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 354.5503 - val_accuracy: 0.5102\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 0.0046 - accuracy: 0.9971 - val_loss: 270.1829 - val_accuracy: 0.5102\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 1s 92ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 220.3555 - val_accuracy: 0.5102\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 196.8487 - val_accuracy: 0.5102\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 7.1076e-04 - accuracy: 1.0000 - val_loss: 175.0607 - val_accuracy: 0.5102\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 0.0063 - accuracy: 0.9971 - val_loss: 145.3181 - val_accuracy: 0.5102\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 0.0037 - accuracy: 0.9971 - val_loss: 141.9774 - val_accuracy: 0.5102\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 1s 91ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 133.5748 - val_accuracy: 0.5102\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 0.0053 - accuracy: 0.9971 - val_loss: 123.7366 - val_accuracy: 0.5102\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 123.9053 - val_accuracy: 0.5102\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 107.8865 - val_accuracy: 0.5102\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 5.3317e-04 - accuracy: 1.0000 - val_loss: 95.0241 - val_accuracy: 0.5102\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 1s 94ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 78.6645 - val_accuracy: 0.5102\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 1s 95ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 67.8624 - val_accuracy: 0.5102\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 8.8052e-04 - accuracy: 1.0000 - val_loss: 57.6786 - val_accuracy: 0.5102\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 55.5553 - val_accuracy: 0.5102\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 4.2781e-04 - accuracy: 1.0000 - val_loss: 49.0055 - val_accuracy: 0.5102\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 2.8114e-04 - accuracy: 1.0000 - val_loss: 43.8498 - val_accuracy: 0.5102\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 9.2865e-04 - accuracy: 1.0000 - val_loss: 39.7574 - val_accuracy: 0.5102\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 1s 91ms/step - loss: 4.7087e-04 - accuracy: 1.0000 - val_loss: 37.4072 - val_accuracy: 0.5102\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 1s 91ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 34.3174 - val_accuracy: 0.5102\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 1s 94ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 33.1586 - val_accuracy: 0.5102\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 1s 94ms/step - loss: 3.0524e-04 - accuracy: 1.0000 - val_loss: 29.0518 - val_accuracy: 0.5102\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 1s 94ms/step - loss: 8.1224e-04 - accuracy: 1.0000 - val_loss: 25.6959 - val_accuracy: 0.5102\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 30.8432 - val_accuracy: 0.5102\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 3.0796e-04 - accuracy: 1.0000 - val_loss: 73.8912 - val_accuracy: 0.5102\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 2.5004e-04 - accuracy: 1.0000 - val_loss: 80.6848 - val_accuracy: 0.5102\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 1s 91ms/step - loss: 3.5885e-04 - accuracy: 1.0000 - val_loss: 76.9189 - val_accuracy: 0.5102\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 1s 94ms/step - loss: 1.0732e-04 - accuracy: 1.0000 - val_loss: 70.4730 - val_accuracy: 0.5102\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 61.4330 - val_accuracy: 0.5102\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 52.4545 - val_accuracy: 0.5102\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 3.3358e-04 - accuracy: 1.0000 - val_loss: 45.0395 - val_accuracy: 0.5102\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 9.9127e-05 - accuracy: 1.0000 - val_loss: 39.6891 - val_accuracy: 0.5102\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 2.0166e-04 - accuracy: 1.0000 - val_loss: 34.5137 - val_accuracy: 0.5102\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 1.3049e-04 - accuracy: 1.0000 - val_loss: 30.4117 - val_accuracy: 0.5102\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 1s 87ms/step - loss: 4.1961e-04 - accuracy: 1.0000 - val_loss: 26.5659 - val_accuracy: 0.5102\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 1s 89ms/step - loss: 1.2019e-04 - accuracy: 1.0000 - val_loss: 23.0410 - val_accuracy: 0.5102\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 1s 90ms/step - loss: 6.4154e-05 - accuracy: 1.0000 - val_loss: 20.0506 - val_accuracy: 0.5102\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 1s 91ms/step - loss: 2.4644e-04 - accuracy: 1.0000 - val_loss: 17.4965 - val_accuracy: 0.5102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDVY6HbxMOlH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1811b84-f34c-407f-e844-c001d005ef7b"
      },
      "source": [
        "# pred_test= model.predict_classes(X_test)\n",
        "pred_test = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict   1\n",
            "Actual     \n",
            "0        72\n",
            "1        75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7pT2q7traXg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da126764-1b6a-4900-b005-058694c11336"
      },
      "source": [
        "print(METRICS)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        72\n",
            "           1       0.51      1.00      0.68        75\n",
            "\n",
            "    accuracy                           0.51       147\n",
            "   macro avg       0.26      0.50      0.34       147\n",
            "weighted avg       0.26      0.51      0.34       147\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iFNNrlWV9tH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59fae03d-06a9-4617-a5b3-92705c53daa3"
      },
      "source": [
        "pred_test"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5pq5z8DHeJ0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d2a8d22c-3245-494a-f2a4-051ed180eb0e"
      },
      "source": [
        "'''\n",
        "img=ww[4] \n",
        "df=Segmenta(img)\n",
        "df.shape\n",
        "'''"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nimg=ww[4] \\ndf=Segmenta(img)\\ndf.shape\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8ofkAXlS-0F"
      },
      "source": [
        "Sample3 =[4,13,25]\n",
        "Sample7 =[2, 5, 17]\n",
        "Sample8 =[0, 3, 9]\n",
        "\n",
        "for i in range(Repetir):\n",
        "  k = 0\n",
        "  for i in Sample7:\n",
        "    img=ww[i]\n",
        "    if( k > 0):\n",
        "      df_old = df_ann.copy()\n",
        "    df_ann=Segmenta(img)\n",
        "    if(k > 0):\n",
        "      df_ann = pd.concat( [df_ann, df_old], ignore_index = True)\n",
        "    k = k + 1\n",
        "#df_ann = df.copy\n",
        "\n",
        "df_teste = np.array(df_ann)\n",
        "names = df_ann.columns\n",
        "df_teste = pd.DataFrame(df_teste,columns=names)\n",
        "Width = df_ann['Width']\n",
        "#del df_ann['Width']\n",
        "names = df_ann.columns\n",
        "del df_ann['Width']\n",
        "result = np.array(df_ann)\n",
        "result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "prediction= np.argmax(model.predict(result), axis=-1)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "_kfx72YkUYiz",
        "outputId": "0351b650-092b-486f-fcae-b4434463a082"
      },
      "source": [
        "df_ann"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>133.780487</td>\n",
              "      <td>139.054779</td>\n",
              "      <td>148.504379</td>\n",
              "      <td>156.894989</td>\n",
              "      <td>160.327606</td>\n",
              "      <td>163.155197</td>\n",
              "      <td>162.033829</td>\n",
              "      <td>159.855255</td>\n",
              "      <td>157.407410</td>\n",
              "      <td>156.817841</td>\n",
              "      <td>156.586334</td>\n",
              "      <td>157.247375</td>\n",
              "      <td>159.295593</td>\n",
              "      <td>162.879623</td>\n",
              "      <td>164.445282</td>\n",
              "      <td>162.881699</td>\n",
              "      <td>162.039764</td>\n",
              "      <td>160.774506</td>\n",
              "      <td>161.235397</td>\n",
              "      <td>162.183685</td>\n",
              "      <td>163.894547</td>\n",
              "      <td>162.736771</td>\n",
              "      <td>161.385391</td>\n",
              "      <td>155.506485</td>\n",
              "      <td>145.198990</td>\n",
              "      <td>128.919052</td>\n",
              "      <td>103.835670</td>\n",
              "      <td>96.422874</td>\n",
              "      <td>154.140549</td>\n",
              "      <td>163.170746</td>\n",
              "      <td>165.158768</td>\n",
              "      <td>164.935608</td>\n",
              "      <td>163.582687</td>\n",
              "      <td>163.669724</td>\n",
              "      <td>160.773895</td>\n",
              "      <td>156.256973</td>\n",
              "      <td>153.078003</td>\n",
              "      <td>152.965759</td>\n",
              "      <td>151.516006</td>\n",
              "      <td>152.703247</td>\n",
              "      <td>...</td>\n",
              "      <td>126.890427</td>\n",
              "      <td>127.961617</td>\n",
              "      <td>128.829071</td>\n",
              "      <td>132.199799</td>\n",
              "      <td>135.682617</td>\n",
              "      <td>137.332458</td>\n",
              "      <td>137.724091</td>\n",
              "      <td>140.557083</td>\n",
              "      <td>143.407181</td>\n",
              "      <td>147.370941</td>\n",
              "      <td>146.395630</td>\n",
              "      <td>145.415939</td>\n",
              "      <td>151.343750</td>\n",
              "      <td>147.706161</td>\n",
              "      <td>137.474335</td>\n",
              "      <td>133.594208</td>\n",
              "      <td>127.212700</td>\n",
              "      <td>126.105637</td>\n",
              "      <td>126.329559</td>\n",
              "      <td>126.105255</td>\n",
              "      <td>127.266258</td>\n",
              "      <td>126.926361</td>\n",
              "      <td>127.473679</td>\n",
              "      <td>131.226746</td>\n",
              "      <td>130.931793</td>\n",
              "      <td>131.933258</td>\n",
              "      <td>131.762802</td>\n",
              "      <td>134.047516</td>\n",
              "      <td>136.807083</td>\n",
              "      <td>132.696838</td>\n",
              "      <td>133.059174</td>\n",
              "      <td>137.314011</td>\n",
              "      <td>141.810928</td>\n",
              "      <td>143.918137</td>\n",
              "      <td>144.662689</td>\n",
              "      <td>144.655273</td>\n",
              "      <td>145.958817</td>\n",
              "      <td>149.133606</td>\n",
              "      <td>147.197723</td>\n",
              "      <td>144.587006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>133.091415</td>\n",
              "      <td>133.601105</td>\n",
              "      <td>138.099731</td>\n",
              "      <td>141.313019</td>\n",
              "      <td>141.074799</td>\n",
              "      <td>141.202225</td>\n",
              "      <td>139.299179</td>\n",
              "      <td>138.988922</td>\n",
              "      <td>137.559555</td>\n",
              "      <td>139.393356</td>\n",
              "      <td>139.108032</td>\n",
              "      <td>136.770081</td>\n",
              "      <td>136.191132</td>\n",
              "      <td>145.160660</td>\n",
              "      <td>140.614960</td>\n",
              "      <td>119.518005</td>\n",
              "      <td>112.537399</td>\n",
              "      <td>113.590034</td>\n",
              "      <td>121.975067</td>\n",
              "      <td>134.113571</td>\n",
              "      <td>139.717453</td>\n",
              "      <td>143.329651</td>\n",
              "      <td>147.770081</td>\n",
              "      <td>151.000000</td>\n",
              "      <td>150.313019</td>\n",
              "      <td>146.789474</td>\n",
              "      <td>144.144043</td>\n",
              "      <td>138.609406</td>\n",
              "      <td>128.409973</td>\n",
              "      <td>128.742401</td>\n",
              "      <td>134.193909</td>\n",
              "      <td>136.232681</td>\n",
              "      <td>137.756241</td>\n",
              "      <td>138.781174</td>\n",
              "      <td>136.473694</td>\n",
              "      <td>136.252075</td>\n",
              "      <td>135.986145</td>\n",
              "      <td>137.221619</td>\n",
              "      <td>137.905823</td>\n",
              "      <td>137.839340</td>\n",
              "      <td>...</td>\n",
              "      <td>104.027718</td>\n",
              "      <td>103.806099</td>\n",
              "      <td>106.858727</td>\n",
              "      <td>110.138504</td>\n",
              "      <td>113.036018</td>\n",
              "      <td>114.709145</td>\n",
              "      <td>115.831024</td>\n",
              "      <td>116.036011</td>\n",
              "      <td>114.578949</td>\n",
              "      <td>113.736839</td>\n",
              "      <td>114.376740</td>\n",
              "      <td>115.213295</td>\n",
              "      <td>117.440437</td>\n",
              "      <td>115.573410</td>\n",
              "      <td>116.445976</td>\n",
              "      <td>118.684212</td>\n",
              "      <td>124.479233</td>\n",
              "      <td>126.379501</td>\n",
              "      <td>126.639893</td>\n",
              "      <td>125.814400</td>\n",
              "      <td>128.016617</td>\n",
              "      <td>132.116348</td>\n",
              "      <td>133.952911</td>\n",
              "      <td>133.670364</td>\n",
              "      <td>131.182831</td>\n",
              "      <td>120.340721</td>\n",
              "      <td>102.019394</td>\n",
              "      <td>94.789474</td>\n",
              "      <td>95.462601</td>\n",
              "      <td>99.950142</td>\n",
              "      <td>104.925217</td>\n",
              "      <td>110.997231</td>\n",
              "      <td>114.094185</td>\n",
              "      <td>116.232681</td>\n",
              "      <td>117.393349</td>\n",
              "      <td>118.495850</td>\n",
              "      <td>117.487534</td>\n",
              "      <td>117.720215</td>\n",
              "      <td>117.168976</td>\n",
              "      <td>118.271461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>118.114456</td>\n",
              "      <td>130.618973</td>\n",
              "      <td>154.452820</td>\n",
              "      <td>148.034912</td>\n",
              "      <td>139.226654</td>\n",
              "      <td>129.245544</td>\n",
              "      <td>122.104858</td>\n",
              "      <td>122.306656</td>\n",
              "      <td>138.920135</td>\n",
              "      <td>140.462601</td>\n",
              "      <td>134.781433</td>\n",
              "      <td>127.309402</td>\n",
              "      <td>119.142212</td>\n",
              "      <td>113.295219</td>\n",
              "      <td>105.751259</td>\n",
              "      <td>99.234261</td>\n",
              "      <td>100.941620</td>\n",
              "      <td>100.975769</td>\n",
              "      <td>101.026978</td>\n",
              "      <td>100.209724</td>\n",
              "      <td>87.884010</td>\n",
              "      <td>25.969667</td>\n",
              "      <td>6.574913</td>\n",
              "      <td>5.191586</td>\n",
              "      <td>1.094041</td>\n",
              "      <td>0.569730</td>\n",
              "      <td>0.431794</td>\n",
              "      <td>0.231824</td>\n",
              "      <td>118.032158</td>\n",
              "      <td>124.633293</td>\n",
              "      <td>151.367020</td>\n",
              "      <td>157.777618</td>\n",
              "      <td>147.900314</td>\n",
              "      <td>137.451004</td>\n",
              "      <td>126.448708</td>\n",
              "      <td>130.677338</td>\n",
              "      <td>145.158813</td>\n",
              "      <td>153.801392</td>\n",
              "      <td>151.837067</td>\n",
              "      <td>149.195404</td>\n",
              "      <td>...</td>\n",
              "      <td>107.773514</td>\n",
              "      <td>111.151352</td>\n",
              "      <td>113.998779</td>\n",
              "      <td>115.787529</td>\n",
              "      <td>117.716522</td>\n",
              "      <td>117.406807</td>\n",
              "      <td>115.193123</td>\n",
              "      <td>113.473854</td>\n",
              "      <td>109.821831</td>\n",
              "      <td>110.640900</td>\n",
              "      <td>112.710861</td>\n",
              "      <td>116.311531</td>\n",
              "      <td>120.376923</td>\n",
              "      <td>111.440788</td>\n",
              "      <td>109.161102</td>\n",
              "      <td>108.006714</td>\n",
              "      <td>109.122543</td>\n",
              "      <td>101.608139</td>\n",
              "      <td>96.809784</td>\n",
              "      <td>108.770454</td>\n",
              "      <td>108.974846</td>\n",
              "      <td>105.119339</td>\n",
              "      <td>103.961136</td>\n",
              "      <td>102.709496</td>\n",
              "      <td>102.295380</td>\n",
              "      <td>104.746231</td>\n",
              "      <td>106.662857</td>\n",
              "      <td>112.354668</td>\n",
              "      <td>116.728249</td>\n",
              "      <td>117.455276</td>\n",
              "      <td>119.118134</td>\n",
              "      <td>119.109589</td>\n",
              "      <td>120.447487</td>\n",
              "      <td>119.681145</td>\n",
              "      <td>119.048019</td>\n",
              "      <td>117.376312</td>\n",
              "      <td>116.429352</td>\n",
              "      <td>114.061737</td>\n",
              "      <td>112.469742</td>\n",
              "      <td>114.073158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>128.468262</td>\n",
              "      <td>123.642456</td>\n",
              "      <td>122.788589</td>\n",
              "      <td>122.366013</td>\n",
              "      <td>123.572311</td>\n",
              "      <td>127.942383</td>\n",
              "      <td>129.524597</td>\n",
              "      <td>130.412460</td>\n",
              "      <td>131.054291</td>\n",
              "      <td>131.080658</td>\n",
              "      <td>132.417252</td>\n",
              "      <td>135.083847</td>\n",
              "      <td>137.652985</td>\n",
              "      <td>140.084915</td>\n",
              "      <td>146.958435</td>\n",
              "      <td>145.210114</td>\n",
              "      <td>118.292374</td>\n",
              "      <td>108.265381</td>\n",
              "      <td>112.145706</td>\n",
              "      <td>119.932495</td>\n",
              "      <td>126.450401</td>\n",
              "      <td>131.546585</td>\n",
              "      <td>134.300766</td>\n",
              "      <td>149.793167</td>\n",
              "      <td>90.547012</td>\n",
              "      <td>73.117966</td>\n",
              "      <td>75.298218</td>\n",
              "      <td>74.494308</td>\n",
              "      <td>126.788383</td>\n",
              "      <td>121.950241</td>\n",
              "      <td>121.557861</td>\n",
              "      <td>120.383667</td>\n",
              "      <td>121.564247</td>\n",
              "      <td>127.649681</td>\n",
              "      <td>130.371216</td>\n",
              "      <td>129.799637</td>\n",
              "      <td>132.095001</td>\n",
              "      <td>133.452438</td>\n",
              "      <td>135.124863</td>\n",
              "      <td>137.007538</td>\n",
              "      <td>...</td>\n",
              "      <td>8.464129</td>\n",
              "      <td>4.047933</td>\n",
              "      <td>1.656605</td>\n",
              "      <td>1.279094</td>\n",
              "      <td>1.453396</td>\n",
              "      <td>2.000744</td>\n",
              "      <td>14.566478</td>\n",
              "      <td>37.312992</td>\n",
              "      <td>52.053875</td>\n",
              "      <td>66.349342</td>\n",
              "      <td>74.567543</td>\n",
              "      <td>81.040482</td>\n",
              "      <td>0.244022</td>\n",
              "      <td>0.701031</td>\n",
              "      <td>0.293230</td>\n",
              "      <td>0.129132</td>\n",
              "      <td>0.793602</td>\n",
              "      <td>1.040174</td>\n",
              "      <td>1.384632</td>\n",
              "      <td>5.276225</td>\n",
              "      <td>15.006694</td>\n",
              "      <td>24.375381</td>\n",
              "      <td>27.935169</td>\n",
              "      <td>12.698054</td>\n",
              "      <td>2.222765</td>\n",
              "      <td>0.869168</td>\n",
              "      <td>0.672547</td>\n",
              "      <td>0.744181</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.717292</td>\n",
              "      <td>0.333404</td>\n",
              "      <td>0.393772</td>\n",
              "      <td>0.271549</td>\n",
              "      <td>0.711340</td>\n",
              "      <td>0.726751</td>\n",
              "      <td>1.129982</td>\n",
              "      <td>1.601126</td>\n",
              "      <td>6.416303</td>\n",
              "      <td>18.555212</td>\n",
              "      <td>33.284931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>105.529854</td>\n",
              "      <td>99.345276</td>\n",
              "      <td>93.104691</td>\n",
              "      <td>92.752991</td>\n",
              "      <td>93.831955</td>\n",
              "      <td>93.853081</td>\n",
              "      <td>94.919205</td>\n",
              "      <td>96.100098</td>\n",
              "      <td>96.234161</td>\n",
              "      <td>97.845734</td>\n",
              "      <td>100.147850</td>\n",
              "      <td>101.799820</td>\n",
              "      <td>105.305786</td>\n",
              "      <td>110.222229</td>\n",
              "      <td>112.531685</td>\n",
              "      <td>113.360886</td>\n",
              "      <td>114.842056</td>\n",
              "      <td>113.954094</td>\n",
              "      <td>117.462814</td>\n",
              "      <td>137.447220</td>\n",
              "      <td>144.022049</td>\n",
              "      <td>144.453644</td>\n",
              "      <td>144.967865</td>\n",
              "      <td>140.790649</td>\n",
              "      <td>139.747482</td>\n",
              "      <td>138.424255</td>\n",
              "      <td>135.231415</td>\n",
              "      <td>132.887985</td>\n",
              "      <td>110.352615</td>\n",
              "      <td>105.287430</td>\n",
              "      <td>100.825531</td>\n",
              "      <td>97.992661</td>\n",
              "      <td>96.609741</td>\n",
              "      <td>96.686874</td>\n",
              "      <td>99.759422</td>\n",
              "      <td>100.850334</td>\n",
              "      <td>99.608810</td>\n",
              "      <td>99.360878</td>\n",
              "      <td>100.905426</td>\n",
              "      <td>103.882469</td>\n",
              "      <td>...</td>\n",
              "      <td>98.711670</td>\n",
              "      <td>107.714424</td>\n",
              "      <td>115.704323</td>\n",
              "      <td>122.116623</td>\n",
              "      <td>123.023888</td>\n",
              "      <td>124.556488</td>\n",
              "      <td>124.845734</td>\n",
              "      <td>124.857674</td>\n",
              "      <td>126.361801</td>\n",
              "      <td>127.301193</td>\n",
              "      <td>132.763092</td>\n",
              "      <td>138.926544</td>\n",
              "      <td>93.725441</td>\n",
              "      <td>94.384758</td>\n",
              "      <td>92.659332</td>\n",
              "      <td>91.554634</td>\n",
              "      <td>90.670341</td>\n",
              "      <td>92.382919</td>\n",
              "      <td>94.002762</td>\n",
              "      <td>94.545464</td>\n",
              "      <td>93.553726</td>\n",
              "      <td>99.482101</td>\n",
              "      <td>99.187325</td>\n",
              "      <td>91.829208</td>\n",
              "      <td>37.998165</td>\n",
              "      <td>53.457306</td>\n",
              "      <td>85.835632</td>\n",
              "      <td>94.012863</td>\n",
              "      <td>100.442619</td>\n",
              "      <td>110.474754</td>\n",
              "      <td>119.363647</td>\n",
              "      <td>122.218552</td>\n",
              "      <td>123.433441</td>\n",
              "      <td>124.575760</td>\n",
              "      <td>123.905426</td>\n",
              "      <td>124.461914</td>\n",
              "      <td>124.897163</td>\n",
              "      <td>126.353539</td>\n",
              "      <td>132.542709</td>\n",
              "      <td>140.015610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>173.533524</td>\n",
              "      <td>163.296967</td>\n",
              "      <td>166.376984</td>\n",
              "      <td>156.545715</td>\n",
              "      <td>146.064529</td>\n",
              "      <td>141.653748</td>\n",
              "      <td>143.174835</td>\n",
              "      <td>148.404602</td>\n",
              "      <td>152.806366</td>\n",
              "      <td>154.981857</td>\n",
              "      <td>153.808304</td>\n",
              "      <td>153.872223</td>\n",
              "      <td>153.789871</td>\n",
              "      <td>151.128708</td>\n",
              "      <td>138.049637</td>\n",
              "      <td>120.164963</td>\n",
              "      <td>111.605324</td>\n",
              "      <td>117.970215</td>\n",
              "      <td>128.884705</td>\n",
              "      <td>136.421082</td>\n",
              "      <td>153.651306</td>\n",
              "      <td>164.151886</td>\n",
              "      <td>164.938522</td>\n",
              "      <td>155.640503</td>\n",
              "      <td>144.858551</td>\n",
              "      <td>142.374649</td>\n",
              "      <td>137.863159</td>\n",
              "      <td>134.692825</td>\n",
              "      <td>172.594833</td>\n",
              "      <td>160.095490</td>\n",
              "      <td>152.564148</td>\n",
              "      <td>143.985565</td>\n",
              "      <td>140.275009</td>\n",
              "      <td>140.623657</td>\n",
              "      <td>141.779144</td>\n",
              "      <td>145.377594</td>\n",
              "      <td>149.650818</td>\n",
              "      <td>150.513107</td>\n",
              "      <td>150.227997</td>\n",
              "      <td>150.423843</td>\n",
              "      <td>...</td>\n",
              "      <td>144.560654</td>\n",
              "      <td>144.667557</td>\n",
              "      <td>146.369217</td>\n",
              "      <td>148.034973</td>\n",
              "      <td>150.020096</td>\n",
              "      <td>151.337051</td>\n",
              "      <td>153.793655</td>\n",
              "      <td>160.936935</td>\n",
              "      <td>168.044189</td>\n",
              "      <td>168.403107</td>\n",
              "      <td>161.512909</td>\n",
              "      <td>152.466385</td>\n",
              "      <td>130.528961</td>\n",
              "      <td>115.413528</td>\n",
              "      <td>75.529739</td>\n",
              "      <td>80.667442</td>\n",
              "      <td>111.556549</td>\n",
              "      <td>115.381355</td>\n",
              "      <td>110.409668</td>\n",
              "      <td>109.371277</td>\n",
              "      <td>113.100311</td>\n",
              "      <td>124.390671</td>\n",
              "      <td>135.678528</td>\n",
              "      <td>141.097076</td>\n",
              "      <td>144.775909</td>\n",
              "      <td>149.688232</td>\n",
              "      <td>150.897522</td>\n",
              "      <td>146.756332</td>\n",
              "      <td>146.612091</td>\n",
              "      <td>146.403381</td>\n",
              "      <td>146.740280</td>\n",
              "      <td>148.010803</td>\n",
              "      <td>149.176544</td>\n",
              "      <td>150.601028</td>\n",
              "      <td>152.131210</td>\n",
              "      <td>159.017822</td>\n",
              "      <td>164.625931</td>\n",
              "      <td>166.672867</td>\n",
              "      <td>164.878204</td>\n",
              "      <td>157.384933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>124.496620</td>\n",
              "      <td>123.467072</td>\n",
              "      <td>123.239227</td>\n",
              "      <td>122.734428</td>\n",
              "      <td>123.160912</td>\n",
              "      <td>123.277672</td>\n",
              "      <td>128.892502</td>\n",
              "      <td>132.505157</td>\n",
              "      <td>130.745117</td>\n",
              "      <td>126.849770</td>\n",
              "      <td>121.359909</td>\n",
              "      <td>112.741905</td>\n",
              "      <td>109.912071</td>\n",
              "      <td>107.024918</td>\n",
              "      <td>104.056610</td>\n",
              "      <td>101.769676</td>\n",
              "      <td>97.684235</td>\n",
              "      <td>95.932007</td>\n",
              "      <td>95.301170</td>\n",
              "      <td>94.197578</td>\n",
              "      <td>95.720551</td>\n",
              "      <td>98.846214</td>\n",
              "      <td>104.907791</td>\n",
              "      <td>108.897118</td>\n",
              "      <td>110.540405</td>\n",
              "      <td>110.027771</td>\n",
              "      <td>107.435043</td>\n",
              "      <td>105.524750</td>\n",
              "      <td>125.463509</td>\n",
              "      <td>124.479538</td>\n",
              "      <td>124.310783</td>\n",
              "      <td>123.659309</td>\n",
              "      <td>124.030258</td>\n",
              "      <td>124.241013</td>\n",
              "      <td>130.564972</td>\n",
              "      <td>132.528671</td>\n",
              "      <td>128.647568</td>\n",
              "      <td>122.567810</td>\n",
              "      <td>114.410828</td>\n",
              "      <td>106.879326</td>\n",
              "      <td>...</td>\n",
              "      <td>163.134918</td>\n",
              "      <td>161.923828</td>\n",
              "      <td>164.791397</td>\n",
              "      <td>169.785339</td>\n",
              "      <td>175.521545</td>\n",
              "      <td>180.034897</td>\n",
              "      <td>182.215027</td>\n",
              "      <td>178.793900</td>\n",
              "      <td>167.814880</td>\n",
              "      <td>126.991104</td>\n",
              "      <td>120.438232</td>\n",
              "      <td>119.987534</td>\n",
              "      <td>157.091858</td>\n",
              "      <td>152.933075</td>\n",
              "      <td>154.964752</td>\n",
              "      <td>159.060165</td>\n",
              "      <td>163.956223</td>\n",
              "      <td>167.731232</td>\n",
              "      <td>169.874695</td>\n",
              "      <td>173.475266</td>\n",
              "      <td>178.983978</td>\n",
              "      <td>180.640808</td>\n",
              "      <td>177.328964</td>\n",
              "      <td>173.687073</td>\n",
              "      <td>169.820938</td>\n",
              "      <td>170.168396</td>\n",
              "      <td>172.279114</td>\n",
              "      <td>172.466385</td>\n",
              "      <td>168.247772</td>\n",
              "      <td>162.952652</td>\n",
              "      <td>164.783554</td>\n",
              "      <td>168.409042</td>\n",
              "      <td>174.985413</td>\n",
              "      <td>178.996078</td>\n",
              "      <td>179.381973</td>\n",
              "      <td>173.080093</td>\n",
              "      <td>157.177643</td>\n",
              "      <td>117.708084</td>\n",
              "      <td>114.993591</td>\n",
              "      <td>116.546463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>96.793625</td>\n",
              "      <td>101.904510</td>\n",
              "      <td>105.550362</td>\n",
              "      <td>100.221878</td>\n",
              "      <td>99.628288</td>\n",
              "      <td>97.211128</td>\n",
              "      <td>84.160950</td>\n",
              "      <td>75.179558</td>\n",
              "      <td>71.256104</td>\n",
              "      <td>66.004395</td>\n",
              "      <td>51.999771</td>\n",
              "      <td>36.345707</td>\n",
              "      <td>26.546888</td>\n",
              "      <td>70.684242</td>\n",
              "      <td>103.949715</td>\n",
              "      <td>111.561935</td>\n",
              "      <td>108.089615</td>\n",
              "      <td>102.458443</td>\n",
              "      <td>98.391258</td>\n",
              "      <td>89.634872</td>\n",
              "      <td>84.964859</td>\n",
              "      <td>85.711517</td>\n",
              "      <td>84.225464</td>\n",
              "      <td>84.985550</td>\n",
              "      <td>85.537987</td>\n",
              "      <td>68.355423</td>\n",
              "      <td>6.109030</td>\n",
              "      <td>0.579142</td>\n",
              "      <td>102.103828</td>\n",
              "      <td>103.086617</td>\n",
              "      <td>100.164993</td>\n",
              "      <td>97.369537</td>\n",
              "      <td>96.890862</td>\n",
              "      <td>92.011909</td>\n",
              "      <td>81.123375</td>\n",
              "      <td>73.750031</td>\n",
              "      <td>70.528389</td>\n",
              "      <td>66.970757</td>\n",
              "      <td>61.517868</td>\n",
              "      <td>51.717430</td>\n",
              "      <td>...</td>\n",
              "      <td>66.690483</td>\n",
              "      <td>67.465607</td>\n",
              "      <td>67.046364</td>\n",
              "      <td>67.681587</td>\n",
              "      <td>68.283508</td>\n",
              "      <td>65.128914</td>\n",
              "      <td>67.898712</td>\n",
              "      <td>71.887863</td>\n",
              "      <td>68.669563</td>\n",
              "      <td>63.698586</td>\n",
              "      <td>53.820110</td>\n",
              "      <td>7.490809</td>\n",
              "      <td>146.698822</td>\n",
              "      <td>160.006943</td>\n",
              "      <td>172.748322</td>\n",
              "      <td>188.026352</td>\n",
              "      <td>197.619858</td>\n",
              "      <td>175.496368</td>\n",
              "      <td>144.079559</td>\n",
              "      <td>147.791199</td>\n",
              "      <td>141.030182</td>\n",
              "      <td>122.693268</td>\n",
              "      <td>122.042671</td>\n",
              "      <td>122.638344</td>\n",
              "      <td>124.636848</td>\n",
              "      <td>124.523537</td>\n",
              "      <td>90.711067</td>\n",
              "      <td>49.290787</td>\n",
              "      <td>55.221416</td>\n",
              "      <td>64.118629</td>\n",
              "      <td>69.016312</td>\n",
              "      <td>71.946815</td>\n",
              "      <td>71.549545</td>\n",
              "      <td>70.240723</td>\n",
              "      <td>69.244774</td>\n",
              "      <td>70.793854</td>\n",
              "      <td>65.013176</td>\n",
              "      <td>62.593712</td>\n",
              "      <td>45.698929</td>\n",
              "      <td>3.332177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.173238</td>\n",
              "      <td>0.085663</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.224041</td>\n",
              "      <td>0.500372</td>\n",
              "      <td>0.730365</td>\n",
              "      <td>1.747263</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.173451</td>\n",
              "      <td>0.536295</td>\n",
              "      <td>0.708258</td>\n",
              "      <td>1.354023</td>\n",
              "      <td>1.180784</td>\n",
              "      <td>1.334042</td>\n",
              "      <td>0.962058</td>\n",
              "      <td>0.622595</td>\n",
              "      <td>0.754809</td>\n",
              "      <td>...</td>\n",
              "      <td>152.447861</td>\n",
              "      <td>154.562225</td>\n",
              "      <td>136.853851</td>\n",
              "      <td>123.747681</td>\n",
              "      <td>130.670731</td>\n",
              "      <td>135.139206</td>\n",
              "      <td>137.194595</td>\n",
              "      <td>136.102448</td>\n",
              "      <td>133.871796</td>\n",
              "      <td>130.042725</td>\n",
              "      <td>126.241356</td>\n",
              "      <td>122.154427</td>\n",
              "      <td>123.228065</td>\n",
              "      <td>122.885628</td>\n",
              "      <td>121.934311</td>\n",
              "      <td>123.025169</td>\n",
              "      <td>127.698463</td>\n",
              "      <td>131.255066</td>\n",
              "      <td>137.279938</td>\n",
              "      <td>148.605042</td>\n",
              "      <td>147.386200</td>\n",
              "      <td>117.261650</td>\n",
              "      <td>131.938339</td>\n",
              "      <td>143.301926</td>\n",
              "      <td>148.930695</td>\n",
              "      <td>149.910828</td>\n",
              "      <td>152.620560</td>\n",
              "      <td>164.954285</td>\n",
              "      <td>185.280045</td>\n",
              "      <td>191.263885</td>\n",
              "      <td>187.956192</td>\n",
              "      <td>151.954071</td>\n",
              "      <td>137.762238</td>\n",
              "      <td>136.213501</td>\n",
              "      <td>133.948547</td>\n",
              "      <td>133.980865</td>\n",
              "      <td>131.396301</td>\n",
              "      <td>125.617599</td>\n",
              "      <td>118.645432</td>\n",
              "      <td>118.309799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>102.123459</td>\n",
              "      <td>103.111115</td>\n",
              "      <td>100.135803</td>\n",
              "      <td>97.395065</td>\n",
              "      <td>92.172836</td>\n",
              "      <td>85.308640</td>\n",
              "      <td>83.827156</td>\n",
              "      <td>84.345680</td>\n",
              "      <td>87.086426</td>\n",
              "      <td>91.037041</td>\n",
              "      <td>96.469139</td>\n",
              "      <td>101.012352</td>\n",
              "      <td>103.790123</td>\n",
              "      <td>107.098770</td>\n",
              "      <td>107.283951</td>\n",
              "      <td>105.530861</td>\n",
              "      <td>103.728394</td>\n",
              "      <td>97.950623</td>\n",
              "      <td>93.913582</td>\n",
              "      <td>72.037041</td>\n",
              "      <td>58.222221</td>\n",
              "      <td>55.827164</td>\n",
              "      <td>52.333328</td>\n",
              "      <td>47.913582</td>\n",
              "      <td>47.691360</td>\n",
              "      <td>50.876541</td>\n",
              "      <td>55.308647</td>\n",
              "      <td>60.740742</td>\n",
              "      <td>102.716057</td>\n",
              "      <td>106.469131</td>\n",
              "      <td>103.197525</td>\n",
              "      <td>97.592598</td>\n",
              "      <td>90.641975</td>\n",
              "      <td>86.493835</td>\n",
              "      <td>86.148148</td>\n",
              "      <td>85.037033</td>\n",
              "      <td>87.753090</td>\n",
              "      <td>92.543213</td>\n",
              "      <td>98.419754</td>\n",
              "      <td>102.765434</td>\n",
              "      <td>...</td>\n",
              "      <td>138.802475</td>\n",
              "      <td>141.123459</td>\n",
              "      <td>145.666672</td>\n",
              "      <td>144.037033</td>\n",
              "      <td>140.962967</td>\n",
              "      <td>138.543213</td>\n",
              "      <td>136.777786</td>\n",
              "      <td>143.790131</td>\n",
              "      <td>148.320999</td>\n",
              "      <td>152.444443</td>\n",
              "      <td>154.037033</td>\n",
              "      <td>152.876526</td>\n",
              "      <td>140.888885</td>\n",
              "      <td>137.827164</td>\n",
              "      <td>135.666672</td>\n",
              "      <td>135.123459</td>\n",
              "      <td>134.308640</td>\n",
              "      <td>130.802475</td>\n",
              "      <td>125.049385</td>\n",
              "      <td>122.580254</td>\n",
              "      <td>122.074074</td>\n",
              "      <td>122.148155</td>\n",
              "      <td>122.666672</td>\n",
              "      <td>123.086418</td>\n",
              "      <td>127.012344</td>\n",
              "      <td>133.283951</td>\n",
              "      <td>138.555573</td>\n",
              "      <td>144.950623</td>\n",
              "      <td>150.555542</td>\n",
              "      <td>153.222229</td>\n",
              "      <td>157.222229</td>\n",
              "      <td>154.061737</td>\n",
              "      <td>145.925934</td>\n",
              "      <td>138.419754</td>\n",
              "      <td>136.135803</td>\n",
              "      <td>141.222229</td>\n",
              "      <td>144.864197</td>\n",
              "      <td>151.604935</td>\n",
              "      <td>153.938278</td>\n",
              "      <td>152.086426</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows × 784 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0           1           2    ...         781         782         783\n",
              "0    133.780487  139.054779  148.504379  ...  149.133606  147.197723  144.587006\n",
              "1    133.091415  133.601105  138.099731  ...  117.720215  117.168976  118.271461\n",
              "2    118.114456  130.618973  154.452820  ...  114.061737  112.469742  114.073158\n",
              "3    128.468262  123.642456  122.788589  ...    6.416303   18.555212   33.284931\n",
              "4    105.529854   99.345276   93.104691  ...  126.353539  132.542709  140.015610\n",
              "..          ...         ...         ...  ...         ...         ...         ...\n",
              "145  173.533524  163.296967  166.376984  ...  166.672867  164.878204  157.384933\n",
              "146  124.496620  123.467072  123.239227  ...  117.708084  114.993591  116.546463\n",
              "147   96.793625  101.904510  105.550362  ...   62.593712   45.698929    3.332177\n",
              "148    0.000000    0.000000    0.000000  ...  125.617599  118.645432  118.309799\n",
              "149  102.123459  103.111115  100.135803  ...  151.604935  153.938278  152.086426\n",
              "\n",
              "[150 rows x 784 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31usb3UnY7lD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd992190-9dd5-4058-b77a-baea68527c9f"
      },
      "source": [
        "df_teste.shape # por que esta saindo 100 ???????"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 785)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVHUUaL8XXs-"
      },
      "source": [
        "#df_ann"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QBV18nYTaNE"
      },
      "source": [
        "img_graos = []\n",
        "Width_new = []\n",
        "k = 0\n",
        "for i in prediction:\n",
        "  if( i == 0):\n",
        "    img_graos.append(df_teste.iloc[k,:])\n",
        "    Width_new.append(Width.iloc[k])\n",
        "\n",
        "  k = k +1\n",
        "\n",
        "img_graos = pd.DataFrame(img_graos, columns=names )\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMCLCNQobH-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27706b2e-1eaf-4228-82d6-411138722595"
      },
      "source": [
        "img_graos.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 785)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "touLevDmbBx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "486a117d-88a2-4587-b6b1-d9f09d74e37b"
      },
      "source": [
        "prediction"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4RSVgX4UhbC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d515bd2-0e7e-43a6-e139-52d34be567e5"
      },
      "source": [
        "img_graos.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 785)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjRbWgmX_LFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09a5d21f-3645-4915-ff08-cb5966db024b"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_Revival import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "#from GetBetterSegm import GetBetter"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marquesgabi_paper_fev_2021'...\n",
            "remote: Enumerating objects: 843, done.\u001b[K\n",
            "remote: Counting objects: 100% (604/604), done.\u001b[K\n",
            "remote: Compressing objects: 100% (602/602), done.\u001b[K\n",
            "remote: Total 843 (delta 395), reused 0 (delta 0), pack-reused 239\u001b[K\n",
            "Receiving objects: 100% (843/843), 6.29 MiB | 8.27 MiB/s, done.\n",
            "Resolving deltas: 100% (532/532), done.\n",
            "/content/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAG_I6FwCvFr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "90f24c71-f28f-4575-aa61-474044382e9c"
      },
      "source": [
        "\n",
        "#!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "#%cd marquesgabi_out_2020\n",
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "#PSD_imageJ = 'Amostra7.csv' \n",
        "#PSD_new = pd.read_csv(PSD_imageJ,sep=';')\n",
        "#encoding='utf8'\n",
        "\n",
        "#PSD_imageJ = 'Areas_ImageJ.csv'\n",
        "PSD_imageJ = 'Amostra7.csv' \n",
        "PSD_new = pd.read_csv(PSD_imageJ)\n",
        "print(PSD_new.head(3))\n",
        "''''''"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Doutorado'...\n",
            "remote: Enumerating objects: 464, done.\u001b[K\n",
            "remote: Counting objects: 100% (214/214), done.\u001b[K\n",
            "remote: Compressing objects: 100% (210/210), done.\u001b[K\n",
            "remote: Total 464 (delta 102), reused 4 (delta 3), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (464/464), 166.12 MiB | 24.37 MiB/s, done.\n",
            "Resolving deltas: 100% (225/225), done.\n",
            "/content/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/Doutorado\n",
            "     ;Area\n",
            "0  1;1.387\n",
            "1  2;1.626\n",
            "2  3;1.336\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB3gOFPeDtoH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "def62b29-57e1-4422-84f2-874af8e7a500"
      },
      "source": [
        "Width.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkB2l0K0YEjz",
        "outputId": "e1169e6e-8afc-4aeb-e84a-b6c4d8e8d885"
      },
      "source": [
        "Width"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([194])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nO6cSz2dIqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c9b1b2b-ca43-4fee-b62a-b10d31cde4f5"
      },
      "source": [
        "img_graos.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 785)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PekBHQOT_6CP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "b9c90a6b-a33f-44a9-b4eb-ee1cbcfd6f9a"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>194.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.173238</td>\n",
              "      <td>0.085663</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.224041</td>\n",
              "      <td>0.500372</td>\n",
              "      <td>0.730365</td>\n",
              "      <td>1.747263</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.173451</td>\n",
              "      <td>0.536295</td>\n",
              "      <td>0.708258</td>\n",
              "      <td>1.354023</td>\n",
              "      <td>1.180784</td>\n",
              "      <td>1.334042</td>\n",
              "      <td>0.962058</td>\n",
              "      <td>0.622595</td>\n",
              "      <td>...</td>\n",
              "      <td>152.447861</td>\n",
              "      <td>154.562225</td>\n",
              "      <td>136.853851</td>\n",
              "      <td>123.747681</td>\n",
              "      <td>130.670731</td>\n",
              "      <td>135.139206</td>\n",
              "      <td>137.194595</td>\n",
              "      <td>136.102448</td>\n",
              "      <td>133.871796</td>\n",
              "      <td>130.042725</td>\n",
              "      <td>126.241356</td>\n",
              "      <td>122.154427</td>\n",
              "      <td>123.228065</td>\n",
              "      <td>122.885628</td>\n",
              "      <td>121.934311</td>\n",
              "      <td>123.025169</td>\n",
              "      <td>127.698463</td>\n",
              "      <td>131.255066</td>\n",
              "      <td>137.279938</td>\n",
              "      <td>148.605042</td>\n",
              "      <td>147.3862</td>\n",
              "      <td>117.26165</td>\n",
              "      <td>131.938339</td>\n",
              "      <td>143.301926</td>\n",
              "      <td>148.930695</td>\n",
              "      <td>149.910828</td>\n",
              "      <td>152.62056</td>\n",
              "      <td>164.954285</td>\n",
              "      <td>185.280045</td>\n",
              "      <td>191.263885</td>\n",
              "      <td>187.956192</td>\n",
              "      <td>151.954071</td>\n",
              "      <td>137.762238</td>\n",
              "      <td>136.213501</td>\n",
              "      <td>133.948547</td>\n",
              "      <td>133.980865</td>\n",
              "      <td>131.396301</td>\n",
              "      <td>125.617599</td>\n",
              "      <td>118.645432</td>\n",
              "      <td>118.309799</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Width    0    1    2  ...         780         781         782         783\n",
              "148  194.0  0.0  0.0  0.0  ...  131.396301  125.617599  118.645432  118.309799\n",
              "\n",
              "[1 rows x 785 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaZPe_AxNBK9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "54e4632b-b442-4925-b212-e997c54efe95"
      },
      "source": [
        "PSD_new.head()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>;Area</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1;1.387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2;1.626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3;1.336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4;0.640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5;2.211</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     ;Area\n",
              "0  1;1.387\n",
              "1  2;1.626\n",
              "2  3;1.336\n",
              "3  4;0.640\n",
              "4  5;2.211"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_ImgJ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "BWbrHZCc_AOY",
        "outputId": "0aee17ff-8769-4322-ca9b-28e98a4b227f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-0d9b65465edd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_ImgJ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df_ImgJ' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "# \n",
        "Area = df_ImgJ['Area'].values\n",
        "# Area = np.concatenate( (Area, [lost_value] ) )\n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79MY9ZHxBW37"
      },
      "source": [
        "len(Diameter_All)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KooHVpH5k2mZ"
      },
      "source": [
        "#\n",
        "\n",
        "PSD_new[';Area'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFcOxMmjJjpL"
      },
      "source": [
        "PSD_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPWCPPf7bzsf"
      },
      "source": [
        "#lost_value = float(PSD_new.columns[1])\n",
        "#Area2 = np.array(PSD_new.iloc[:,1])\n",
        "#Area2 = np.concatenate( (Area2, [lost_value] ) )\n",
        "Area2 = PSD_new[';Area'].values\n",
        "for A in Area2:\n",
        "  Diam1.append((4*A/np.pi)**0.5) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfk_fNXGDK5_"
      },
      "source": [
        "wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        "wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        "X = pd.DataFrame([Diam1,Diameter_All])\n",
        "wts = pd.DataFrame([wt1,wt2])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWf2nmnEp6yX"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzFRjY4BLtbh"
      },
      "source": [
        "Diameter_All"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OieAXw_by3nz"
      },
      "source": [
        "A = plt.hist(X,weights=wts,bins=7)\n",
        "plt.legend(['True','CNN'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WryLryB9ovi3"
      },
      "source": [
        "print('ImgJ:','media=',np.mean(np.array(Diam1)),'desvio=',np.std(np.array(Diam1)),'pontos=',len(Diam1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Nyv-Nopbkc"
      },
      "source": [
        "print('Software:','media=',np.mean(np.array(Diameter_All)),'desvio=',np.std(np.array(Diameter_All)),'pontos=',len(Diameter_All))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE6PjA0SZ8ZQ"
      },
      "source": [
        "# Software: media= 1.3185563233999378 desvio= 0.2728642468732428 pontos= 66 theshold =0.8 e repete=80\n",
        "# Software: media= 1.2650227960747715 desvio= 0.22942393421076387 pontos= 20 theshold =0.5 e repete=40"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpdrvEySy8Ij"
      },
      "source": [
        "np.mean(np.array(Diameter_All))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMK89w-fzCVe"
      },
      "source": [
        "# Freq1 = [19.12043703, 29.22484843, 19.35872174, 20.82190224, 11.47409056] # avarage 4 samples\n",
        "Freq1 = [20.69301557, 28.55598044, 18.50768331, 22.7106327, 8.905907357] # avarage 10 samples\n",
        "#Freq2 = [16.93792791, 31.38008965, 24.93810752, 18.56158392, 6.233810752, 0.4]\n",
        "Freq2 = [16.93792791, 31.38008965, 24.93810752, 18.56158392, 6.633810752]\n",
        "Freq3 = [22.22489, 30.15078, 25.10463, 19.30926, 2.810434]\n",
        "barWidth = 0.25\n",
        "\n",
        "br1 = range(len(Freq1))\n",
        "# Set position of bar on X axis\n",
        "br2 = [x + barWidth for x in br1]\n",
        "br3 = [x + barWidth for x in br2]\n",
        "# labels = [0.8, 1.0, 1.2, 1.4, 1.6, 1.8]\n",
        "labels = [0.8, 1.0, 1.2, 1.4, 1.6]\n",
        "\n",
        "xx=[]\n",
        "for a in labels:\n",
        "  xx.append(str(a))\n",
        "plt.bar(br1, Freq1 , color=\"green\", align=\"center\", width=0.3, tick_label= xx) \n",
        "plt.bar(br2, Freq2 , color=\"red\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.bar(br3, Freq3 , color=\"blue\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.legend(['CNN 1','CNN 2','True'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}