{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_CNN_B_filtro_e_densas_iguais_e_augmentation_jul_13_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/Qualificacao/PSD_histogram_CNN_B_filtro_e_densas_iguais_e_augmentation_jul_13_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from keras.preprocessing import image"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZEvJvfoibE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60712467-ffb8-4a8d-ad72-1f6c3329e70d"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mahotas in /usr/local/lib/python3.7/dist-packages (1.4.11)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT"
      },
      "source": [
        "import mahotas.features.texture as mht\n",
        "import mahotas.features"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VcTdaNVh9EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79d8903b-773e-4507-8c1b-1fa8afbb8e69"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'marquesgabi_fev_2020' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v7SRrc8mH2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3579c4e6-d001-4701-e8ae-b9a27e256a27"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip'\n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Doutorado' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[4] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHgqAnaFyCjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2285ec3d-ac8e-4465-9a42-c19c9c36546e"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'MarquesGabi_Routines' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from segment_filter_not_conclude import Segmenta  # got image provided segmented"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5MN5a_v4np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d79f1867-dd4d-44bb-f083-d8b467f13a27"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     165   75.302734   75.439896  ...  208.299286  228.457962  211.743774\n",
            "1     146   74.692436  126.028145  ...    0.551511    0.387127    1.428223\n",
            "2     180  137.142715  134.348145  ...    1.046420    0.913087    0.646420\n",
            "3     199  178.009186  166.880630  ...   93.637611   94.321030   84.062698\n",
            "4     184  168.048676  179.222565  ...  192.234406  184.006592  154.443283\n",
            "5     112  109.500000  107.062500  ...    1.562500    1.000000    0.875000\n",
            "6     126  179.024689  201.864212  ...  147.790131  144.925934  141.913589\n",
            "7     128  131.719727  130.054688  ...  176.222656  183.421875  189.036133\n",
            "8     196  153.387756  150.408157  ...   96.693878   98.285713   97.571426\n",
            "9     150  156.513428  138.329956  ...  114.962135   81.515915   84.939560\n",
            "10    133   68.277016   72.033241  ...  161.055405  161.340729  163.922455\n",
            "11    143  173.849716  178.568970  ...  133.665558  130.657196  134.863174\n",
            "12    136  158.370255  150.806229  ...  127.548447  125.846886  121.195511\n",
            "13    128  109.586914  112.504883  ...   69.611328   67.745117   65.417969\n",
            "14    104  202.397949  224.473389  ...  123.165688  151.965988  164.630188\n",
            "15    174  101.752426  110.625977  ...  173.365051  172.122604  170.828659\n",
            "16    111  134.554672  134.263458  ...  152.377563  151.995865  154.999512\n",
            "17    161  186.983002  180.741028  ...   99.107758  100.945175  111.678650\n",
            "18    170  133.110336  129.493149  ...    1.000000    1.000000    1.000000\n",
            "19    184   26.252361   29.106804  ...  136.121445  133.191864  143.724945\n",
            "20    177   86.306068   89.695137  ...    1.130007    0.148457    1.341409\n",
            "21    163  148.332077  149.807419  ...  148.808121  154.811646  151.169968\n",
            "22    101  110.908539  111.112640  ...    1.063131    0.413489    0.076855\n",
            "23    152  254.360092  249.203583  ...  158.594193  142.421753  130.021454\n",
            "24    137  119.883629  121.946075  ...    1.106452    2.895572    5.532154\n",
            "25    155  182.657959  173.537628  ...    1.303143    0.226181    0.755213\n",
            "26    114  168.421677  179.771637  ...  154.265625  150.978760  143.421982\n",
            "27    177  177.641281  171.306320  ...  182.309357  158.324738  147.785172\n",
            "28    100   64.799995   57.067196  ...    1.000000    1.000000    1.000000\n",
            "29    102  158.010010  153.773178  ...  101.060379  107.295670  115.191086\n",
            "30    153   17.901405   28.186169  ...    0.654406    0.314537    1.399504\n",
            "31    131  131.662415  132.461212  ...    0.267059    0.541868    1.473166\n",
            "32    138  106.039696  100.317368  ...   30.585382   38.162571   40.888050\n",
            "33    135  130.600540  134.037903  ...    0.961262    0.035336    0.864966\n",
            "34    199  178.386627  176.501282  ...  185.576065  185.561371  194.456100\n",
            "35    127  139.375092  135.457794  ...  105.401077  117.891693  127.329971\n",
            "36    167  174.010147  172.720764  ...  174.936981  178.821899  182.632019\n",
            "37    168   65.416664   59.722221  ...  127.694443  132.833328  137.861115\n",
            "38    182  182.491135  191.579895  ...  100.431961  102.822495  102.952660\n",
            "39    113  158.572403  157.782516  ...  146.992233  142.758011  142.891296\n",
            "40    150   87.103470  107.664177  ...  135.261505  159.904358  172.242493\n",
            "41    107  151.451660  151.715607  ...   58.407372   50.924889   45.883747\n",
            "42    140  177.199997  192.879990  ...  174.319992  115.639999  126.079994\n",
            "43    111  213.529816  221.546051  ...  217.648483  187.875656  149.820801\n",
            "44    181   78.733589   87.520683  ...    1.230060    0.129025    1.333323\n",
            "45    182  148.449722   80.449707  ...    1.254438    0.124260    1.331361\n",
            "46    116  152.334122  149.432800  ...  249.725327  251.639709  253.217590\n",
            "47    133  248.365646  248.432129  ...  132.376724  132.193909  133.590027\n",
            "48    118  186.626541  187.710709  ...  150.005737  147.109741  146.726517\n",
            "49    180  133.836060  130.350632  ...  120.335815  125.133331  128.960495\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzpQ1Pz0fX5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "181b181f-9390-4631-873a-ac3ef0e2ac25"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines\n",
        "# filename = 'model_ANN.pkl'\n",
        "filename = 'model_ANN_new.pkl'\n",
        "model = joblib.load(filename)\n",
        "'''"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n!git clone https://github.com/ucfilho/MarquesGabi_Routines\\n%cd MarquesGabi_Routines\\n# filename = 'model_ANN.pkl'\\nfilename = 'model_ANN_new.pkl'\\nmodel = joblib.load(filename)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2emP4rNjQy",
        "outputId": "1c3d8355-361d-4b3a-ff8a-a1c3605e3beb"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'MarquesGabi_Routines' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "# make the CNN\n",
        "# model.add(Input(shape=(28, 28, 1)))\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=64, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=200))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=200))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbPQ1FSRG6A",
        "outputId": "de7ae2f5-8a60-4d80-9331-fe3a8bc00fa4"
      },
      "source": [
        "\n",
        "# gives us back a <keras.callbacks.History object at 0x112e61a90>\n",
        "model.fit(X_train, Y_train, epochs=200, batch_size=32)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 17s 19ms/step - loss: 0.6159 - accuracy: 0.7136\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2616 - accuracy: 0.8771\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1938 - accuracy: 0.9230\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0939 - accuracy: 0.9764\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0319 - accuracy: 0.9851\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0314 - accuracy: 0.9904\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0098 - accuracy: 0.9951\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0107 - accuracy: 0.9978\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0154 - accuracy: 0.9986\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0089 - accuracy: 0.9928\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0143 - accuracy: 0.9959\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0483 - accuracy: 0.9840\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1101 - accuracy: 0.9694\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2099 - accuracy: 0.9744\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0411 - accuracy: 0.9816\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0400 - accuracy: 0.9928\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0256 - accuracy: 0.9849\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0133 - accuracy: 0.9952\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0209 - accuracy: 0.9896\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0164 - accuracy: 0.9973\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0088 - accuracy: 0.9983\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 0.9986\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5686e-04 - accuracy: 1.0000\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7674e-04 - accuracy: 1.0000\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0750e-04 - accuracy: 1.0000\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2122e-04 - accuracy: 1.0000\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.0525e-04 - accuracy: 1.0000\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.3749e-04 - accuracy: 1.0000\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.1393e-04 - accuracy: 1.0000\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6130e-05 - accuracy: 1.0000\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.3272e-05 - accuracy: 1.0000\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.8738e-05 - accuracy: 1.0000\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.0807e-05 - accuracy: 1.0000\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2913e-04 - accuracy: 1.0000\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.7939e-05 - accuracy: 1.0000\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2898e-04 - accuracy: 1.0000\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6106e-05 - accuracy: 1.0000\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.1949e-05 - accuracy: 1.0000\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9871e-05 - accuracy: 1.0000\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.5896e-05 - accuracy: 1.0000\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.2108e-04 - accuracy: 1.0000\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3494e-05 - accuracy: 1.0000\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.4575e-04 - accuracy: 1.0000\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5482e-05 - accuracy: 1.0000\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.4577e-05 - accuracy: 1.0000\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6181e-04 - accuracy: 1.0000\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.4912e-05 - accuracy: 1.0000\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.3101e-05 - accuracy: 1.0000\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.8603e-05 - accuracy: 1.0000\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9480e-05 - accuracy: 1.0000\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.8080e-05 - accuracy: 1.0000\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.0065e-05 - accuracy: 1.0000\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.0702e-04 - accuracy: 1.0000\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8791e-05 - accuracy: 1.0000\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.1195e-05 - accuracy: 1.0000\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.0000e-06 - accuracy: 1.0000\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.9953e-05 - accuracy: 1.0000\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.5893e-05 - accuracy: 1.0000\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.0706e-04 - accuracy: 1.0000\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0457e-05 - accuracy: 1.0000\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4687e-05 - accuracy: 1.0000\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.2206e-06 - accuracy: 1.0000\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.6083e-06 - accuracy: 1.0000\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4547e-05 - accuracy: 1.0000\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2841e-05 - accuracy: 1.0000\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.9190e-05 - accuracy: 1.0000\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6752e-05 - accuracy: 1.0000\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.6556e-06 - accuracy: 1.0000\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4406e-05 - accuracy: 1.0000\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7636e-05 - accuracy: 1.0000\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.2526e-05 - accuracy: 1.0000\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.8030e-06 - accuracy: 1.0000\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.0861e-06 - accuracy: 1.0000\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2269e-06 - accuracy: 1.0000\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 7.0316e-06 - accuracy: 1.0000\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.2266e-05 - accuracy: 1.0000\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8585e-05 - accuracy: 1.0000\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0417e-05 - accuracy: 1.0000\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.8680e-05 - accuracy: 1.0000\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.3141e-05 - accuracy: 1.0000\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5895e-06 - accuracy: 1.0000\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.8275e-06 - accuracy: 1.0000\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.6734e-06 - accuracy: 1.0000\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.8712e-06 - accuracy: 1.0000\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8795e-05 - accuracy: 1.0000\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2422e-05 - accuracy: 1.0000\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0344e-05 - accuracy: 1.0000\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0270e-05 - accuracy: 1.0000\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7418e-05 - accuracy: 1.0000\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.4035e-06 - accuracy: 1.0000\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.6419e-06 - accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.9389e-06 - accuracy: 1.0000\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.1856e-06 - accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3122e-05 - accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.8079e-06 - accuracy: 1.0000\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.4993e-06 - accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7136e-06 - accuracy: 1.0000\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.2684e-06 - accuracy: 1.0000\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.7872e-06 - accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.0450e-05 - accuracy: 1.0000\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9647e-06 - accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.2393e-06 - accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7949e-06 - accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1633e-06 - accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.5943e-05 - accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2052e-05 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.5240e-06 - accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4599e-06 - accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0695e-06 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.4319e-06 - accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8273e-04 - accuracy: 1.0000\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 0.9986\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0849 - accuracy: 0.9618\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.3110 - accuracy: 0.9309\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1267 - accuracy: 0.9627\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1149 - accuracy: 0.9511\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0168 - accuracy: 0.9967\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0546 - accuracy: 0.9812\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0040 - accuracy: 0.9995\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0480 - accuracy: 0.9864\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0708 - accuracy: 0.9824\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0499 - accuracy: 0.9833\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 0.9935\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.6623e-04 - accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.8098e-04 - accuracy: 1.0000\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.8686e-04 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.0520e-04 - accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.8081e-04 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.3857e-04 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1854e-04 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1528e-04 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5474e-04 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.1675e-04 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0746e-04 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5999e-05 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.0996e-05 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.0955e-05 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.6621e-05 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.7277e-04 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4846e-05 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.7266e-05 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4186e-05 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4130e-04 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.5044e-05 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2007e-05 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.4785e-05 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5890e-04 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.7202e-05 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.1762e-05 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.7951e-04 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.3990e-04 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.0881e-05 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7971e-04 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.3018e-05 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8173e-04 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.2714e-04 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6405e-05 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6060e-05 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.0971e-05 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.5949e-05 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.7703e-05 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9157e-06 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.0273e-06 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.7118e-05 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.7553e-06 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.8958e-05 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.7702e-05 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.8663e-05 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.9036e-04 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1260e-04 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.1110e-04 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.6544e-05 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2357e-05 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.6154e-06 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.3953e-05 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7903e-04 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.9701e-06 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.9385e-06 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4949e-04 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7123e-06 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.3866e-05 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 3.6250e-05 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.1828e-06 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2555e-05 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2700e-05 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.4230e-06 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.8842e-05 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4917e-06 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.6414e-06 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3633e-06 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3b30345d10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRaP8bHWNeZA",
        "outputId": "8ef58376-5f34-4efe-bd02-b4ca0c98aacf"
      },
      "source": [
        "\n",
        "# Fit with data augmentation\n",
        "# Note: if you run this AFTER calling the previous model.fit(), it will CONTINUE training where it left off\n",
        "batch_size = 5\n",
        "data_generator = image.ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "train_generator = data_generator.flow(X_train, Y_train, batch_size)\n",
        "steps_per_epoch = X_train.shape[0] // batch_size\n",
        "\n",
        "model.fit(train_generator, validation_data=(X_test, Y_test), steps_per_epoch=steps_per_epoch, epochs=200)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "68/68 [==============================] - 2s 20ms/step - loss: 0.9277 - accuracy: 0.6982 - val_loss: 170.0074 - val_accuracy: 0.5102\n",
            "Epoch 2/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.4417 - accuracy: 0.8254 - val_loss: 45.4445 - val_accuracy: 0.5102\n",
            "Epoch 3/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.3603 - accuracy: 0.8609 - val_loss: 131.2180 - val_accuracy: 0.5102\n",
            "Epoch 4/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2910 - accuracy: 0.8935 - val_loss: 40.7562 - val_accuracy: 0.5102\n",
            "Epoch 5/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2500 - accuracy: 0.8905 - val_loss: 30.9924 - val_accuracy: 0.5102\n",
            "Epoch 6/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2367 - accuracy: 0.9112 - val_loss: 59.4079 - val_accuracy: 0.5102\n",
            "Epoch 7/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1908 - accuracy: 0.9201 - val_loss: 10.4077 - val_accuracy: 0.4898\n",
            "Epoch 8/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2434 - accuracy: 0.9053 - val_loss: 28.6777 - val_accuracy: 0.5102\n",
            "Epoch 9/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.2451 - accuracy: 0.8905 - val_loss: 37.4256 - val_accuracy: 0.5102\n",
            "Epoch 10/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2388 - accuracy: 0.9112 - val_loss: 6.7921 - val_accuracy: 0.4898\n",
            "Epoch 11/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.2544 - accuracy: 0.8964 - val_loss: 4.3944 - val_accuracy: 0.4898\n",
            "Epoch 12/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2517 - accuracy: 0.9083 - val_loss: 5.5016 - val_accuracy: 0.4898\n",
            "Epoch 13/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1953 - accuracy: 0.9260 - val_loss: 9.0312 - val_accuracy: 0.4898\n",
            "Epoch 14/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2139 - accuracy: 0.9053 - val_loss: 48.6041 - val_accuracy: 0.5102\n",
            "Epoch 15/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.2031 - accuracy: 0.9142 - val_loss: 20.4050 - val_accuracy: 0.5102\n",
            "Epoch 16/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.2060 - accuracy: 0.9290 - val_loss: 25.6924 - val_accuracy: 0.5102\n",
            "Epoch 17/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.2073 - accuracy: 0.9142 - val_loss: 5.0914 - val_accuracy: 0.4898\n",
            "Epoch 18/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.2337 - accuracy: 0.9290 - val_loss: 9.6620 - val_accuracy: 0.5102\n",
            "Epoch 19/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2246 - accuracy: 0.9112 - val_loss: 22.0562 - val_accuracy: 0.5102\n",
            "Epoch 20/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2197 - accuracy: 0.9112 - val_loss: 5.1109 - val_accuracy: 0.4898\n",
            "Epoch 21/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2007 - accuracy: 0.9379 - val_loss: 1.4330 - val_accuracy: 0.5238\n",
            "Epoch 22/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1937 - accuracy: 0.9231 - val_loss: 33.5650 - val_accuracy: 0.5102\n",
            "Epoch 23/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.2016 - accuracy: 0.9260 - val_loss: 22.4433 - val_accuracy: 0.5102\n",
            "Epoch 24/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1936 - accuracy: 0.9408 - val_loss: 3.8089 - val_accuracy: 0.5102\n",
            "Epoch 25/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2156 - accuracy: 0.9260 - val_loss: 16.0388 - val_accuracy: 0.5102\n",
            "Epoch 26/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1416 - accuracy: 0.9497 - val_loss: 10.0428 - val_accuracy: 0.4898\n",
            "Epoch 27/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1034 - accuracy: 0.9586 - val_loss: 5.2578 - val_accuracy: 0.4898\n",
            "Epoch 28/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1976 - accuracy: 0.9379 - val_loss: 0.8527 - val_accuracy: 0.4898\n",
            "Epoch 29/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1404 - accuracy: 0.9497 - val_loss: 10.6872 - val_accuracy: 0.5102\n",
            "Epoch 30/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2020 - accuracy: 0.9142 - val_loss: 0.8976 - val_accuracy: 0.5102\n",
            "Epoch 31/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.2054 - accuracy: 0.9408 - val_loss: 1.3940 - val_accuracy: 0.5102\n",
            "Epoch 32/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1260 - accuracy: 0.9675 - val_loss: 4.8728 - val_accuracy: 0.5102\n",
            "Epoch 33/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2079 - accuracy: 0.9260 - val_loss: 3.0717 - val_accuracy: 0.5102\n",
            "Epoch 34/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1605 - accuracy: 0.9527 - val_loss: 1.1028 - val_accuracy: 0.4898\n",
            "Epoch 35/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1707 - accuracy: 0.9290 - val_loss: 2.1973 - val_accuracy: 0.4898\n",
            "Epoch 36/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0935 - accuracy: 0.9527 - val_loss: 0.6834 - val_accuracy: 0.6259\n",
            "Epoch 37/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1833 - accuracy: 0.9201 - val_loss: 9.8352 - val_accuracy: 0.4898\n",
            "Epoch 38/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1303 - accuracy: 0.9527 - val_loss: 3.4722 - val_accuracy: 0.5102\n",
            "Epoch 39/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1996 - accuracy: 0.9290 - val_loss: 5.7386 - val_accuracy: 0.4898\n",
            "Epoch 40/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1389 - accuracy: 0.9556 - val_loss: 0.4330 - val_accuracy: 0.7279\n",
            "Epoch 41/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1754 - accuracy: 0.9349 - val_loss: 17.0673 - val_accuracy: 0.4898\n",
            "Epoch 42/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1882 - accuracy: 0.9231 - val_loss: 1.8533 - val_accuracy: 0.5850\n",
            "Epoch 43/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1449 - accuracy: 0.9467 - val_loss: 17.2358 - val_accuracy: 0.5102\n",
            "Epoch 44/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1913 - accuracy: 0.9349 - val_loss: 3.3467 - val_accuracy: 0.5102\n",
            "Epoch 45/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1386 - accuracy: 0.9497 - val_loss: 19.1296 - val_accuracy: 0.5102\n",
            "Epoch 46/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1525 - accuracy: 0.9320 - val_loss: 9.9207 - val_accuracy: 0.5102\n",
            "Epoch 47/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1556 - accuracy: 0.9497 - val_loss: 9.6069 - val_accuracy: 0.5102\n",
            "Epoch 48/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1485 - accuracy: 0.9497 - val_loss: 8.7974 - val_accuracy: 0.5102\n",
            "Epoch 49/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1130 - accuracy: 0.9527 - val_loss: 11.4361 - val_accuracy: 0.5102\n",
            "Epoch 50/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1477 - accuracy: 0.9645 - val_loss: 1.8254 - val_accuracy: 0.5102\n",
            "Epoch 51/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1128 - accuracy: 0.9615 - val_loss: 5.4508 - val_accuracy: 0.5102\n",
            "Epoch 52/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0961 - accuracy: 0.9704 - val_loss: 9.6521 - val_accuracy: 0.5102\n",
            "Epoch 53/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1990 - accuracy: 0.9408 - val_loss: 2.0069 - val_accuracy: 0.5102\n",
            "Epoch 54/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1236 - accuracy: 0.9586 - val_loss: 9.4002 - val_accuracy: 0.4898\n",
            "Epoch 55/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1413 - accuracy: 0.9379 - val_loss: 7.9940 - val_accuracy: 0.5102\n",
            "Epoch 56/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1506 - accuracy: 0.9438 - val_loss: 6.5117 - val_accuracy: 0.5102\n",
            "Epoch 57/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0856 - accuracy: 0.9763 - val_loss: 0.8809 - val_accuracy: 0.5374\n",
            "Epoch 58/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1922 - accuracy: 0.9379 - val_loss: 48.7207 - val_accuracy: 0.5102\n",
            "Epoch 59/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1106 - accuracy: 0.9556 - val_loss: 46.7558 - val_accuracy: 0.5102\n",
            "Epoch 60/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1074 - accuracy: 0.9586 - val_loss: 4.0015 - val_accuracy: 0.5102\n",
            "Epoch 61/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1850 - accuracy: 0.9497 - val_loss: 10.0656 - val_accuracy: 0.4898\n",
            "Epoch 62/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1499 - accuracy: 0.9527 - val_loss: 43.9906 - val_accuracy: 0.4898\n",
            "Epoch 63/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1744 - accuracy: 0.9260 - val_loss: 18.2040 - val_accuracy: 0.4898\n",
            "Epoch 64/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1573 - accuracy: 0.9675 - val_loss: 0.3422 - val_accuracy: 0.8844\n",
            "Epoch 65/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1182 - accuracy: 0.9497 - val_loss: 14.9202 - val_accuracy: 0.5102\n",
            "Epoch 66/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1581 - accuracy: 0.9379 - val_loss: 31.0874 - val_accuracy: 0.5102\n",
            "Epoch 67/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1665 - accuracy: 0.9320 - val_loss: 1.7623 - val_accuracy: 0.5102\n",
            "Epoch 68/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1020 - accuracy: 0.9497 - val_loss: 15.9704 - val_accuracy: 0.5102\n",
            "Epoch 69/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1698 - accuracy: 0.9467 - val_loss: 21.2780 - val_accuracy: 0.4898\n",
            "Epoch 70/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1207 - accuracy: 0.9556 - val_loss: 13.5841 - val_accuracy: 0.4898\n",
            "Epoch 71/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0978 - accuracy: 0.9645 - val_loss: 0.4677 - val_accuracy: 0.8027\n",
            "Epoch 72/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1211 - accuracy: 0.9556 - val_loss: 2.8965 - val_accuracy: 0.5102\n",
            "Epoch 73/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1682 - accuracy: 0.9556 - val_loss: 10.7639 - val_accuracy: 0.4898\n",
            "Epoch 74/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1254 - accuracy: 0.9556 - val_loss: 13.9474 - val_accuracy: 0.4898\n",
            "Epoch 75/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0923 - accuracy: 0.9615 - val_loss: 12.9088 - val_accuracy: 0.4898\n",
            "Epoch 76/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1399 - accuracy: 0.9497 - val_loss: 31.2032 - val_accuracy: 0.4898\n",
            "Epoch 77/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1024 - accuracy: 0.9467 - val_loss: 16.1270 - val_accuracy: 0.4898\n",
            "Epoch 78/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0837 - accuracy: 0.9615 - val_loss: 0.7321 - val_accuracy: 0.4898\n",
            "Epoch 79/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0325 - accuracy: 0.9882 - val_loss: 0.6787 - val_accuracy: 0.4898\n",
            "Epoch 80/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1407 - accuracy: 0.9467 - val_loss: 3.7581 - val_accuracy: 0.5102\n",
            "Epoch 81/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1169 - accuracy: 0.9704 - val_loss: 16.8961 - val_accuracy: 0.4898\n",
            "Epoch 82/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1044 - accuracy: 0.9615 - val_loss: 14.6937 - val_accuracy: 0.5102\n",
            "Epoch 83/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1337 - accuracy: 0.9586 - val_loss: 11.4631 - val_accuracy: 0.5102\n",
            "Epoch 84/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1441 - accuracy: 0.9527 - val_loss: 28.6837 - val_accuracy: 0.4898\n",
            "Epoch 85/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1054 - accuracy: 0.9586 - val_loss: 6.7125 - val_accuracy: 0.4898\n",
            "Epoch 86/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0907 - accuracy: 0.9763 - val_loss: 0.4427 - val_accuracy: 0.8231\n",
            "Epoch 87/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1394 - accuracy: 0.9467 - val_loss: 1.6218 - val_accuracy: 0.5238\n",
            "Epoch 88/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1054 - accuracy: 0.9675 - val_loss: 6.3551 - val_accuracy: 0.4898\n",
            "Epoch 89/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0888 - accuracy: 0.9645 - val_loss: 6.5654 - val_accuracy: 0.5102\n",
            "Epoch 90/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1010 - accuracy: 0.9735 - val_loss: 5.4590 - val_accuracy: 0.4898\n",
            "Epoch 91/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1476 - accuracy: 0.9497 - val_loss: 2.8711 - val_accuracy: 0.4898\n",
            "Epoch 92/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1304 - accuracy: 0.9645 - val_loss: 2.0984 - val_accuracy: 0.4898\n",
            "Epoch 93/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1055 - accuracy: 0.9704 - val_loss: 14.2014 - val_accuracy: 0.4898\n",
            "Epoch 94/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0611 - accuracy: 0.9734 - val_loss: 32.0198 - val_accuracy: 0.4898\n",
            "Epoch 95/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1455 - accuracy: 0.9527 - val_loss: 3.0521 - val_accuracy: 0.4898\n",
            "Epoch 96/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1320 - accuracy: 0.9618 - val_loss: 5.3918 - val_accuracy: 0.4898\n",
            "Epoch 97/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1636 - accuracy: 0.9556 - val_loss: 0.5590 - val_accuracy: 0.7755\n",
            "Epoch 98/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0995 - accuracy: 0.9793 - val_loss: 21.5951 - val_accuracy: 0.4898\n",
            "Epoch 99/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1501 - accuracy: 0.9467 - val_loss: 11.8397 - val_accuracy: 0.4898\n",
            "Epoch 100/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1385 - accuracy: 0.9497 - val_loss: 10.0081 - val_accuracy: 0.4898\n",
            "Epoch 101/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1319 - accuracy: 0.9586 - val_loss: 13.2594 - val_accuracy: 0.4898\n",
            "Epoch 102/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0974 - accuracy: 0.9615 - val_loss: 15.4548 - val_accuracy: 0.4898\n",
            "Epoch 103/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0953 - accuracy: 0.9645 - val_loss: 2.1250 - val_accuracy: 0.5102\n",
            "Epoch 104/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0613 - accuracy: 0.9793 - val_loss: 6.0669 - val_accuracy: 0.4898\n",
            "Epoch 105/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1049 - accuracy: 0.9645 - val_loss: 1.1462 - val_accuracy: 0.5102\n",
            "Epoch 106/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1474 - accuracy: 0.9527 - val_loss: 0.6704 - val_accuracy: 0.5714\n",
            "Epoch 107/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0803 - accuracy: 0.9586 - val_loss: 6.2391 - val_accuracy: 0.4898\n",
            "Epoch 108/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0904 - accuracy: 0.9615 - val_loss: 1.8208 - val_accuracy: 0.5102\n",
            "Epoch 109/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0909 - accuracy: 0.9556 - val_loss: 11.8803 - val_accuracy: 0.4898\n",
            "Epoch 110/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1249 - accuracy: 0.9615 - val_loss: 11.2423 - val_accuracy: 0.5102\n",
            "Epoch 111/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1158 - accuracy: 0.9675 - val_loss: 7.5575 - val_accuracy: 0.4898\n",
            "Epoch 112/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0748 - accuracy: 0.9675 - val_loss: 0.4880 - val_accuracy: 0.6667\n",
            "Epoch 113/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0816 - accuracy: 0.9704 - val_loss: 48.7951 - val_accuracy: 0.4898\n",
            "Epoch 114/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0968 - accuracy: 0.9645 - val_loss: 10.6959 - val_accuracy: 0.4898\n",
            "Epoch 115/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1102 - accuracy: 0.9645 - val_loss: 14.6516 - val_accuracy: 0.4898\n",
            "Epoch 116/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1036 - accuracy: 0.9704 - val_loss: 23.5874 - val_accuracy: 0.4898\n",
            "Epoch 117/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0903 - accuracy: 0.9675 - val_loss: 0.4941 - val_accuracy: 0.8095\n",
            "Epoch 118/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0639 - accuracy: 0.9763 - val_loss: 3.2955 - val_accuracy: 0.4898\n",
            "Epoch 119/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0791 - accuracy: 0.9763 - val_loss: 25.0915 - val_accuracy: 0.4898\n",
            "Epoch 120/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0747 - accuracy: 0.9763 - val_loss: 18.9343 - val_accuracy: 0.4898\n",
            "Epoch 121/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1435 - accuracy: 0.9586 - val_loss: 16.8939 - val_accuracy: 0.4898\n",
            "Epoch 122/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1503 - accuracy: 0.9586 - val_loss: 28.6320 - val_accuracy: 0.5102\n",
            "Epoch 123/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0833 - accuracy: 0.9556 - val_loss: 131.6216 - val_accuracy: 0.5102\n",
            "Epoch 124/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0919 - accuracy: 0.9556 - val_loss: 146.1945 - val_accuracy: 0.5102\n",
            "Epoch 125/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1109 - accuracy: 0.9675 - val_loss: 29.8206 - val_accuracy: 0.5102\n",
            "Epoch 126/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1027 - accuracy: 0.9704 - val_loss: 13.6188 - val_accuracy: 0.4898\n",
            "Epoch 127/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0943 - accuracy: 0.9645 - val_loss: 28.8866 - val_accuracy: 0.4898\n",
            "Epoch 128/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0957 - accuracy: 0.9675 - val_loss: 13.1145 - val_accuracy: 0.4898\n",
            "Epoch 129/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1283 - accuracy: 0.9615 - val_loss: 10.6278 - val_accuracy: 0.4898\n",
            "Epoch 130/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1584 - accuracy: 0.9675 - val_loss: 17.2479 - val_accuracy: 0.4898\n",
            "Epoch 131/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1411 - accuracy: 0.9438 - val_loss: 6.8964 - val_accuracy: 0.4898\n",
            "Epoch 132/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0801 - accuracy: 0.9704 - val_loss: 11.0564 - val_accuracy: 0.4898\n",
            "Epoch 133/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0736 - accuracy: 0.9704 - val_loss: 120.4986 - val_accuracy: 0.4898\n",
            "Epoch 134/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0972 - accuracy: 0.9645 - val_loss: 38.4487 - val_accuracy: 0.4898\n",
            "Epoch 135/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0939 - accuracy: 0.9645 - val_loss: 35.3981 - val_accuracy: 0.4898\n",
            "Epoch 136/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1327 - accuracy: 0.9497 - val_loss: 19.9810 - val_accuracy: 0.5102\n",
            "Epoch 137/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1209 - accuracy: 0.9793 - val_loss: 10.6521 - val_accuracy: 0.4898\n",
            "Epoch 138/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1403 - accuracy: 0.9527 - val_loss: 1.7554 - val_accuracy: 0.5442\n",
            "Epoch 139/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1008 - accuracy: 0.9556 - val_loss: 43.4803 - val_accuracy: 0.5102\n",
            "Epoch 140/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0553 - accuracy: 0.9793 - val_loss: 32.8603 - val_accuracy: 0.5102\n",
            "Epoch 141/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1043 - accuracy: 0.9645 - val_loss: 9.9069 - val_accuracy: 0.5102\n",
            "Epoch 142/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0889 - accuracy: 0.9704 - val_loss: 9.3610 - val_accuracy: 0.5102\n",
            "Epoch 143/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1113 - accuracy: 0.9529 - val_loss: 18.6172 - val_accuracy: 0.4898\n",
            "Epoch 144/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1093 - accuracy: 0.9645 - val_loss: 0.7585 - val_accuracy: 0.6327\n",
            "Epoch 145/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1139 - accuracy: 0.9615 - val_loss: 6.6662 - val_accuracy: 0.5102\n",
            "Epoch 146/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0805 - accuracy: 0.9675 - val_loss: 7.4560 - val_accuracy: 0.4966\n",
            "Epoch 147/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0424 - accuracy: 0.9822 - val_loss: 11.6704 - val_accuracy: 0.4898\n",
            "Epoch 148/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0763 - accuracy: 0.9734 - val_loss: 7.7188 - val_accuracy: 0.4898\n",
            "Epoch 149/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0951 - accuracy: 0.9793 - val_loss: 0.7042 - val_accuracy: 0.6259\n",
            "Epoch 150/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0524 - accuracy: 0.9793 - val_loss: 4.1135 - val_accuracy: 0.5102\n",
            "Epoch 151/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0699 - accuracy: 0.9675 - val_loss: 4.5648 - val_accuracy: 0.4898\n",
            "Epoch 152/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0708 - accuracy: 0.9704 - val_loss: 17.0157 - val_accuracy: 0.4898\n",
            "Epoch 153/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0839 - accuracy: 0.9615 - val_loss: 31.1152 - val_accuracy: 0.5102\n",
            "Epoch 154/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0578 - accuracy: 0.9793 - val_loss: 22.7246 - val_accuracy: 0.4898\n",
            "Epoch 155/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1615 - accuracy: 0.9615 - val_loss: 11.8551 - val_accuracy: 0.5102\n",
            "Epoch 156/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0948 - accuracy: 0.9586 - val_loss: 11.1694 - val_accuracy: 0.4898\n",
            "Epoch 157/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1202 - accuracy: 0.9615 - val_loss: 8.2012 - val_accuracy: 0.5102\n",
            "Epoch 158/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0637 - accuracy: 0.9822 - val_loss: 0.6308 - val_accuracy: 0.7211\n",
            "Epoch 159/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0615 - accuracy: 0.9793 - val_loss: 5.7493 - val_accuracy: 0.5102\n",
            "Epoch 160/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0739 - accuracy: 0.9734 - val_loss: 48.5481 - val_accuracy: 0.5102\n",
            "Epoch 161/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0674 - accuracy: 0.9645 - val_loss: 17.2117 - val_accuracy: 0.5102\n",
            "Epoch 162/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1151 - accuracy: 0.9615 - val_loss: 4.9224 - val_accuracy: 0.5102\n",
            "Epoch 163/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0867 - accuracy: 0.9556 - val_loss: 0.8943 - val_accuracy: 0.7279\n",
            "Epoch 164/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0690 - accuracy: 0.9793 - val_loss: 12.6286 - val_accuracy: 0.4898\n",
            "Epoch 165/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0699 - accuracy: 0.9734 - val_loss: 25.0952 - val_accuracy: 0.5102\n",
            "Epoch 166/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0734 - accuracy: 0.9763 - val_loss: 30.7238 - val_accuracy: 0.5102\n",
            "Epoch 167/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1162 - accuracy: 0.9556 - val_loss: 22.3875 - val_accuracy: 0.4898\n",
            "Epoch 168/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1265 - accuracy: 0.9586 - val_loss: 144.1166 - val_accuracy: 0.5102\n",
            "Epoch 169/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0985 - accuracy: 0.9645 - val_loss: 86.9109 - val_accuracy: 0.5102\n",
            "Epoch 170/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0320 - accuracy: 0.9852 - val_loss: 57.5123 - val_accuracy: 0.5102\n",
            "Epoch 171/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1272 - accuracy: 0.9645 - val_loss: 165.5559 - val_accuracy: 0.5102\n",
            "Epoch 172/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0888 - accuracy: 0.9645 - val_loss: 89.6824 - val_accuracy: 0.5102\n",
            "Epoch 173/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0365 - accuracy: 0.9882 - val_loss: 62.9583 - val_accuracy: 0.5102\n",
            "Epoch 174/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0947 - accuracy: 0.9615 - val_loss: 0.9402 - val_accuracy: 0.6803\n",
            "Epoch 175/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1195 - accuracy: 0.9763 - val_loss: 28.4982 - val_accuracy: 0.5102\n",
            "Epoch 176/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0715 - accuracy: 0.9704 - val_loss: 22.1441 - val_accuracy: 0.4898\n",
            "Epoch 177/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0186 - accuracy: 0.9941 - val_loss: 11.9118 - val_accuracy: 0.5102\n",
            "Epoch 178/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0273 - accuracy: 0.9852 - val_loss: 2.4205 - val_accuracy: 0.5578\n",
            "Epoch 179/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0752 - accuracy: 0.9793 - val_loss: 56.1648 - val_accuracy: 0.5102\n",
            "Epoch 180/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0989 - accuracy: 0.9615 - val_loss: 45.5739 - val_accuracy: 0.5102\n",
            "Epoch 181/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1141 - accuracy: 0.9675 - val_loss: 2.5672 - val_accuracy: 0.4898\n",
            "Epoch 182/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0753 - accuracy: 0.9734 - val_loss: 5.4651 - val_accuracy: 0.4898\n",
            "Epoch 183/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0599 - accuracy: 0.9763 - val_loss: 0.4296 - val_accuracy: 0.7823\n",
            "Epoch 184/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0719 - accuracy: 0.9734 - val_loss: 2.1736 - val_accuracy: 0.5102\n",
            "Epoch 185/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0713 - accuracy: 0.9704 - val_loss: 8.3024 - val_accuracy: 0.4898\n",
            "Epoch 186/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1172 - accuracy: 0.9675 - val_loss: 12.2248 - val_accuracy: 0.4898\n",
            "Epoch 187/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0507 - accuracy: 0.9704 - val_loss: 10.4308 - val_accuracy: 0.4898\n",
            "Epoch 188/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0586 - accuracy: 0.9763 - val_loss: 11.5129 - val_accuracy: 0.4898\n",
            "Epoch 189/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0675 - accuracy: 0.9793 - val_loss: 2.4914 - val_accuracy: 0.5102\n",
            "Epoch 190/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0581 - accuracy: 0.9822 - val_loss: 28.2039 - val_accuracy: 0.4898\n",
            "Epoch 191/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0758 - accuracy: 0.9675 - val_loss: 20.8786 - val_accuracy: 0.4898\n",
            "Epoch 192/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0362 - accuracy: 0.9793 - val_loss: 20.9422 - val_accuracy: 0.5102\n",
            "Epoch 193/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0498 - accuracy: 0.9704 - val_loss: 6.8400 - val_accuracy: 0.5510\n",
            "Epoch 194/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0201 - accuracy: 0.9911 - val_loss: 1.0341 - val_accuracy: 0.6190\n",
            "Epoch 195/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0791 - accuracy: 0.9822 - val_loss: 9.1761 - val_accuracy: 0.4898\n",
            "Epoch 196/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0936 - accuracy: 0.9675 - val_loss: 70.9329 - val_accuracy: 0.5102\n",
            "Epoch 197/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1497 - accuracy: 0.9586 - val_loss: 13.1943 - val_accuracy: 0.5102\n",
            "Epoch 198/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0922 - accuracy: 0.9645 - val_loss: 10.1692 - val_accuracy: 0.4898\n",
            "Epoch 199/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0771 - accuracy: 0.9645 - val_loss: 0.9460 - val_accuracy: 0.5714\n",
            "Epoch 200/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0727 - accuracy: 0.9704 - val_loss: 6.3374 - val_accuracy: 0.4898\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3b2fa889d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-3hvIYqRcV5",
        "outputId": "34ba682f-2d7e-43d3-edf1-2c0f839068df"
      },
      "source": [
        "# X_train.shape\n",
        "steps_per_epoch"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIgDLfqrNhp_",
        "outputId": "c4b6342f-994d-4741-ad37-89c5bf895447"
      },
      "source": [
        "# gives us back a <keras.callbacks.History object at 0x112e61a90>\n",
        "model.fit(X_train, Y_train, epochs=200, batch_size=32)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0105 - accuracy: 1.0000\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0061 - accuracy: 1.0000\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.7266e-04 - accuracy: 1.0000\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.8532e-04 - accuracy: 1.0000\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.0510e-04 - accuracy: 1.0000\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.9245e-04 - accuracy: 1.0000\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.6401e-04 - accuracy: 1.0000\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.0439e-04 - accuracy: 1.0000\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.4653e-04 - accuracy: 1.0000\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.6278e-04 - accuracy: 1.0000\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.1076e-04 - accuracy: 1.0000\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.5794e-04 - accuracy: 1.0000\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.1206e-04 - accuracy: 1.0000\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.9384e-04 - accuracy: 1.0000\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.2574e-04 - accuracy: 1.0000\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.9177e-04 - accuracy: 1.0000\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7788e-04 - accuracy: 1.0000\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.6084e-04 - accuracy: 1.0000\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.6466e-04 - accuracy: 1.0000\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5913e-04 - accuracy: 1.0000\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.7529e-04 - accuracy: 1.0000\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.3391e-04 - accuracy: 1.0000\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.6112e-04 - accuracy: 1.0000\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.0587e-04 - accuracy: 1.0000\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.5000e-04 - accuracy: 1.0000\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.3346e-04 - accuracy: 1.0000\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.2028e-04 - accuracy: 1.0000\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.0848e-04 - accuracy: 1.0000\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.6838e-04 - accuracy: 1.0000\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.3017e-04 - accuracy: 1.0000\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.4280e-04 - accuracy: 1.0000\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.2457e-04 - accuracy: 1.0000\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.0462e-04 - accuracy: 1.0000\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.9268e-04 - accuracy: 1.0000\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0432e-04 - accuracy: 1.0000\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.9868e-04 - accuracy: 1.0000\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0253e-04 - accuracy: 1.0000\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.7015e-05 - accuracy: 1.0000\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.0257e-04 - accuracy: 1.0000\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.4751e-04 - accuracy: 1.0000\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.3747e-05 - accuracy: 1.0000\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.8685e-04 - accuracy: 1.0000\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4643e-04 - accuracy: 1.0000\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5715e-04 - accuracy: 1.0000\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.8380e-04 - accuracy: 1.0000\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0349e-04 - accuracy: 1.0000\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9134e-04 - accuracy: 1.0000\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.5856e-04 - accuracy: 1.0000\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.0964e-05 - accuracy: 1.0000\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.6394e-04 - accuracy: 1.0000\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2050e-04 - accuracy: 1.0000\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3839e-04 - accuracy: 1.0000\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0651e-04 - accuracy: 1.0000\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.5678e-04 - accuracy: 1.0000\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3341e-04 - accuracy: 1.0000\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5099e-04 - accuracy: 1.0000\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.1626e-04 - accuracy: 1.0000\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0411e-04 - accuracy: 1.0000\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.2658e-05 - accuracy: 1.0000\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.6516e-05 - accuracy: 1.0000\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4312e-04 - accuracy: 1.0000\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.1784e-04 - accuracy: 1.0000\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5026e-04 - accuracy: 1.0000\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1347e-04 - accuracy: 1.0000\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.0936e-05 - accuracy: 1.0000\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4389e-04 - accuracy: 1.0000\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.4557e-05 - accuracy: 1.0000\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7971e-04 - accuracy: 1.0000\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.6568e-05 - accuracy: 1.0000\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 4.5693e-05 - accuracy: 1.0000\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.0907e-04 - accuracy: 1.0000\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.9601e-04 - accuracy: 1.0000\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.4017e-04 - accuracy: 1.0000\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6906e-04 - accuracy: 1.0000\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.3639e-05 - accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5770e-05 - accuracy: 1.0000\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0287e-04 - accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9581e-05 - accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.7620e-05 - accuracy: 1.0000\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6267e-04 - accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.7066e-04 - accuracy: 1.0000\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2171e-04 - accuracy: 1.0000\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3484e-04 - accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1229e-04 - accuracy: 1.0000\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8993e-05 - accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3436e-04 - accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.6143e-04 - accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2196e-04 - accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9922e-04 - accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6637e-04 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.5094e-04 - accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2171e-04 - accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5349e-04 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.9178e-04 - accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.3561e-05 - accuracy: 1.0000\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.1757e-05 - accuracy: 1.0000\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.6682e-05 - accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6456e-05 - accuracy: 1.0000\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.7758e-05 - accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.2216e-05 - accuracy: 1.0000\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7868e-04 - accuracy: 1.0000\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.1832e-04 - accuracy: 1.0000\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.4620e-04 - accuracy: 1.0000\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2846e-05 - accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2308e-04 - accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5686e-05 - accuracy: 1.0000\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.8097e-05 - accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.9664e-05 - accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.7281e-05 - accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.0673e-05 - accuracy: 1.0000\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.7282e-05 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.1172e-05 - accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0925e-04 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4820e-05 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.8781e-05 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.5614e-05 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.9448e-05 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3889e-04 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.5805e-04 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1045e-04 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.6980e-04 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.8827e-05 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.7868e-05 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.0255e-04 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.9269e-05 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.6315e-05 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1847e-04 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.6900e-04 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.0158e-05 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.0950e-05 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5485e-05 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5499e-04 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5001e-04 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.8260e-05 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.9836e-05 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.7336e-06 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.4273e-05 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4205e-04 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.6484e-05 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9741e-05 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.1114e-05 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.9864e-05 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.0035e-05 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6263e-05 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.8619e-05 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.8101e-05 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0273e-04 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5512e-05 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.4135e-06 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5974e-05 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1048e-04 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.1111e-05 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4810e-04 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.4206e-05 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.4526e-05 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0145e-04 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 4.2743e-06 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.7229e-06 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.3491e-05 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.3929e-05 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.9144e-05 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1219e-05 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5241e-05 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.6167e-05 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.1819e-05 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.1159e-05 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.8401e-05 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4369e-05 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.8676e-05 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9214e-05 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3328e-04 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9599e-05 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.8389e-05 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.2775e-05 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0382e-05 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.0574e-06 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3993e-05 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3b2f666090>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpf0XlSARX78",
        "outputId": "ec60aa60-bfc6-4916-9e12-2905598862c3"
      },
      "source": [
        "pred_test= model.predict_classes(X_test)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predict   0   1\n",
            "Actual         \n",
            "0        71   1\n",
            "1         2  73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iFNNrlWV9tH",
        "outputId": "30225cf5-9bea-4197-f078-3dfef192717d"
      },
      "source": [
        "pred_test"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QISvYcJBgWbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f2e98cf-b367-4757-ff8e-f3749f07161d"
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[0] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  prediction = model.predict_classes(result)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0   124.0    0.000000    0.000000  ...    0.000000    0.000000    0.000000\n",
            "1   117.0  177.598206  177.623062  ...   98.639572   91.359558   96.984512\n",
            "2   127.0   75.739410   75.409943  ...   57.261581   58.935459   62.211670\n",
            "5   190.0   77.613190   77.997665  ...   49.993790   53.464928   59.269695\n",
            "7   177.0   75.359726  107.797653  ...   57.421776   54.379036   54.671799\n",
            "10  105.0   65.644447   62.720009  ...   41.284447   41.000004   40.457783\n",
            "11  111.0   69.206718   66.934174  ...   37.065094   29.576088   18.495981\n",
            "12  197.0   58.950920   53.335926  ...   56.756531   42.201351   39.788506\n",
            "14  198.0   93.701355   94.796951  ...   75.085899   68.096718   69.287209\n",
            "16  103.0   69.152794   67.450089  ...   37.832970   33.912716   25.103588\n",
            "17  134.0   86.545563   87.061493  ...  101.385834  103.502121  104.187355\n",
            "18  199.0   95.126076   91.837631  ...   61.657154   69.488449   84.189285\n",
            "20  147.0   57.285717   57.376419  ...   44.716557    1.793651    0.437642\n",
            "22  138.0   81.282288   83.277664  ...   93.018898   94.307487   98.579292\n",
            "23  184.0   60.937614   60.006615  ...   62.235344   63.923435   62.292057\n",
            "24  155.0   99.867439  101.469437  ...   39.828808   40.439503   42.180687\n",
            "25  167.0   69.362411   66.612221  ...  136.253296  144.494400  146.154831\n",
            "26  181.0   84.105125   81.768204  ...   57.947224   55.761787   52.047928\n",
            "27  149.0   42.622040   49.571098  ...   90.962791   81.728043   80.457825\n",
            "29  194.0   50.064720   45.521412  ...   78.657761   74.721428   57.557442\n",
            "31  138.0  101.002945   98.171181  ...   71.437515   68.962814   72.566681\n",
            "32  143.0  104.787766  103.936081  ...   57.718468   18.879993    1.261137\n",
            "33  144.0   41.672840   39.663582  ...   82.296303   84.000000   83.804016\n",
            "35  193.0  100.699806  105.598511  ...   30.089157   28.202366   17.298395\n",
            "36  155.0   40.334404   39.524078  ...   98.998787   93.593681   83.621315\n",
            "38  115.0  109.733902  114.189705  ...  194.440735  163.735107  161.688843\n",
            "39  172.0   58.356411   56.354248  ...   84.256355   70.957275   69.126556\n",
            "40  110.0  103.046608  101.857185  ...   33.923637   26.840660   27.145452\n",
            "41  133.0   42.094181   44.637119  ...   92.210526   89.797791   83.891968\n",
            "42  166.0   72.652924   70.349541  ...   34.288864   60.327766   96.649727\n",
            "44  155.0   84.387939   83.365829  ...   50.499279   36.089035   18.533985\n",
            "45  195.0   41.101646   39.391193  ...   99.757492   93.619232   98.907921\n",
            "46  200.0   61.911201   62.180401  ...  135.992004  139.027588  138.930405\n",
            "47  124.0  111.259102  110.885529  ...    0.000000    0.000000    0.000000\n",
            "48  118.0  108.023842  113.096519  ...   64.096237   65.358810   71.214020\n",
            "\n",
            "[35 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRbWgmX_LFH",
        "outputId": "adaf39b7-2a74-438b-ae9b-3645ed48847d"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "from GetBetterSegm import GetBetter"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'marquesgabi_paper_fev_2021' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAG_I6FwCvFr",
        "outputId": "ec495e23-a85e-4e84-82b7-f3df2634963c"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "%cd marquesgabi_out_2020\n",
        "PSD_imageJ = 'Areas_ImageJ.csv'\n",
        "PSD_new = pd.read_csv(PSD_imageJ)\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'marquesgabi_out_2020' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020\n",
            "   Juntas   Area\n",
            "0       1  2.001\n",
            "1       2  0.820\n",
            "2       3  1.270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "PekBHQOT_6CP",
        "outputId": "b894f4a7-86da-4c92-a357-ad4b071ffa67"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>124.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.437045</td>\n",
              "      <td>1.126951</td>\n",
              "      <td>2.434963</td>\n",
              "      <td>3.932362</td>\n",
              "      <td>6.077003</td>\n",
              "      <td>9.662850</td>\n",
              "      <td>12.052028</td>\n",
              "      <td>14.109260</td>\n",
              "      <td>16.774193</td>\n",
              "      <td>18.580645</td>\n",
              "      <td>19.332985</td>\n",
              "      <td>20.097815</td>\n",
              "      <td>21.316336</td>\n",
              "      <td>24.052029</td>\n",
              "      <td>34.014565</td>\n",
              "      <td>47.539021</td>\n",
              "      <td>54.558788</td>\n",
              "      <td>57.858479</td>\n",
              "      <td>59.818935</td>\n",
              "      <td>60.486988</td>\n",
              "      <td>59.793961</td>\n",
              "      <td>59.489071</td>\n",
              "      <td>58.176899</td>\n",
              "      <td>57.856396</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.155047</td>\n",
              "      <td>1.272633</td>\n",
              "      <td>1.632674</td>\n",
              "      <td>1.675338</td>\n",
              "      <td>2.182102</td>\n",
              "      <td>2.364204</td>\n",
              "      <td>...</td>\n",
              "      <td>0.935484</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.021852</td>\n",
              "      <td>0.524454</td>\n",
              "      <td>0.473465</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.590010</td>\n",
              "      <td>3.003122</td>\n",
              "      <td>3.636836</td>\n",
              "      <td>3.040583</td>\n",
              "      <td>0.539022</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>117.0</td>\n",
              "      <td>177.598206</td>\n",
              "      <td>177.623062</td>\n",
              "      <td>161.736893</td>\n",
              "      <td>70.109436</td>\n",
              "      <td>63.866539</td>\n",
              "      <td>71.174583</td>\n",
              "      <td>73.351891</td>\n",
              "      <td>74.613190</td>\n",
              "      <td>74.206154</td>\n",
              "      <td>74.218422</td>\n",
              "      <td>73.758934</td>\n",
              "      <td>71.831551</td>\n",
              "      <td>68.214912</td>\n",
              "      <td>64.105049</td>\n",
              "      <td>56.175030</td>\n",
              "      <td>49.072685</td>\n",
              "      <td>43.910149</td>\n",
              "      <td>41.533787</td>\n",
              "      <td>40.493099</td>\n",
              "      <td>40.061363</td>\n",
              "      <td>40.379940</td>\n",
              "      <td>41.137043</td>\n",
              "      <td>42.384907</td>\n",
              "      <td>46.571045</td>\n",
              "      <td>56.397480</td>\n",
              "      <td>66.049599</td>\n",
              "      <td>71.879906</td>\n",
              "      <td>76.097748</td>\n",
              "      <td>196.874252</td>\n",
              "      <td>196.703049</td>\n",
              "      <td>188.071304</td>\n",
              "      <td>71.050034</td>\n",
              "      <td>62.997810</td>\n",
              "      <td>74.484703</td>\n",
              "      <td>76.563957</td>\n",
              "      <td>76.982536</td>\n",
              "      <td>77.845711</td>\n",
              "      <td>77.878738</td>\n",
              "      <td>75.814445</td>\n",
              "      <td>...</td>\n",
              "      <td>116.266708</td>\n",
              "      <td>117.063995</td>\n",
              "      <td>118.537949</td>\n",
              "      <td>120.572289</td>\n",
              "      <td>121.820953</td>\n",
              "      <td>121.126816</td>\n",
              "      <td>122.346626</td>\n",
              "      <td>117.639931</td>\n",
              "      <td>106.619400</td>\n",
              "      <td>98.100449</td>\n",
              "      <td>90.443642</td>\n",
              "      <td>95.382202</td>\n",
              "      <td>52.457447</td>\n",
              "      <td>48.030243</td>\n",
              "      <td>46.667103</td>\n",
              "      <td>49.047707</td>\n",
              "      <td>50.730297</td>\n",
              "      <td>50.981152</td>\n",
              "      <td>57.172039</td>\n",
              "      <td>57.358170</td>\n",
              "      <td>53.646797</td>\n",
              "      <td>46.977356</td>\n",
              "      <td>74.018333</td>\n",
              "      <td>94.635040</td>\n",
              "      <td>102.830521</td>\n",
              "      <td>119.627518</td>\n",
              "      <td>139.344162</td>\n",
              "      <td>132.953918</td>\n",
              "      <td>123.598289</td>\n",
              "      <td>120.219666</td>\n",
              "      <td>120.585144</td>\n",
              "      <td>121.410912</td>\n",
              "      <td>120.493019</td>\n",
              "      <td>119.528015</td>\n",
              "      <td>120.342682</td>\n",
              "      <td>117.303307</td>\n",
              "      <td>108.077209</td>\n",
              "      <td>98.639572</td>\n",
              "      <td>91.359558</td>\n",
              "      <td>96.984512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>127.0</td>\n",
              "      <td>75.739410</td>\n",
              "      <td>75.409943</td>\n",
              "      <td>74.833214</td>\n",
              "      <td>73.969299</td>\n",
              "      <td>73.363701</td>\n",
              "      <td>72.417938</td>\n",
              "      <td>69.430283</td>\n",
              "      <td>67.942154</td>\n",
              "      <td>67.325806</td>\n",
              "      <td>65.022881</td>\n",
              "      <td>60.963860</td>\n",
              "      <td>58.639591</td>\n",
              "      <td>59.143158</td>\n",
              "      <td>54.533325</td>\n",
              "      <td>51.555336</td>\n",
              "      <td>50.613426</td>\n",
              "      <td>51.463821</td>\n",
              "      <td>75.113159</td>\n",
              "      <td>90.858391</td>\n",
              "      <td>91.001358</td>\n",
              "      <td>90.452408</td>\n",
              "      <td>93.391220</td>\n",
              "      <td>94.706116</td>\n",
              "      <td>108.794289</td>\n",
              "      <td>126.101120</td>\n",
              "      <td>118.448944</td>\n",
              "      <td>96.106453</td>\n",
              "      <td>88.772713</td>\n",
              "      <td>72.974075</td>\n",
              "      <td>73.341995</td>\n",
              "      <td>74.309807</td>\n",
              "      <td>77.003220</td>\n",
              "      <td>80.457634</td>\n",
              "      <td>77.305161</td>\n",
              "      <td>70.765144</td>\n",
              "      <td>68.587074</td>\n",
              "      <td>63.275837</td>\n",
              "      <td>59.411491</td>\n",
              "      <td>59.862236</td>\n",
              "      <td>...</td>\n",
              "      <td>42.250420</td>\n",
              "      <td>47.287308</td>\n",
              "      <td>51.983013</td>\n",
              "      <td>51.966274</td>\n",
              "      <td>51.061878</td>\n",
              "      <td>51.390972</td>\n",
              "      <td>50.322769</td>\n",
              "      <td>50.064480</td>\n",
              "      <td>50.443798</td>\n",
              "      <td>53.547398</td>\n",
              "      <td>56.700352</td>\n",
              "      <td>58.799305</td>\n",
              "      <td>88.567680</td>\n",
              "      <td>88.217255</td>\n",
              "      <td>88.911896</td>\n",
              "      <td>89.598854</td>\n",
              "      <td>88.627998</td>\n",
              "      <td>89.288559</td>\n",
              "      <td>88.579956</td>\n",
              "      <td>85.570709</td>\n",
              "      <td>67.726265</td>\n",
              "      <td>44.972839</td>\n",
              "      <td>43.295059</td>\n",
              "      <td>42.718952</td>\n",
              "      <td>38.955296</td>\n",
              "      <td>33.500900</td>\n",
              "      <td>36.781261</td>\n",
              "      <td>41.534443</td>\n",
              "      <td>46.838799</td>\n",
              "      <td>53.010788</td>\n",
              "      <td>56.103416</td>\n",
              "      <td>58.269325</td>\n",
              "      <td>57.246265</td>\n",
              "      <td>57.208073</td>\n",
              "      <td>54.218365</td>\n",
              "      <td>50.636616</td>\n",
              "      <td>53.676117</td>\n",
              "      <td>57.261581</td>\n",
              "      <td>58.935459</td>\n",
              "      <td>62.211670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>190.0</td>\n",
              "      <td>77.613190</td>\n",
              "      <td>77.997665</td>\n",
              "      <td>72.510139</td>\n",
              "      <td>71.488533</td>\n",
              "      <td>74.430908</td>\n",
              "      <td>78.200890</td>\n",
              "      <td>81.308365</td>\n",
              "      <td>78.656181</td>\n",
              "      <td>77.130081</td>\n",
              "      <td>73.938713</td>\n",
              "      <td>63.467251</td>\n",
              "      <td>58.996120</td>\n",
              "      <td>60.886089</td>\n",
              "      <td>61.631798</td>\n",
              "      <td>60.422264</td>\n",
              "      <td>64.441101</td>\n",
              "      <td>64.964867</td>\n",
              "      <td>65.213295</td>\n",
              "      <td>63.359661</td>\n",
              "      <td>51.213848</td>\n",
              "      <td>52.174290</td>\n",
              "      <td>60.200657</td>\n",
              "      <td>62.866142</td>\n",
              "      <td>64.500053</td>\n",
              "      <td>69.590904</td>\n",
              "      <td>72.872459</td>\n",
              "      <td>76.891960</td>\n",
              "      <td>80.249084</td>\n",
              "      <td>77.706039</td>\n",
              "      <td>80.283104</td>\n",
              "      <td>82.905708</td>\n",
              "      <td>76.826141</td>\n",
              "      <td>73.362091</td>\n",
              "      <td>76.635674</td>\n",
              "      <td>77.057289</td>\n",
              "      <td>79.245430</td>\n",
              "      <td>79.682327</td>\n",
              "      <td>74.721649</td>\n",
              "      <td>63.423595</td>\n",
              "      <td>...</td>\n",
              "      <td>41.112793</td>\n",
              "      <td>37.548920</td>\n",
              "      <td>33.309582</td>\n",
              "      <td>33.967533</td>\n",
              "      <td>34.310577</td>\n",
              "      <td>37.421936</td>\n",
              "      <td>38.814186</td>\n",
              "      <td>41.019829</td>\n",
              "      <td>39.881439</td>\n",
              "      <td>37.123325</td>\n",
              "      <td>47.015293</td>\n",
              "      <td>80.454842</td>\n",
              "      <td>73.489410</td>\n",
              "      <td>105.888535</td>\n",
              "      <td>137.227234</td>\n",
              "      <td>82.703148</td>\n",
              "      <td>76.905594</td>\n",
              "      <td>81.946251</td>\n",
              "      <td>82.384377</td>\n",
              "      <td>84.392342</td>\n",
              "      <td>86.302498</td>\n",
              "      <td>89.945038</td>\n",
              "      <td>91.651855</td>\n",
              "      <td>92.883759</td>\n",
              "      <td>92.712799</td>\n",
              "      <td>91.159996</td>\n",
              "      <td>76.337387</td>\n",
              "      <td>46.146812</td>\n",
              "      <td>38.451965</td>\n",
              "      <td>34.206421</td>\n",
              "      <td>32.113571</td>\n",
              "      <td>36.994015</td>\n",
              "      <td>43.320885</td>\n",
              "      <td>50.344479</td>\n",
              "      <td>50.513901</td>\n",
              "      <td>50.775177</td>\n",
              "      <td>49.691517</td>\n",
              "      <td>49.993790</td>\n",
              "      <td>53.464928</td>\n",
              "      <td>59.269695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>177.0</td>\n",
              "      <td>75.359726</td>\n",
              "      <td>107.797653</td>\n",
              "      <td>123.991295</td>\n",
              "      <td>118.290840</td>\n",
              "      <td>87.559036</td>\n",
              "      <td>81.610794</td>\n",
              "      <td>102.345383</td>\n",
              "      <td>110.453262</td>\n",
              "      <td>114.304909</td>\n",
              "      <td>120.418358</td>\n",
              "      <td>121.272675</td>\n",
              "      <td>121.391472</td>\n",
              "      <td>122.573067</td>\n",
              "      <td>125.855904</td>\n",
              "      <td>118.316986</td>\n",
              "      <td>95.139671</td>\n",
              "      <td>48.208618</td>\n",
              "      <td>39.192822</td>\n",
              "      <td>42.423634</td>\n",
              "      <td>43.567875</td>\n",
              "      <td>53.460846</td>\n",
              "      <td>94.536713</td>\n",
              "      <td>113.042534</td>\n",
              "      <td>93.344498</td>\n",
              "      <td>80.316597</td>\n",
              "      <td>78.235329</td>\n",
              "      <td>78.456215</td>\n",
              "      <td>76.995331</td>\n",
              "      <td>84.284042</td>\n",
              "      <td>100.904686</td>\n",
              "      <td>119.140457</td>\n",
              "      <td>112.725266</td>\n",
              "      <td>98.626465</td>\n",
              "      <td>84.961304</td>\n",
              "      <td>76.564873</td>\n",
              "      <td>92.263435</td>\n",
              "      <td>106.915848</td>\n",
              "      <td>117.500801</td>\n",
              "      <td>122.704773</td>\n",
              "      <td>...</td>\n",
              "      <td>97.060387</td>\n",
              "      <td>100.324516</td>\n",
              "      <td>100.394043</td>\n",
              "      <td>97.067337</td>\n",
              "      <td>88.930984</td>\n",
              "      <td>61.325573</td>\n",
              "      <td>54.229721</td>\n",
              "      <td>56.571384</td>\n",
              "      <td>56.541122</td>\n",
              "      <td>56.497971</td>\n",
              "      <td>56.182411</td>\n",
              "      <td>56.661076</td>\n",
              "      <td>90.143692</td>\n",
              "      <td>92.498886</td>\n",
              "      <td>95.266747</td>\n",
              "      <td>96.286186</td>\n",
              "      <td>93.996162</td>\n",
              "      <td>89.082123</td>\n",
              "      <td>72.991600</td>\n",
              "      <td>43.285069</td>\n",
              "      <td>39.362057</td>\n",
              "      <td>87.701797</td>\n",
              "      <td>107.911224</td>\n",
              "      <td>108.963867</td>\n",
              "      <td>100.050972</td>\n",
              "      <td>94.212280</td>\n",
              "      <td>94.479683</td>\n",
              "      <td>94.612007</td>\n",
              "      <td>94.633812</td>\n",
              "      <td>98.844284</td>\n",
              "      <td>97.635315</td>\n",
              "      <td>86.906784</td>\n",
              "      <td>63.476364</td>\n",
              "      <td>54.346996</td>\n",
              "      <td>57.138462</td>\n",
              "      <td>54.495735</td>\n",
              "      <td>56.935326</td>\n",
              "      <td>57.421776</td>\n",
              "      <td>54.379036</td>\n",
              "      <td>54.671799</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Width           0           1  ...        781        782        783\n",
              "0  124.0    0.000000    0.000000  ...   0.000000   0.000000   0.000000\n",
              "1  117.0  177.598206  177.623062  ...  98.639572  91.359558  96.984512\n",
              "2  127.0   75.739410   75.409943  ...  57.261581  58.935459  62.211670\n",
              "5  190.0   77.613190   77.997665  ...  49.993790  53.464928  59.269695\n",
              "7  177.0   75.359726  107.797653  ...  57.421776  54.379036  54.671799\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "Area = np.array(PSD_new['Area'])\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "Vfk_fNXGDK5_",
        "outputId": "d81dcd15-5a67-4063-9e66-ed11a2a2f310"
      },
      "source": [
        " wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        " wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        " X = pd.DataFrame([Diam1,Diameter_All])\n",
        " wts = pd.DataFrame([wt1,wt2])\n",
        "plt.hist(X,weights=wts)\n",
        "plt.legend(['Image J','CNN'])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f3b2fdede90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "ZZHa1j4HT9Dq",
        "outputId": "dbc9224d-a7c4-40c8-8388-b22838479ddf"
      },
      "source": [
        "counts, bins, bars = plt.hist(X,weights=wts)\n",
        "print(bars)\n",
        "print(bins)\n",
        "print(counts)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<a list of 2 Lists of Patches objects>\n",
            "[0.72648699 0.88558221 1.04467743 1.20377265 1.36286787 1.52196309\n",
            " 1.68105831 1.84015353 1.99924875 2.15834397 2.31743919]\n",
            "[[ 6.31578947  9.47368421 23.15789474 33.68421053 15.78947368  8.42105263\n",
            "   1.05263158  0.          1.05263158  1.05263158]\n",
            " [ 2.85714286 17.14285714 22.85714286  8.57142857 17.14285714 17.14285714\n",
            "  14.28571429  0.          0.          0.        ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o_vDGeWUwIZ",
        "outputId": "11da28e8-9d05-4ce0-9cf1-2ba9c3439c59"
      },
      "source": [
        "print(counts.sum())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200.00000000000014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "KcH52-6iJQ8t",
        "outputId": "ba8f1430-bc12-41e7-ea3d-f29fa21ea449"
      },
      "source": [
        "\n",
        "plt.hist([Diam1,Diameter_All])\n",
        "plt.legend(['Image J','CNN'])\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f3b2f91be50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r11AxFK_JIii",
        "outputId": "7fcc4b82-4d92-4cd5-ca88-3714a4ee4ea3"
      },
      "source": [
        "[Diam1,Diameter_All]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.59616801403081,\n",
              "  1.0217907939900581,\n",
              "  1.2716187407449044,\n",
              "  1.104429030701514,\n",
              "  1.2163487785097904,\n",
              "  1.6013445735058454,\n",
              "  1.1715597420637607,\n",
              "  1.2534662333717612,\n",
              "  1.2676073151634049,\n",
              "  1.309600575274104,\n",
              "  1.292966945531582,\n",
              "  1.7658322811231006,\n",
              "  1.3564037533648712,\n",
              "  1.2407040781688483,\n",
              "  2.130217298173151,\n",
              "  1.4228319915327,\n",
              "  1.0651086490865755,\n",
              "  1.3008210311003705,\n",
              "  1.336545951796433,\n",
              "  0.8927754224911278,\n",
              "  1.4494292838262302,\n",
              "  1.4052738287907582,\n",
              "  1.6421697097891788,\n",
              "  1.2329833804288621,\n",
              "  1.19042665178928,\n",
              "  1.1682948223612457,\n",
              "  1.1518314137121108,\n",
              "  0.9607802401865855,\n",
              "  2.317439190074449,\n",
              "  1.0591147430338594,\n",
              "  1.4308630919602832,\n",
              "  0.7535680705496237,\n",
              "  0.8608283307581511,\n",
              "  1.2776122636975893,\n",
              "  1.3745862957220916,\n",
              "  1.259546137598783,\n",
              "  1.2978813187979172,\n",
              "  1.2412170838050638,\n",
              "  1.6009469708743893,\n",
              "  1.3149369953539032,\n",
              "  1.417901703622935,\n",
              "  1.2478669653497139,\n",
              "  1.1055812783082735,\n",
              "  0.9561307405997607,\n",
              "  0.9487783503683882,\n",
              "  1.1238565871041026,\n",
              "  1.2058356273089446,\n",
              "  1.2801012827406097,\n",
              "  0.8733100751144249,\n",
              "  0.9194732501297403,\n",
              "  1.6425573339441792,\n",
              "  1.085826790250066,\n",
              "  1.0639125693728595,\n",
              "  1.0875842666474016,\n",
              "  1.417901703622935,\n",
              "  1.550443891425932,\n",
              "  0.7825779328716171,\n",
              "  1.4690612745308145,\n",
              "  1.053086721720641,\n",
              "  1.2676073151634049,\n",
              "  0.7744003006005755,\n",
              "  1.3787482149724068,\n",
              "  1.363892581861956,\n",
              "  1.299352006316543,\n",
              "  1.2870449283923413,\n",
              "  1.11817763925502,\n",
              "  0.9474354220939228,\n",
              "  1.5218484589055707,\n",
              "  1.3526437911676632,\n",
              "  1.1556938532445284,\n",
              "  1.6013445735058454,\n",
              "  1.274619025074578,\n",
              "  1.422384489715834,\n",
              "  1.3408259533459403,\n",
              "  1.172646028567008,\n",
              "  1.1490645795125545,\n",
              "  1.459060149136146,\n",
              "  1.2483770274864237,\n",
              "  1.336545951796433,\n",
              "  0.9601174044814821,\n",
              "  1.4867225193896279,\n",
              "  1.4277452542806772,\n",
              "  1.35028849808504,\n",
              "  0.7560982446653928,\n",
              "  1.259040600296622,\n",
              "  1.13456827900627,\n",
              "  1.6549133695530214,\n",
              "  1.1204526724091788,\n",
              "  1.1176081573544434,\n",
              "  0.9153095762832032,\n",
              "  1.1639273497938836,\n",
              "  1.3066806149514323,\n",
              "  1.1529362882239027,\n",
              "  1.3047303442899274,\n",
              "  1.3066806149514323],\n",
              " [1.2050462112763163,\n",
              "  1.1370194090268468,\n",
              "  1.0823538941477302,\n",
              "  1.6386885891721048,\n",
              "  1.3751980898036082,\n",
              "  0.8912536229125034,\n",
              "  1.027449450422443,\n",
              "  1.7035033246447802,\n",
              "  1.51586171768035,\n",
              "  0.9988998254326201,\n",
              "  1.0725409791352554,\n",
              "  1.7005126126297643,\n",
              "  1.167618552631259,\n",
              "  1.283162676083402,\n",
              "  1.683694130651539,\n",
              "  1.4653763837250726,\n",
              "  1.604415278919866,\n",
              "  1.6886649280191488,\n",
              "  1.2423144124260457,\n",
              "  1.6175100403399518,\n",
              "  1.082514630948658,\n",
              "  1.1020393559585284,\n",
              "  1.0556700007698157,\n",
              "  1.6905054182783497,\n",
              "  1.3781797427031288,\n",
              "  1.0424739539120382,\n",
              "  1.3953975116863941,\n",
              "  0.7264869902684433,\n",
              "  1.1395502040375511,\n",
              "  1.5012184055107816,\n",
              "  1.5263618991810295,\n",
              "  1.5247919221501713,\n",
              "  1.66986997562247,\n",
              "  0.9237517248271501,\n",
              "  0.9487727360572904]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    }
  ]
}