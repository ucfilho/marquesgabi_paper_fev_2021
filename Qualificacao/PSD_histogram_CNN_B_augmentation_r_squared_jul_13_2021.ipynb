{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_CNN_B_augmentation_r_squared_jul_13_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/Qualificacao/PSD_histogram_CNN_B_augmentation_r_squared_jul_13_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from keras.preprocessing import image\n",
        "from sklearn.metrics import r2_score"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZEvJvfoibE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7867b680-394a-456f-8b90-79e759354d6d"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mahotas\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/ad/553b246b0a35dccc3ed58dc8889a67124bf5ab858e9c6b7255d56086e70c/mahotas-1.4.11-cp37-cp37m-manylinux2010_x86_64.whl (5.7MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7MB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.19.5)\n",
            "Installing collected packages: mahotas\n",
            "Successfully installed mahotas-1.4.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT"
      },
      "source": [
        "import mahotas.features.texture as mht\n",
        "import mahotas.features"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VcTdaNVh9EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c87b164-6a50-4f3d-dd7b-eec0e909b154"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_fev_2020'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 73 (delta 37), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n",
            "/content/marquesgabi_fev_2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v7SRrc8mH2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b991c3a1-806f-4ff9-95bc-ef5ccfa46146"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip'\n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Doutorado'...\n",
            "remote: Enumerating objects: 361, done.\u001b[K\n",
            "remote: Counting objects: 100% (111/111), done.\u001b[K\n",
            "remote: Compressing objects: 100% (110/110), done.\u001b[K\n",
            "remote: Total 361 (delta 38), reused 0 (delta 0), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (361/361), 202.49 MiB | 28.40 MiB/s, done.\n",
            "Resolving deltas: 100% (155/155), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[4] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHgqAnaFyCjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5284d6d5-eb72-4634-e62f-7c6f54d29027"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (160/160), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 3 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 7.67 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from segment_filter_not_conclude import Segmenta  # got image provided segmented"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5MN5a_v4np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0814e810-1f8f-49d3-abf0-f35754ba0434"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     173  136.032104  135.526520  ...  113.509865  108.506096  108.751938\n",
            "1     175   64.631996   71.256004  ...  126.796791  112.961594   95.508789\n",
            "2     185  112.722061  124.327148  ...  136.917953  137.842987  136.229370\n",
            "3     185  194.708801  157.562515  ...  187.278259  205.118271  172.416855\n",
            "4     113  186.488922  180.061295  ...  118.307465  119.903831  118.914719\n",
            "5     126  109.592590  108.802467  ...  128.370361  130.481476  132.259262\n",
            "6     180  124.392593  129.398529  ...  149.203461  136.563950  150.416321\n",
            "7     113  105.986847   98.876511  ...  149.873825  153.521805  154.920959\n",
            "8     140  110.879997  109.399994  ...   50.279999   62.840000   45.119999\n",
            "9     140  199.479996  209.799988  ...  114.959999  129.879990  137.639999\n",
            "10    129  127.934380  129.951813  ...  200.378693  208.210815  202.750565\n",
            "11    143  147.429688  173.579453  ...  184.624924  102.251801  125.684731\n",
            "12    162  123.234566  120.336533  ...  192.615295  154.166595  153.871826\n",
            "13    104  178.042908  169.016296  ...  236.571045  225.068054  226.588776\n",
            "14    128  144.124023  141.672852  ...  138.221680  143.590820  145.330078\n",
            "15    117   72.076561   71.503838  ...  170.520340  161.110672  153.019363\n",
            "16    122  127.584785  120.715668  ...  146.263641  159.805161  174.443695\n",
            "17    144  186.336426  211.862671  ...  160.647400   87.117294  105.204483\n",
            "18    172  206.517044  199.159561  ...  158.753387  172.012985  179.282333\n",
            "19    164   93.450333   93.026764  ...    0.826294    0.224271    1.370613\n",
            "20    132   87.073471   80.984398  ...  166.111115  154.328751  142.422424\n",
            "21    163   44.820614   40.011406  ...    0.810079    0.231962    1.373066\n",
            "22    170    1.356540    0.183945  ...    1.000000    0.602768    0.004014\n",
            "23    151   79.674400   91.336082  ...    0.623788    0.332398    1.405245\n",
            "24    148  201.417114  199.416367  ...  178.823227  164.908707  154.816666\n",
            "25    156  148.572647  159.818542  ...   90.634453   75.927681   52.130180\n",
            "26    200   91.654396  125.138008  ...  134.985199  132.133194  124.118011\n",
            "27    162  110.956558  117.136414  ...  157.726715  150.594589  141.784332\n",
            "28    171  120.797585   93.857796  ...  176.468750  175.318726  170.228683\n",
            "29    185  121.960869  120.776352  ...  247.729050  217.430359  133.298813\n",
            "30    135   81.989738   26.558517  ...  118.636581  102.970261   65.626938\n",
            "31    190  112.698830  118.035553  ...    2.293185    1.296731    0.216620\n",
            "32    105  126.457787  121.275566  ...    0.035556    0.902222    1.604445\n",
            "33    108   63.717422   58.927292  ...  142.891617  144.455414  143.865555\n",
            "34    114  135.832565  132.880875  ...   99.003708   96.377960   85.622658\n",
            "35    187  129.398315  143.078445  ...  190.478363  184.822021  176.134933\n",
            "36    104  130.165680  156.575455  ...  204.322510  214.662735  210.689377\n",
            "37    169  130.615692  123.365517  ...    1.000000    1.000000    1.000000\n",
            "38    114  153.419510  162.432129  ...    0.012927    0.779624    1.551554\n",
            "39    174  252.850204  248.098984  ...  165.671570  168.507355  166.232010\n",
            "40    147    1.417234    0.369615  ...    0.702948    1.417234    0.419501\n",
            "41    182  180.420135  163.414215  ...  212.952682  194.733734  149.739655\n",
            "42    164  110.783463  106.606186  ...  152.138016  146.803085  139.178467\n",
            "43    114  183.400742  181.924591  ...  119.507545  128.931976  132.280090\n",
            "44    162  155.509979  157.674301  ...    0.135802    0.436671    1.000000\n",
            "45    121  220.028061  220.783905  ...   40.519093   53.205791   94.773438\n",
            "46    153  108.535690  110.699646  ...  177.317368  189.632446  206.812347\n",
            "47    122  134.655472  123.636917  ...  209.031693  179.325195  149.755157\n",
            "48    194  177.622910  128.243164  ...   63.123814   87.926765  103.549454\n",
            "49    120  137.490005  141.656677  ...    1.000000    1.000000    1.000000\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzpQ1Pz0fX5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7724f744-8ef5-4df9-e119-298adce14ddf"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines\n",
        "# filename = 'model_ANN.pkl'\n",
        "filename = 'model_ANN_new.pkl'\n",
        "model = joblib.load(filename)\n",
        "'''"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n!git clone https://github.com/ucfilho/MarquesGabi_Routines\\n%cd MarquesGabi_Routines\\n# filename = 'model_ANN.pkl'\\nfilename = 'model_ANN_new.pkl'\\nmodel = joblib.load(filename)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2emP4rNjQy",
        "outputId": "242f4d39-ea59-4618-828c-1ed3163c1af5"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (160/160), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 3 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 23.37 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "# make the CNN\n",
        "# model.add(Input(shape=(28, 28, 1)))\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=32, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=200))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=10))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbPQ1FSRG6A",
        "outputId": "5ef379e4-a63a-47af-ea69-848ca4c11f9f"
      },
      "source": [
        "\n",
        "# gives us back a <keras.callbacks.History object at 0x112e61a90>\n",
        "model.fit(X_train, Y_train, epochs=200, batch_size=32)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 46s 17ms/step - loss: 0.6827 - accuracy: 0.6075\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.3739 - accuracy: 0.8293\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1742 - accuracy: 0.9294\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0984 - accuracy: 0.9631\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0566 - accuracy: 0.9828\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0541 - accuracy: 0.9829\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0422 - accuracy: 0.9840\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0266 - accuracy: 0.9946\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0110 - accuracy: 0.9993\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0048 - accuracy: 0.9993\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0119 - accuracy: 0.9958\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0028 - accuracy: 0.9986\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0033 - accuracy: 0.9986\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 0.9986\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0155 - accuracy: 0.9941\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.9920\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0247 - accuracy: 0.9858\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0357 - accuracy: 0.9849\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0697 - accuracy: 0.9728\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0206 - accuracy: 0.9946\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.9887\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0055 - accuracy: 0.9967\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.7009e-04 - accuracy: 1.0000\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0075 - accuracy: 0.9945\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.3488e-04 - accuracy: 1.0000\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.6474e-04 - accuracy: 1.0000\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5466e-04 - accuracy: 1.0000\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4400e-04 - accuracy: 1.0000\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6994e-04 - accuracy: 1.0000\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4109e-04 - accuracy: 1.0000\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.4437e-05 - accuracy: 1.0000\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9989e-04 - accuracy: 1.0000\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0659e-04 - accuracy: 1.0000\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5749e-04 - accuracy: 1.0000\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7977e-05 - accuracy: 1.0000\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0129e-04 - accuracy: 1.0000\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.7699e-05 - accuracy: 1.0000\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2323e-04 - accuracy: 1.0000\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6738e-05 - accuracy: 1.0000\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.9756e-05 - accuracy: 1.0000\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.1051e-05 - accuracy: 1.0000\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7881e-05 - accuracy: 1.0000\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.4381e-05 - accuracy: 1.0000\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.8650e-05 - accuracy: 1.0000\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.9235e-05 - accuracy: 1.0000\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2638e-05 - accuracy: 1.0000\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 4.6633e-05 - accuracy: 1.0000\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 1.8733e-04 - accuracy: 1.0000\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5301e-04 - accuracy: 1.0000\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.2168e-05 - accuracy: 1.0000\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0942e-04 - accuracy: 1.0000\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1781e-04 - accuracy: 1.0000\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.2081e-05 - accuracy: 1.0000\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2564e-04 - accuracy: 1.0000\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.2687e-05 - accuracy: 1.0000\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.8792e-05 - accuracy: 1.0000\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.2040e-05 - accuracy: 1.0000\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.6050e-04 - accuracy: 1.0000\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6489e-05 - accuracy: 1.0000\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.1677e-05 - accuracy: 1.0000\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4197e-04 - accuracy: 1.0000\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.4193e-05 - accuracy: 1.0000\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.2930e-05 - accuracy: 1.0000\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.0921e-05 - accuracy: 1.0000\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.7277e-05 - accuracy: 1.0000\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9366e-05 - accuracy: 1.0000\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.4077e-05 - accuracy: 1.0000\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5452e-04 - accuracy: 1.0000\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.6247e-05 - accuracy: 1.0000\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.5549e-05 - accuracy: 1.0000\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.4590e-05 - accuracy: 1.0000\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.4908e-05 - accuracy: 1.0000\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.2041e-05 - accuracy: 1.0000\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9087e-05 - accuracy: 1.0000\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.8688e-05 - accuracy: 1.0000\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0324e-05 - accuracy: 1.0000\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.6382e-05 - accuracy: 1.0000\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.3800e-05 - accuracy: 1.0000\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6943e-05 - accuracy: 1.0000\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5942e-04 - accuracy: 1.0000\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.3038e-05 - accuracy: 1.0000\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0920e-05 - accuracy: 1.0000\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6692e-05 - accuracy: 1.0000\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8014e-05 - accuracy: 1.0000\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1757e-05 - accuracy: 1.0000\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.8233e-05 - accuracy: 1.0000\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.0878e-05 - accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.1452e-05 - accuracy: 1.0000\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2503e-05 - accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0105e-05 - accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4824e-05 - accuracy: 1.0000\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4349e-05 - accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.9611e-05 - accuracy: 1.0000\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4517e-05 - accuracy: 1.0000\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4897e-04 - accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4192e-05 - accuracy: 1.0000\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5459e-05 - accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2247e-05 - accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.2456e-05 - accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.2885e-05 - accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.8097e-06 - accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4198e-05 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.1871e-06 - accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6576e-05 - accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1048e-05 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6825e-04 - accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.3948e-05 - accuracy: 1.0000\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.7182e-05 - accuracy: 1.0000\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6506e-05 - accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.0600e-06 - accuracy: 1.0000\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.0303e-05 - accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.1333e-05 - accuracy: 1.0000\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3108e-05 - accuracy: 1.0000\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.6696e-05 - accuracy: 1.0000\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0061e-05 - accuracy: 1.0000\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1021e-05 - accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3686e-05 - accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.0759e-04 - accuracy: 1.0000\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.7475e-05 - accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.4469e-06 - accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1478e-05 - accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.4034e-05 - accuracy: 1.0000\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.0096e-06 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.4921e-05 - accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.3201e-05 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7520e-05 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.3898e-06 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4133e-04 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.7071e-06 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3672e-05 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.0078e-05 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1746e-05 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.0987e-05 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6584e-05 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8717e-05 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.2768e-05 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2616e-04 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.9286e-05 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.0151e-05 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.3646e-05 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0494e-04 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0783e-05 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.1675e-06 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2875e-05 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0307e-05 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.1824e-06 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.2590e-05 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.0724e-05 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7198e-05 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.4689e-05 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.5115e-06 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7885e-05 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.4979e-05 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.1276e-06 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.9366e-06 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.8002e-06 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4011e-05 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.9256e-06 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.5137e-06 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0222e-05 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.9222e-06 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.5675e-05 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2381e-05 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.8169e-06 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.9602e-06 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5710e-05 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4410e-05 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.6038e-06 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2757e-05 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.1057e-06 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1682e-05 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7302e-05 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.6175e-06 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.9274e-06 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.1837e-06 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.7494e-06 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.5328e-06 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.3242e-06 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4610e-05 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.1707e-06 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.4879e-06 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.4318e-06 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.9794e-06 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.2185e-06 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3927e-05 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.6218e-06 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7340e-06 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.8212e-06 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3587e-06 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f604a8755d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRaP8bHWNeZA",
        "outputId": "de346bcd-66d9-4088-82bf-c20cf414e697"
      },
      "source": [
        "\n",
        "# Fit with data augmentation\n",
        "# Note: if you run this AFTER calling the previous model.fit(), it will CONTINUE training where it left off\n",
        "batch_size = 5\n",
        "data_generator = image.ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "train_generator = data_generator.flow(X_train, Y_train, batch_size)\n",
        "steps_per_epoch = X_train.shape[0] // batch_size\n",
        "\n",
        "model.fit(train_generator, validation_data=(X_test, Y_test), steps_per_epoch=steps_per_epoch, epochs=200)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "68/68 [==============================] - 2s 17ms/step - loss: 0.8081 - accuracy: 0.6065 - val_loss: 15.4351 - val_accuracy: 0.5102\n",
            "Epoch 2/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.5892 - accuracy: 0.7189 - val_loss: 8.9320 - val_accuracy: 0.5102\n",
            "Epoch 3/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6206 - accuracy: 0.7012 - val_loss: 13.6313 - val_accuracy: 0.5102\n",
            "Epoch 4/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.5635 - accuracy: 0.8254 - val_loss: 27.4758 - val_accuracy: 0.5102\n",
            "Epoch 5/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.4845 - accuracy: 0.8314 - val_loss: 32.8097 - val_accuracy: 0.5102\n",
            "Epoch 6/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.4020 - accuracy: 0.8876 - val_loss: 33.2896 - val_accuracy: 0.5102\n",
            "Epoch 7/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.4062 - accuracy: 0.8817 - val_loss: 15.9340 - val_accuracy: 0.5102\n",
            "Epoch 8/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.3669 - accuracy: 0.8817 - val_loss: 40.6383 - val_accuracy: 0.5102\n",
            "Epoch 9/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.3856 - accuracy: 0.8639 - val_loss: 6.3816 - val_accuracy: 0.5102\n",
            "Epoch 10/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.3634 - accuracy: 0.8876 - val_loss: 12.8829 - val_accuracy: 0.5102\n",
            "Epoch 11/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.3833 - accuracy: 0.8728 - val_loss: 0.8175 - val_accuracy: 0.4898\n",
            "Epoch 12/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.3222 - accuracy: 0.8698 - val_loss: 2.9283 - val_accuracy: 0.5102\n",
            "Epoch 13/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.3604 - accuracy: 0.9024 - val_loss: 5.7295 - val_accuracy: 0.5102\n",
            "Epoch 14/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.3112 - accuracy: 0.8935 - val_loss: 30.2621 - val_accuracy: 0.5102\n",
            "Epoch 15/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.3700 - accuracy: 0.8698 - val_loss: 18.7210 - val_accuracy: 0.5102\n",
            "Epoch 16/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.3406 - accuracy: 0.8876 - val_loss: 8.1167 - val_accuracy: 0.5102\n",
            "Epoch 17/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2731 - accuracy: 0.9320 - val_loss: 0.9017 - val_accuracy: 0.4898\n",
            "Epoch 18/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2628 - accuracy: 0.9320 - val_loss: 0.6608 - val_accuracy: 0.5850\n",
            "Epoch 19/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.2860 - accuracy: 0.9112 - val_loss: 4.3523 - val_accuracy: 0.5102\n",
            "Epoch 20/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2776 - accuracy: 0.8994 - val_loss: 0.9516 - val_accuracy: 0.4898\n",
            "Epoch 21/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.2086 - accuracy: 0.9527 - val_loss: 4.6075 - val_accuracy: 0.5102\n",
            "Epoch 22/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.3064 - accuracy: 0.9024 - val_loss: 0.9850 - val_accuracy: 0.4898\n",
            "Epoch 23/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2383 - accuracy: 0.9290 - val_loss: 11.4635 - val_accuracy: 0.5102\n",
            "Epoch 24/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2491 - accuracy: 0.9231 - val_loss: 40.4362 - val_accuracy: 0.5102\n",
            "Epoch 25/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1969 - accuracy: 0.9497 - val_loss: 48.3659 - val_accuracy: 0.5102\n",
            "Epoch 26/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2504 - accuracy: 0.9260 - val_loss: 26.0630 - val_accuracy: 0.5102\n",
            "Epoch 27/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2123 - accuracy: 0.9438 - val_loss: 8.0043 - val_accuracy: 0.5102\n",
            "Epoch 28/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2129 - accuracy: 0.9349 - val_loss: 6.3010 - val_accuracy: 0.5102\n",
            "Epoch 29/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1576 - accuracy: 0.9408 - val_loss: 0.4492 - val_accuracy: 0.8299\n",
            "Epoch 30/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2098 - accuracy: 0.9260 - val_loss: 1.0709 - val_accuracy: 0.4898\n",
            "Epoch 31/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2148 - accuracy: 0.9112 - val_loss: 1.0736 - val_accuracy: 0.4898\n",
            "Epoch 32/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1816 - accuracy: 0.9290 - val_loss: 6.2504 - val_accuracy: 0.5102\n",
            "Epoch 33/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1647 - accuracy: 0.9231 - val_loss: 37.2470 - val_accuracy: 0.5102\n",
            "Epoch 34/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2311 - accuracy: 0.9024 - val_loss: 22.0808 - val_accuracy: 0.5102\n",
            "Epoch 35/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2212 - accuracy: 0.9142 - val_loss: 4.8899 - val_accuracy: 0.5102\n",
            "Epoch 36/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1706 - accuracy: 0.9320 - val_loss: 10.6473 - val_accuracy: 0.5102\n",
            "Epoch 37/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2398 - accuracy: 0.9320 - val_loss: 21.6202 - val_accuracy: 0.5102\n",
            "Epoch 38/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2654 - accuracy: 0.8964 - val_loss: 20.9482 - val_accuracy: 0.5102\n",
            "Epoch 39/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1514 - accuracy: 0.9467 - val_loss: 18.4224 - val_accuracy: 0.5102\n",
            "Epoch 40/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1260 - accuracy: 0.9556 - val_loss: 3.8500 - val_accuracy: 0.5102\n",
            "Epoch 41/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1868 - accuracy: 0.9349 - val_loss: 0.6986 - val_accuracy: 0.5102\n",
            "Epoch 42/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2181 - accuracy: 0.9201 - val_loss: 1.1603 - val_accuracy: 0.5102\n",
            "Epoch 43/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1579 - accuracy: 0.9438 - val_loss: 0.6034 - val_accuracy: 0.5782\n",
            "Epoch 44/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1997 - accuracy: 0.9408 - val_loss: 1.0868 - val_accuracy: 0.4898\n",
            "Epoch 45/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1076 - accuracy: 0.9645 - val_loss: 1.0897 - val_accuracy: 0.4898\n",
            "Epoch 46/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1837 - accuracy: 0.9349 - val_loss: 6.1039 - val_accuracy: 0.5102\n",
            "Epoch 47/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1965 - accuracy: 0.9260 - val_loss: 2.5838 - val_accuracy: 0.4898\n",
            "Epoch 48/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1593 - accuracy: 0.9320 - val_loss: 10.9490 - val_accuracy: 0.5102\n",
            "Epoch 49/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1441 - accuracy: 0.9438 - val_loss: 6.1823 - val_accuracy: 0.5102\n",
            "Epoch 50/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1477 - accuracy: 0.9408 - val_loss: 33.9734 - val_accuracy: 0.4898\n",
            "Epoch 51/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1051 - accuracy: 0.9675 - val_loss: 1.1008 - val_accuracy: 0.4898\n",
            "Epoch 52/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1692 - accuracy: 0.9320 - val_loss: 1.4415 - val_accuracy: 0.4898\n",
            "Epoch 53/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1634 - accuracy: 0.9379 - val_loss: 1.0590 - val_accuracy: 0.4898\n",
            "Epoch 54/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1362 - accuracy: 0.9497 - val_loss: 13.9061 - val_accuracy: 0.5102\n",
            "Epoch 55/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1217 - accuracy: 0.9586 - val_loss: 1.8179 - val_accuracy: 0.5102\n",
            "Epoch 56/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1944 - accuracy: 0.9349 - val_loss: 2.5545 - val_accuracy: 0.4898\n",
            "Epoch 57/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1227 - accuracy: 0.9467 - val_loss: 1.1001 - val_accuracy: 0.4898\n",
            "Epoch 58/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1179 - accuracy: 0.9408 - val_loss: 36.9455 - val_accuracy: 0.4898\n",
            "Epoch 59/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1596 - accuracy: 0.9349 - val_loss: 9.4286 - val_accuracy: 0.5102\n",
            "Epoch 60/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1504 - accuracy: 0.9467 - val_loss: 43.6716 - val_accuracy: 0.4898\n",
            "Epoch 61/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1479 - accuracy: 0.9408 - val_loss: 12.5121 - val_accuracy: 0.4898\n",
            "Epoch 62/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1071 - accuracy: 0.9497 - val_loss: 0.9956 - val_accuracy: 0.5306\n",
            "Epoch 63/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1349 - accuracy: 0.9497 - val_loss: 29.7971 - val_accuracy: 0.4898\n",
            "Epoch 64/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1563 - accuracy: 0.9438 - val_loss: 1.9486 - val_accuracy: 0.4898\n",
            "Epoch 65/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1397 - accuracy: 0.9349 - val_loss: 4.3696 - val_accuracy: 0.5102\n",
            "Epoch 66/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1963 - accuracy: 0.9112 - val_loss: 1.0914 - val_accuracy: 0.4898\n",
            "Epoch 67/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1812 - accuracy: 0.9379 - val_loss: 3.8475 - val_accuracy: 0.5102\n",
            "Epoch 68/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1448 - accuracy: 0.9467 - val_loss: 10.0845 - val_accuracy: 0.5102\n",
            "Epoch 69/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0871 - accuracy: 0.9586 - val_loss: 1.0868 - val_accuracy: 0.4898\n",
            "Epoch 70/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1381 - accuracy: 0.9527 - val_loss: 18.8954 - val_accuracy: 0.4898\n",
            "Epoch 71/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1484 - accuracy: 0.9438 - val_loss: 10.1291 - val_accuracy: 0.4898\n",
            "Epoch 72/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1073 - accuracy: 0.9618 - val_loss: 31.4946 - val_accuracy: 0.4898\n",
            "Epoch 73/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1428 - accuracy: 0.9467 - val_loss: 11.4578 - val_accuracy: 0.4898\n",
            "Epoch 74/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1260 - accuracy: 0.9467 - val_loss: 1.0910 - val_accuracy: 0.4898\n",
            "Epoch 75/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0878 - accuracy: 0.9734 - val_loss: 6.4110 - val_accuracy: 0.4898\n",
            "Epoch 76/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0866 - accuracy: 0.9645 - val_loss: 1.0894 - val_accuracy: 0.4898\n",
            "Epoch 77/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1343 - accuracy: 0.9527 - val_loss: 27.2981 - val_accuracy: 0.4898\n",
            "Epoch 78/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1141 - accuracy: 0.9467 - val_loss: 34.1437 - val_accuracy: 0.4898\n",
            "Epoch 79/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1384 - accuracy: 0.9467 - val_loss: 32.1239 - val_accuracy: 0.4898\n",
            "Epoch 80/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1402 - accuracy: 0.9467 - val_loss: 22.6235 - val_accuracy: 0.4898\n",
            "Epoch 81/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1557 - accuracy: 0.9527 - val_loss: 27.7066 - val_accuracy: 0.4898\n",
            "Epoch 82/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1495 - accuracy: 0.9379 - val_loss: 30.3402 - val_accuracy: 0.4898\n",
            "Epoch 83/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0816 - accuracy: 0.9734 - val_loss: 27.0587 - val_accuracy: 0.4898\n",
            "Epoch 84/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1039 - accuracy: 0.9704 - val_loss: 38.3342 - val_accuracy: 0.4898\n",
            "Epoch 85/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1125 - accuracy: 0.9615 - val_loss: 30.9098 - val_accuracy: 0.4898\n",
            "Epoch 86/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0999 - accuracy: 0.9556 - val_loss: 8.4759 - val_accuracy: 0.4898\n",
            "Epoch 87/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1498 - accuracy: 0.9615 - val_loss: 9.7686 - val_accuracy: 0.5102\n",
            "Epoch 88/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0909 - accuracy: 0.9704 - val_loss: 21.5176 - val_accuracy: 0.4898\n",
            "Epoch 89/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1244 - accuracy: 0.9408 - val_loss: 11.4208 - val_accuracy: 0.4898\n",
            "Epoch 90/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1205 - accuracy: 0.9408 - val_loss: 9.3478 - val_accuracy: 0.4898\n",
            "Epoch 91/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1099 - accuracy: 0.9586 - val_loss: 32.7608 - val_accuracy: 0.4898\n",
            "Epoch 92/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1297 - accuracy: 0.9320 - val_loss: 8.5971 - val_accuracy: 0.4898\n",
            "Epoch 93/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1436 - accuracy: 0.9349 - val_loss: 0.6833 - val_accuracy: 0.5986\n",
            "Epoch 94/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1082 - accuracy: 0.9527 - val_loss: 1.0979 - val_accuracy: 0.4898\n",
            "Epoch 95/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0997 - accuracy: 0.9645 - val_loss: 1.0985 - val_accuracy: 0.4898\n",
            "Epoch 96/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0923 - accuracy: 0.9556 - val_loss: 5.5691 - val_accuracy: 0.5102\n",
            "Epoch 97/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0775 - accuracy: 0.9704 - val_loss: 8.1864 - val_accuracy: 0.4898\n",
            "Epoch 98/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0879 - accuracy: 0.9615 - val_loss: 10.6986 - val_accuracy: 0.5102\n",
            "Epoch 99/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0829 - accuracy: 0.9615 - val_loss: 13.4961 - val_accuracy: 0.4898\n",
            "Epoch 100/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1319 - accuracy: 0.9586 - val_loss: 10.7110 - val_accuracy: 0.4898\n",
            "Epoch 101/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1168 - accuracy: 0.9467 - val_loss: 12.2024 - val_accuracy: 0.4898\n",
            "Epoch 102/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0929 - accuracy: 0.9645 - val_loss: 1.0988 - val_accuracy: 0.4898\n",
            "Epoch 103/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0885 - accuracy: 0.9675 - val_loss: 5.5628 - val_accuracy: 0.4898\n",
            "Epoch 104/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0731 - accuracy: 0.9734 - val_loss: 3.9068 - val_accuracy: 0.4898\n",
            "Epoch 105/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1092 - accuracy: 0.9704 - val_loss: 21.7339 - val_accuracy: 0.4898\n",
            "Epoch 106/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0792 - accuracy: 0.9704 - val_loss: 25.8494 - val_accuracy: 0.4898\n",
            "Epoch 107/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0667 - accuracy: 0.9763 - val_loss: 47.5640 - val_accuracy: 0.4898\n",
            "Epoch 108/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1619 - accuracy: 0.9467 - val_loss: 10.3732 - val_accuracy: 0.4898\n",
            "Epoch 109/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1111 - accuracy: 0.9586 - val_loss: 1.6042 - val_accuracy: 0.4898\n",
            "Epoch 110/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0791 - accuracy: 0.9734 - val_loss: 1.1010 - val_accuracy: 0.4898\n",
            "Epoch 111/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0769 - accuracy: 0.9734 - val_loss: 8.5162 - val_accuracy: 0.5102\n",
            "Epoch 112/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0860 - accuracy: 0.9763 - val_loss: 5.1452 - val_accuracy: 0.5102\n",
            "Epoch 113/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1197 - accuracy: 0.9438 - val_loss: 1.1047 - val_accuracy: 0.4898\n",
            "Epoch 114/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0656 - accuracy: 0.9852 - val_loss: 4.8426 - val_accuracy: 0.5102\n",
            "Epoch 115/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0804 - accuracy: 0.9615 - val_loss: 2.4435 - val_accuracy: 0.4898\n",
            "Epoch 116/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1010 - accuracy: 0.9586 - val_loss: 33.5597 - val_accuracy: 0.4898\n",
            "Epoch 117/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0767 - accuracy: 0.9676 - val_loss: 49.2581 - val_accuracy: 0.4898\n",
            "Epoch 118/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1136 - accuracy: 0.9615 - val_loss: 11.9351 - val_accuracy: 0.4898\n",
            "Epoch 119/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0969 - accuracy: 0.9675 - val_loss: 16.9056 - val_accuracy: 0.4898\n",
            "Epoch 120/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0609 - accuracy: 0.9734 - val_loss: 3.3462 - val_accuracy: 0.5102\n",
            "Epoch 121/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1211 - accuracy: 0.9556 - val_loss: 0.9897 - val_accuracy: 0.5102\n",
            "Epoch 122/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0868 - accuracy: 0.9734 - val_loss: 6.2602 - val_accuracy: 0.5102\n",
            "Epoch 123/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0754 - accuracy: 0.9675 - val_loss: 1.0855 - val_accuracy: 0.4966\n",
            "Epoch 124/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1370 - accuracy: 0.9586 - val_loss: 4.6673 - val_accuracy: 0.4898\n",
            "Epoch 125/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0631 - accuracy: 0.9822 - val_loss: 21.7923 - val_accuracy: 0.4898\n",
            "Epoch 126/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0870 - accuracy: 0.9586 - val_loss: 2.8730 - val_accuracy: 0.5102\n",
            "Epoch 127/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0667 - accuracy: 0.9763 - val_loss: 2.1598 - val_accuracy: 0.4898\n",
            "Epoch 128/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0670 - accuracy: 0.9763 - val_loss: 0.8056 - val_accuracy: 0.5374\n",
            "Epoch 129/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0798 - accuracy: 0.9734 - val_loss: 1.8137 - val_accuracy: 0.5034\n",
            "Epoch 130/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0692 - accuracy: 0.9793 - val_loss: 48.2635 - val_accuracy: 0.4898\n",
            "Epoch 131/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0822 - accuracy: 0.9615 - val_loss: 20.5782 - val_accuracy: 0.4898\n",
            "Epoch 132/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0662 - accuracy: 0.9824 - val_loss: 63.3564 - val_accuracy: 0.4898\n",
            "Epoch 133/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1349 - accuracy: 0.9615 - val_loss: 48.7788 - val_accuracy: 0.4898\n",
            "Epoch 134/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0497 - accuracy: 0.9793 - val_loss: 6.6189 - val_accuracy: 0.5102\n",
            "Epoch 135/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0567 - accuracy: 0.9793 - val_loss: 0.5373 - val_accuracy: 0.7075\n",
            "Epoch 136/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0846 - accuracy: 0.9704 - val_loss: 19.7334 - val_accuracy: 0.4898\n",
            "Epoch 137/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0692 - accuracy: 0.9734 - val_loss: 120.3485 - val_accuracy: 0.4898\n",
            "Epoch 138/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0849 - accuracy: 0.9675 - val_loss: 71.3117 - val_accuracy: 0.4898\n",
            "Epoch 139/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0832 - accuracy: 0.9675 - val_loss: 24.4632 - val_accuracy: 0.4898\n",
            "Epoch 140/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0840 - accuracy: 0.9763 - val_loss: 181.4823 - val_accuracy: 0.4898\n",
            "Epoch 141/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0619 - accuracy: 0.9734 - val_loss: 64.1304 - val_accuracy: 0.4898\n",
            "Epoch 142/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0968 - accuracy: 0.9615 - val_loss: 162.2970 - val_accuracy: 0.4898\n",
            "Epoch 143/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1014 - accuracy: 0.9645 - val_loss: 9.9314 - val_accuracy: 0.4898\n",
            "Epoch 144/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0925 - accuracy: 0.9586 - val_loss: 2.0561 - val_accuracy: 0.5102\n",
            "Epoch 145/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0541 - accuracy: 0.9793 - val_loss: 2.1242 - val_accuracy: 0.4898\n",
            "Epoch 146/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1469 - accuracy: 0.9379 - val_loss: 21.1190 - val_accuracy: 0.4898\n",
            "Epoch 147/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0621 - accuracy: 0.9734 - val_loss: 2.5597 - val_accuracy: 0.4898\n",
            "Epoch 148/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0794 - accuracy: 0.9704 - val_loss: 10.3943 - val_accuracy: 0.4898\n",
            "Epoch 149/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0851 - accuracy: 0.9645 - val_loss: 55.8152 - val_accuracy: 0.4898\n",
            "Epoch 150/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0553 - accuracy: 0.9822 - val_loss: 18.5876 - val_accuracy: 0.4898\n",
            "Epoch 151/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0879 - accuracy: 0.9734 - val_loss: 4.2121 - val_accuracy: 0.4898\n",
            "Epoch 152/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1006 - accuracy: 0.9675 - val_loss: 16.3993 - val_accuracy: 0.5102\n",
            "Epoch 153/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0341 - accuracy: 0.9852 - val_loss: 16.2539 - val_accuracy: 0.4898\n",
            "Epoch 154/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0820 - accuracy: 0.9645 - val_loss: 1.7814 - val_accuracy: 0.5102\n",
            "Epoch 155/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1090 - accuracy: 0.9586 - val_loss: 1.1086 - val_accuracy: 0.4898\n",
            "Epoch 156/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0678 - accuracy: 0.9734 - val_loss: 1.3869 - val_accuracy: 0.5306\n",
            "Epoch 157/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0676 - accuracy: 0.9706 - val_loss: 29.9749 - val_accuracy: 0.5102\n",
            "Epoch 158/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0700 - accuracy: 0.9615 - val_loss: 12.5975 - val_accuracy: 0.5102\n",
            "Epoch 159/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0727 - accuracy: 0.9704 - val_loss: 30.9397 - val_accuracy: 0.5102\n",
            "Epoch 160/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0447 - accuracy: 0.9763 - val_loss: 41.5811 - val_accuracy: 0.5102\n",
            "Epoch 161/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0584 - accuracy: 0.9763 - val_loss: 42.2848 - val_accuracy: 0.5102\n",
            "Epoch 162/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0907 - accuracy: 0.9763 - val_loss: 0.8861 - val_accuracy: 0.5374\n",
            "Epoch 163/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0956 - accuracy: 0.9704 - val_loss: 1.4079 - val_accuracy: 0.4898\n",
            "Epoch 164/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0577 - accuracy: 0.9645 - val_loss: 3.2637 - val_accuracy: 0.5102\n",
            "Epoch 165/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0657 - accuracy: 0.9704 - val_loss: 24.3171 - val_accuracy: 0.5102\n",
            "Epoch 166/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0389 - accuracy: 0.9911 - val_loss: 2.9678 - val_accuracy: 0.4898\n",
            "Epoch 167/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0616 - accuracy: 0.9763 - val_loss: 13.6311 - val_accuracy: 0.4898\n",
            "Epoch 168/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0184 - accuracy: 0.9970 - val_loss: 6.0151 - val_accuracy: 0.5102\n",
            "Epoch 169/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1522 - accuracy: 0.9556 - val_loss: 12.8104 - val_accuracy: 0.4898\n",
            "Epoch 170/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0675 - accuracy: 0.9822 - val_loss: 3.4771 - val_accuracy: 0.4898\n",
            "Epoch 171/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0550 - accuracy: 0.9852 - val_loss: 5.3882 - val_accuracy: 0.5102\n",
            "Epoch 172/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0479 - accuracy: 0.9882 - val_loss: 7.6836 - val_accuracy: 0.5102\n",
            "Epoch 173/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0553 - accuracy: 0.9793 - val_loss: 23.4329 - val_accuracy: 0.4898\n",
            "Epoch 174/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0969 - accuracy: 0.9704 - val_loss: 15.8256 - val_accuracy: 0.5102\n",
            "Epoch 175/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0877 - accuracy: 0.9763 - val_loss: 1.8259 - val_accuracy: 0.5102\n",
            "Epoch 176/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0705 - accuracy: 0.9793 - val_loss: 2.4040 - val_accuracy: 0.5102\n",
            "Epoch 177/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0992 - accuracy: 0.9675 - val_loss: 5.2648 - val_accuracy: 0.5102\n",
            "Epoch 178/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0412 - accuracy: 0.9852 - val_loss: 5.6818 - val_accuracy: 0.5102\n",
            "Epoch 179/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0551 - accuracy: 0.9763 - val_loss: 43.2346 - val_accuracy: 0.5102\n",
            "Epoch 180/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1105 - accuracy: 0.9645 - val_loss: 14.4075 - val_accuracy: 0.5102\n",
            "Epoch 181/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1032 - accuracy: 0.9615 - val_loss: 16.2553 - val_accuracy: 0.5102\n",
            "Epoch 182/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0524 - accuracy: 0.9941 - val_loss: 1.1015 - val_accuracy: 0.4898\n",
            "Epoch 183/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0600 - accuracy: 0.9704 - val_loss: 41.3154 - val_accuracy: 0.4898\n",
            "Epoch 184/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0750 - accuracy: 0.9645 - val_loss: 6.0788 - val_accuracy: 0.4898\n",
            "Epoch 185/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0492 - accuracy: 0.9734 - val_loss: 16.4133 - val_accuracy: 0.4898\n",
            "Epoch 186/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1112 - accuracy: 0.9734 - val_loss: 44.3601 - val_accuracy: 0.4898\n",
            "Epoch 187/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0776 - accuracy: 0.9734 - val_loss: 56.1396 - val_accuracy: 0.4898\n",
            "Epoch 188/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0687 - accuracy: 0.9734 - val_loss: 85.5834 - val_accuracy: 0.4898\n",
            "Epoch 189/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1185 - accuracy: 0.9675 - val_loss: 1.0953 - val_accuracy: 0.4898\n",
            "Epoch 190/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0588 - accuracy: 0.9793 - val_loss: 11.5799 - val_accuracy: 0.5102\n",
            "Epoch 191/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0934 - accuracy: 0.9527 - val_loss: 5.8082 - val_accuracy: 0.4898\n",
            "Epoch 192/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0706 - accuracy: 0.9763 - val_loss: 7.2406 - val_accuracy: 0.4898\n",
            "Epoch 193/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0484 - accuracy: 0.9793 - val_loss: 26.7753 - val_accuracy: 0.4898\n",
            "Epoch 194/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1207 - accuracy: 0.9467 - val_loss: 0.6360 - val_accuracy: 0.6735\n",
            "Epoch 195/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0783 - accuracy: 0.9763 - val_loss: 12.8993 - val_accuracy: 0.5102\n",
            "Epoch 196/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0539 - accuracy: 0.9793 - val_loss: 13.3786 - val_accuracy: 0.5102\n",
            "Epoch 197/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0974 - accuracy: 0.9615 - val_loss: 13.2650 - val_accuracy: 0.5102\n",
            "Epoch 198/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0680 - accuracy: 0.9645 - val_loss: 0.7779 - val_accuracy: 0.5714\n",
            "Epoch 199/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0357 - accuracy: 0.9882 - val_loss: 0.9883 - val_accuracy: 0.5170\n",
            "Epoch 200/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0444 - accuracy: 0.9852 - val_loss: 15.7845 - val_accuracy: 0.5102\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6049dac8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-3hvIYqRcV5",
        "outputId": "aebe5f8f-6020-4183-deb8-8b4e4bc38e2a"
      },
      "source": [
        "# X_train.shape\n",
        "steps_per_epoch"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIgDLfqrNhp_",
        "outputId": "8748f2f0-d555-4adc-a56e-4c06f4813f2f"
      },
      "source": [
        "# gives us back a <keras.callbacks.History object at 0x112e61a90>\n",
        "model.fit(X_train, Y_train, epochs=200, batch_size=32)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0058 - accuracy: 0.9971\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 0.9971\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0090 - accuracy: 0.9971\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0050 - accuracy: 0.9971\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.5682e-04 - accuracy: 1.0000\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.1751e-04 - accuracy: 1.0000\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.0210e-04 - accuracy: 1.0000\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.9614e-04 - accuracy: 1.0000\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.4913e-04 - accuracy: 1.0000\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.8547e-04 - accuracy: 1.0000\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.5150e-04 - accuracy: 1.0000\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.1633e-04 - accuracy: 1.0000\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.3285e-04 - accuracy: 1.0000\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.9932e-04 - accuracy: 1.0000\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.1488e-04 - accuracy: 1.0000\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.7119e-04 - accuracy: 1.0000\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.0247e-04 - accuracy: 1.0000\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.7472e-04 - accuracy: 1.0000\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.9466e-04 - accuracy: 1.0000\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6319e-04 - accuracy: 1.0000\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.4701e-04 - accuracy: 1.0000\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.4640e-04 - accuracy: 1.0000\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.4069e-04 - accuracy: 1.0000\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.0431e-04 - accuracy: 1.0000\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.9787e-04 - accuracy: 1.0000\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.8595e-04 - accuracy: 1.0000\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.4621e-04 - accuracy: 1.0000\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.4901e-04 - accuracy: 1.0000\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3555e-04 - accuracy: 1.0000\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4980e-04 - accuracy: 1.0000\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.3816e-04 - accuracy: 1.0000\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.9630e-04 - accuracy: 1.0000\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.8698e-04 - accuracy: 1.0000\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6051e-04 - accuracy: 1.0000\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.6294e-04 - accuracy: 1.0000\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.2450e-04 - accuracy: 1.0000\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.9939e-04 - accuracy: 1.0000\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.9065e-04 - accuracy: 1.0000\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6313e-04 - accuracy: 1.0000\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0572e-04 - accuracy: 1.0000\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.3007e-04 - accuracy: 1.0000\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5218e-04 - accuracy: 1.0000\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.9983e-04 - accuracy: 1.0000\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2179e-04 - accuracy: 1.0000\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6163e-04 - accuracy: 1.0000\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.5594e-04 - accuracy: 1.0000\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0354e-04 - accuracy: 1.0000\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7241e-04 - accuracy: 1.0000\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.2889e-04 - accuracy: 1.0000\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1323e-04 - accuracy: 1.0000\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2332e-04 - accuracy: 1.0000\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4783e-04 - accuracy: 1.0000\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6702e-04 - accuracy: 1.0000\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8401e-04 - accuracy: 1.0000\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7818e-04 - accuracy: 1.0000\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6353e-04 - accuracy: 1.0000\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.9730e-04 - accuracy: 1.0000\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.1297e-04 - accuracy: 1.0000\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.2107e-04 - accuracy: 1.0000\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4794e-04 - accuracy: 1.0000\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.4896e-04 - accuracy: 1.0000\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7408e-04 - accuracy: 1.0000\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7922e-04 - accuracy: 1.0000\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.3563e-04 - accuracy: 1.0000\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0077e-04 - accuracy: 1.0000\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9059e-04 - accuracy: 1.0000\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4439e-04 - accuracy: 1.0000\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4754e-04 - accuracy: 1.0000\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6346e-04 - accuracy: 1.0000\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7875e-04 - accuracy: 1.0000\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9048e-04 - accuracy: 1.0000\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.7987e-04 - accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4436e-04 - accuracy: 1.0000\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2719e-04 - accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.7298e-05 - accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1863e-04 - accuracy: 1.0000\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1659e-04 - accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6882e-05 - accuracy: 1.0000\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0789e-04 - accuracy: 1.0000\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3360e-04 - accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2746e-04 - accuracy: 1.0000\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5821e-04 - accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2115e-04 - accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1961e-04 - accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0718e-04 - accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3716e-04 - accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.9525e-05 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.2559e-05 - accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3438e-04 - accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1198e-04 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.8066e-04 - accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1139e-04 - accuracy: 1.0000\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3177e-04 - accuracy: 1.0000\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.0793e-05 - accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.3126e-05 - accuracy: 1.0000\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.5151e-04 - accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.8731e-05 - accuracy: 1.0000\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2109e-04 - accuracy: 1.0000\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2717e-04 - accuracy: 1.0000\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5119e-04 - accuracy: 1.0000\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.3375e-05 - accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.0801e-05 - accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.4064e-05 - accuracy: 1.0000\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4983e-04 - accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8072e-04 - accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0647e-04 - accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.6252e-05 - accuracy: 1.0000\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3406e-04 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.4448e-05 - accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.3745e-05 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.1621e-05 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3285e-04 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1680e-04 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.3616e-05 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.4849e-05 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2334e-04 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0732e-05 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3496e-04 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.7227e-05 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.4341e-05 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.8234e-05 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.9257e-05 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.9840e-05 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.5163e-05 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.3607e-05 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.1795e-05 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.1937e-04 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0575e-04 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.7640e-05 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.1734e-05 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4381e-04 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.3197e-05 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2664e-05 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2974e-04 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.1222e-05 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7626e-05 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.3989e-05 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.2260e-05 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.1419e-05 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.3405e-05 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4824e-05 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.9602e-05 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.9462e-05 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.9926e-05 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.6043e-05 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.4863e-05 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.1764e-05 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0287e-04 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7670e-05 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6281e-05 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.9673e-05 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6990e-05 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.2164e-05 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.0685e-05 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.7978e-05 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.1335e-05 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.8703e-04 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.1531e-05 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.8346e-05 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.5070e-05 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.8030e-05 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.0884e-05 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.0935e-05 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.0868e-05 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6002e-05 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3193e-05 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.8911e-05 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.7313e-05 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.4060e-05 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.3398e-05 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.4273e-05 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.9475e-05 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3327e-05 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.2023e-05 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f604a585e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpf0XlSARX78",
        "outputId": "3c18924a-fd79-44ad-d0b8-223b6ccf61b2"
      },
      "source": [
        "pred_test= model.predict_classes(X_test)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predict   0   1\n",
            "Actual         \n",
            "0        68   4\n",
            "1         2  73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iFNNrlWV9tH",
        "outputId": "5012cfce-fc50-4259-89e0-20fe4fce3ba2"
      },
      "source": [
        "pred_test"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QISvYcJBgWbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b50b516d-7ce3-49fd-eef0-21a761441a21"
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[0] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  prediction = model.predict_classes(result)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "2   110.0   75.473053   76.030411  ...   32.605618   33.027107   35.333221\n",
            "5   194.0  130.847168  132.532135  ...   54.409924   44.967159   29.347006\n",
            "8   134.0  144.090454  142.638901  ...   68.534416   61.687458   60.067497\n",
            "9   195.0   71.041977   84.708199  ...   74.042374   75.499207   75.105240\n",
            "12  160.0   99.181870   98.168747  ...    7.008750    3.735000    1.103125\n",
            "13  148.0   64.582916   65.488678  ...   37.293644   35.994888   37.498905\n",
            "14  118.0   94.796318   95.415115  ...   55.293304   49.029873   40.695488\n",
            "15  150.0  107.920708  107.651558  ...    0.610133    0.317511    0.000000\n",
            "18  116.0   79.747917   77.019020  ...   66.771698   67.416168   73.161713\n",
            "20  164.0   80.077332   74.550865  ...   60.543129   59.400955   57.758480\n",
            "26  133.0  143.548477  159.731293  ...   95.595573   98.468140  101.066483\n",
            "27  194.0  105.766388  106.628113  ...  107.786575  110.362732  102.603775\n",
            "28  104.0  113.807701  117.032547  ...  105.125748   94.643501   65.000008\n",
            "30  182.0   76.911247   78.757401  ...  100.017746  107.603561  113.485214\n",
            "34  101.0   20.171162   23.644350  ...    0.000000    0.000000    0.000000\n",
            "35  109.0   59.726288   73.253761  ...   61.734280   58.585049   54.236427\n",
            "36  175.0   86.457603   82.816002  ...   40.724796   18.145599   24.321600\n",
            "37  142.0  124.817108  118.749664  ...  117.794090  126.328896  128.130936\n",
            "38  189.0   96.647461  108.792870  ...   86.481483   90.374496   86.643341\n",
            "40  177.0   73.836861   68.306709  ...   41.282322   41.965141   37.323948\n",
            "41  114.0   73.237305   75.249611  ...   87.861801   92.305939   94.822403\n",
            "42  104.0   89.331375   91.261841  ...   45.057697   45.218941   44.378700\n",
            "43  114.0   87.965530   85.687294  ...   90.918129   90.677742   91.830704\n",
            "47  141.0   67.274635   67.768776  ...   38.693878   12.294502    3.055027\n",
            "48  107.0  123.173637  121.897018  ...   35.822254   36.934231   38.020092\n",
            "49  144.0   64.010033   66.314041  ...  125.476089  119.289352  118.740738\n",
            "\n",
            "[26 rows x 785 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRbWgmX_LFH",
        "outputId": "0a92ff66-d0fb-4e3c-d685-f33bc3e4fbb2"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "from GetBetterSegm import GetBetter"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_paper_fev_2021'...\n",
            "remote: Enumerating objects: 608, done.\u001b[K\n",
            "remote: Counting objects: 100% (369/369), done.\u001b[K\n",
            "remote: Compressing objects: 100% (368/368), done.\u001b[K\n",
            "remote: Total 608 (delta 227), reused 0 (delta 0), pack-reused 239\u001b[K\n",
            "Receiving objects: 100% (608/608), 5.20 MiB | 14.10 MiB/s, done.\n",
            "Resolving deltas: 100% (364/364), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAG_I6FwCvFr",
        "outputId": "7add3462-580a-4240-dc82-57a1a4063539"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "%cd marquesgabi_out_2020\n",
        "PSD_imageJ = 'Areas_ImageJ.csv'\n",
        "PSD_new = pd.read_csv(PSD_imageJ)\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_out_2020'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 146 (delta 75), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (146/146), 1.00 MiB | 4.00 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020\n",
            "   Juntas   Area\n",
            "0       1  2.001\n",
            "1       2  0.820\n",
            "2       3  1.270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "PekBHQOT_6CP",
        "outputId": "00529ba8-bfd1-4ba4-b563-5a911ab68a57"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>110.0</td>\n",
              "      <td>75.473053</td>\n",
              "      <td>76.030411</td>\n",
              "      <td>73.175201</td>\n",
              "      <td>70.941162</td>\n",
              "      <td>67.378510</td>\n",
              "      <td>64.454880</td>\n",
              "      <td>64.446609</td>\n",
              "      <td>63.617519</td>\n",
              "      <td>63.470081</td>\n",
              "      <td>63.538837</td>\n",
              "      <td>65.595039</td>\n",
              "      <td>65.200989</td>\n",
              "      <td>65.887268</td>\n",
              "      <td>72.753387</td>\n",
              "      <td>78.546448</td>\n",
              "      <td>80.297844</td>\n",
              "      <td>76.209908</td>\n",
              "      <td>67.651230</td>\n",
              "      <td>55.898506</td>\n",
              "      <td>40.471077</td>\n",
              "      <td>29.651237</td>\n",
              "      <td>26.339500</td>\n",
              "      <td>24.533884</td>\n",
              "      <td>24.026117</td>\n",
              "      <td>24.019505</td>\n",
              "      <td>23.228098</td>\n",
              "      <td>22.069422</td>\n",
              "      <td>19.791737</td>\n",
              "      <td>76.083305</td>\n",
              "      <td>76.402641</td>\n",
              "      <td>73.124954</td>\n",
              "      <td>69.929581</td>\n",
              "      <td>66.952065</td>\n",
              "      <td>66.144127</td>\n",
              "      <td>65.049583</td>\n",
              "      <td>63.363632</td>\n",
              "      <td>63.086609</td>\n",
              "      <td>64.111404</td>\n",
              "      <td>64.424797</td>\n",
              "      <td>...</td>\n",
              "      <td>38.055866</td>\n",
              "      <td>35.231407</td>\n",
              "      <td>33.899830</td>\n",
              "      <td>30.505453</td>\n",
              "      <td>29.682646</td>\n",
              "      <td>30.502480</td>\n",
              "      <td>30.827106</td>\n",
              "      <td>31.529255</td>\n",
              "      <td>31.565619</td>\n",
              "      <td>32.523636</td>\n",
              "      <td>33.311405</td>\n",
              "      <td>35.448265</td>\n",
              "      <td>58.763634</td>\n",
              "      <td>59.895538</td>\n",
              "      <td>59.383141</td>\n",
              "      <td>58.310078</td>\n",
              "      <td>48.786118</td>\n",
              "      <td>20.341818</td>\n",
              "      <td>7.453223</td>\n",
              "      <td>8.727273</td>\n",
              "      <td>13.044298</td>\n",
              "      <td>25.320000</td>\n",
              "      <td>32.901817</td>\n",
              "      <td>35.826778</td>\n",
              "      <td>37.564957</td>\n",
              "      <td>37.584129</td>\n",
              "      <td>37.374874</td>\n",
              "      <td>37.486610</td>\n",
              "      <td>35.632065</td>\n",
              "      <td>34.091568</td>\n",
              "      <td>33.368923</td>\n",
              "      <td>31.403967</td>\n",
              "      <td>30.345123</td>\n",
              "      <td>30.465456</td>\n",
              "      <td>30.507769</td>\n",
              "      <td>31.182148</td>\n",
              "      <td>31.160992</td>\n",
              "      <td>32.605618</td>\n",
              "      <td>33.027107</td>\n",
              "      <td>35.333221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>194.0</td>\n",
              "      <td>130.847168</td>\n",
              "      <td>132.532135</td>\n",
              "      <td>128.218399</td>\n",
              "      <td>108.236046</td>\n",
              "      <td>91.338280</td>\n",
              "      <td>95.278770</td>\n",
              "      <td>99.874802</td>\n",
              "      <td>102.401527</td>\n",
              "      <td>90.277283</td>\n",
              "      <td>86.299179</td>\n",
              "      <td>75.294395</td>\n",
              "      <td>47.462536</td>\n",
              "      <td>60.220737</td>\n",
              "      <td>65.779884</td>\n",
              "      <td>68.588051</td>\n",
              "      <td>70.759377</td>\n",
              "      <td>72.220955</td>\n",
              "      <td>71.192154</td>\n",
              "      <td>67.700600</td>\n",
              "      <td>64.770210</td>\n",
              "      <td>60.479218</td>\n",
              "      <td>52.312355</td>\n",
              "      <td>47.616108</td>\n",
              "      <td>47.464127</td>\n",
              "      <td>48.800507</td>\n",
              "      <td>45.222866</td>\n",
              "      <td>43.691036</td>\n",
              "      <td>39.702938</td>\n",
              "      <td>139.877014</td>\n",
              "      <td>143.101379</td>\n",
              "      <td>131.263992</td>\n",
              "      <td>101.095215</td>\n",
              "      <td>93.153145</td>\n",
              "      <td>95.376968</td>\n",
              "      <td>98.482925</td>\n",
              "      <td>95.297691</td>\n",
              "      <td>89.322655</td>\n",
              "      <td>90.051117</td>\n",
              "      <td>80.615044</td>\n",
              "      <td>...</td>\n",
              "      <td>152.493759</td>\n",
              "      <td>90.002754</td>\n",
              "      <td>38.472950</td>\n",
              "      <td>45.812309</td>\n",
              "      <td>40.935802</td>\n",
              "      <td>46.026672</td>\n",
              "      <td>52.419273</td>\n",
              "      <td>54.016891</td>\n",
              "      <td>53.781799</td>\n",
              "      <td>44.529808</td>\n",
              "      <td>21.073862</td>\n",
              "      <td>3.652885</td>\n",
              "      <td>49.200546</td>\n",
              "      <td>54.865761</td>\n",
              "      <td>78.770531</td>\n",
              "      <td>109.402473</td>\n",
              "      <td>124.292259</td>\n",
              "      <td>123.391205</td>\n",
              "      <td>112.303207</td>\n",
              "      <td>111.235298</td>\n",
              "      <td>113.782532</td>\n",
              "      <td>114.211494</td>\n",
              "      <td>112.456589</td>\n",
              "      <td>96.182800</td>\n",
              "      <td>93.082993</td>\n",
              "      <td>106.277603</td>\n",
              "      <td>114.239761</td>\n",
              "      <td>124.725250</td>\n",
              "      <td>158.016571</td>\n",
              "      <td>94.975433</td>\n",
              "      <td>32.515884</td>\n",
              "      <td>39.599319</td>\n",
              "      <td>41.596233</td>\n",
              "      <td>48.762989</td>\n",
              "      <td>54.401840</td>\n",
              "      <td>56.031349</td>\n",
              "      <td>58.366558</td>\n",
              "      <td>54.409924</td>\n",
              "      <td>44.967159</td>\n",
              "      <td>29.347006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>134.0</td>\n",
              "      <td>144.090454</td>\n",
              "      <td>142.638901</td>\n",
              "      <td>121.627762</td>\n",
              "      <td>80.546898</td>\n",
              "      <td>69.831810</td>\n",
              "      <td>72.615509</td>\n",
              "      <td>77.593239</td>\n",
              "      <td>78.198265</td>\n",
              "      <td>80.400757</td>\n",
              "      <td>81.155945</td>\n",
              "      <td>80.248169</td>\n",
              "      <td>78.338608</td>\n",
              "      <td>78.383614</td>\n",
              "      <td>79.245277</td>\n",
              "      <td>81.712196</td>\n",
              "      <td>79.040329</td>\n",
              "      <td>73.411011</td>\n",
              "      <td>66.994881</td>\n",
              "      <td>59.662289</td>\n",
              "      <td>53.321899</td>\n",
              "      <td>48.899086</td>\n",
              "      <td>47.734909</td>\n",
              "      <td>46.317894</td>\n",
              "      <td>46.883717</td>\n",
              "      <td>46.887058</td>\n",
              "      <td>50.588551</td>\n",
              "      <td>54.717533</td>\n",
              "      <td>55.020943</td>\n",
              "      <td>140.207626</td>\n",
              "      <td>127.107155</td>\n",
              "      <td>85.461128</td>\n",
              "      <td>68.211853</td>\n",
              "      <td>68.835602</td>\n",
              "      <td>71.984634</td>\n",
              "      <td>76.453552</td>\n",
              "      <td>81.636002</td>\n",
              "      <td>82.677444</td>\n",
              "      <td>83.382713</td>\n",
              "      <td>82.911560</td>\n",
              "      <td>...</td>\n",
              "      <td>95.053696</td>\n",
              "      <td>97.386505</td>\n",
              "      <td>96.039665</td>\n",
              "      <td>94.912903</td>\n",
              "      <td>95.072845</td>\n",
              "      <td>95.372688</td>\n",
              "      <td>97.425491</td>\n",
              "      <td>94.812889</td>\n",
              "      <td>81.422821</td>\n",
              "      <td>68.449318</td>\n",
              "      <td>65.128761</td>\n",
              "      <td>64.180664</td>\n",
              "      <td>81.352196</td>\n",
              "      <td>85.392746</td>\n",
              "      <td>87.499901</td>\n",
              "      <td>84.954117</td>\n",
              "      <td>83.820679</td>\n",
              "      <td>85.573402</td>\n",
              "      <td>87.060379</td>\n",
              "      <td>86.743149</td>\n",
              "      <td>84.824684</td>\n",
              "      <td>81.001564</td>\n",
              "      <td>77.687462</td>\n",
              "      <td>74.573181</td>\n",
              "      <td>76.629539</td>\n",
              "      <td>85.195816</td>\n",
              "      <td>90.787704</td>\n",
              "      <td>94.191132</td>\n",
              "      <td>93.546448</td>\n",
              "      <td>93.795952</td>\n",
              "      <td>92.166626</td>\n",
              "      <td>89.615952</td>\n",
              "      <td>88.483406</td>\n",
              "      <td>88.235245</td>\n",
              "      <td>90.120972</td>\n",
              "      <td>89.653152</td>\n",
              "      <td>81.198715</td>\n",
              "      <td>68.534416</td>\n",
              "      <td>61.687458</td>\n",
              "      <td>60.067497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>195.0</td>\n",
              "      <td>71.041977</td>\n",
              "      <td>84.708199</td>\n",
              "      <td>92.258911</td>\n",
              "      <td>96.483246</td>\n",
              "      <td>96.705780</td>\n",
              "      <td>78.777153</td>\n",
              "      <td>77.127266</td>\n",
              "      <td>81.474983</td>\n",
              "      <td>87.153954</td>\n",
              "      <td>88.843956</td>\n",
              "      <td>89.853104</td>\n",
              "      <td>87.950356</td>\n",
              "      <td>86.104752</td>\n",
              "      <td>86.436539</td>\n",
              "      <td>86.878899</td>\n",
              "      <td>51.163506</td>\n",
              "      <td>44.390247</td>\n",
              "      <td>50.231510</td>\n",
              "      <td>58.976864</td>\n",
              "      <td>63.509514</td>\n",
              "      <td>65.872307</td>\n",
              "      <td>61.893547</td>\n",
              "      <td>62.031960</td>\n",
              "      <td>62.125954</td>\n",
              "      <td>60.667961</td>\n",
              "      <td>62.248657</td>\n",
              "      <td>66.347626</td>\n",
              "      <td>66.666466</td>\n",
              "      <td>65.717857</td>\n",
              "      <td>78.532837</td>\n",
              "      <td>88.164505</td>\n",
              "      <td>94.773163</td>\n",
              "      <td>110.803871</td>\n",
              "      <td>77.581619</td>\n",
              "      <td>75.929054</td>\n",
              "      <td>84.270142</td>\n",
              "      <td>89.854866</td>\n",
              "      <td>88.044525</td>\n",
              "      <td>87.541519</td>\n",
              "      <td>...</td>\n",
              "      <td>77.220146</td>\n",
              "      <td>74.931602</td>\n",
              "      <td>79.298859</td>\n",
              "      <td>81.320900</td>\n",
              "      <td>79.874352</td>\n",
              "      <td>79.622841</td>\n",
              "      <td>83.409340</td>\n",
              "      <td>75.733223</td>\n",
              "      <td>66.981018</td>\n",
              "      <td>68.166023</td>\n",
              "      <td>72.072037</td>\n",
              "      <td>75.385117</td>\n",
              "      <td>117.868225</td>\n",
              "      <td>120.345757</td>\n",
              "      <td>94.862358</td>\n",
              "      <td>62.580441</td>\n",
              "      <td>68.473351</td>\n",
              "      <td>55.314377</td>\n",
              "      <td>38.240788</td>\n",
              "      <td>32.697388</td>\n",
              "      <td>32.494411</td>\n",
              "      <td>30.512508</td>\n",
              "      <td>30.282867</td>\n",
              "      <td>30.603132</td>\n",
              "      <td>33.433640</td>\n",
              "      <td>41.249813</td>\n",
              "      <td>67.793617</td>\n",
              "      <td>76.981834</td>\n",
              "      <td>75.981224</td>\n",
              "      <td>76.160423</td>\n",
              "      <td>76.744751</td>\n",
              "      <td>79.892365</td>\n",
              "      <td>78.199875</td>\n",
              "      <td>75.089600</td>\n",
              "      <td>79.941826</td>\n",
              "      <td>79.506645</td>\n",
              "      <td>74.034172</td>\n",
              "      <td>74.042374</td>\n",
              "      <td>75.499207</td>\n",
              "      <td>75.105240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>160.0</td>\n",
              "      <td>99.181870</td>\n",
              "      <td>98.168747</td>\n",
              "      <td>111.627495</td>\n",
              "      <td>126.980621</td>\n",
              "      <td>134.995621</td>\n",
              "      <td>42.406250</td>\n",
              "      <td>42.867500</td>\n",
              "      <td>64.128128</td>\n",
              "      <td>61.524364</td>\n",
              "      <td>48.227501</td>\n",
              "      <td>43.994999</td>\n",
              "      <td>44.546249</td>\n",
              "      <td>44.994377</td>\n",
              "      <td>42.890621</td>\n",
              "      <td>36.845623</td>\n",
              "      <td>27.733747</td>\n",
              "      <td>14.973751</td>\n",
              "      <td>2.142500</td>\n",
              "      <td>0.987500</td>\n",
              "      <td>1.187500</td>\n",
              "      <td>0.168750</td>\n",
              "      <td>0.065625</td>\n",
              "      <td>0.555625</td>\n",
              "      <td>0.332500</td>\n",
              "      <td>0.026250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>100.489998</td>\n",
              "      <td>101.714378</td>\n",
              "      <td>115.938110</td>\n",
              "      <td>132.131256</td>\n",
              "      <td>156.235001</td>\n",
              "      <td>54.718121</td>\n",
              "      <td>42.041248</td>\n",
              "      <td>50.350620</td>\n",
              "      <td>44.723122</td>\n",
              "      <td>40.981247</td>\n",
              "      <td>46.888123</td>\n",
              "      <td>...</td>\n",
              "      <td>22.873747</td>\n",
              "      <td>19.498123</td>\n",
              "      <td>13.490624</td>\n",
              "      <td>12.847499</td>\n",
              "      <td>13.895000</td>\n",
              "      <td>12.259374</td>\n",
              "      <td>10.715001</td>\n",
              "      <td>10.509376</td>\n",
              "      <td>10.375626</td>\n",
              "      <td>9.061250</td>\n",
              "      <td>4.136250</td>\n",
              "      <td>1.113125</td>\n",
              "      <td>75.343124</td>\n",
              "      <td>75.967491</td>\n",
              "      <td>76.273125</td>\n",
              "      <td>76.413750</td>\n",
              "      <td>73.822502</td>\n",
              "      <td>62.439995</td>\n",
              "      <td>43.059376</td>\n",
              "      <td>32.747498</td>\n",
              "      <td>28.297497</td>\n",
              "      <td>25.875624</td>\n",
              "      <td>24.429998</td>\n",
              "      <td>24.187498</td>\n",
              "      <td>24.434376</td>\n",
              "      <td>22.863125</td>\n",
              "      <td>20.918751</td>\n",
              "      <td>23.060001</td>\n",
              "      <td>23.353750</td>\n",
              "      <td>22.016874</td>\n",
              "      <td>16.084373</td>\n",
              "      <td>12.914374</td>\n",
              "      <td>11.921874</td>\n",
              "      <td>11.262499</td>\n",
              "      <td>11.310625</td>\n",
              "      <td>10.706875</td>\n",
              "      <td>9.464374</td>\n",
              "      <td>7.008750</td>\n",
              "      <td>3.735000</td>\n",
              "      <td>1.103125</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Width           0           1  ...        781        782        783\n",
              "2   110.0   75.473053   76.030411  ...  32.605618  33.027107  35.333221\n",
              "5   194.0  130.847168  132.532135  ...  54.409924  44.967159  29.347006\n",
              "8   134.0  144.090454  142.638901  ...  68.534416  61.687458  60.067497\n",
              "9   195.0   71.041977   84.708199  ...  74.042374  75.499207  75.105240\n",
              "12  160.0   99.181870   98.168747  ...   7.008750   3.735000   1.103125\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "Area = np.array(PSD_new['Area'])\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "Vfk_fNXGDK5_",
        "outputId": "cbb8c974-b2a8-41f0-90ea-cc05ae7d9070"
      },
      "source": [
        " wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        " wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        " X = pd.DataFrame([Diam1,Diameter_All])\n",
        " wts = pd.DataFrame([wt1,wt2])\n",
        "plt.hist(X,weights=wts)\n",
        "plt.legend(['Image J','CNN'])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f604a66c350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "PLckZ0Db05Om",
        "outputId": "99333c34-190b-4554-ff38-dfb4a23c047a"
      },
      "source": [
        "Obj = plt.hist(X, density=True, histtype='step', cumulative=True,label='Reversed emp.')\n",
        "Y1, Y2 = Obj[0]\n",
        "print('r_squared =',r2_score(Y1, Y2))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r_squared = 0.9179343488878418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "ZZHa1j4HT9Dq",
        "outputId": "cb03c372-cac5-483e-927e-29bada655b20"
      },
      "source": [
        "counts, bins, bars = plt.hist(X,weights=wts)\n",
        "print(bars)\n",
        "print(bins)\n",
        "print(counts)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<a list of 2 Lists of Patches objects>\n",
            "[0.73640011 0.89450402 1.05260793 1.21071183 1.36881574 1.52691965\n",
            " 1.68502356 1.84312747 2.00123137 2.15933528 2.31743919]\n",
            "[[ 7.36842105  8.42105263 24.21052632 33.68421053 14.73684211  8.42105263\n",
            "   1.05263158  0.          1.05263158  1.05263158]\n",
            " [23.07692308 11.53846154 23.07692308 15.38461538  7.69230769 19.23076923\n",
            "   0.          0.          0.          0.        ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o_vDGeWUwIZ",
        "outputId": "ccc03ce3-5f8e-4869-f3a9-217a8b76e56e"
      },
      "source": [
        "print(counts.sum())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200.00000000000006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "KcH52-6iJQ8t",
        "outputId": "c358fa66-8776-4055-a5c9-337e66431311"
      },
      "source": [
        "\n",
        "plt.hist([Diam1,Diameter_All])\n",
        "plt.legend(['Image J','CNN'])\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f5ffe081cd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r11AxFK_JIii",
        "outputId": "03c9c8b4-81f5-4913-e143-260c4f89bfc9"
      },
      "source": [
        "[Diam1,Diameter_All]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.59616801403081,\n",
              "  1.0217907939900581,\n",
              "  1.2716187407449044,\n",
              "  1.104429030701514,\n",
              "  1.2163487785097904,\n",
              "  1.6013445735058454,\n",
              "  1.1715597420637607,\n",
              "  1.2534662333717612,\n",
              "  1.2676073151634049,\n",
              "  1.309600575274104,\n",
              "  1.292966945531582,\n",
              "  1.7658322811231006,\n",
              "  1.3564037533648712,\n",
              "  1.2407040781688483,\n",
              "  2.130217298173151,\n",
              "  1.4228319915327,\n",
              "  1.0651086490865755,\n",
              "  1.3008210311003705,\n",
              "  1.336545951796433,\n",
              "  0.8927754224911278,\n",
              "  1.4494292838262302,\n",
              "  1.4052738287907582,\n",
              "  1.6421697097891788,\n",
              "  1.2329833804288621,\n",
              "  1.19042665178928,\n",
              "  1.1682948223612457,\n",
              "  1.1518314137121108,\n",
              "  0.9607802401865855,\n",
              "  2.317439190074449,\n",
              "  1.0591147430338594,\n",
              "  1.4308630919602832,\n",
              "  0.7535680705496237,\n",
              "  0.8608283307581511,\n",
              "  1.2776122636975893,\n",
              "  1.3745862957220916,\n",
              "  1.259546137598783,\n",
              "  1.2978813187979172,\n",
              "  1.2412170838050638,\n",
              "  1.6009469708743893,\n",
              "  1.3149369953539032,\n",
              "  1.417901703622935,\n",
              "  1.2478669653497139,\n",
              "  1.1055812783082735,\n",
              "  0.9561307405997607,\n",
              "  0.9487783503683882,\n",
              "  1.1238565871041026,\n",
              "  1.2058356273089446,\n",
              "  1.2801012827406097,\n",
              "  0.8733100751144249,\n",
              "  0.9194732501297403,\n",
              "  1.6425573339441792,\n",
              "  1.085826790250066,\n",
              "  1.0639125693728595,\n",
              "  1.0875842666474016,\n",
              "  1.417901703622935,\n",
              "  1.550443891425932,\n",
              "  0.7825779328716171,\n",
              "  1.4690612745308145,\n",
              "  1.053086721720641,\n",
              "  1.2676073151634049,\n",
              "  0.7744003006005755,\n",
              "  1.3787482149724068,\n",
              "  1.363892581861956,\n",
              "  1.299352006316543,\n",
              "  1.2870449283923413,\n",
              "  1.11817763925502,\n",
              "  0.9474354220939228,\n",
              "  1.5218484589055707,\n",
              "  1.3526437911676632,\n",
              "  1.1556938532445284,\n",
              "  1.6013445735058454,\n",
              "  1.274619025074578,\n",
              "  1.422384489715834,\n",
              "  1.3408259533459403,\n",
              "  1.172646028567008,\n",
              "  1.1490645795125545,\n",
              "  1.459060149136146,\n",
              "  1.2483770274864237,\n",
              "  1.336545951796433,\n",
              "  0.9601174044814821,\n",
              "  1.4867225193896279,\n",
              "  1.4277452542806772,\n",
              "  1.35028849808504,\n",
              "  0.7560982446653928,\n",
              "  1.259040600296622,\n",
              "  1.13456827900627,\n",
              "  1.6549133695530214,\n",
              "  1.1204526724091788,\n",
              "  1.1176081573544434,\n",
              "  0.9153095762832032,\n",
              "  1.1639273497938836,\n",
              "  1.3066806149514323,\n",
              "  1.1529362882239027,\n",
              "  1.3047303442899274,\n",
              "  1.3066806149514323],\n",
              " [0.7916708435942904,\n",
              "  1.55514127101818,\n",
              "  1.054458532755461,\n",
              "  1.490412162201014,\n",
              "  1.2411102704798136,\n",
              "  1.3012720926813273,\n",
              "  1.1360478675304007,\n",
              "  1.6036938245129442,\n",
              "  0.7661135533739949,\n",
              "  1.3106656484623902,\n",
              "  1.1545646880600418,\n",
              "  1.4350467089468757,\n",
              "  0.9874951742294082,\n",
              "  1.3318302720995878,\n",
              "  0.7364001095743897,\n",
              "  0.7513331625302545,\n",
              "  1.5693659356531722,\n",
              "  1.0670837110472298,\n",
              "  1.602087140234888,\n",
              "  1.550359891374445,\n",
              "  0.8048889795916161,\n",
              "  0.7747595111453517,\n",
              "  0.9303129181181193,\n",
              "  0.951783536945879,\n",
              "  1.067346363842749,\n",
              "  1.203984410316336]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}