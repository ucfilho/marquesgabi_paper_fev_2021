{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_final_amostra_08_set_16_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/Qualificacao/Histograma_Final/PSD_histogram_final_amostra_08_set_16_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from sklearn.metrics import r2_score\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZEvJvfoibE4",
        "outputId": "80bc9513-ba06-4e57-fbf5-457d4b6adf82"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mahotas\n",
            "  Downloading mahotas-1.4.11-cp37-cp37m-manylinux2010_x86_64.whl (5.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7 MB 11.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.19.5)\n",
            "Installing collected packages: mahotas\n",
            "Successfully installed mahotas-1.4.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT"
      },
      "source": [
        "import mahotas.features.texture as mht\n",
        "import mahotas.features"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VcTdaNVh9EE",
        "outputId": "d90a6a1c-0323-4702-f205-d60a420b1d0a"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marquesgabi_fev_2020'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 73 (delta 37), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n",
            "/content/marquesgabi_fev_2020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v7SRrc8mH2N",
        "outputId": "67e97f0f-d4b0-4e6c-e816-fe0c7f9255f8"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip' \n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Doutorado'...\n",
            "remote: Enumerating objects: 464, done.\u001b[K\n",
            "remote: Counting objects: 100% (214/214), done.\u001b[K\n",
            "remote: Compressing objects: 100% (213/213), done.\u001b[K\n",
            "remote: Total 464 (delta 102), reused 0 (delta 0), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (464/464), 203.28 MiB | 28.55 MiB/s, done.\n",
            "Resolving deltas: 100% (219/219), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqIYzUcnrdMp",
        "outputId": "5fcdac46-3af5-48b1-cb5a-1fbc94676566"
      },
      "source": [
        "labels =[]\n",
        "with zipfile.ZipFile(Transfere, \"r\") as f:\n",
        "  for f in f.namelist():\n",
        "    labels.append(f)\n",
        "print(labels)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Fotos_Grandes-3cdAmostra/Q6-8-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-4.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[9] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHgqAnaFyCjp",
        "outputId": "d88c6f8c-a732-4622-fa82-e02d2526413d"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 21.81 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from segment_filter_not_conclude import Segmenta  # got image provided segmented"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN5MN5a_v4np",
        "outputId": "407fdab8-27f1-4038-bb47-ae382d45b02b"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     110   64.383797   64.875694  ...   90.766937   93.777184   93.352066\n",
            "1     143   90.485153   90.230965  ...   63.545158   56.168808   44.303780\n",
            "2     107   39.529827   18.400124  ...   43.338982   17.656301    4.504062\n",
            "3     178   33.783241   16.141775  ...   77.305893   83.510674   86.715073\n",
            "4     146   12.698817   35.434040  ...   59.528240   68.356911   69.110901\n",
            "5     147   58.684807   60.807255  ...   97.002274   97.501137   93.623581\n",
            "6     167   65.341286   66.689919  ...   68.813477   67.593468   69.703751\n",
            "7     177  124.720001   84.219246  ...    1.785885    1.770404    1.965240\n",
            "8     123   49.053078   53.893978  ...   41.104172   42.021614   41.980244\n",
            "9     198   59.406685   54.396179  ...   63.030296   61.462200   63.969894\n",
            "10    161   93.364838   99.240082  ...    7.086957    6.994329    6.347826\n",
            "11    121  104.039963  106.268082  ...   75.218842   77.984909   78.759445\n",
            "12    164   52.486019   48.236763  ...   18.886377   12.209398    6.220107\n",
            "13    175   29.220793   33.243202  ...   72.967995   75.931198   75.995193\n",
            "14    171   80.931976   81.647614  ...    0.000000    0.000000    0.000000\n",
            "15    135   92.789680   83.777664  ...   87.356316   91.244385  106.155441\n",
            "16    112   92.500000   93.187500  ...    8.187500    8.125000    7.250000\n",
            "17    151   49.623569   70.877548  ...   84.383232   84.677116   83.893997\n",
            "18    165   80.182007   83.600708  ...   33.788319   25.214619   23.944464\n",
            "19    134   86.963020  105.944977  ...   73.777458   86.580536   92.583206\n",
            "20    167   58.749973   62.405827  ...   38.728680   43.271721   44.740906\n",
            "21    107  102.467636  103.636475  ...   22.936939   28.736309   41.098263\n",
            "22    137   66.191856   68.320793  ...   92.296768   87.805359   88.651978\n",
            "23    137   33.046776   17.627949  ...   19.529863   15.764078    7.364910\n",
            "24    121   72.680420   76.619904  ...    0.000000    0.000000    0.000000\n",
            "25    171   50.696423   53.254406  ...   96.944916   87.929718   88.887344\n",
            "26    124   54.456814   69.544220  ...    0.855359    0.768991    0.400624\n",
            "27    113   19.631685   18.850418  ...   37.985901   43.162109   41.917065\n",
            "28    186   45.023010   40.094349  ...   24.112846   21.092846   13.626317\n",
            "29    199   42.516350   66.954666  ...   94.586899   99.064865  106.731590\n",
            "30    115   49.448845   49.476215  ...   96.036743   92.270996   89.104424\n",
            "31    182   78.195274   78.952667  ...   98.893501   96.786995   98.112442\n",
            "32    157   61.418598   78.626762  ...   83.489632   87.491585   95.750900\n",
            "33    151   71.830658   66.410034  ...   27.253149   39.955528   55.041714\n",
            "34    180    0.293827    2.810864  ...   48.111614   50.599014   49.016792\n",
            "35    147   83.195015   68.687080  ...    0.732426    6.247166    3.104309\n",
            "36    152   43.001385   55.997227  ...   43.532551   43.897507   42.612186\n",
            "37    185  101.571327   93.255836  ...   70.404587   69.067284   68.836403\n",
            "38    199   40.254539   41.193707  ...   63.103252   61.926262   70.854477\n",
            "39    136   40.078724   46.578724  ...  104.298439  105.339096  108.538933\n",
            "40    168   82.138893   90.944443  ...   61.805557   64.138893   66.111115\n",
            "41    150   60.290489   69.041771  ...   69.673424   72.050491   77.548088\n",
            "42    131   81.965561   82.630905  ...   56.826118   56.929661   55.142120\n",
            "43    179   88.268753   35.761024  ...   86.104897   88.966232   91.274399\n",
            "44    151   86.616859   84.750282  ...   68.150299   68.367134   65.383934\n",
            "45    185    0.274887    0.101154  ...   68.650894   61.252506   51.045113\n",
            "46    168   89.250000   94.777779  ...   79.722221   72.916664   64.472221\n",
            "47    191   68.088104   81.883728  ...   88.619377   83.646591   87.429214\n",
            "48    189   84.183807   85.947884  ...   48.681755   34.438957   30.430727\n",
            "49    108   50.609051   54.716045  ...  104.253777  104.938271  104.652954\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "xzpQ1Pz0fX5L",
        "outputId": "d88bd322-1fd9-4e32-95c1-40169dbd03e5"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines\n",
        "# filename = 'model_ANN.pkl'\n",
        "filename = 'model_ANN_new.pkl'\n",
        "model = joblib.load(filename)\n",
        "'''"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n!git clone https://github.com/ucfilho/MarquesGabi_Routines\\n%cd MarquesGabi_Routines\\n# filename = 'model_ANN.pkl'\\nfilename = 'model_ANN_new.pkl'\\nmodel = joblib.load(filename)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2emP4rNjQy",
        "outputId": "f4065ccf-48bd-412e-c066-525a0776e645"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 8.70 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIFPGE_-vx3T"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "Description = '3 layers of Convolution: 64, 128, 256 '\n",
        "N1 = 20\n",
        "N2 = 20\n",
        "\n",
        "# make the CNN\n",
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=64, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=N1))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=N2))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbPQ1FSRG6A",
        "outputId": "cb5cf52f-101b-4c6e-d414-0d7baeddb710"
      },
      "source": [
        "\n",
        "# training the model\n",
        "r = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), \n",
        "              epochs=200, batch_size=32)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 3s 142ms/step - loss: 0.4878 - accuracy: 0.7843 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 0.2035 - accuracy: 0.9155 - val_loss: 0.6928 - val_accuracy: 0.5102\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 0.1684 - accuracy: 0.9329 - val_loss: 0.6928 - val_accuracy: 0.5102\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0873 - accuracy: 0.9708 - val_loss: 0.6934 - val_accuracy: 0.5102\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0774 - accuracy: 0.9738 - val_loss: 0.6933 - val_accuracy: 0.5102\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 0.0851 - accuracy: 0.9621 - val_loss: 0.6925 - val_accuracy: 0.5102\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 0.0617 - accuracy: 0.9767 - val_loss: 0.6924 - val_accuracy: 0.5102\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0934 - accuracy: 0.9621 - val_loss: 0.6939 - val_accuracy: 0.5102\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 0.0360 - accuracy: 0.9883 - val_loss: 0.6944 - val_accuracy: 0.5102\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0302 - accuracy: 0.9942 - val_loss: 0.6949 - val_accuracy: 0.5102\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0274 - accuracy: 0.9913 - val_loss: 0.6969 - val_accuracy: 0.5102\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0167 - accuracy: 0.9971 - val_loss: 0.6969 - val_accuracy: 0.5102\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.6966 - val_accuracy: 0.5102\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.6996 - val_accuracy: 0.5102\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.7045 - val_accuracy: 0.5102\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.7070 - val_accuracy: 0.5102\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 0.0063 - accuracy: 0.9971 - val_loss: 0.7092 - val_accuracy: 0.5102\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.7049 - val_accuracy: 0.5102\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.7015 - val_accuracy: 0.5102\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.6992 - val_accuracy: 0.5102\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.7022 - val_accuracy: 0.5102\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.7060 - val_accuracy: 0.5102\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.7090 - val_accuracy: 0.5102\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 4.3803e-04 - accuracy: 1.0000 - val_loss: 0.7090 - val_accuracy: 0.5102\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 3.1517e-04 - accuracy: 1.0000 - val_loss: 0.7042 - val_accuracy: 0.5102\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.6964 - val_accuracy: 0.5102\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 0.0057 - accuracy: 0.9971 - val_loss: 0.7490 - val_accuracy: 0.5102\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0400 - accuracy: 0.9854 - val_loss: 0.7354 - val_accuracy: 0.5102\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.0274 - accuracy: 0.9883 - val_loss: 0.9073 - val_accuracy: 0.5102\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 0.0244 - accuracy: 0.9913 - val_loss: 1.5461 - val_accuracy: 0.5102\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0326 - accuracy: 0.9883 - val_loss: 1.2574 - val_accuracy: 0.5102\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0596 - accuracy: 0.9796 - val_loss: 1.8742 - val_accuracy: 0.5102\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.1347 - accuracy: 0.9621 - val_loss: 4.7779 - val_accuracy: 0.5102\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0590 - accuracy: 0.9825 - val_loss: 3.7212 - val_accuracy: 0.5102\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0615 - accuracy: 0.9738 - val_loss: 2.6935 - val_accuracy: 0.5102\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0292 - accuracy: 0.9913 - val_loss: 3.0503 - val_accuracy: 0.5102\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 1.9719 - val_accuracy: 0.5102\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0119 - accuracy: 0.9942 - val_loss: 2.7502 - val_accuracy: 0.5102\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0256 - accuracy: 0.9883 - val_loss: 1.6523 - val_accuracy: 0.5102\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0088 - accuracy: 0.9971 - val_loss: 1.9122 - val_accuracy: 0.5102\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 2.4662 - val_accuracy: 0.5102\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 2.3294 - val_accuracy: 0.5102\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 2.3226 - val_accuracy: 0.5102\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 2.2462 - val_accuracy: 0.5102\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 1.9592 - val_accuracy: 0.5102\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 9.3853e-04 - accuracy: 1.0000 - val_loss: 1.8642 - val_accuracy: 0.5102\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 5.4215e-04 - accuracy: 1.0000 - val_loss: 1.9031 - val_accuracy: 0.5102\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 2.0967e-04 - accuracy: 1.0000 - val_loss: 1.9655 - val_accuracy: 0.5102\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.8185 - val_accuracy: 0.5102\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 8.2195e-04 - accuracy: 1.0000 - val_loss: 1.7678 - val_accuracy: 0.5102\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 2.8877e-04 - accuracy: 1.0000 - val_loss: 1.8068 - val_accuracy: 0.5102\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.7791e-04 - accuracy: 1.0000 - val_loss: 1.8453 - val_accuracy: 0.5102\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 3.2109e-04 - accuracy: 1.0000 - val_loss: 1.8512 - val_accuracy: 0.5102\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 3.2917e-04 - accuracy: 1.0000 - val_loss: 1.8588 - val_accuracy: 0.5102\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 2.1537e-04 - accuracy: 1.0000 - val_loss: 1.8669 - val_accuracy: 0.5102\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 3.8186e-04 - accuracy: 1.0000 - val_loss: 1.8373 - val_accuracy: 0.5102\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 3.4941e-04 - accuracy: 1.0000 - val_loss: 1.7384 - val_accuracy: 0.5170\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.9533 - val_accuracy: 0.5170\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 5.2490e-04 - accuracy: 1.0000 - val_loss: 3.3928 - val_accuracy: 0.5102\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 3.6761 - val_accuracy: 0.5102\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 2.4657e-04 - accuracy: 1.0000 - val_loss: 3.1184 - val_accuracy: 0.5170\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 1.9382 - val_accuracy: 0.5374\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 5.1418e-04 - accuracy: 1.0000 - val_loss: 0.8549 - val_accuracy: 0.6667\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 3.5968e-04 - accuracy: 1.0000 - val_loss: 0.3136 - val_accuracy: 0.8503\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 8.3909e-04 - accuracy: 1.0000 - val_loss: 0.2069 - val_accuracy: 0.9116\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 7.1244e-04 - accuracy: 1.0000 - val_loss: 0.2517 - val_accuracy: 0.9320\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.9771e-04 - accuracy: 1.0000 - val_loss: 0.2719 - val_accuracy: 0.9388\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 5.3238e-04 - accuracy: 1.0000 - val_loss: 0.2345 - val_accuracy: 0.9456\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 2.5158e-04 - accuracy: 1.0000 - val_loss: 0.2262 - val_accuracy: 0.9048\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 2.4136e-04 - accuracy: 1.0000 - val_loss: 0.2166 - val_accuracy: 0.9048\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.0646e-04 - accuracy: 1.0000 - val_loss: 0.2046 - val_accuracy: 0.9116\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 1.0105e-04 - accuracy: 1.0000 - val_loss: 0.1827 - val_accuracy: 0.9320\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 9.9309e-04 - accuracy: 1.0000 - val_loss: 0.3665 - val_accuracy: 0.8231\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 5.3627e-04 - accuracy: 1.0000 - val_loss: 0.6093 - val_accuracy: 0.7687\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 1.6231e-04 - accuracy: 1.0000 - val_loss: 0.5479 - val_accuracy: 0.7891\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 2.2174e-04 - accuracy: 1.0000 - val_loss: 0.3588 - val_accuracy: 0.8571\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 3.0947e-04 - accuracy: 1.0000 - val_loss: 0.3596 - val_accuracy: 0.8639\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 7.4768e-04 - accuracy: 1.0000 - val_loss: 0.3551 - val_accuracy: 0.8639\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 1.7845e-04 - accuracy: 1.0000 - val_loss: 0.3191 - val_accuracy: 0.8639\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.1201e-04 - accuracy: 1.0000 - val_loss: 0.3269 - val_accuracy: 0.8571\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.8652e-04 - accuracy: 1.0000 - val_loss: 0.3134 - val_accuracy: 0.8639\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 7.0201e-04 - accuracy: 1.0000 - val_loss: 0.2764 - val_accuracy: 0.8844\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 8.0237e-05 - accuracy: 1.0000 - val_loss: 0.4394 - val_accuracy: 0.8707\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 4.5404e-05 - accuracy: 1.0000 - val_loss: 0.4436 - val_accuracy: 0.8707\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 8.8819e-04 - accuracy: 1.0000 - val_loss: 0.2956 - val_accuracy: 0.9048\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.2893e-04 - accuracy: 1.0000 - val_loss: 0.4025 - val_accuracy: 0.8912\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 3.7155e-05 - accuracy: 1.0000 - val_loss: 0.3974 - val_accuracy: 0.8844\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 4.8163e-05 - accuracy: 1.0000 - val_loss: 0.4116 - val_accuracy: 0.8912\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 5.9873e-05 - accuracy: 1.0000 - val_loss: 0.4147 - val_accuracy: 0.8980\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 3.7589e-04 - accuracy: 1.0000 - val_loss: 0.3206 - val_accuracy: 0.9456\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.3311e-04 - accuracy: 1.0000 - val_loss: 0.2391 - val_accuracy: 0.9524\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 3.3888e-05 - accuracy: 1.0000 - val_loss: 0.2398 - val_accuracy: 0.9592\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 4.3480e-05 - accuracy: 1.0000 - val_loss: 0.2467 - val_accuracy: 0.9592\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 3.5688e-05 - accuracy: 1.0000 - val_loss: 0.2455 - val_accuracy: 0.9660\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 1.5221e-04 - accuracy: 1.0000 - val_loss: 0.2628 - val_accuracy: 0.9660\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 2.3856e-04 - accuracy: 1.0000 - val_loss: 0.2680 - val_accuracy: 0.9456\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 8.2198e-05 - accuracy: 1.0000 - val_loss: 0.3230 - val_accuracy: 0.9048\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 4.0527e-05 - accuracy: 1.0000 - val_loss: 0.3293 - val_accuracy: 0.9048\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0028 - accuracy: 0.9971 - val_loss: 9.5366 - val_accuracy: 0.5102\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 31.9177 - val_accuracy: 0.5102\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 1.0317e-04 - accuracy: 1.0000 - val_loss: 34.3587 - val_accuracy: 0.5102\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 5.2705e-04 - accuracy: 1.0000 - val_loss: 30.8535 - val_accuracy: 0.5102\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 8.5410e-04 - accuracy: 1.0000 - val_loss: 25.2412 - val_accuracy: 0.5102\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 2.7088e-04 - accuracy: 1.0000 - val_loss: 21.0625 - val_accuracy: 0.5102\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 7.7375e-05 - accuracy: 1.0000 - val_loss: 18.4581 - val_accuracy: 0.5102\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.1514e-04 - accuracy: 1.0000 - val_loss: 16.2850 - val_accuracy: 0.5102\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.0421e-04 - accuracy: 1.0000 - val_loss: 14.4721 - val_accuracy: 0.5102\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 1.0815e-04 - accuracy: 1.0000 - val_loss: 13.0578 - val_accuracy: 0.5102\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 6.4885e-05 - accuracy: 1.0000 - val_loss: 11.5776 - val_accuracy: 0.5102\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 5.1649e-05 - accuracy: 1.0000 - val_loss: 10.0208 - val_accuracy: 0.5102\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 1.7517e-04 - accuracy: 1.0000 - val_loss: 8.8584 - val_accuracy: 0.5102\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 7.1409 - val_accuracy: 0.5170\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 5.9968e-05 - accuracy: 1.0000 - val_loss: 6.6243 - val_accuracy: 0.5170\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 1.1083e-04 - accuracy: 1.0000 - val_loss: 6.0003 - val_accuracy: 0.5170\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 2.7126e-04 - accuracy: 1.0000 - val_loss: 4.4018 - val_accuracy: 0.5442\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 7.4109 - val_accuracy: 0.5102\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0542 - accuracy: 0.9913 - val_loss: 40.0599 - val_accuracy: 0.5102\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0800 - accuracy: 0.9708 - val_loss: 112.3274 - val_accuracy: 0.5102\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0089 - accuracy: 0.9971 - val_loss: 117.3296 - val_accuracy: 0.5102\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0576 - accuracy: 0.9738 - val_loss: 176.6109 - val_accuracy: 0.5102\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0587 - accuracy: 0.9767 - val_loss: 525.6026 - val_accuracy: 0.5102\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.1150 - accuracy: 0.9738 - val_loss: 7.8370 - val_accuracy: 0.5102\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 0.0533 - accuracy: 0.9767 - val_loss: 212.6420 - val_accuracy: 0.5102\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0203 - accuracy: 0.9883 - val_loss: 280.3264 - val_accuracy: 0.5102\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0308 - accuracy: 0.9854 - val_loss: 296.1548 - val_accuracy: 0.5102\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 238.1971 - val_accuracy: 0.5102\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0035 - accuracy: 0.9971 - val_loss: 222.3469 - val_accuracy: 0.5102\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0069 - accuracy: 0.9971 - val_loss: 197.0046 - val_accuracy: 0.5102\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 166.8453 - val_accuracy: 0.5102\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 5.5960e-04 - accuracy: 1.0000 - val_loss: 142.7133 - val_accuracy: 0.5102\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 123.1742 - val_accuracy: 0.5102\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 5.0601e-04 - accuracy: 1.0000 - val_loss: 106.9243 - val_accuracy: 0.5102\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 9.6830e-04 - accuracy: 1.0000 - val_loss: 96.1621 - val_accuracy: 0.5102\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 8.2540e-04 - accuracy: 1.0000 - val_loss: 83.7535 - val_accuracy: 0.5102\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 3.0042e-04 - accuracy: 1.0000 - val_loss: 73.3172 - val_accuracy: 0.5102\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 4.9844e-04 - accuracy: 1.0000 - val_loss: 64.7262 - val_accuracy: 0.5102\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 3.5300e-04 - accuracy: 1.0000 - val_loss: 57.4215 - val_accuracy: 0.5102\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 3.8224e-04 - accuracy: 1.0000 - val_loss: 50.9665 - val_accuracy: 0.5102\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 2.8298e-04 - accuracy: 1.0000 - val_loss: 45.0523 - val_accuracy: 0.5102\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 39.4406 - val_accuracy: 0.5102\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 4.6430e-04 - accuracy: 1.0000 - val_loss: 34.8288 - val_accuracy: 0.5102\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 4.8651e-04 - accuracy: 1.0000 - val_loss: 30.7385 - val_accuracy: 0.5102\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 5.2004e-04 - accuracy: 1.0000 - val_loss: 27.4534 - val_accuracy: 0.5102\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 4.1342e-04 - accuracy: 1.0000 - val_loss: 24.6146 - val_accuracy: 0.5102\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 1.6606e-04 - accuracy: 1.0000 - val_loss: 22.1357 - val_accuracy: 0.5102\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 2.1561e-04 - accuracy: 1.0000 - val_loss: 19.0082 - val_accuracy: 0.5102\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 1.5207e-04 - accuracy: 1.0000 - val_loss: 16.1788 - val_accuracy: 0.5102\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 2.1971e-04 - accuracy: 1.0000 - val_loss: 13.7292 - val_accuracy: 0.5102\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.4285e-04 - accuracy: 1.0000 - val_loss: 11.5158 - val_accuracy: 0.5102\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 4.0891e-04 - accuracy: 1.0000 - val_loss: 8.7738 - val_accuracy: 0.5102\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 5.7332e-05 - accuracy: 1.0000 - val_loss: 6.5220 - val_accuracy: 0.5306\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 7.7740e-05 - accuracy: 1.0000 - val_loss: 4.8122 - val_accuracy: 0.5442\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 6.8509e-04 - accuracy: 1.0000 - val_loss: 3.6240 - val_accuracy: 0.5986\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 4.6677e-04 - accuracy: 1.0000 - val_loss: 3.2223 - val_accuracy: 0.6054\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 3.8508e-04 - accuracy: 1.0000 - val_loss: 4.9144 - val_accuracy: 0.5442\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 1.6123e-04 - accuracy: 1.0000 - val_loss: 4.5848 - val_accuracy: 0.5646\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 6.9421e-05 - accuracy: 1.0000 - val_loss: 3.5865 - val_accuracy: 0.5918\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.2974e-04 - accuracy: 1.0000 - val_loss: 2.3478 - val_accuracy: 0.6667\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 1.4761e-04 - accuracy: 1.0000 - val_loss: 1.2664 - val_accuracy: 0.7279\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.9792 - val_accuracy: 0.7211\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 7.5455e-05 - accuracy: 1.0000 - val_loss: 2.8025 - val_accuracy: 0.5714\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 9.3871e-04 - accuracy: 1.0000 - val_loss: 1.7935 - val_accuracy: 0.6054\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 5.6208e-05 - accuracy: 1.0000 - val_loss: 1.0168 - val_accuracy: 0.7075\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1071 - val_accuracy: 0.9864\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 3.5096e-04 - accuracy: 1.0000 - val_loss: 0.1729 - val_accuracy: 0.9320\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 3.3920e-04 - accuracy: 1.0000 - val_loss: 0.5597 - val_accuracy: 0.7755\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 1.5240e-04 - accuracy: 1.0000 - val_loss: 2.4468 - val_accuracy: 0.5102\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 6.5295e-05 - accuracy: 1.0000 - val_loss: 2.9976 - val_accuracy: 0.5102\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 4.3727e-05 - accuracy: 1.0000 - val_loss: 2.7441 - val_accuracy: 0.5102\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 5.4985e-05 - accuracy: 1.0000 - val_loss: 2.2191 - val_accuracy: 0.5442\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 7.7686e-05 - accuracy: 1.0000 - val_loss: 1.6175 - val_accuracy: 0.6190\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 9.0165e-05 - accuracy: 1.0000 - val_loss: 1.0145 - val_accuracy: 0.6939\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 1.7732e-05 - accuracy: 1.0000 - val_loss: 0.5844 - val_accuracy: 0.7959\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 4.2964e-05 - accuracy: 1.0000 - val_loss: 0.3343 - val_accuracy: 0.8776\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 8.7264e-05 - accuracy: 1.0000 - val_loss: 0.2022 - val_accuracy: 0.9320\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 2.5039e-05 - accuracy: 1.0000 - val_loss: 0.1696 - val_accuracy: 0.9524\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 9.2319e-05 - accuracy: 1.0000 - val_loss: 0.1638 - val_accuracy: 0.9728\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 2.6297e-05 - accuracy: 1.0000 - val_loss: 0.1642 - val_accuracy: 0.9728\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 2.9036e-05 - accuracy: 1.0000 - val_loss: 0.1648 - val_accuracy: 0.9728\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 8.1118e-05 - accuracy: 1.0000 - val_loss: 0.1618 - val_accuracy: 0.9728\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 8.2108e-05 - accuracy: 1.0000 - val_loss: 0.1606 - val_accuracy: 0.9728\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 2.3572e-04 - accuracy: 1.0000 - val_loss: 0.1196 - val_accuracy: 0.9728\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 1.2785e-04 - accuracy: 1.0000 - val_loss: 0.1387 - val_accuracy: 0.9524\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 6.3776e-05 - accuracy: 1.0000 - val_loss: 0.1042 - val_accuracy: 0.9796\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 7.0103e-05 - accuracy: 1.0000 - val_loss: 0.1137 - val_accuracy: 0.9796\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 1.2357e-04 - accuracy: 1.0000 - val_loss: 0.1145 - val_accuracy: 0.9864\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 5.3626e-05 - accuracy: 1.0000 - val_loss: 0.1242 - val_accuracy: 0.9728\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.0175e-05 - accuracy: 1.0000 - val_loss: 0.1391 - val_accuracy: 0.9660\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 8.9179e-04 - accuracy: 1.0000 - val_loss: 0.3592 - val_accuracy: 0.8980\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 5.5243e-04 - accuracy: 1.0000 - val_loss: 16.7910 - val_accuracy: 0.5102\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 6.5349e-05 - accuracy: 1.0000 - val_loss: 20.6950 - val_accuracy: 0.5102\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 3.6471e-04 - accuracy: 1.0000 - val_loss: 19.5015 - val_accuracy: 0.5102\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 3.0222e-04 - accuracy: 1.0000 - val_loss: 16.8013 - val_accuracy: 0.5102\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 11.5879 - val_accuracy: 0.5102\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.2316e-04 - accuracy: 1.0000 - val_loss: 8.7365 - val_accuracy: 0.5102\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.4166e-04 - accuracy: 1.0000 - val_loss: 7.3559 - val_accuracy: 0.5102\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 6.8851e-05 - accuracy: 1.0000 - val_loss: 6.2172 - val_accuracy: 0.5102\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 4.4606e-05 - accuracy: 1.0000 - val_loss: 5.2462 - val_accuracy: 0.5170\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 2.4930e-04 - accuracy: 1.0000 - val_loss: 8.5320 - val_accuracy: 0.5102\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 1.1686e-04 - accuracy: 1.0000 - val_loss: 10.0751 - val_accuracy: 0.5102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2Q1QDQTGqqV"
      },
      "source": [
        "pred_test= model.predict(X_test)\n",
        "Rows, Cols = pred_test.shape\n",
        "Prediction =[]\n",
        "for i in range(Rows):\n",
        "  if(pred_test[0,0] > pred_test[0,1]):\n",
        "    Prediction.append(0)\n",
        "  else:\n",
        "    Prediction.append(1)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaBx_Qdz5PIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb4947d4-df2d-46b1-b866-f0fc223d0617"
      },
      "source": [
        "# pred_test= model.predict_classes(X_test)\n",
        "pred_test = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict   1\n",
            "Actual     \n",
            "0        72\n",
            "1        75\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXognw8S5VLf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a4a1806-b299-4860-84c2-2114ddc4b142"
      },
      "source": [
        "print(METRICS)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        72\n",
            "           1       0.51      1.00      0.68        75\n",
            "\n",
            "    accuracy                           0.51       147\n",
            "   macro avg       0.26      0.50      0.34       147\n",
            "weighted avg       0.26      0.51      0.34       147\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iFNNrlWV9tH"
      },
      "source": [
        "#pred_test"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2_t9FpLG42n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f07879a-e4d0-4021-fc2f-37e9e6432f74"
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[9] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  prediction_02 = model.predict(result)\n",
        "  Rows, Cols = prediction_02.shape\n",
        "  Prediction =[]\n",
        "  for i in range(Rows):\n",
        "    if(prediction_02[0,0] > prediction_02[0,1]):\n",
        "      Prediction.append(0)\n",
        "    else:\n",
        "      Prediction.append(1)\n",
        "  loc_grao =[];k=0\n",
        "  for i in Prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0   175.0   90.908791   93.843201  ...   54.561592   56.212799   60.622395\n",
            "1   113.0   93.945412   97.006584  ...   48.239250   47.497688   48.451878\n",
            "2   128.0   97.116211   99.363281  ...   68.506836   69.538086   70.903320\n",
            "3   113.0   55.317329   50.582584  ...   79.221161   77.526894   75.636223\n",
            "4   103.0   44.139313   44.324627  ...   80.520966   83.156845   84.875954\n",
            "5   126.0  100.098770  103.777786  ...   84.580246   84.407410   82.901230\n",
            "6   103.0   75.207085   79.317368  ...   48.486191   50.137520   52.151291\n",
            "7   118.0   61.940247   42.980747  ...   59.563911   65.319160   66.721054\n",
            "8   144.0   80.584106   81.441360  ...   21.012346   11.175926    2.733025\n",
            "9   193.0   78.870438   82.588287  ...   58.678005   61.540684   60.298397\n",
            "10  115.0   54.232891   54.505249  ...   71.764153   65.775269   60.527782\n",
            "11  139.0   37.910252   41.747738  ...   95.483040   95.530617   93.005676\n",
            "12  127.0   72.716972   69.978546  ...   37.081406   37.520924   40.069504\n",
            "13  148.0   52.889702   58.456543  ...   71.714394   74.084740   77.341866\n",
            "14  102.0   64.473671   65.976936  ...    0.502499    0.000000    0.000000\n",
            "15  109.0   27.504333   29.651291  ...   54.844707   56.472179   58.599442\n",
            "16  131.0   44.427483   46.335411  ...    0.064740    0.000000    0.000000\n",
            "17  112.0    3.187500    0.875000  ...   64.437500   65.125000   71.687500\n",
            "18  115.0  101.861618  100.100250  ...   97.988724  100.968994  104.946304\n",
            "19  127.0    8.778473    7.008617  ...   18.410440    3.359291    1.189286\n",
            "20  132.0   68.566582   47.006432  ...   91.698814   93.837471   93.247940\n",
            "21  119.0   67.480972   74.269897  ...   82.716270   82.650520   82.512108\n",
            "22  106.0   83.529373   88.953362  ...   41.224277   41.008545   40.566395\n",
            "23  121.0   40.062500   42.538692  ...   86.842911   71.537399   66.334061\n",
            "24  104.0   50.640541   50.236694  ...    9.088758    9.263314   10.155326\n",
            "25  198.0   90.181091   97.064072  ...  106.429947  114.076408  120.995308\n",
            "26  167.0   76.041878   65.138374  ...   98.676323   99.654381  100.279144\n",
            "27  199.0   77.076759   83.895279  ...   54.677681   57.442383   54.615841\n",
            "28  192.0  100.019096   98.139748  ...   52.830292   54.386280   54.036892\n",
            "29  137.0   16.200384   19.152058  ...   85.642174   88.352448   89.613136\n",
            "30  179.0   76.830193   81.640396  ...  106.282196  111.904221  113.564217\n",
            "31  151.0   77.581253   78.240601  ...   59.213894   55.374596   53.410202\n",
            "32  139.0   89.572685   95.993271  ...   63.024940   75.702858   81.150505\n",
            "33  129.0  105.028603  110.129501  ...    7.271619    7.357911    7.718587\n",
            "34  181.0   61.037544   68.010536  ...  118.319138  121.799561  116.173904\n",
            "35  187.0   99.487717  101.322433  ...   87.497665   89.344444   96.859779\n",
            "36  165.0   54.922493   58.354786  ...   34.481617   23.126390    7.146594\n",
            "37  164.0    1.217133    1.964902  ...   76.127304   73.677582   81.355743\n",
            "38  189.0   62.396442   52.536350  ...   81.755829   60.662552   50.887520\n",
            "39  135.0   72.198021   73.065399  ...   72.186607   73.062660   72.408493\n",
            "40  186.0   95.419243   94.314606  ...   28.034801   49.578682   99.850624\n",
            "41  125.0   75.682884   75.230911  ...   77.832321   86.698700   88.207939\n",
            "42  183.0   46.261665   46.034309  ...   42.185017   42.296726   43.295143\n",
            "43  115.0  135.992874  152.267822  ...   74.717270   76.051491   76.467216\n",
            "44  114.0   91.935059   92.220688  ...   68.094185   70.255157   72.284706\n",
            "45  184.0   37.716442   39.302929  ...  102.460762   99.660675   98.318985\n",
            "46  122.0   95.485352   79.736351  ...    0.000000    0.000000    0.000000\n",
            "47  196.0   86.285713   85.816322  ...   95.346939   96.775505   97.020409\n",
            "48  197.0    0.671700    0.290036  ...   73.358315   53.474838   36.001678\n",
            "49  167.0  164.918823  135.040451  ...    5.741189    2.147693    1.082326\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRbWgmX_LFH",
        "outputId": "c8badc78-4d44-4e60-969c-c42a0f622299"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "from GetBetterSegm import GetBetter"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marquesgabi_paper_fev_2021'...\n",
            "remote: Enumerating objects: 722, done.\u001b[K\n",
            "remote: Counting objects: 100% (483/483), done.\u001b[K\n",
            "remote: Compressing objects: 100% (481/481), done.\u001b[K\n",
            "remote: Total 722 (delta 308), reused 0 (delta 0), pack-reused 239\u001b[K\n",
            "Receiving objects: 100% (722/722), 5.82 MiB | 7.22 MiB/s, done.\n",
            "Resolving deltas: 100% (445/445), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAG_I6FwCvFr",
        "outputId": "6bc73445-a8dc-47fd-9195-8b9306cc378d"
      },
      "source": [
        "#!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "\n",
        "%cd Doutorado\n",
        "\n",
        "PSD_imageJ = 'Amostra8.csv' \n",
        "PSD_new = pd.read_csv(PSD_imageJ,sep=';')\n",
        "#encoding='utf8'\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Doutorado'...\n",
            "remote: Enumerating objects: 464, done.\u001b[K\n",
            "remote: Counting objects: 100% (214/214), done.\u001b[K\n",
            "remote: Compressing objects: 100% (213/213), done.\u001b[K\n",
            "remote: Total 464 (delta 102), reused 0 (delta 0), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (464/464), 166.11 MiB | 15.67 MiB/s, done.\n",
            "Resolving deltas: 100% (225/225), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/Doutorado\n",
            "   Unnamed: 0   Area\n",
            "0           1  0.807\n",
            "1           2  1.407\n",
            "2           3  1.177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tEPjIBnv_xM",
        "outputId": "7aae4884-7648-4c32-b619-76d8e62de331"
      },
      "source": [
        "PSD_new.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfagXc-Mv3oa"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "PekBHQOT_6CP",
        "outputId": "95a71a14-5938-4f98-8797-f50b8b599e2e"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>175.0</td>\n",
              "      <td>90.908791</td>\n",
              "      <td>93.843201</td>\n",
              "      <td>87.049591</td>\n",
              "      <td>82.111992</td>\n",
              "      <td>71.963196</td>\n",
              "      <td>52.600006</td>\n",
              "      <td>49.023994</td>\n",
              "      <td>58.559994</td>\n",
              "      <td>61.660797</td>\n",
              "      <td>61.891193</td>\n",
              "      <td>55.947197</td>\n",
              "      <td>55.956795</td>\n",
              "      <td>55.115196</td>\n",
              "      <td>55.246399</td>\n",
              "      <td>55.867195</td>\n",
              "      <td>57.316795</td>\n",
              "      <td>56.932796</td>\n",
              "      <td>56.225597</td>\n",
              "      <td>50.905598</td>\n",
              "      <td>45.350399</td>\n",
              "      <td>38.499195</td>\n",
              "      <td>31.497595</td>\n",
              "      <td>35.409599</td>\n",
              "      <td>70.299194</td>\n",
              "      <td>88.294403</td>\n",
              "      <td>94.291199</td>\n",
              "      <td>96.454384</td>\n",
              "      <td>99.321602</td>\n",
              "      <td>83.753601</td>\n",
              "      <td>83.449593</td>\n",
              "      <td>81.263992</td>\n",
              "      <td>72.319992</td>\n",
              "      <td>54.564796</td>\n",
              "      <td>48.308800</td>\n",
              "      <td>51.267197</td>\n",
              "      <td>61.289597</td>\n",
              "      <td>65.987198</td>\n",
              "      <td>65.327995</td>\n",
              "      <td>56.782394</td>\n",
              "      <td>...</td>\n",
              "      <td>73.195198</td>\n",
              "      <td>77.351997</td>\n",
              "      <td>75.272003</td>\n",
              "      <td>68.030396</td>\n",
              "      <td>55.169598</td>\n",
              "      <td>53.985596</td>\n",
              "      <td>51.535995</td>\n",
              "      <td>49.023994</td>\n",
              "      <td>50.427197</td>\n",
              "      <td>52.001595</td>\n",
              "      <td>54.782398</td>\n",
              "      <td>58.519993</td>\n",
              "      <td>89.854393</td>\n",
              "      <td>76.014397</td>\n",
              "      <td>40.254398</td>\n",
              "      <td>40.686398</td>\n",
              "      <td>47.793598</td>\n",
              "      <td>51.617596</td>\n",
              "      <td>54.555191</td>\n",
              "      <td>53.758396</td>\n",
              "      <td>56.627197</td>\n",
              "      <td>58.419193</td>\n",
              "      <td>61.401596</td>\n",
              "      <td>60.889595</td>\n",
              "      <td>59.639999</td>\n",
              "      <td>58.827194</td>\n",
              "      <td>58.691193</td>\n",
              "      <td>61.798401</td>\n",
              "      <td>70.737595</td>\n",
              "      <td>76.859192</td>\n",
              "      <td>79.156799</td>\n",
              "      <td>74.443192</td>\n",
              "      <td>63.838394</td>\n",
              "      <td>55.710396</td>\n",
              "      <td>52.275200</td>\n",
              "      <td>53.942398</td>\n",
              "      <td>54.011196</td>\n",
              "      <td>54.561592</td>\n",
              "      <td>56.212799</td>\n",
              "      <td>60.622395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>113.0</td>\n",
              "      <td>93.945412</td>\n",
              "      <td>97.006584</td>\n",
              "      <td>97.523148</td>\n",
              "      <td>97.381866</td>\n",
              "      <td>98.850349</td>\n",
              "      <td>102.256554</td>\n",
              "      <td>106.305267</td>\n",
              "      <td>106.428841</td>\n",
              "      <td>102.839066</td>\n",
              "      <td>95.988411</td>\n",
              "      <td>90.580780</td>\n",
              "      <td>89.544830</td>\n",
              "      <td>86.848305</td>\n",
              "      <td>86.588776</td>\n",
              "      <td>87.055840</td>\n",
              "      <td>86.838905</td>\n",
              "      <td>85.449684</td>\n",
              "      <td>87.504189</td>\n",
              "      <td>89.785187</td>\n",
              "      <td>94.087006</td>\n",
              "      <td>97.058975</td>\n",
              "      <td>98.950272</td>\n",
              "      <td>97.276367</td>\n",
              "      <td>91.460876</td>\n",
              "      <td>65.584229</td>\n",
              "      <td>24.487038</td>\n",
              "      <td>27.346621</td>\n",
              "      <td>29.238859</td>\n",
              "      <td>92.132584</td>\n",
              "      <td>95.235741</td>\n",
              "      <td>96.116928</td>\n",
              "      <td>95.865387</td>\n",
              "      <td>97.365349</td>\n",
              "      <td>103.225632</td>\n",
              "      <td>109.107529</td>\n",
              "      <td>110.265495</td>\n",
              "      <td>105.716347</td>\n",
              "      <td>97.452904</td>\n",
              "      <td>91.881271</td>\n",
              "      <td>...</td>\n",
              "      <td>76.309418</td>\n",
              "      <td>77.273941</td>\n",
              "      <td>78.460411</td>\n",
              "      <td>81.508415</td>\n",
              "      <td>82.187958</td>\n",
              "      <td>72.030540</td>\n",
              "      <td>58.027645</td>\n",
              "      <td>48.216541</td>\n",
              "      <td>49.334721</td>\n",
              "      <td>50.602947</td>\n",
              "      <td>50.177464</td>\n",
              "      <td>50.759182</td>\n",
              "      <td>75.697235</td>\n",
              "      <td>75.063507</td>\n",
              "      <td>69.699966</td>\n",
              "      <td>65.795044</td>\n",
              "      <td>70.549454</td>\n",
              "      <td>75.687042</td>\n",
              "      <td>80.229065</td>\n",
              "      <td>82.518051</td>\n",
              "      <td>82.223671</td>\n",
              "      <td>81.758789</td>\n",
              "      <td>80.712906</td>\n",
              "      <td>80.110031</td>\n",
              "      <td>80.676407</td>\n",
              "      <td>81.210510</td>\n",
              "      <td>79.770851</td>\n",
              "      <td>78.357582</td>\n",
              "      <td>80.011826</td>\n",
              "      <td>80.686897</td>\n",
              "      <td>83.857224</td>\n",
              "      <td>88.855583</td>\n",
              "      <td>90.518600</td>\n",
              "      <td>83.773201</td>\n",
              "      <td>68.022316</td>\n",
              "      <td>50.682983</td>\n",
              "      <td>48.154831</td>\n",
              "      <td>48.239250</td>\n",
              "      <td>47.497688</td>\n",
              "      <td>48.451878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>128.0</td>\n",
              "      <td>97.116211</td>\n",
              "      <td>99.363281</td>\n",
              "      <td>101.120117</td>\n",
              "      <td>103.237305</td>\n",
              "      <td>105.067383</td>\n",
              "      <td>107.893555</td>\n",
              "      <td>112.124023</td>\n",
              "      <td>112.666016</td>\n",
              "      <td>111.428711</td>\n",
              "      <td>108.452148</td>\n",
              "      <td>108.463867</td>\n",
              "      <td>111.670898</td>\n",
              "      <td>116.499023</td>\n",
              "      <td>119.522461</td>\n",
              "      <td>119.367188</td>\n",
              "      <td>115.756836</td>\n",
              "      <td>103.452148</td>\n",
              "      <td>84.054688</td>\n",
              "      <td>78.672852</td>\n",
              "      <td>82.488281</td>\n",
              "      <td>91.420898</td>\n",
              "      <td>99.040039</td>\n",
              "      <td>102.558594</td>\n",
              "      <td>103.543945</td>\n",
              "      <td>103.448242</td>\n",
              "      <td>104.626953</td>\n",
              "      <td>105.044922</td>\n",
              "      <td>106.189453</td>\n",
              "      <td>93.198242</td>\n",
              "      <td>97.667969</td>\n",
              "      <td>99.371094</td>\n",
              "      <td>101.927734</td>\n",
              "      <td>103.997070</td>\n",
              "      <td>107.600586</td>\n",
              "      <td>110.039062</td>\n",
              "      <td>110.556641</td>\n",
              "      <td>109.406250</td>\n",
              "      <td>107.149414</td>\n",
              "      <td>107.971680</td>\n",
              "      <td>...</td>\n",
              "      <td>75.171875</td>\n",
              "      <td>74.318359</td>\n",
              "      <td>75.764648</td>\n",
              "      <td>74.640625</td>\n",
              "      <td>70.477539</td>\n",
              "      <td>69.561523</td>\n",
              "      <td>68.022461</td>\n",
              "      <td>67.680664</td>\n",
              "      <td>68.113281</td>\n",
              "      <td>66.895508</td>\n",
              "      <td>64.541016</td>\n",
              "      <td>66.837891</td>\n",
              "      <td>69.626953</td>\n",
              "      <td>61.082031</td>\n",
              "      <td>64.163086</td>\n",
              "      <td>69.704102</td>\n",
              "      <td>68.667969</td>\n",
              "      <td>62.959961</td>\n",
              "      <td>57.387695</td>\n",
              "      <td>50.792969</td>\n",
              "      <td>43.843750</td>\n",
              "      <td>52.233398</td>\n",
              "      <td>69.923828</td>\n",
              "      <td>73.101562</td>\n",
              "      <td>75.121094</td>\n",
              "      <td>77.145508</td>\n",
              "      <td>75.133789</td>\n",
              "      <td>74.646484</td>\n",
              "      <td>73.572266</td>\n",
              "      <td>72.032227</td>\n",
              "      <td>69.437500</td>\n",
              "      <td>64.932617</td>\n",
              "      <td>68.591797</td>\n",
              "      <td>70.328125</td>\n",
              "      <td>68.709961</td>\n",
              "      <td>67.735352</td>\n",
              "      <td>70.108398</td>\n",
              "      <td>68.506836</td>\n",
              "      <td>69.538086</td>\n",
              "      <td>70.903320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>113.0</td>\n",
              "      <td>55.317329</td>\n",
              "      <td>50.582584</td>\n",
              "      <td>40.319603</td>\n",
              "      <td>31.308245</td>\n",
              "      <td>31.285299</td>\n",
              "      <td>44.234940</td>\n",
              "      <td>56.308247</td>\n",
              "      <td>49.948311</td>\n",
              "      <td>39.828648</td>\n",
              "      <td>34.750881</td>\n",
              "      <td>36.108387</td>\n",
              "      <td>45.409508</td>\n",
              "      <td>67.635437</td>\n",
              "      <td>72.662315</td>\n",
              "      <td>76.263519</td>\n",
              "      <td>81.892479</td>\n",
              "      <td>88.524628</td>\n",
              "      <td>94.578667</td>\n",
              "      <td>83.537628</td>\n",
              "      <td>57.942669</td>\n",
              "      <td>50.040489</td>\n",
              "      <td>50.095623</td>\n",
              "      <td>51.504505</td>\n",
              "      <td>50.867023</td>\n",
              "      <td>48.694340</td>\n",
              "      <td>49.185844</td>\n",
              "      <td>51.109249</td>\n",
              "      <td>52.339729</td>\n",
              "      <td>51.660976</td>\n",
              "      <td>38.463234</td>\n",
              "      <td>29.579685</td>\n",
              "      <td>34.796227</td>\n",
              "      <td>48.913307</td>\n",
              "      <td>64.110893</td>\n",
              "      <td>67.814545</td>\n",
              "      <td>66.693550</td>\n",
              "      <td>63.517269</td>\n",
              "      <td>63.498009</td>\n",
              "      <td>65.109406</td>\n",
              "      <td>...</td>\n",
              "      <td>18.223980</td>\n",
              "      <td>19.114262</td>\n",
              "      <td>57.729187</td>\n",
              "      <td>79.645081</td>\n",
              "      <td>85.255699</td>\n",
              "      <td>85.751274</td>\n",
              "      <td>85.465034</td>\n",
              "      <td>86.755501</td>\n",
              "      <td>87.624008</td>\n",
              "      <td>86.560036</td>\n",
              "      <td>82.773201</td>\n",
              "      <td>79.917999</td>\n",
              "      <td>15.190069</td>\n",
              "      <td>27.460258</td>\n",
              "      <td>49.176914</td>\n",
              "      <td>64.606628</td>\n",
              "      <td>71.842201</td>\n",
              "      <td>69.585632</td>\n",
              "      <td>63.759106</td>\n",
              "      <td>62.019577</td>\n",
              "      <td>56.211060</td>\n",
              "      <td>45.090691</td>\n",
              "      <td>33.692146</td>\n",
              "      <td>27.762472</td>\n",
              "      <td>26.758478</td>\n",
              "      <td>28.575220</td>\n",
              "      <td>29.598244</td>\n",
              "      <td>30.757305</td>\n",
              "      <td>30.099930</td>\n",
              "      <td>27.752369</td>\n",
              "      <td>47.988724</td>\n",
              "      <td>76.606468</td>\n",
              "      <td>85.381790</td>\n",
              "      <td>89.653061</td>\n",
              "      <td>92.600433</td>\n",
              "      <td>94.179100</td>\n",
              "      <td>88.873131</td>\n",
              "      <td>79.221161</td>\n",
              "      <td>77.526894</td>\n",
              "      <td>75.636223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>103.0</td>\n",
              "      <td>44.139313</td>\n",
              "      <td>44.324627</td>\n",
              "      <td>43.736637</td>\n",
              "      <td>44.789894</td>\n",
              "      <td>47.107738</td>\n",
              "      <td>49.484589</td>\n",
              "      <td>51.815910</td>\n",
              "      <td>52.250069</td>\n",
              "      <td>52.216419</td>\n",
              "      <td>52.491093</td>\n",
              "      <td>53.379677</td>\n",
              "      <td>53.997265</td>\n",
              "      <td>57.913280</td>\n",
              "      <td>63.175793</td>\n",
              "      <td>66.810539</td>\n",
              "      <td>65.993866</td>\n",
              "      <td>65.415497</td>\n",
              "      <td>59.901871</td>\n",
              "      <td>50.421623</td>\n",
              "      <td>40.165993</td>\n",
              "      <td>37.454517</td>\n",
              "      <td>43.620789</td>\n",
              "      <td>49.606464</td>\n",
              "      <td>50.404846</td>\n",
              "      <td>47.493729</td>\n",
              "      <td>45.068245</td>\n",
              "      <td>45.602695</td>\n",
              "      <td>45.392685</td>\n",
              "      <td>43.943066</td>\n",
              "      <td>44.550285</td>\n",
              "      <td>44.341595</td>\n",
              "      <td>44.922424</td>\n",
              "      <td>48.912056</td>\n",
              "      <td>49.968140</td>\n",
              "      <td>52.697613</td>\n",
              "      <td>53.767838</td>\n",
              "      <td>54.092190</td>\n",
              "      <td>53.878590</td>\n",
              "      <td>55.169476</td>\n",
              "      <td>...</td>\n",
              "      <td>81.607689</td>\n",
              "      <td>87.510979</td>\n",
              "      <td>87.992928</td>\n",
              "      <td>85.535957</td>\n",
              "      <td>85.633797</td>\n",
              "      <td>83.807045</td>\n",
              "      <td>80.808266</td>\n",
              "      <td>79.398140</td>\n",
              "      <td>78.700439</td>\n",
              "      <td>80.295502</td>\n",
              "      <td>82.432182</td>\n",
              "      <td>84.255722</td>\n",
              "      <td>42.296165</td>\n",
              "      <td>37.901402</td>\n",
              "      <td>31.382790</td>\n",
              "      <td>28.226601</td>\n",
              "      <td>26.355076</td>\n",
              "      <td>25.825903</td>\n",
              "      <td>28.010273</td>\n",
              "      <td>31.796303</td>\n",
              "      <td>36.733528</td>\n",
              "      <td>45.966820</td>\n",
              "      <td>53.649353</td>\n",
              "      <td>56.291641</td>\n",
              "      <td>59.631157</td>\n",
              "      <td>62.474403</td>\n",
              "      <td>66.674332</td>\n",
              "      <td>71.644257</td>\n",
              "      <td>75.173904</td>\n",
              "      <td>80.493919</td>\n",
              "      <td>81.281647</td>\n",
              "      <td>81.120461</td>\n",
              "      <td>80.583176</td>\n",
              "      <td>81.333115</td>\n",
              "      <td>82.135818</td>\n",
              "      <td>81.137802</td>\n",
              "      <td>79.019043</td>\n",
              "      <td>80.520966</td>\n",
              "      <td>83.156845</td>\n",
              "      <td>84.875954</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Width          0          1  ...        781        782        783\n",
              "0  175.0  90.908791  93.843201  ...  54.561592  56.212799  60.622395\n",
              "1  113.0  93.945412  97.006584  ...  48.239250  47.497688  48.451878\n",
              "2  128.0  97.116211  99.363281  ...  68.506836  69.538086  70.903320\n",
              "3  113.0  55.317329  50.582584  ...  79.221161  77.526894  75.636223\n",
              "4  103.0  44.139313  44.324627  ...  80.520966  83.156845  84.875954\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "VaZPe_AxNBK9",
        "outputId": "dc1c3fd6-a62b-4b64-f87b-f2304ca91c41"
      },
      "source": [
        "PSD_new.head()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Area</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1.407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1.289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1.743</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0   Area\n",
              "0           1  0.807\n",
              "1           2  1.407\n",
              "2           3  1.177\n",
              "3           4  1.289\n",
              "4           5  1.743"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dExnOsslHrab"
      },
      "source": [
        "#lost_value = float(PSD_new.columns[1])\n",
        "\n",
        "# Area = np.array(PSD_new.iloc[:,1])\n",
        "Area = PSD_new['Area'].values\n",
        "# Area = np.concatenate( (Area, [lost_value] ) )\n",
        "# Area = np.concatenate( (Area, [lost_value] ) )\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8IJgjcFLssj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "38a4351c-273a-4d1a-f607-2e2ce22b96df"
      },
      "source": [
        "PSD_new"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Area</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1.407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1.289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1.743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>28</td>\n",
              "      <td>2.097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>29</td>\n",
              "      <td>1.871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>30</td>\n",
              "      <td>1.315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>31</td>\n",
              "      <td>1.034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>32</td>\n",
              "      <td>2.095</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>99 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Unnamed: 0   Area\n",
              "0            1  0.807\n",
              "1            2  1.407\n",
              "2            3  1.177\n",
              "3            4  1.289\n",
              "4            5  1.743\n",
              "..         ...    ...\n",
              "94          28  2.097\n",
              "95          29  1.871\n",
              "96          30  1.315\n",
              "97          31  1.034\n",
              "98          32  2.095\n",
              "\n",
              "[99 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_sp1BcwKM9q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8052d4f-e0a0-44ab-d30d-b5f2ac9430a8"
      },
      "source": [
        "PSD_new.iloc[:,1].values"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.807, 1.407, 1.177, 1.289, 1.743, 1.425, 2.553, 0.968, 1.43 ,\n",
              "       0.722, 1.235, 1.058, 1.342, 1.207, 1.682, 1.474, 1.997, 1.187,\n",
              "       2.082, 2.877, 1.386, 1.176, 0.96 , 1.147, 1.02 , 1.249, 1.704,\n",
              "       1.602, 1.303, 1.707, 2.264, 1.233, 0.84 , 1.105, 1.343, 0.811,\n",
              "       2.03 , 1.844, 2.266, 1.472, 1.009, 1.851, 0.941, 2.252, 1.269,\n",
              "       1.082, 1.065, 1.995, 2.063, 0.969, 1.389, 1.721, 1.355, 1.178,\n",
              "       1.529, 1.371, 1.423, 2.756, 0.854, 0.811, 0.69 , 1.752, 0.978,\n",
              "       1.108, 1.149, 0.994, 1.594, 1.492, 1.322, 1.564, 1.29 , 1.057,\n",
              "       1.193, 1.413, 1.477, 2.21 , 1.27 , 1.865, 1.088, 2.316, 1.855,\n",
              "       0.882, 1.587, 1.075, 2.179, 1.749, 0.957, 1.24 , 1.586, 2.507,\n",
              "       1.864, 1.281, 2.137, 1.282, 2.097, 1.871, 1.315, 1.034, 2.095])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J705kDqsE8f",
        "outputId": "7460d49d-6e89-4d24-bcbd-4bbc49467e09"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(490, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlLynvcaBJZz"
      },
      "source": [
        "wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        "wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        "X = pd.DataFrame([Diam1,Diameter_All])\n",
        "wts = pd.DataFrame([wt1,wt2])"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "JPaPY_JpBFff",
        "outputId": "257d4032-4312-4da6-9657-5c6b036aa729"
      },
      "source": [
        "A = plt.hist(X,weights=wts,bins=7)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TelhvGgqBbV6"
      },
      "source": [
        "B = A[0][0]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70IntnlyBemv",
        "outputId": "89fb5792-7cde-46e4-cb75-1ad8f36e0d53"
      },
      "source": [
        "Novo = []\n",
        "k = 0\n",
        "soma = 0\n",
        "for i in B:\n",
        "  if(k<4):\n",
        "    Novo.append(i)\n",
        "  else:\n",
        "    soma = soma + i\n",
        "  k = k + 1\n",
        "Novo.append(soma)\n",
        "print(Novo)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.0101010101010102, 10.101010101010102, 27.272727272727273, 30.303030303030305, 31.313131313131322]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "mK1GBUHWiIr4",
        "outputId": "dbf304b9-8706-4b03-ee19-761a0228d4f8"
      },
      "source": [
        "# Freq = [13.6, 24.8, 20.4, 24.8, 9.6, 0.] # average 4 \n",
        "Freq = [13.6, 24.8, 20.4, 24.8, 9.6] # average 4 \n",
        "Freq = [15, 22.2, 22.6, 23, 11.8] # average 10 \n",
        "# Freq2 = [14.6, 24.6, 21.2, 20.6, 14., 0.2]\n",
        "Freq2 = [14.6, 24.6, 21.2, 20.6, 14.2]\n",
        "Freq3 = Novo\n",
        "barWidth = 0.25\n",
        "\n",
        "br1 = range(len(Freq))\n",
        "# Set position of bar on X axis\n",
        "br2 = [x + barWidth for x in br1]\n",
        "br3 = [x + barWidth for x in br2]\n",
        "# labels = [0.8, 1.0, 1.2, 1.4, 1.6, 1.8]\n",
        "labels = [0.8, 1.0, 1.2, 1.4, 1.6]\n",
        "xx=[]\n",
        "for a in labels:\n",
        "  xx.append(str(a))\n",
        "plt.bar(br1, Freq , color=\"green\", align=\"center\", width=0.3, tick_label= xx) \n",
        "plt.bar(br2, Freq2 , color=\"red\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.bar(br3, Freq3 , color=\"blue\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.legend(['CNN 1','CNN 2','True'])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f48506c0a90>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfk_fNXGDK5_"
      },
      "source": [
        " wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        " wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        " X = pd.DataFrame([Diam1,Diameter_All])\n",
        " wts = pd.DataFrame([wt1,wt2])\n",
        "plt.hist(X,weights=wts)\n",
        "plt.legend(['Image J','CNN'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nGDbBEeiUij"
      },
      "source": [
        "# plt.hist(x, bins=bins, density=True, histtype='step', cumulative=-1,label='Reversed emp.')\n",
        "plt.hist(X, density=True, histtype='step', cumulative=True,label='Reversed emp.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xENlBUUxfTu"
      },
      "source": [
        "Obj = plt.hist(X, density=True, histtype='step', cumulative=True,label='Reversed emp.')\n",
        "Y1, Y2 = Obj[0]\n",
        "Rsquared = r2_score(Y1, Y2)\n",
        "print('r_squared =',Rsquared)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2XboMiFbkaa"
      },
      "source": [
        "acc_train = r.history['accuracy'][-1]\n",
        "acc_test = r.history['val_accuracy'][-1]\n",
        "loss_train = r.history['loss'][-1]\n",
        "loss_test = r.history['val_loss'][-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euTd_-CYN1v0"
      },
      "source": [
        "df = pd.DataFrame({'N1':N1, 'N2':N2,'R^2':Rsquared,\n",
        "                   'acc train':acc_train,'acc test':acc_test,\n",
        "                   'loss train':loss_train,'loss test':loss_test,\n",
        "                   'Details':Description},\n",
        "                  index= [0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KukfpGTTKlj"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZHa1j4HT9Dq"
      },
      "source": [
        "counts, bins, bars = plt.hist(X,weights=wts)\n",
        "print(bars)\n",
        "print(bins)\n",
        "print(counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o_vDGeWUwIZ"
      },
      "source": [
        "print(counts.sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcH52-6iJQ8t"
      },
      "source": [
        "\n",
        "plt.hist([Diam1,Diameter_All])\n",
        "plt.legend(['Image J','CNN'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r11AxFK_JIii"
      },
      "source": [
        "[Diam1,Diameter_All]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xS7NSM92s_8"
      },
      "source": [
        " bins_list = [0.8, 1.0, 1.2, 1.4,1.6, 1.8,2.0]\n",
        " wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        " wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        " X = pd.DataFrame([Diam1,Diameter_All])\n",
        " wts = pd.DataFrame([wt1,wt2])\n",
        "plt.hist(X,weights=wts,bins = bins_list)\n",
        "plt.legend(['Image J','CNN'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jD80rFZs37Wm"
      },
      "source": [
        "yy = plt.hist(X,weights=wts,bins = bins_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edcwJoJP4GBs"
      },
      "source": [
        "yy[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0zY-NBY6Rgx"
      },
      "source": [
        "bins_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ujCjAa85RG5"
      },
      "source": [
        "k =0\n",
        "for kk in yy[0][0]:\n",
        "  name = str(bins_list[k])\n",
        "  df[name] = yy[0][1][k]\n",
        "  k = k+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMQRVn4D69RC"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMWpAwMq5Hw2"
      },
      "source": [
        "Arq = \"output.xlsx\"\n",
        "df.to_excel(Arq)\n",
        "files.download(Arq)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}