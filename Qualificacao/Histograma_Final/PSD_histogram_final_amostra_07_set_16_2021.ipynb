{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_final_amostra_07_set_16_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/Qualificacao/Histograma_Final/PSD_histogram_final_amostra_07_set_16_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from sklearn.metrics import r2_score\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZEvJvfoibE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8538a467-768f-4b87-ffb3-b37cb990f9a0"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mahotas\n",
            "  Downloading mahotas-1.4.11-cp37-cp37m-manylinux2010_x86_64.whl (5.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7 MB 11.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.19.5)\n",
            "Installing collected packages: mahotas\n",
            "Successfully installed mahotas-1.4.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT"
      },
      "source": [
        "import mahotas.features.texture as mht\n",
        "import mahotas.features"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VcTdaNVh9EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96e5fcaf-1510-4b15-ea54-4ce4cd700bd1"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marquesgabi_fev_2020'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 73 (delta 37), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n",
            "/content/marquesgabi_fev_2020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v7SRrc8mH2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db9233cd-721d-4995-c4c3-777e69649361"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip' \n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Doutorado'...\n",
            "remote: Enumerating objects: 464, done.\u001b[K\n",
            "remote: Counting objects: 100% (214/214), done.\u001b[K\n",
            "remote: Compressing objects: 100% (210/210), done.\u001b[K\n",
            "remote: Total 464 (delta 102), reused 4 (delta 3), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (464/464), 166.12 MiB | 16.92 MiB/s, done.\n",
            "Resolving deltas: 100% (225/225), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqIYzUcnrdMp",
        "outputId": "91e72b37-6dfa-4a9e-9d58-fa31ad3f3697"
      },
      "source": [
        "labels =[]\n",
        "with zipfile.ZipFile(Transfere, \"r\") as f:\n",
        "  for f in f.namelist():\n",
        "    labels.append(f)\n",
        "print(labels)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Fotos_Grandes-3cdAmostra/Q6-8-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-4.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[2] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHgqAnaFyCjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46cdcde5-c8fe-410d-e750-515778836592"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 19.14 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from segment_filter_not_conclude import Segmenta  # got image provided segmented"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5MN5a_v4np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6235ad01-e3ef-4cc8-a9ba-62c05aa69e31"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     198  134.034363  137.265884  ...  127.490662  136.614410  144.750931\n",
            "1     161   28.655954   28.221174  ...    8.270321    7.741022    7.608696\n",
            "2     114   84.064018   79.719292  ...  134.701141  133.116333  132.822723\n",
            "3     191  125.372086  119.136894  ...   81.289879   82.203651   82.527977\n",
            "4     116  195.925079  211.775253  ...  114.219971  117.461357  120.033295\n",
            "5     143  166.455048  169.940186  ...  115.729721  113.477280  122.339142\n",
            "6     161    0.264650    0.034026  ...  121.446129  117.463150  114.041595\n",
            "7     161   84.085075   55.843102  ...    8.081285    8.398867    8.478261\n",
            "8     181  116.788414  120.419250  ...   69.469101   27.926346   36.522053\n",
            "9     191  136.704681  139.203995  ...   92.697632   90.647926   91.195015\n",
            "10    149   49.917709   59.215755  ...  119.587555  125.237915  127.361969\n",
            "11    183   46.731556   48.901222  ...    7.036580    7.590910    7.608469\n",
            "12    172   75.073013   73.045975  ...  123.409958  119.507309  106.416992\n",
            "13    108   64.249657   61.703701  ...  102.438950   99.831276  107.537720\n",
            "14    199  242.916382  205.641815  ...  107.860474  110.350677  110.443314\n",
            "15    157  130.593704  131.635773  ...   96.609886   21.059597    2.294130\n",
            "16    108  154.201645  152.508911  ...  150.008224  150.665283  151.891632\n",
            "17    130   94.656570   96.326630  ...   10.351006   11.362368   11.281420\n",
            "18    144   71.529320   69.883484  ...    9.225309    8.938272    9.128858\n",
            "19    124  138.862625  141.867828  ...   89.718002   68.905296   59.413109\n",
            "20    129   93.239883   99.452980  ...   59.633133   59.601891   59.772308\n",
            "21    188    0.000000    0.000000  ...   60.932549   88.122681  133.781799\n",
            "22    187    0.000000    0.000000  ...  136.211212  137.816986  137.997772\n",
            "23    177   95.498955   92.573166  ...  142.964417  144.920151  147.969788\n",
            "24    120   62.641113   71.822227  ...   10.365556   10.524445   10.827778\n",
            "25    144  102.773155  103.861107  ...    1.566358    0.411265    0.006173\n",
            "26    176  134.636353  121.049072  ...    6.761880    8.260847    8.401342\n",
            "27    152  145.268005  147.734741  ...   95.781158   98.819939  101.604568\n",
            "28    172   92.415901   92.934029  ...   29.381289   25.329367   23.355328\n",
            "29    128  127.961914  128.949219  ...   84.498047   77.912109   73.361328\n",
            "30    119  165.833908  167.294113  ...   90.435974   92.667816   93.795853\n",
            "31    165  110.225853  116.823250  ...    8.271919    8.305896    7.690873\n",
            "32    166  144.912750  138.141953  ...  120.440994  128.291763  131.258240\n",
            "33    143   49.692795   79.911674  ...   58.922543   55.157318   53.119076\n",
            "34    143  124.287643  119.787468  ...   59.514503   61.120979   61.903221\n",
            "35    151  145.609070  150.261475  ...   73.136009   72.163765   71.604233\n",
            "36    148  128.401749  131.240326  ...  121.211838  118.253471  113.146095\n",
            "37    173   46.974506   41.510635  ...    7.768985    7.884593    8.263691\n",
            "38    192  123.875427  122.361534  ...  102.910149   87.047295   90.423607\n",
            "39    196  141.795914  141.102036  ...    7.326530    6.959184    6.897959\n",
            "40    149   76.732178   74.248055  ...  118.212624  117.923080  118.061539\n",
            "41    163    0.000000    0.000000  ...  131.103165  134.350586  126.653885\n",
            "42    171  191.721466  194.888443  ...   99.587151  100.483185  100.542618\n",
            "43    139  152.130310  132.377716  ...   80.545929   84.109711   88.675217\n",
            "44    115   96.270233   96.510841  ...  151.996521  157.716660  169.860794\n",
            "45    106   64.309013   64.368813  ...    1.069776    0.539694    0.000000\n",
            "46    171  178.175095  181.488800  ...  112.590126  110.885056  111.638031\n",
            "47    200  131.072403  143.430405  ...  105.913193  100.612801   95.653206\n",
            "48    181   49.854740   75.255241  ...  142.484070  145.644424  145.663239\n",
            "49    120  148.867783  150.784439  ...   11.445555   11.048889   10.621112\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzpQ1Pz0fX5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "543f3b5c-f8e1-4fda-edf8-257af4de197e"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines\n",
        "# filename = 'model_ANN.pkl'\n",
        "filename = 'model_ANN_new.pkl'\n",
        "model = joblib.load(filename)\n",
        "'''"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n!git clone https://github.com/ucfilho/MarquesGabi_Routines\\n%cd MarquesGabi_Routines\\n# filename = 'model_ANN.pkl'\\nfilename = 'model_ANN_new.pkl'\\nmodel = joblib.load(filename)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2emP4rNjQy",
        "outputId": "7fecd1a5-8ac2-4e20-8fb1-fd162527ef68"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 18.77 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIFPGE_-vx3T"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "Description = '3 layers of Convolution: 64, 128, 256 '\n",
        "N1 = 20\n",
        "N2 = 20\n",
        "\n",
        "# make the CNN\n",
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=64, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=N1))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=N2))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbPQ1FSRG6A",
        "outputId": "46208e96-15d8-4180-8f01-5cf64af69b0a"
      },
      "source": [
        "\n",
        "# training the model\n",
        "r = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), \n",
        "              epochs=200, batch_size=32)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 3s 151ms/step - loss: 0.5774 - accuracy: 0.7609 - val_loss: 0.6931 - val_accuracy: 0.4898\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.2710 - accuracy: 0.8746 - val_loss: 0.6928 - val_accuracy: 0.5102\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.1922 - accuracy: 0.9067 - val_loss: 0.6928 - val_accuracy: 0.5102\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.1474 - accuracy: 0.9388 - val_loss: 0.6927 - val_accuracy: 0.5102\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.1136 - accuracy: 0.9592 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0769 - accuracy: 0.9767 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0496 - accuracy: 0.9796 - val_loss: 0.6925 - val_accuracy: 0.5102\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0335 - accuracy: 0.9942 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0256 - accuracy: 0.9913 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0279 - accuracy: 0.9942 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0184 - accuracy: 0.9942 - val_loss: 0.6928 - val_accuracy: 0.5102\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0107 - accuracy: 0.9971 - val_loss: 0.6932 - val_accuracy: 0.5102\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.6941 - val_accuracy: 0.5102\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.6951 - val_accuracy: 0.5102\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0085 - accuracy: 0.9971 - val_loss: 0.6969 - val_accuracy: 0.5102\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.6999 - val_accuracy: 0.5102\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.6926 - val_accuracy: 0.5102\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0106 - accuracy: 0.9942 - val_loss: 0.6975 - val_accuracy: 0.5102\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0142 - accuracy: 0.9942 - val_loss: 0.7014 - val_accuracy: 0.5102\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0084 - accuracy: 0.9971 - val_loss: 0.6903 - val_accuracy: 0.5102\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.6981 - val_accuracy: 0.5102\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 1s 128ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.7109 - val_accuracy: 0.5102\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.6853 - val_accuracy: 0.5102\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0278 - accuracy: 0.9971 - val_loss: 0.7678 - val_accuracy: 0.5102\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0165 - accuracy: 0.9883 - val_loss: 0.7951 - val_accuracy: 0.5102\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.9420 - val_accuracy: 0.5102\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0341 - accuracy: 0.9854 - val_loss: 0.7904 - val_accuracy: 0.5102\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.9534 - val_accuracy: 0.5102\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.9747 - val_accuracy: 0.5102\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.8816 - val_accuracy: 0.5102\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.8977 - val_accuracy: 0.5102\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.9688 - val_accuracy: 0.5102\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 6.9004e-04 - accuracy: 1.0000 - val_loss: 1.0132 - val_accuracy: 0.5102\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.8936 - val_accuracy: 0.5102\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.8434 - val_accuracy: 0.5102\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 1.1944 - val_accuracy: 0.5102\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 2.5163 - val_accuracy: 0.5102\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0058 - accuracy: 0.9971 - val_loss: 1.8836 - val_accuracy: 0.5102\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.8311 - val_accuracy: 0.5102\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 2.3322 - val_accuracy: 0.5102\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0063 - accuracy: 0.9971 - val_loss: 3.3375 - val_accuracy: 0.5102\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0081 - accuracy: 0.9942 - val_loss: 8.0016 - val_accuracy: 0.5102\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 9.2844 - val_accuracy: 0.5102\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 6.7938 - val_accuracy: 0.5102\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0050 - accuracy: 0.9971 - val_loss: 7.5900 - val_accuracy: 0.5102\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 10.2727 - val_accuracy: 0.5102\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 4.4938e-04 - accuracy: 1.0000 - val_loss: 10.7628 - val_accuracy: 0.5102\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0105 - accuracy: 0.9971 - val_loss: 7.4260 - val_accuracy: 0.5102\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 5.4351 - val_accuracy: 0.5102\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 5.6974e-04 - accuracy: 1.0000 - val_loss: 4.9599 - val_accuracy: 0.5102\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 5.8418e-04 - accuracy: 1.0000 - val_loss: 4.8656 - val_accuracy: 0.5102\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 4.9381e-04 - accuracy: 1.0000 - val_loss: 4.8974 - val_accuracy: 0.5102\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0195 - accuracy: 0.9913 - val_loss: 6.6161 - val_accuracy: 0.5102\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0149 - accuracy: 0.9883 - val_loss: 1.3791 - val_accuracy: 0.5102\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0145 - accuracy: 0.9942 - val_loss: 3.3428 - val_accuracy: 0.4898\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0295 - accuracy: 0.9883 - val_loss: 16.2720 - val_accuracy: 0.4898\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0118 - accuracy: 0.9913 - val_loss: 11.3597 - val_accuracy: 0.4898\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0191 - accuracy: 0.9971 - val_loss: 3.9018 - val_accuracy: 0.4898\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0435 - accuracy: 0.9913 - val_loss: 24.2095 - val_accuracy: 0.5102\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0340 - accuracy: 0.9913 - val_loss: 100.8889 - val_accuracy: 0.5102\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0195 - accuracy: 0.9971 - val_loss: 149.6899 - val_accuracy: 0.5102\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0391 - accuracy: 0.9913 - val_loss: 125.6799 - val_accuracy: 0.5102\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0343 - accuracy: 0.9883 - val_loss: 66.6135 - val_accuracy: 0.5102\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0476 - accuracy: 0.9883 - val_loss: 19.2278 - val_accuracy: 0.5102\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0074 - accuracy: 0.9971 - val_loss: 83.4474 - val_accuracy: 0.5102\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 80.2484 - val_accuracy: 0.5102\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 69.1610 - val_accuracy: 0.5102\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 73.3957 - val_accuracy: 0.5102\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 5.3699e-04 - accuracy: 1.0000 - val_loss: 66.5826 - val_accuracy: 0.5102\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 58.8600 - val_accuracy: 0.5102\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 8.1646e-04 - accuracy: 1.0000 - val_loss: 50.4184 - val_accuracy: 0.5102\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 7.6504e-04 - accuracy: 1.0000 - val_loss: 43.8269 - val_accuracy: 0.5102\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 2.4331e-04 - accuracy: 1.0000 - val_loss: 38.2414 - val_accuracy: 0.5102\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 40.7839 - val_accuracy: 0.5102\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 36.2436 - val_accuracy: 0.5102\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 4.8206e-04 - accuracy: 1.0000 - val_loss: 31.0885 - val_accuracy: 0.5102\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 4.9456e-04 - accuracy: 1.0000 - val_loss: 28.4558 - val_accuracy: 0.5102\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 30.4836 - val_accuracy: 0.5102\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 2.9480e-04 - accuracy: 1.0000 - val_loss: 23.5471 - val_accuracy: 0.5102\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 15.3444 - val_accuracy: 0.5102\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 2.3821e-04 - accuracy: 1.0000 - val_loss: 11.4296 - val_accuracy: 0.5102\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 2.1561e-04 - accuracy: 1.0000 - val_loss: 10.0858 - val_accuracy: 0.5102\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 5.2457e-04 - accuracy: 1.0000 - val_loss: 6.7214 - val_accuracy: 0.5102\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 2.0649e-04 - accuracy: 1.0000 - val_loss: 4.9987 - val_accuracy: 0.5102\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 1.5826e-04 - accuracy: 1.0000 - val_loss: 4.4816 - val_accuracy: 0.5238\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 9.2777e-05 - accuracy: 1.0000 - val_loss: 4.3095 - val_accuracy: 0.5238\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 1.1849e-04 - accuracy: 1.0000 - val_loss: 4.2702 - val_accuracy: 0.5238\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 1.0780e-04 - accuracy: 1.0000 - val_loss: 4.1903 - val_accuracy: 0.5306\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 7.4001e-05 - accuracy: 1.0000 - val_loss: 4.0350 - val_accuracy: 0.5374\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 7.7395e-04 - accuracy: 1.0000 - val_loss: 6.7956 - val_accuracy: 0.5102\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 3.1318e-04 - accuracy: 1.0000 - val_loss: 8.1233 - val_accuracy: 0.5102\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 2.1651e-04 - accuracy: 1.0000 - val_loss: 7.9081 - val_accuracy: 0.5102\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 2.9465e-04 - accuracy: 1.0000 - val_loss: 7.2277 - val_accuracy: 0.5102\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 4.6351e-04 - accuracy: 1.0000 - val_loss: 6.2371 - val_accuracy: 0.5170\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 1.2836e-04 - accuracy: 1.0000 - val_loss: 5.2329 - val_accuracy: 0.5238\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 6.6322e-05 - accuracy: 1.0000 - val_loss: 4.1478 - val_accuracy: 0.5510\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 4.4555e-05 - accuracy: 1.0000 - val_loss: 3.5398 - val_accuracy: 0.5782\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 2.2431e-04 - accuracy: 1.0000 - val_loss: 2.9532 - val_accuracy: 0.5850\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 5.6838e-04 - accuracy: 1.0000 - val_loss: 2.5116 - val_accuracy: 0.6054\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 4.4893e-05 - accuracy: 1.0000 - val_loss: 1.7810 - val_accuracy: 0.6599\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 4.7246e-05 - accuracy: 1.0000 - val_loss: 1.3668 - val_accuracy: 0.7075\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 2.7758e-04 - accuracy: 1.0000 - val_loss: 1.0736 - val_accuracy: 0.7483\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 7.2193e-04 - accuracy: 1.0000 - val_loss: 2.6331 - val_accuracy: 0.6395\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 4.0083e-05 - accuracy: 1.0000 - val_loss: 2.8986 - val_accuracy: 0.6122\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 1.1551e-04 - accuracy: 1.0000 - val_loss: 2.5137 - val_accuracy: 0.6327\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 1.0045e-04 - accuracy: 1.0000 - val_loss: 2.0715 - val_accuracy: 0.6531\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 7.5899e-05 - accuracy: 1.0000 - val_loss: 1.8032 - val_accuracy: 0.6803\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 9.3004e-05 - accuracy: 1.0000 - val_loss: 1.2694 - val_accuracy: 0.7279\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 1.4562e-04 - accuracy: 1.0000 - val_loss: 1.1116 - val_accuracy: 0.7415\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 1.2219e-04 - accuracy: 1.0000 - val_loss: 1.1029 - val_accuracy: 0.7347\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 4.4319e-05 - accuracy: 1.0000 - val_loss: 1.0904 - val_accuracy: 0.7483\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 5.7820e-05 - accuracy: 1.0000 - val_loss: 1.0319 - val_accuracy: 0.7551\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 6.0920e-05 - accuracy: 1.0000 - val_loss: 0.7016 - val_accuracy: 0.8299\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 1.8543e-04 - accuracy: 1.0000 - val_loss: 1.4679 - val_accuracy: 0.7279\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 9.0141e-05 - accuracy: 1.0000 - val_loss: 1.6091 - val_accuracy: 0.7347\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 5.4412e-05 - accuracy: 1.0000 - val_loss: 1.0714 - val_accuracy: 0.7687\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 3.5911e-05 - accuracy: 1.0000 - val_loss: 0.6225 - val_accuracy: 0.8707\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 7.9202e-05 - accuracy: 1.0000 - val_loss: 0.4195 - val_accuracy: 0.8844\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 8.9409e-05 - accuracy: 1.0000 - val_loss: 1.5706 - val_accuracy: 0.7483\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 7.4876e-04 - accuracy: 1.0000 - val_loss: 0.6742 - val_accuracy: 0.7143\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 2.9016e-05 - accuracy: 1.0000 - val_loss: 0.6886 - val_accuracy: 0.7143\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6453 - val_accuracy: 0.7415\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 1.6029e-04 - accuracy: 1.0000 - val_loss: 2.8710 - val_accuracy: 0.5102\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 2.6428e-04 - accuracy: 1.0000 - val_loss: 2.9603 - val_accuracy: 0.5102\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0041 - accuracy: 0.9971 - val_loss: 8.0327 - val_accuracy: 0.4898\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0401 - accuracy: 0.9913 - val_loss: 9.0875 - val_accuracy: 0.4898\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0264 - accuracy: 0.9854 - val_loss: 9.0699 - val_accuracy: 0.5102\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 8.8313 - val_accuracy: 0.4898\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 7.8892 - val_accuracy: 0.4898\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 10.3601 - val_accuracy: 0.4898\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 4.6837e-04 - accuracy: 1.0000 - val_loss: 7.2443 - val_accuracy: 0.4898\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 4.3861e-04 - accuracy: 1.0000 - val_loss: 6.2790 - val_accuracy: 0.4898\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 4.1047 - val_accuracy: 0.4898\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 3.1342e-04 - accuracy: 1.0000 - val_loss: 2.0852 - val_accuracy: 0.5034\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 4.5538 - val_accuracy: 0.5102\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0040 - accuracy: 0.9971 - val_loss: 52.7460 - val_accuracy: 0.5102\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0109 - accuracy: 0.9942 - val_loss: 15.2471 - val_accuracy: 0.5102\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0112 - accuracy: 0.9942 - val_loss: 0.5395 - val_accuracy: 0.8163\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0084 - accuracy: 0.9971 - val_loss: 13.8579 - val_accuracy: 0.5102\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 2.2708e-04 - accuracy: 1.0000 - val_loss: 33.8921 - val_accuracy: 0.5102\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 1.9648e-04 - accuracy: 1.0000 - val_loss: 33.0599 - val_accuracy: 0.5102\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 1.6874e-04 - accuracy: 1.0000 - val_loss: 29.8116 - val_accuracy: 0.5102\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 4.7929e-04 - accuracy: 1.0000 - val_loss: 28.7678 - val_accuracy: 0.5102\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 7.2317e-04 - accuracy: 1.0000 - val_loss: 25.6287 - val_accuracy: 0.5102\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 5.8589e-04 - accuracy: 1.0000 - val_loss: 21.7891 - val_accuracy: 0.5102\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 1.7485e-04 - accuracy: 1.0000 - val_loss: 18.3945 - val_accuracy: 0.5102\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 2.4516e-04 - accuracy: 1.0000 - val_loss: 14.3721 - val_accuracy: 0.5102\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 5.9478e-04 - accuracy: 1.0000 - val_loss: 9.0827 - val_accuracy: 0.5170\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 2.8270e-05 - accuracy: 1.0000 - val_loss: 4.5891 - val_accuracy: 0.5442\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 4.1691e-05 - accuracy: 1.0000 - val_loss: 2.5680 - val_accuracy: 0.6122\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 9.7247e-05 - accuracy: 1.0000 - val_loss: 1.5092 - val_accuracy: 0.6939\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 4.6485e-05 - accuracy: 1.0000 - val_loss: 0.8328 - val_accuracy: 0.7755\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 7.2991e-05 - accuracy: 1.0000 - val_loss: 0.6769 - val_accuracy: 0.8095\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 7.6576e-04 - accuracy: 1.0000 - val_loss: 4.8778 - val_accuracy: 0.5714\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 2.4898e-05 - accuracy: 1.0000 - val_loss: 12.3618 - val_accuracy: 0.5102\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 4.1772e-05 - accuracy: 1.0000 - val_loss: 14.5710 - val_accuracy: 0.5102\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 5.6105e-05 - accuracy: 1.0000 - val_loss: 13.0647 - val_accuracy: 0.5102\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 1.0830e-04 - accuracy: 1.0000 - val_loss: 11.4696 - val_accuracy: 0.5102\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 1s 138ms/step - loss: 9.1370e-04 - accuracy: 1.0000 - val_loss: 11.0505 - val_accuracy: 0.5102\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 1s 138ms/step - loss: 9.9517e-04 - accuracy: 1.0000 - val_loss: 19.3045 - val_accuracy: 0.5102\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 7.5524e-05 - accuracy: 1.0000 - val_loss: 20.5578 - val_accuracy: 0.5102\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 9.7668e-05 - accuracy: 1.0000 - val_loss: 19.6144 - val_accuracy: 0.5102\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 7.3391e-05 - accuracy: 1.0000 - val_loss: 17.6477 - val_accuracy: 0.5102\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 1.0986e-04 - accuracy: 1.0000 - val_loss: 15.2356 - val_accuracy: 0.5102\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 1.2981e-04 - accuracy: 1.0000 - val_loss: 12.7953 - val_accuracy: 0.5102\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 5.8457e-05 - accuracy: 1.0000 - val_loss: 10.4949 - val_accuracy: 0.5170\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 5.8186e-05 - accuracy: 1.0000 - val_loss: 8.4525 - val_accuracy: 0.5238\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 1.5581e-05 - accuracy: 1.0000 - val_loss: 6.5104 - val_accuracy: 0.5374\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 1.1331e-04 - accuracy: 1.0000 - val_loss: 4.7744 - val_accuracy: 0.5646\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 7.9455e-05 - accuracy: 1.0000 - val_loss: 2.9835 - val_accuracy: 0.6599\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 5.3405e-05 - accuracy: 1.0000 - val_loss: 2.1476 - val_accuracy: 0.7143\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 9.6478e-05 - accuracy: 1.0000 - val_loss: 1.3563 - val_accuracy: 0.7619\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 1.1523e-04 - accuracy: 1.0000 - val_loss: 1.0598 - val_accuracy: 0.7891\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 1.3973e-05 - accuracy: 1.0000 - val_loss: 0.7991 - val_accuracy: 0.8299\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 1.1797e-04 - accuracy: 1.0000 - val_loss: 0.7462 - val_accuracy: 0.8299\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 2.6391e-05 - accuracy: 1.0000 - val_loss: 0.6423 - val_accuracy: 0.8707\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 3.3160e-04 - accuracy: 1.0000 - val_loss: 0.3002 - val_accuracy: 0.9388\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 3.6071e-05 - accuracy: 1.0000 - val_loss: 0.1272 - val_accuracy: 0.9728\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 6.4906e-04 - accuracy: 1.0000 - val_loss: 0.1462 - val_accuracy: 0.9592\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 1.0351e-04 - accuracy: 1.0000 - val_loss: 0.2183 - val_accuracy: 0.9116\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 6.4607e-05 - accuracy: 1.0000 - val_loss: 0.1132 - val_accuracy: 0.9524\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 2s 136ms/step - loss: 1.7904e-04 - accuracy: 1.0000 - val_loss: 0.0913 - val_accuracy: 0.9796\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 2.9889e-05 - accuracy: 1.0000 - val_loss: 0.1062 - val_accuracy: 0.9728\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 1.8301e-05 - accuracy: 1.0000 - val_loss: 0.1106 - val_accuracy: 0.9728\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 2.7163e-05 - accuracy: 1.0000 - val_loss: 0.1181 - val_accuracy: 0.9728\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 3.2455e-05 - accuracy: 1.0000 - val_loss: 0.1254 - val_accuracy: 0.9660\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 2.3269e-05 - accuracy: 1.0000 - val_loss: 0.1346 - val_accuracy: 0.9660\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 2.4930e-04 - accuracy: 1.0000 - val_loss: 0.1541 - val_accuracy: 0.9660\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 7.2046e-05 - accuracy: 1.0000 - val_loss: 0.1282 - val_accuracy: 0.9592\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 3.4785e-04 - accuracy: 1.0000 - val_loss: 0.8236 - val_accuracy: 0.8163\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 4.6919e-05 - accuracy: 1.0000 - val_loss: 1.0570 - val_accuracy: 0.7687\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 6.2201e-05 - accuracy: 1.0000 - val_loss: 1.0895 - val_accuracy: 0.7823\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 5.8219e-05 - accuracy: 1.0000 - val_loss: 1.2536 - val_accuracy: 0.7687\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 2.2411e-04 - accuracy: 1.0000 - val_loss: 1.1945 - val_accuracy: 0.7823\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 1.7580e-05 - accuracy: 1.0000 - val_loss: 0.8229 - val_accuracy: 0.8367\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 1.4520e-05 - accuracy: 1.0000 - val_loss: 0.7009 - val_accuracy: 0.8639\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 6.8117e-05 - accuracy: 1.0000 - val_loss: 0.5095 - val_accuracy: 0.9116\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 1.0031e-05 - accuracy: 1.0000 - val_loss: 0.3849 - val_accuracy: 0.9184\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 3.9792e-05 - accuracy: 1.0000 - val_loss: 0.3913 - val_accuracy: 0.9184\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 7.8462e-05 - accuracy: 1.0000 - val_loss: 0.3768 - val_accuracy: 0.9252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEE2dRY62O66"
      },
      "source": [
        "pred_test= model.predict(X_test)\n",
        "Rows, Cols = pred_test.shape\n",
        "Prediction =[]\n",
        "for i in range(Rows):\n",
        "  if(pred_test[0,0] > pred_test[0,1]):\n",
        "    Prediction.append(0)\n",
        "  else:\n",
        "    Prediction.append(1)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIPQG40CyIXq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb8e06f0-716f-4fe9-c900-fa7ac141c9b5"
      },
      "source": [
        "# pred_test= model.predict_classes(X_test)\n",
        "pred_test = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict   0   1\n",
            "Actual         \n",
            "0        62  10\n",
            "1         1  74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ3YoP6I0axi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d159a2a-25ec-4b5b-8344-17f5ec844b18"
      },
      "source": [
        "print(METRICS)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.86      0.92        72\n",
            "           1       0.88      0.99      0.93        75\n",
            "\n",
            "    accuracy                           0.93       147\n",
            "   macro avg       0.93      0.92      0.92       147\n",
            "weighted avg       0.93      0.93      0.92       147\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iFNNrlWV9tH"
      },
      "source": [
        "#pred_test"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QISvYcJBgWbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2230e695-0bf4-410c-b470-ca984797297b"
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[2] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  prediction_02 = model.predict(result)\n",
        "  Rows, Cols = prediction_02.shape\n",
        "  Prediction =[]\n",
        "  for i in range(Rows):\n",
        "    if(prediction_02[0,0] > prediction_02[0,1]):\n",
        "      Prediction.append(0)\n",
        "    else:\n",
        "      Prediction.append(1)\n",
        "  loc_grao =[];k=0\n",
        "  for i in Prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0   115.0   96.093842   95.426918  ...   84.199539   83.373306   82.275833\n",
            "1   188.0   80.697586   80.514702  ...   59.295155   57.668179   54.706650\n",
            "2   183.0  146.967484  154.082062  ...  146.333359  147.932983  148.330978\n",
            "3   189.0   87.762688   93.455421  ...  119.716049  124.367638  128.422516\n",
            "4   171.0  147.521255  160.530609  ...   95.571266   83.656960   76.451935\n",
            "5   178.0   97.999374   84.530113  ...   61.699032   65.814552   88.952782\n",
            "6   140.0  109.479996  108.320000  ...  200.800003  199.839996  203.800003\n",
            "7   135.0   61.692894   59.511604  ...    0.000000    0.000000    0.000000\n",
            "8   152.0   91.831718   91.170357  ...  108.120499  104.918976  103.397499\n",
            "9   117.0  147.934982  148.922485  ...  114.103661  112.123238  107.578415\n",
            "10  150.0  117.228798  103.165497  ...  113.361069  107.792892  106.836800\n",
            "11  190.0  138.991119  144.054413  ...  156.194229  159.975159  159.149918\n",
            "12  159.0   73.023491   72.754997  ...  139.204453  142.492859  143.192825\n",
            "13  152.0  115.117729  114.770081  ...  110.574783  111.336563  111.695969\n",
            "14  111.0  146.215576  146.781250  ...  136.919159  134.488678  134.438522\n",
            "15  181.0  132.242645  129.024918  ...  131.017334  141.290283  144.727371\n",
            "16  152.0  166.075470  168.887787  ...  126.250000  130.105942  131.421051\n",
            "17  106.0  114.760773  114.606270  ...  187.427551  189.653275  190.882874\n",
            "18  121.0   80.590942   81.511101  ...   85.813599   88.415611   89.497231\n",
            "19  144.0   85.617287   92.290909  ...    8.853395    8.459877    8.690586\n",
            "20  195.0    1.635477    0.876292  ...   19.039474    1.699724    0.161999\n",
            "21  142.0  124.585999  126.516960  ...  121.087891  127.620316  127.627472\n",
            "22  119.0   77.183395   80.083046  ...  130.899658  129.823532  128.404831\n",
            "23  133.0  140.767303   79.204987  ...   10.509695    9.844875   10.044321\n",
            "24  118.0  177.196198  201.896576  ...  131.501877  130.648376  128.042801\n",
            "25  131.0  124.135773  123.388367  ...   10.777460   10.100752    9.670357\n",
            "26  134.0  104.755859   97.354195  ...   67.675652   68.179771   67.545784\n",
            "27  118.0  156.766449  159.950012  ...  130.158844  111.793167  102.175812\n",
            "28  145.0    0.000000    0.000000  ...   80.038765   79.932365   77.004898\n",
            "29  115.0  129.506683  130.906006  ...   80.874100   50.485062    4.346994\n",
            "30  181.0  127.682350  145.853333  ...  100.417419   95.169197   91.750313\n",
            "31  175.0  107.559990  110.452805  ...  124.080002  124.972786  122.835205\n",
            "32  124.0  112.499474  114.345474  ...   86.701347   91.634750   96.053062\n",
            "33  137.0  119.034843  108.175339  ...  200.565231  220.561707  225.160583\n",
            "34  153.0  133.053879  130.816879  ...  124.196342  123.988342  123.393608\n",
            "35  126.0   95.222229  113.074074  ...   95.901237   80.962967   71.456787\n",
            "36  103.0   26.671883   38.339710  ...   85.945511   85.241966   86.971809\n",
            "37  135.0   60.823647   59.164715  ...  127.263596  125.196869  122.830994\n",
            "38  159.0  131.142517  134.069550  ...  112.373749  142.390015  167.116409\n",
            "39  119.0   94.325256  101.702423  ...   89.051903   47.159172    6.619377\n",
            "40  122.0   79.065575   80.496368  ...  122.848412   65.592575   42.556835\n",
            "41  148.0   74.582909   73.348434  ...   99.952522   99.753113  100.432442\n",
            "42  185.0  139.782898  141.200912  ...  124.224136  126.384018  130.045776\n",
            "43  102.0   83.012314   81.668976  ...    2.491350    1.091503    0.547866\n",
            "44  130.0  143.104614  144.794098  ...  111.315277  113.172310  112.509590\n",
            "45  122.0  151.111237  144.971497  ...  143.441803  141.482391  138.429443\n",
            "46  188.0  127.387497  128.353104  ...    7.046174    7.394748    6.751471\n",
            "47  115.0   60.400829   60.541927  ...  117.400597  116.517273  116.652023\n",
            "48  147.0   77.843536   78.682549  ...  119.959190  122.825401  129.786850\n",
            "49  190.0  104.468483  115.407646  ...  111.734840  129.639648  134.338165\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRbWgmX_LFH",
        "outputId": "9d060cbc-46ad-4111-de9c-0c86f8ac61b0"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "from GetBetterSegm import GetBetter"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marquesgabi_paper_fev_2021'...\n",
            "remote: Enumerating objects: 717, done.\u001b[K\n",
            "remote: Counting objects: 100% (478/478), done.\u001b[K\n",
            "remote: Compressing objects: 100% (476/476), done.\u001b[K\n",
            "remote: Total 717 (delta 304), reused 0 (delta 0), pack-reused 239\u001b[K\n",
            "Receiving objects: 100% (717/717), 5.80 MiB | 12.72 MiB/s, done.\n",
            "Resolving deltas: 100% (441/441), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAG_I6FwCvFr",
        "outputId": "ab2760f6-f826-47a6-a0e1-ff9a2dd164bf"
      },
      "source": [
        "#!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "\n",
        "%cd Doutorado\n",
        "\n",
        "PSD_imageJ = 'Amostra7.csv' \n",
        "PSD_new = pd.read_csv(PSD_imageJ,sep=';')\n",
        "#encoding='utf8'\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Doutorado'...\n",
            "remote: Enumerating objects: 464, done.\u001b[K\n",
            "remote: Counting objects: 100% (214/214), done.\u001b[K\n",
            "remote: Compressing objects: 100% (210/210), done.\u001b[K\n",
            "remote: Total 464 (delta 102), reused 4 (delta 3), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (464/464), 166.12 MiB | 26.68 MiB/s, done.\n",
            "Resolving deltas: 100% (225/225), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/Doutorado\n",
            "   Unnamed: 0   Area\n",
            "0           1  1.387\n",
            "1           2  1.626\n",
            "2           3  1.336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tEPjIBnv_xM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80e862fc-ea49-47af-f8fe-f722acdaf74d"
      },
      "source": [
        "PSD_new.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfagXc-Mv3oa"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PekBHQOT_6CP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "2230c164-091c-4fa8-fbf9-413995b4dbb7"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>115.0</td>\n",
              "      <td>96.093842</td>\n",
              "      <td>95.426918</td>\n",
              "      <td>93.942749</td>\n",
              "      <td>92.670609</td>\n",
              "      <td>92.734283</td>\n",
              "      <td>92.247169</td>\n",
              "      <td>88.010353</td>\n",
              "      <td>87.639687</td>\n",
              "      <td>84.642265</td>\n",
              "      <td>83.812317</td>\n",
              "      <td>82.680595</td>\n",
              "      <td>81.216553</td>\n",
              "      <td>78.838333</td>\n",
              "      <td>78.270317</td>\n",
              "      <td>76.605972</td>\n",
              "      <td>74.957733</td>\n",
              "      <td>80.938072</td>\n",
              "      <td>85.006729</td>\n",
              "      <td>91.435608</td>\n",
              "      <td>107.377838</td>\n",
              "      <td>125.787361</td>\n",
              "      <td>134.620178</td>\n",
              "      <td>137.569748</td>\n",
              "      <td>135.910904</td>\n",
              "      <td>133.979355</td>\n",
              "      <td>131.776108</td>\n",
              "      <td>127.526413</td>\n",
              "      <td>125.368538</td>\n",
              "      <td>96.820641</td>\n",
              "      <td>98.590012</td>\n",
              "      <td>99.057007</td>\n",
              "      <td>96.816711</td>\n",
              "      <td>93.437805</td>\n",
              "      <td>91.867294</td>\n",
              "      <td>85.782005</td>\n",
              "      <td>81.151230</td>\n",
              "      <td>81.570961</td>\n",
              "      <td>81.176254</td>\n",
              "      <td>80.003853</td>\n",
              "      <td>...</td>\n",
              "      <td>116.835602</td>\n",
              "      <td>115.075302</td>\n",
              "      <td>110.569756</td>\n",
              "      <td>96.195755</td>\n",
              "      <td>89.594330</td>\n",
              "      <td>90.364227</td>\n",
              "      <td>90.430466</td>\n",
              "      <td>89.381683</td>\n",
              "      <td>87.519463</td>\n",
              "      <td>86.834099</td>\n",
              "      <td>86.316055</td>\n",
              "      <td>85.152359</td>\n",
              "      <td>155.566711</td>\n",
              "      <td>153.221008</td>\n",
              "      <td>146.309402</td>\n",
              "      <td>116.527176</td>\n",
              "      <td>103.550087</td>\n",
              "      <td>106.486649</td>\n",
              "      <td>108.648918</td>\n",
              "      <td>109.465248</td>\n",
              "      <td>109.687622</td>\n",
              "      <td>109.142761</td>\n",
              "      <td>109.387825</td>\n",
              "      <td>112.089676</td>\n",
              "      <td>113.443626</td>\n",
              "      <td>113.870544</td>\n",
              "      <td>115.424042</td>\n",
              "      <td>113.249374</td>\n",
              "      <td>108.850883</td>\n",
              "      <td>103.528542</td>\n",
              "      <td>97.900261</td>\n",
              "      <td>90.278633</td>\n",
              "      <td>88.207108</td>\n",
              "      <td>88.478256</td>\n",
              "      <td>88.980789</td>\n",
              "      <td>88.248985</td>\n",
              "      <td>86.746536</td>\n",
              "      <td>84.199539</td>\n",
              "      <td>83.373306</td>\n",
              "      <td>82.275833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>188.0</td>\n",
              "      <td>80.697586</td>\n",
              "      <td>80.514702</td>\n",
              "      <td>79.857399</td>\n",
              "      <td>77.430054</td>\n",
              "      <td>113.011765</td>\n",
              "      <td>138.821625</td>\n",
              "      <td>135.437286</td>\n",
              "      <td>123.810768</td>\n",
              "      <td>116.729736</td>\n",
              "      <td>114.435486</td>\n",
              "      <td>113.796738</td>\n",
              "      <td>114.867813</td>\n",
              "      <td>119.852417</td>\n",
              "      <td>127.047974</td>\n",
              "      <td>133.988678</td>\n",
              "      <td>140.865555</td>\n",
              "      <td>145.954742</td>\n",
              "      <td>154.988663</td>\n",
              "      <td>162.173370</td>\n",
              "      <td>168.009491</td>\n",
              "      <td>166.521484</td>\n",
              "      <td>156.706650</td>\n",
              "      <td>85.479858</td>\n",
              "      <td>78.024437</td>\n",
              "      <td>89.156181</td>\n",
              "      <td>97.187408</td>\n",
              "      <td>98.731094</td>\n",
              "      <td>99.920776</td>\n",
              "      <td>81.214111</td>\n",
              "      <td>82.429604</td>\n",
              "      <td>83.415115</td>\n",
              "      <td>79.687187</td>\n",
              "      <td>104.478951</td>\n",
              "      <td>132.251236</td>\n",
              "      <td>128.864639</td>\n",
              "      <td>124.540512</td>\n",
              "      <td>121.434586</td>\n",
              "      <td>122.021729</td>\n",
              "      <td>122.947929</td>\n",
              "      <td>...</td>\n",
              "      <td>94.139885</td>\n",
              "      <td>70.661377</td>\n",
              "      <td>47.642822</td>\n",
              "      <td>45.698505</td>\n",
              "      <td>48.630146</td>\n",
              "      <td>50.650517</td>\n",
              "      <td>53.714798</td>\n",
              "      <td>54.915337</td>\n",
              "      <td>58.211864</td>\n",
              "      <td>58.679935</td>\n",
              "      <td>55.635582</td>\n",
              "      <td>54.906292</td>\n",
              "      <td>156.756882</td>\n",
              "      <td>158.076050</td>\n",
              "      <td>160.020355</td>\n",
              "      <td>162.378906</td>\n",
              "      <td>166.004517</td>\n",
              "      <td>168.139420</td>\n",
              "      <td>171.636032</td>\n",
              "      <td>173.105927</td>\n",
              "      <td>192.468994</td>\n",
              "      <td>163.751907</td>\n",
              "      <td>98.518333</td>\n",
              "      <td>102.583061</td>\n",
              "      <td>101.322311</td>\n",
              "      <td>103.980537</td>\n",
              "      <td>105.161621</td>\n",
              "      <td>109.290176</td>\n",
              "      <td>110.533722</td>\n",
              "      <td>101.990494</td>\n",
              "      <td>71.367134</td>\n",
              "      <td>48.929379</td>\n",
              "      <td>49.115437</td>\n",
              "      <td>51.761883</td>\n",
              "      <td>52.888184</td>\n",
              "      <td>54.258484</td>\n",
              "      <td>56.354462</td>\n",
              "      <td>59.295155</td>\n",
              "      <td>57.668179</td>\n",
              "      <td>54.706650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>183.0</td>\n",
              "      <td>146.967484</td>\n",
              "      <td>154.082062</td>\n",
              "      <td>160.773941</td>\n",
              "      <td>165.516251</td>\n",
              "      <td>172.210342</td>\n",
              "      <td>172.858780</td>\n",
              "      <td>171.472748</td>\n",
              "      <td>169.219315</td>\n",
              "      <td>149.140762</td>\n",
              "      <td>99.472809</td>\n",
              "      <td>87.141777</td>\n",
              "      <td>87.676582</td>\n",
              "      <td>87.781242</td>\n",
              "      <td>93.395805</td>\n",
              "      <td>111.680710</td>\n",
              "      <td>124.939148</td>\n",
              "      <td>134.176178</td>\n",
              "      <td>141.091812</td>\n",
              "      <td>140.148865</td>\n",
              "      <td>140.146698</td>\n",
              "      <td>144.232605</td>\n",
              "      <td>152.467682</td>\n",
              "      <td>144.888504</td>\n",
              "      <td>139.561905</td>\n",
              "      <td>140.140442</td>\n",
              "      <td>137.153366</td>\n",
              "      <td>132.851334</td>\n",
              "      <td>132.558456</td>\n",
              "      <td>142.057678</td>\n",
              "      <td>149.614441</td>\n",
              "      <td>156.474976</td>\n",
              "      <td>164.608749</td>\n",
              "      <td>175.116730</td>\n",
              "      <td>175.083405</td>\n",
              "      <td>175.911179</td>\n",
              "      <td>176.755676</td>\n",
              "      <td>168.775162</td>\n",
              "      <td>138.567795</td>\n",
              "      <td>85.625755</td>\n",
              "      <td>...</td>\n",
              "      <td>176.679688</td>\n",
              "      <td>178.081711</td>\n",
              "      <td>173.441452</td>\n",
              "      <td>164.809311</td>\n",
              "      <td>147.254684</td>\n",
              "      <td>135.623642</td>\n",
              "      <td>134.531906</td>\n",
              "      <td>138.001770</td>\n",
              "      <td>141.251495</td>\n",
              "      <td>142.189621</td>\n",
              "      <td>139.907837</td>\n",
              "      <td>142.203751</td>\n",
              "      <td>121.489487</td>\n",
              "      <td>156.603271</td>\n",
              "      <td>171.880600</td>\n",
              "      <td>207.452682</td>\n",
              "      <td>233.219055</td>\n",
              "      <td>225.079956</td>\n",
              "      <td>206.567001</td>\n",
              "      <td>199.207565</td>\n",
              "      <td>176.551147</td>\n",
              "      <td>173.049042</td>\n",
              "      <td>175.521194</td>\n",
              "      <td>175.880173</td>\n",
              "      <td>178.310989</td>\n",
              "      <td>180.268417</td>\n",
              "      <td>179.156174</td>\n",
              "      <td>176.357880</td>\n",
              "      <td>176.191467</td>\n",
              "      <td>180.335037</td>\n",
              "      <td>183.465576</td>\n",
              "      <td>188.133698</td>\n",
              "      <td>187.523727</td>\n",
              "      <td>183.782440</td>\n",
              "      <td>175.415375</td>\n",
              "      <td>156.100052</td>\n",
              "      <td>145.020477</td>\n",
              "      <td>146.333359</td>\n",
              "      <td>147.932983</td>\n",
              "      <td>148.330978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>189.0</td>\n",
              "      <td>87.762688</td>\n",
              "      <td>93.455421</td>\n",
              "      <td>103.034294</td>\n",
              "      <td>128.411530</td>\n",
              "      <td>76.382713</td>\n",
              "      <td>40.854599</td>\n",
              "      <td>83.451309</td>\n",
              "      <td>103.031555</td>\n",
              "      <td>102.142670</td>\n",
              "      <td>94.153633</td>\n",
              "      <td>85.847740</td>\n",
              "      <td>82.887512</td>\n",
              "      <td>82.807953</td>\n",
              "      <td>83.537720</td>\n",
              "      <td>82.310013</td>\n",
              "      <td>85.012344</td>\n",
              "      <td>88.063103</td>\n",
              "      <td>95.410149</td>\n",
              "      <td>92.842262</td>\n",
              "      <td>89.045273</td>\n",
              "      <td>94.570656</td>\n",
              "      <td>110.820305</td>\n",
              "      <td>100.610420</td>\n",
              "      <td>95.913582</td>\n",
              "      <td>94.650208</td>\n",
              "      <td>91.912216</td>\n",
              "      <td>83.156372</td>\n",
              "      <td>76.854599</td>\n",
              "      <td>88.401924</td>\n",
              "      <td>93.869690</td>\n",
              "      <td>103.711929</td>\n",
              "      <td>87.909470</td>\n",
              "      <td>44.209877</td>\n",
              "      <td>45.647465</td>\n",
              "      <td>90.452667</td>\n",
              "      <td>100.518524</td>\n",
              "      <td>99.615913</td>\n",
              "      <td>96.625519</td>\n",
              "      <td>93.175575</td>\n",
              "      <td>...</td>\n",
              "      <td>108.216736</td>\n",
              "      <td>120.666672</td>\n",
              "      <td>121.021942</td>\n",
              "      <td>117.540482</td>\n",
              "      <td>114.596710</td>\n",
              "      <td>112.676254</td>\n",
              "      <td>113.086411</td>\n",
              "      <td>116.787384</td>\n",
              "      <td>120.599457</td>\n",
              "      <td>124.541847</td>\n",
              "      <td>125.786003</td>\n",
              "      <td>121.478745</td>\n",
              "      <td>161.936890</td>\n",
              "      <td>162.342941</td>\n",
              "      <td>165.042542</td>\n",
              "      <td>171.736633</td>\n",
              "      <td>190.220856</td>\n",
              "      <td>204.102875</td>\n",
              "      <td>189.211243</td>\n",
              "      <td>70.155006</td>\n",
              "      <td>70.388206</td>\n",
              "      <td>72.201645</td>\n",
              "      <td>69.310013</td>\n",
              "      <td>69.170090</td>\n",
              "      <td>66.218109</td>\n",
              "      <td>84.840881</td>\n",
              "      <td>110.314133</td>\n",
              "      <td>119.861450</td>\n",
              "      <td>126.746239</td>\n",
              "      <td>130.757202</td>\n",
              "      <td>129.935532</td>\n",
              "      <td>129.839508</td>\n",
              "      <td>124.840881</td>\n",
              "      <td>114.015091</td>\n",
              "      <td>111.486961</td>\n",
              "      <td>113.530869</td>\n",
              "      <td>116.432106</td>\n",
              "      <td>119.716049</td>\n",
              "      <td>124.367638</td>\n",
              "      <td>128.422516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>171.0</td>\n",
              "      <td>147.521255</td>\n",
              "      <td>160.530609</td>\n",
              "      <td>161.353256</td>\n",
              "      <td>115.071053</td>\n",
              "      <td>108.602409</td>\n",
              "      <td>106.392220</td>\n",
              "      <td>102.871582</td>\n",
              "      <td>100.127831</td>\n",
              "      <td>100.278648</td>\n",
              "      <td>99.846252</td>\n",
              "      <td>100.633217</td>\n",
              "      <td>108.223518</td>\n",
              "      <td>118.712196</td>\n",
              "      <td>130.721634</td>\n",
              "      <td>142.073944</td>\n",
              "      <td>133.603714</td>\n",
              "      <td>123.162689</td>\n",
              "      <td>144.683899</td>\n",
              "      <td>148.086182</td>\n",
              "      <td>147.325256</td>\n",
              "      <td>145.272568</td>\n",
              "      <td>144.795502</td>\n",
              "      <td>145.248459</td>\n",
              "      <td>144.821533</td>\n",
              "      <td>144.267517</td>\n",
              "      <td>146.731430</td>\n",
              "      <td>153.315384</td>\n",
              "      <td>159.380508</td>\n",
              "      <td>138.079773</td>\n",
              "      <td>152.058319</td>\n",
              "      <td>157.057999</td>\n",
              "      <td>125.200615</td>\n",
              "      <td>118.911491</td>\n",
              "      <td>106.017670</td>\n",
              "      <td>102.391357</td>\n",
              "      <td>103.026016</td>\n",
              "      <td>104.020035</td>\n",
              "      <td>109.531342</td>\n",
              "      <td>120.459770</td>\n",
              "      <td>...</td>\n",
              "      <td>138.437744</td>\n",
              "      <td>132.125717</td>\n",
              "      <td>107.134804</td>\n",
              "      <td>105.193565</td>\n",
              "      <td>107.781982</td>\n",
              "      <td>109.367294</td>\n",
              "      <td>112.882187</td>\n",
              "      <td>114.367432</td>\n",
              "      <td>110.603706</td>\n",
              "      <td>97.595749</td>\n",
              "      <td>83.967003</td>\n",
              "      <td>79.534248</td>\n",
              "      <td>121.891457</td>\n",
              "      <td>123.277321</td>\n",
              "      <td>123.912453</td>\n",
              "      <td>124.880394</td>\n",
              "      <td>126.611198</td>\n",
              "      <td>127.404388</td>\n",
              "      <td>131.443344</td>\n",
              "      <td>137.352875</td>\n",
              "      <td>137.611877</td>\n",
              "      <td>139.194199</td>\n",
              "      <td>140.929489</td>\n",
              "      <td>138.638870</td>\n",
              "      <td>126.726212</td>\n",
              "      <td>118.727913</td>\n",
              "      <td>116.475189</td>\n",
              "      <td>121.653748</td>\n",
              "      <td>127.836052</td>\n",
              "      <td>120.596832</td>\n",
              "      <td>109.573845</td>\n",
              "      <td>110.584892</td>\n",
              "      <td>112.195168</td>\n",
              "      <td>114.130402</td>\n",
              "      <td>115.314590</td>\n",
              "      <td>115.335037</td>\n",
              "      <td>109.161133</td>\n",
              "      <td>95.571266</td>\n",
              "      <td>83.656960</td>\n",
              "      <td>76.451935</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Width           0           1  ...         781         782         783\n",
              "0  115.0   96.093842   95.426918  ...   84.199539   83.373306   82.275833\n",
              "1  188.0   80.697586   80.514702  ...   59.295155   57.668179   54.706650\n",
              "2  183.0  146.967484  154.082062  ...  146.333359  147.932983  148.330978\n",
              "3  189.0   87.762688   93.455421  ...  119.716049  124.367638  128.422516\n",
              "4  171.0  147.521255  160.530609  ...   95.571266   83.656960   76.451935\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q09DRGPtM75"
      },
      "source": [
        "#lost_value = float(PSD_new.columns[1])\n",
        "\n",
        "# Area = np.array(PSD_new.iloc[:,1])\n",
        "Area = PSD_new['Area'].values\n",
        "# Area = np.concatenate( (Area, [lost_value] ) )\n",
        "# Area = np.concatenate( (Area, [lost_value] ) )\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aUb2_-jsY1Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84d8c6af-a352-4bf3-84a5-13bb6a458e02"
      },
      "source": [
        "PSD_new.iloc[:,1].values"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.387, 1.626, 1.336, 0.64 , 2.211, 1.12 , 0.974, 1.237, 1.29 ,\n",
              "       3.755, 2.778, 1.256, 1.386, 1.302, 1.071, 1.497, 1.518, 1.244,\n",
              "       1.532, 1.325, 1.519, 1.895, 1.22 , 1.241, 1.301, 1.429, 0.667,\n",
              "       2.157, 1.052, 2.082, 1.517, 1.281, 0.784, 1.067, 2.764, 1.215,\n",
              "       0.943, 2.182, 1.486, 1.569, 2.667, 0.709, 1.006, 1.6  , 1.408,\n",
              "       3.16 , 2.465, 2.284, 1.273, 1.256, 3.021, 1.701, 1.955, 5.248,\n",
              "       1.627, 1.367, 1.592, 2.718, 1.658, 1.128, 2.192, 1.508, 2.547,\n",
              "       1.945, 1.606, 3.482, 1.756, 1.457, 1.864, 1.821, 1.314, 1.715,\n",
              "       1.015, 1.345, 1.265, 1.844, 1.396, 1.785, 1.694, 1.413, 1.368,\n",
              "       2.21 , 1.034, 1.367, 1.943, 1.008, 1.279, 1.579, 1.444, 1.879,\n",
              "       1.466, 2.154, 1.794, 3.149, 1.883, 1.692, 1.163, 1.297, 2.949,\n",
              "       1.09 , 1.444, 1.524])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J705kDqsE8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4bcd472-be3d-4e75-a07c-c3b69982edb9"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(490, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8UtfP5P8mqk"
      },
      "source": [
        "wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        "wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        "X = pd.DataFrame([Diam1,Diameter_All])\n",
        "wts = pd.DataFrame([wt1,wt2])"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "sxq4aUCt8nmQ",
        "outputId": "9a808882-a651-4fed-a0fd-9ebc515e8db0"
      },
      "source": [
        "A = plt.hist(X,weights=wts,bins=7)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvXJTakQ8q1s"
      },
      "source": [
        "B = A[0][0]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jksgWcYt8uFz",
        "outputId": "e02d4075-a79b-496d-c463-8b3fc4f45871"
      },
      "source": [
        "Novo = []\n",
        "k = 0\n",
        "soma = 0\n",
        "for i in B:\n",
        "  if(k<4):\n",
        "    Novo.append(i)\n",
        "  else:\n",
        "    soma = soma + i\n",
        "  k = k + 1\n",
        "Novo.append(soma)\n",
        "print(Novo)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.941176470588235, 13.725490196078429, 51.96078431372539, 18.627450980392226, 12.745098039215733]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK1GBUHWiIr4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "fc7dea5f-356e-4c29-ef6a-8f8a8e9be994"
      },
      "source": [
        "#Freq = [12.8, 23.2, 29.2, 18.4, 12.0, 0.8]\n",
        "#Freq2 = [16.4, 22.2, 29.6, 20.8, 8., 0.2]\n",
        "# Freq = [12.8, 23.2, 29.2, 18.4, 12.8] # average sample 4\n",
        "# Freq2 = [16.4, 22.2, 29.6, 20.8, 8.2] # average sample 10\n",
        "Freq = [14.4, 21.8, 27.6, 21.0, 12.2] # average sample 10\n",
        "Freq2 = [16.4, 22.2, 29.6, 20.8, 8.2] # average sample 10\n",
        "Freq3 = Novo\n",
        "barWidth = 0.25\n",
        "\n",
        "br1 = range(len(Freq))\n",
        "# Set position of bar on X axis\n",
        "br2 = [x + barWidth for x in br1]\n",
        "br3 = [x + barWidth for x in br2]\n",
        "#labels = [0.8, 1.0, 1.2, 1.4, 1.6, 1.8]\n",
        "labels = [0.8, 1.0, 1.2, 1.4, 1.6]\n",
        "xx=[]\n",
        "for a in labels:\n",
        "  xx.append(str(a))\n",
        "plt.bar(br1, Freq , color=\"green\", align=\"center\", width=0.3, tick_label= xx) \n",
        "plt.bar(br2, Freq2 , color=\"red\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.bar(br3, Freq3 , color=\"blue\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.legend(['CNN 1','CNN 2','True'])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f66088be1d0>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}