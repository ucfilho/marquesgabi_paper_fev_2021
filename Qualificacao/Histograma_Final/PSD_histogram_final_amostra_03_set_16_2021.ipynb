{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_final_amostra_03_set_16_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/Qualificacao/Histograma_Final/PSD_histogram_final_amostra_03_set_16_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from sklearn.metrics import r2_score\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZEvJvfoibE4",
        "outputId": "4a985a38-f92c-4158-f84b-2eafa2a07045"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mahotas\n",
            "  Downloading mahotas-1.4.11-cp37-cp37m-manylinux2010_x86_64.whl (5.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7 MB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.19.5)\n",
            "Installing collected packages: mahotas\n",
            "Successfully installed mahotas-1.4.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT"
      },
      "source": [
        "import mahotas.features.texture as mht\n",
        "import mahotas.features"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VcTdaNVh9EE",
        "outputId": "929d53e6-c815-4cbc-ad11-36d53e78f9c3"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marquesgabi_fev_2020'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 73 (delta 37), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n",
            "/content/marquesgabi_fev_2020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v7SRrc8mH2N",
        "outputId": "cd97d5c8-0646-4163-8ce6-2d3d3769f962"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip' \n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Doutorado'...\n",
            "remote: Enumerating objects: 464, done.\u001b[K\n",
            "remote: Counting objects: 100% (214/214), done.\u001b[K\n",
            "remote: Compressing objects: 100% (213/213), done.\u001b[K\n",
            "remote: Total 464 (delta 102), reused 0 (delta 0), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (464/464), 166.11 MiB | 24.78 MiB/s, done.\n",
            "Resolving deltas: 100% (225/225), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqIYzUcnrdMp",
        "outputId": "82f4c46e-e960-4c0a-aa0f-7de0f9e485b7"
      },
      "source": [
        "labels =[]\n",
        "with zipfile.ZipFile(Transfere, \"r\") as f:\n",
        "  for f in f.namelist():\n",
        "    labels.append(f)\n",
        "print(labels)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Fotos_Grandes-3cdAmostra/Q6-8-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-4.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[4] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHgqAnaFyCjp",
        "outputId": "e79f5c1f-0dce-4783-eba3-514a5149cc12"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 20.10 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from segment_filter_not_conclude import Segmenta  # got image provided segmented"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN5MN5a_v4np",
        "outputId": "14be68e4-2e73-4f8c-b294-a9c43e9b82a5"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     101  139.175873  139.407028  ...    0.036663    0.950593    1.601118\n",
            "1     170  105.942986  110.783966  ...    1.000000    1.000000    1.000000\n",
            "2     170  162.908234  167.893158  ...  151.608582  148.534134  145.589355\n",
            "3     118    1.056306    0.242746  ...    0.036196    0.717323    1.530882\n",
            "4     176  193.473648  198.352264  ...  122.911156  116.719521  112.084198\n",
            "5     113  254.357117  253.815018  ...   33.507557   14.504032    7.412796\n",
            "6     155  244.199768  247.511658  ...  115.263321  134.880753  168.666656\n",
            "7     152  149.311630  149.171051  ...  115.795700  112.760384  120.896118\n",
            "8     142  146.324341  146.340607  ...  170.546722  167.662964  165.873245\n",
            "9     192  100.726562   89.881508  ...  118.449638  118.084183  118.415359\n",
            "10    197  100.081741  111.449570  ...    0.297276    1.584375    0.762658\n",
            "11    189   79.844986   84.547325  ...   55.010975   32.411522    1.733882\n",
            "12    105  169.386688  162.702240  ...  183.422256  144.293350  129.368896\n",
            "13    102    1.075356    0.447905  ...  139.236847  130.973877  127.017700\n",
            "14    148  137.333832  139.615784  ...  142.162170  144.225723  146.878754\n",
            "15    111  167.552399  173.696045  ...  178.839783  174.540619  173.716339\n",
            "16    181  198.459915  185.357315  ...  101.714905   86.441345   74.468330\n",
            "17    189  140.561050  145.139908  ...  163.031540  140.122086  127.982162\n",
            "18    161   89.315689   83.693756  ...    0.776938    0.247637    1.378072\n",
            "19    145  148.882706  148.947113  ...    1.000000    1.000000    1.000000\n",
            "20    188  104.429153  137.296509  ...    1.363513    0.195111    1.342236\n",
            "21    189  250.096024  240.235947  ...  175.928665  180.054871  168.005493\n",
            "22    127  149.623413  116.035706  ...   92.361519   88.354210   96.067398\n",
            "23    152  147.020081  134.910660  ...    0.639197    0.323407    1.402355\n",
            "24    190  185.325424  190.522079  ...  105.060379  133.546356  155.048843\n",
            "25    166  204.885315  161.179977  ...  113.497879  107.376976  155.499481\n",
            "26    114  128.309021  131.974152  ...  189.641418  181.659882  171.311493\n",
            "27    162  135.057007  121.954124  ...  182.615616  214.464874  225.761322\n",
            "28    194   96.392174   99.711761  ...    1.851631    0.825699    1.000000\n",
            "29    123  180.033905  167.359451  ...  188.117920  185.145630  181.234985\n",
            "30    167  163.191452  156.522232  ...  199.190048  172.937576  150.188538\n",
            "31    156  178.289291  176.312302  ...    0.717291    0.307035    1.409599\n",
            "32    190  156.607193  170.217834  ...    2.065042    1.977728    0.953463\n",
            "33    135  220.930511  213.823807  ...  174.288498  183.579193  192.038330\n",
            "34    195  109.480927  110.413048  ...  129.225891  122.394226  117.888878\n",
            "35    108  147.827148  133.576126  ...  164.683121  172.790131  180.112488\n",
            "36    153    0.415054    0.584989  ...  139.818878  158.406998  157.658218\n",
            "37    110  182.875046  182.865784  ...  118.688263  119.342804  118.218178\n",
            "38    153  159.010010  170.930130  ...  191.972549  183.394424  168.506989\n",
            "39    147    0.045351    0.902494  ...  171.494339  187.761917  167.671219\n",
            "40    117  120.994743  113.485649  ...    1.733655    2.759077    1.795675\n",
            "41    180  113.231117  124.492836  ...  152.023712  158.014832  158.920013\n",
            "42    155   86.475639   82.609161  ...  131.003922  131.522263  143.964630\n",
            "43    169  118.133362  140.786896  ...  129.884552  134.083786  142.580887\n",
            "44    156  128.763321  133.504929  ...   77.651543   30.174229   27.799473\n",
            "45    171  159.874023  167.901306  ...  174.911575  160.681824  135.322495\n",
            "46    146  156.713074  152.672531  ...    0.543629    0.379246    1.420341\n",
            "47    115  169.950470  168.848312  ...   78.640747   69.124458   71.422073\n",
            "48    124  166.320496  171.636826  ...  171.222656  162.683655  167.652451\n",
            "49    196  163.285706  162.938766  ...  200.489792  148.673462  137.285706\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2emP4rNjQy",
        "outputId": "e991f09e-9755-40e9-d7ac-333d7bf5a64f"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 13.50 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIFPGE_-vx3T"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "Description = '3 layers of Convolution: 64, 128, 256 '\n",
        "N1 = 20\n",
        "N2 = 20\n",
        "\n",
        "# make the CNN\n",
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=64, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=N1))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=N2))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbPQ1FSRG6A",
        "outputId": "52d2193e-391f-472f-96d0-fc9c2b70dbe0"
      },
      "source": [
        "\n",
        "# training the model\n",
        "r = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), \n",
        "              epochs=200, batch_size=32)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 3s 156ms/step - loss: 0.5802 - accuracy: 0.7114 - val_loss: 0.6937 - val_accuracy: 0.4898\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.3900 - accuracy: 0.7930 - val_loss: 0.6936 - val_accuracy: 0.4898\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.2290 - accuracy: 0.9125 - val_loss: 0.6937 - val_accuracy: 0.4898\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.1657 - accuracy: 0.9359 - val_loss: 0.6938 - val_accuracy: 0.4898\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.1329 - accuracy: 0.9534 - val_loss: 0.6939 - val_accuracy: 0.4898\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.1139 - accuracy: 0.9563 - val_loss: 0.6956 - val_accuracy: 0.4898\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.1119 - accuracy: 0.9504 - val_loss: 0.6947 - val_accuracy: 0.4898\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.1176 - accuracy: 0.9475 - val_loss: 0.6976 - val_accuracy: 0.4898\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0723 - accuracy: 0.9650 - val_loss: 0.6966 - val_accuracy: 0.4898\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0832 - accuracy: 0.9679 - val_loss: 0.6966 - val_accuracy: 0.4898\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0279 - accuracy: 0.9913 - val_loss: 0.6930 - val_accuracy: 0.4898\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0229 - accuracy: 0.9913 - val_loss: 0.6924 - val_accuracy: 0.4898\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0284 - accuracy: 0.9913 - val_loss: 0.6900 - val_accuracy: 0.5034\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0277 - accuracy: 0.9913 - val_loss: 0.6893 - val_accuracy: 0.5102\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0139 - accuracy: 0.9942 - val_loss: 0.6900 - val_accuracy: 0.5102\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.6916 - val_accuracy: 0.5102\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0118 - accuracy: 0.9971 - val_loss: 0.6920 - val_accuracy: 0.5102\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0178 - accuracy: 0.9942 - val_loss: 0.6959 - val_accuracy: 0.5102\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0104 - accuracy: 0.9971 - val_loss: 0.7023 - val_accuracy: 0.5102\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0094 - accuracy: 0.9971 - val_loss: 0.7304 - val_accuracy: 0.5102\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.7643 - val_accuracy: 0.5102\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0088 - accuracy: 0.9971 - val_loss: 0.8594 - val_accuracy: 0.5102\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0086 - accuracy: 0.9942 - val_loss: 0.8529 - val_accuracy: 0.5102\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0103 - accuracy: 0.9971 - val_loss: 0.7130 - val_accuracy: 0.5102\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.7188 - val_accuracy: 0.5102\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.7133 - val_accuracy: 0.5102\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.7391 - val_accuracy: 0.5102\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.7792 - val_accuracy: 0.5102\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.8205 - val_accuracy: 0.5102\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.9445 - val_accuracy: 0.5102\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 1.0815 - val_accuracy: 0.5102\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 6.1959e-04 - accuracy: 1.0000 - val_loss: 1.1839 - val_accuracy: 0.5102\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.2113 - val_accuracy: 0.5102\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0042 - accuracy: 0.9971 - val_loss: 1.3990 - val_accuracy: 0.5102\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0082 - accuracy: 0.9971 - val_loss: 1.7090 - val_accuracy: 0.5102\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0076 - accuracy: 0.9971 - val_loss: 1.6668 - val_accuracy: 0.5102\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0049 - accuracy: 0.9971 - val_loss: 2.2920 - val_accuracy: 0.5102\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 2.9451 - val_accuracy: 0.5102\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 3.4867 - val_accuracy: 0.5102\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.5120 - val_accuracy: 0.5102\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 8.0184e-04 - accuracy: 1.0000 - val_loss: 3.4908 - val_accuracy: 0.5102\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 8.4717e-04 - accuracy: 1.0000 - val_loss: 3.4749 - val_accuracy: 0.5102\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 6.2384e-04 - accuracy: 1.0000 - val_loss: 2.9054 - val_accuracy: 0.5102\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 4.3575e-04 - accuracy: 1.0000 - val_loss: 2.7073 - val_accuracy: 0.5102\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.8125 - val_accuracy: 0.5102\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 6.7020e-04 - accuracy: 1.0000 - val_loss: 3.1188 - val_accuracy: 0.5102\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 2.8244e-04 - accuracy: 1.0000 - val_loss: 3.1396 - val_accuracy: 0.5102\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 4.3003e-04 - accuracy: 1.0000 - val_loss: 3.1561 - val_accuracy: 0.5102\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 3.0997e-04 - accuracy: 1.0000 - val_loss: 3.1835 - val_accuracy: 0.5102\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 3.8667e-04 - accuracy: 1.0000 - val_loss: 3.0313 - val_accuracy: 0.5102\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 3.1043e-04 - accuracy: 1.0000 - val_loss: 2.7736 - val_accuracy: 0.5102\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 1.3319e-04 - accuracy: 1.0000 - val_loss: 2.5570 - val_accuracy: 0.5102\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 2.6463e-04 - accuracy: 1.0000 - val_loss: 2.4612 - val_accuracy: 0.5102\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 4.5932e-04 - accuracy: 1.0000 - val_loss: 2.3794 - val_accuracy: 0.5102\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 1.8343e-04 - accuracy: 1.0000 - val_loss: 2.2095 - val_accuracy: 0.5102\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 8.5370e-04 - accuracy: 1.0000 - val_loss: 0.5831 - val_accuracy: 0.6939\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 3.0755e-04 - accuracy: 1.0000 - val_loss: 0.2788 - val_accuracy: 0.8435\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 3.8662e-04 - accuracy: 1.0000 - val_loss: 0.2000 - val_accuracy: 0.9184\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 3.7355 - val_accuracy: 0.5102\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0091 - accuracy: 0.9971 - val_loss: 27.7079 - val_accuracy: 0.5102\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0290 - accuracy: 0.9971 - val_loss: 80.5597 - val_accuracy: 0.5102\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0638 - accuracy: 0.9738 - val_loss: 180.3285 - val_accuracy: 0.5102\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0619 - accuracy: 0.9767 - val_loss: 241.3244 - val_accuracy: 0.5102\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0337 - accuracy: 0.9942 - val_loss: 275.2249 - val_accuracy: 0.5102\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0961 - accuracy: 0.9796 - val_loss: 113.0343 - val_accuracy: 0.5102\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.1004 - accuracy: 0.9650 - val_loss: 85.7268 - val_accuracy: 0.5102\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0819 - accuracy: 0.9738 - val_loss: 20.0099 - val_accuracy: 0.5102\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0304 - accuracy: 0.9942 - val_loss: 16.6078 - val_accuracy: 0.5102\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0231 - accuracy: 0.9883 - val_loss: 63.6118 - val_accuracy: 0.5102\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0244 - accuracy: 0.9883 - val_loss: 35.3213 - val_accuracy: 0.5102\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 0.0086 - accuracy: 0.9971 - val_loss: 82.7389 - val_accuracy: 0.5102\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 85.7184 - val_accuracy: 0.5102\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 62.7117 - val_accuracy: 0.5102\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 3.7673e-04 - accuracy: 1.0000 - val_loss: 51.4010 - val_accuracy: 0.5102\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 8.9168e-04 - accuracy: 1.0000 - val_loss: 43.0424 - val_accuracy: 0.5102\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0050 - accuracy: 0.9971 - val_loss: 30.0440 - val_accuracy: 0.5102\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 32.4466 - val_accuracy: 0.5102\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0045 - accuracy: 0.9971 - val_loss: 45.6726 - val_accuracy: 0.5102\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 6.0641e-04 - accuracy: 1.0000 - val_loss: 44.3098 - val_accuracy: 0.5102\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 33.9226 - val_accuracy: 0.5102\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 4.5741e-04 - accuracy: 1.0000 - val_loss: 28.0499 - val_accuracy: 0.5102\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 3.1898e-04 - accuracy: 1.0000 - val_loss: 23.9303 - val_accuracy: 0.5102\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 5.0737e-04 - accuracy: 1.0000 - val_loss: 20.7970 - val_accuracy: 0.5102\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 7.1524e-04 - accuracy: 1.0000 - val_loss: 18.5562 - val_accuracy: 0.5102\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 5.2670e-04 - accuracy: 1.0000 - val_loss: 16.8146 - val_accuracy: 0.5102\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 1.9105e-04 - accuracy: 1.0000 - val_loss: 15.5564 - val_accuracy: 0.5102\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 2.5951e-04 - accuracy: 1.0000 - val_loss: 14.1556 - val_accuracy: 0.5102\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 1.4396e-04 - accuracy: 1.0000 - val_loss: 12.7173 - val_accuracy: 0.5102\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 3.8990e-04 - accuracy: 1.0000 - val_loss: 11.0717 - val_accuracy: 0.5102\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 2.5844e-04 - accuracy: 1.0000 - val_loss: 9.6271 - val_accuracy: 0.5102\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 4.4506e-04 - accuracy: 1.0000 - val_loss: 8.2434 - val_accuracy: 0.5102\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 6.4676e-04 - accuracy: 1.0000 - val_loss: 5.8936 - val_accuracy: 0.5102\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 1.3929e-04 - accuracy: 1.0000 - val_loss: 3.9640 - val_accuracy: 0.5102\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 1.8875e-04 - accuracy: 1.0000 - val_loss: 3.0226 - val_accuracy: 0.5102\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 2.2151e-04 - accuracy: 1.0000 - val_loss: 2.4796 - val_accuracy: 0.5102\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 2.8100e-04 - accuracy: 1.0000 - val_loss: 2.3514 - val_accuracy: 0.5238\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 1.1433e-04 - accuracy: 1.0000 - val_loss: 1.9799 - val_accuracy: 0.5510\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 3.3854e-04 - accuracy: 1.0000 - val_loss: 1.2890 - val_accuracy: 0.6327\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 1.8625e-04 - accuracy: 1.0000 - val_loss: 0.8880 - val_accuracy: 0.6735\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 4.8049e-04 - accuracy: 1.0000 - val_loss: 1.0250 - val_accuracy: 0.6735\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 2.2699 - val_accuracy: 0.5510\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 8.7640e-04 - accuracy: 1.0000 - val_loss: 5.0226 - val_accuracy: 0.5170\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 8.9113e-04 - accuracy: 1.0000 - val_loss: 4.2432 - val_accuracy: 0.5170\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 1.6092e-04 - accuracy: 1.0000 - val_loss: 4.0731 - val_accuracy: 0.5170\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 2.3096e-04 - accuracy: 1.0000 - val_loss: 4.5192 - val_accuracy: 0.5170\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 8.6469e-04 - accuracy: 1.0000 - val_loss: 4.4998 - val_accuracy: 0.5170\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 1.4186e-04 - accuracy: 1.0000 - val_loss: 4.1861 - val_accuracy: 0.5170\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 2.0587e-04 - accuracy: 1.0000 - val_loss: 3.3584 - val_accuracy: 0.5238\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 6.2196e-05 - accuracy: 1.0000 - val_loss: 2.6483 - val_accuracy: 0.5578\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 1.8211e-04 - accuracy: 1.0000 - val_loss: 2.2731 - val_accuracy: 0.5850\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 1.0436e-04 - accuracy: 1.0000 - val_loss: 2.0664 - val_accuracy: 0.6122\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 1.0565e-04 - accuracy: 1.0000 - val_loss: 1.8129 - val_accuracy: 0.6327\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 1.0195e-04 - accuracy: 1.0000 - val_loss: 1.5038 - val_accuracy: 0.6599\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 2.0970e-04 - accuracy: 1.0000 - val_loss: 0.7402 - val_accuracy: 0.7891\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 7.8789e-05 - accuracy: 1.0000 - val_loss: 0.3940 - val_accuracy: 0.8912\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 8.7762e-05 - accuracy: 1.0000 - val_loss: 0.2432 - val_accuracy: 0.9456\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 5.2852e-04 - accuracy: 1.0000 - val_loss: 0.1744 - val_accuracy: 0.9456\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 3.4450e-05 - accuracy: 1.0000 - val_loss: 0.2044 - val_accuracy: 0.9252\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 6.1353e-05 - accuracy: 1.0000 - val_loss: 0.2056 - val_accuracy: 0.9320\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 4.7181e-05 - accuracy: 1.0000 - val_loss: 0.2110 - val_accuracy: 0.9456\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 6.2909e-05 - accuracy: 1.0000 - val_loss: 0.2129 - val_accuracy: 0.9456\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 5.6326e-05 - accuracy: 1.0000 - val_loss: 0.2388 - val_accuracy: 0.9456\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 3.6439e-05 - accuracy: 1.0000 - val_loss: 0.2531 - val_accuracy: 0.9320\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 7.9475e-05 - accuracy: 1.0000 - val_loss: 0.2649 - val_accuracy: 0.9320\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 1.0821e-04 - accuracy: 1.0000 - val_loss: 0.2689 - val_accuracy: 0.9252\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 4.4475e-04 - accuracy: 1.0000 - val_loss: 0.1537 - val_accuracy: 0.9592\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 1.1644e-04 - accuracy: 1.0000 - val_loss: 0.4631 - val_accuracy: 0.8571\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 5.8631e-05 - accuracy: 1.0000 - val_loss: 0.6265 - val_accuracy: 0.8367\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 1.7922e-04 - accuracy: 1.0000 - val_loss: 0.5894 - val_accuracy: 0.8367\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 4.2657e-05 - accuracy: 1.0000 - val_loss: 0.7288 - val_accuracy: 0.8095\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 2.7956e-05 - accuracy: 1.0000 - val_loss: 0.5591 - val_accuracy: 0.8435\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 5.9454e-04 - accuracy: 1.0000 - val_loss: 0.3858 - val_accuracy: 0.8776\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 1.2231e-04 - accuracy: 1.0000 - val_loss: 0.3265 - val_accuracy: 0.9048\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 3.5064e-05 - accuracy: 1.0000 - val_loss: 0.3560 - val_accuracy: 0.9048\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 2.8369e-04 - accuracy: 1.0000 - val_loss: 0.2385 - val_accuracy: 0.9660\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 3.5436e-05 - accuracy: 1.0000 - val_loss: 0.2489 - val_accuracy: 0.9660\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 6.5525e-05 - accuracy: 1.0000 - val_loss: 0.2406 - val_accuracy: 0.9660\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 8.9029e-05 - accuracy: 1.0000 - val_loss: 0.2434 - val_accuracy: 0.9660\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 1.1465e-04 - accuracy: 1.0000 - val_loss: 0.2459 - val_accuracy: 0.9660\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 1.5130e-04 - accuracy: 1.0000 - val_loss: 0.1922 - val_accuracy: 0.9728\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 3.4990e-04 - accuracy: 1.0000 - val_loss: 0.3217 - val_accuracy: 0.9252\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 6.5545e-05 - accuracy: 1.0000 - val_loss: 0.3166 - val_accuracy: 0.9116\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 5.7442e-05 - accuracy: 1.0000 - val_loss: 0.2982 - val_accuracy: 0.9252\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 1.6962e-05 - accuracy: 1.0000 - val_loss: 0.2877 - val_accuracy: 0.9388\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 1.6190e-04 - accuracy: 1.0000 - val_loss: 0.3677 - val_accuracy: 0.9048\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 2.7343e-05 - accuracy: 1.0000 - val_loss: 0.3788 - val_accuracy: 0.9184\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 4.7086e-05 - accuracy: 1.0000 - val_loss: 0.4108 - val_accuracy: 0.9184\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 3.4449e-05 - accuracy: 1.0000 - val_loss: 0.4342 - val_accuracy: 0.9184\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 1.1555e-04 - accuracy: 1.0000 - val_loss: 0.4816 - val_accuracy: 0.9184\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 1.4727e-04 - accuracy: 1.0000 - val_loss: 0.4240 - val_accuracy: 0.9252\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 1.8885e-04 - accuracy: 1.0000 - val_loss: 0.3783 - val_accuracy: 0.9252\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 1.9686e-05 - accuracy: 1.0000 - val_loss: 0.3217 - val_accuracy: 0.9320\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 2.6277e-04 - accuracy: 1.0000 - val_loss: 0.2010 - val_accuracy: 0.9728\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 2.6636e-05 - accuracy: 1.0000 - val_loss: 0.2074 - val_accuracy: 0.9320\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 4.7086e-05 - accuracy: 1.0000 - val_loss: 0.2739 - val_accuracy: 0.8912\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 2.8471e-05 - accuracy: 1.0000 - val_loss: 0.2655 - val_accuracy: 0.9116\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 3.6022e-04 - accuracy: 1.0000 - val_loss: 0.1709 - val_accuracy: 0.9592\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 2.0755e-05 - accuracy: 1.0000 - val_loss: 0.2093 - val_accuracy: 0.9252\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2863 - val_accuracy: 0.9048\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 5.5150e-05 - accuracy: 1.0000 - val_loss: 0.3365 - val_accuracy: 0.9116\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 2.9344e-05 - accuracy: 1.0000 - val_loss: 0.3216 - val_accuracy: 0.9184\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 3.9028e-05 - accuracy: 1.0000 - val_loss: 0.3149 - val_accuracy: 0.9184\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 1.5222e-05 - accuracy: 1.0000 - val_loss: 0.3084 - val_accuracy: 0.9184\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 3.3441e-05 - accuracy: 1.0000 - val_loss: 0.3277 - val_accuracy: 0.9252\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 1.4373e-04 - accuracy: 1.0000 - val_loss: 0.3494 - val_accuracy: 0.9252\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 2.1054e-04 - accuracy: 1.0000 - val_loss: 0.3524 - val_accuracy: 0.9184\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 1.0642e-04 - accuracy: 1.0000 - val_loss: 0.3521 - val_accuracy: 0.9252\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 4.9076e-05 - accuracy: 1.0000 - val_loss: 0.3726 - val_accuracy: 0.9252\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 1.3316e-05 - accuracy: 1.0000 - val_loss: 0.3743 - val_accuracy: 0.9320\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 2s 151ms/step - loss: 2.8713e-04 - accuracy: 1.0000 - val_loss: 0.4060 - val_accuracy: 0.9116\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 5.1983e-05 - accuracy: 1.0000 - val_loss: 0.2975 - val_accuracy: 0.9184\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 3.9711e-05 - accuracy: 1.0000 - val_loss: 0.1997 - val_accuracy: 0.9320\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 3.7857e-05 - accuracy: 1.0000 - val_loss: 0.1706 - val_accuracy: 0.9252\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 1.7272e-04 - accuracy: 1.0000 - val_loss: 0.1447 - val_accuracy: 0.9524\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 1.6685e-04 - accuracy: 1.0000 - val_loss: 0.1292 - val_accuracy: 0.9524\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 4.4623e-05 - accuracy: 1.0000 - val_loss: 0.1319 - val_accuracy: 0.9456\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 9.1285e-05 - accuracy: 1.0000 - val_loss: 0.1415 - val_accuracy: 0.9524\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 3.5560e-05 - accuracy: 1.0000 - val_loss: 0.1482 - val_accuracy: 0.9456\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 4.6679e-05 - accuracy: 1.0000 - val_loss: 0.1477 - val_accuracy: 0.9456\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 1.8872e-05 - accuracy: 1.0000 - val_loss: 0.1572 - val_accuracy: 0.9456\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 1.2896e-04 - accuracy: 1.0000 - val_loss: 0.1745 - val_accuracy: 0.9524\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 6.8362e-06 - accuracy: 1.0000 - val_loss: 0.1762 - val_accuracy: 0.9456\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 3.1614e-05 - accuracy: 1.0000 - val_loss: 0.1888 - val_accuracy: 0.9456\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 3.6120e-05 - accuracy: 1.0000 - val_loss: 0.2033 - val_accuracy: 0.9456\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 2.4368e-05 - accuracy: 1.0000 - val_loss: 0.2181 - val_accuracy: 0.9456\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 5.8982e-06 - accuracy: 1.0000 - val_loss: 0.2325 - val_accuracy: 0.9456\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 5.6354e-05 - accuracy: 1.0000 - val_loss: 0.1996 - val_accuracy: 0.9728\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 1.8143e-05 - accuracy: 1.0000 - val_loss: 0.1950 - val_accuracy: 0.9728\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 2.2038e-05 - accuracy: 1.0000 - val_loss: 0.2078 - val_accuracy: 0.9728\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 1.1538e-05 - accuracy: 1.0000 - val_loss: 0.2059 - val_accuracy: 0.9728\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 4.9169e-05 - accuracy: 1.0000 - val_loss: 0.1969 - val_accuracy: 0.9796\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 1.0181e-04 - accuracy: 1.0000 - val_loss: 0.1919 - val_accuracy: 0.9728\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 2.8509e-05 - accuracy: 1.0000 - val_loss: 0.1925 - val_accuracy: 0.9728\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 5.3214e-05 - accuracy: 1.0000 - val_loss: 0.1783 - val_accuracy: 0.9796\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 1.0958e-05 - accuracy: 1.0000 - val_loss: 0.1748 - val_accuracy: 0.9796\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 3.8437e-05 - accuracy: 1.0000 - val_loss: 0.1752 - val_accuracy: 0.9728\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 1.3534e-05 - accuracy: 1.0000 - val_loss: 0.1871 - val_accuracy: 0.9728\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 3.1939e-05 - accuracy: 1.0000 - val_loss: 0.1991 - val_accuracy: 0.9728\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 5.0483e-05 - accuracy: 1.0000 - val_loss: 0.2062 - val_accuracy: 0.9728\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 1.8228e-05 - accuracy: 1.0000 - val_loss: 0.2050 - val_accuracy: 0.9796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDVY6HbxMOlH",
        "outputId": "69cbefa1-85e0-43d6-a3e7-974170bd75aa"
      },
      "source": [
        "# pred_test= model.predict_classes(X_test)\n",
        "pred_test = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict   0   1\n",
            "Actual         \n",
            "0        71   1\n",
            "1         2  73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7pT2q7traXg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dfc895d-e47d-4ab8-c292-9a1ee706dc05"
      },
      "source": [
        "print(METRICS)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98        72\n",
            "           1       0.99      0.97      0.98        75\n",
            "\n",
            "    accuracy                           0.98       147\n",
            "   macro avg       0.98      0.98      0.98       147\n",
            "weighted avg       0.98      0.98      0.98       147\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElpxWbBnpgLX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "9c40144a-6f9a-453a-ccc3-aeed2c985e69"
      },
      "source": [
        "'''\n",
        "#X =np.array(df.copy())/255.0 \n",
        "X =np.array(df.copy())\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)\n",
        "model = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh', solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  \n",
        "prediction = model.predict(X_test)  \n",
        "y =np.copy(y_test)\n",
        "data = {'y_true': y_test,'y_predict': prediction}  \n",
        "# este dado esta no formato de dicionario\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)\n",
        "print(METRICS)\n",
        "#X =np.array(df.copy())/255.0 X =np.array(df_all.copy())X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)model = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh',                       solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  prediction = model.predict(X_test)  y =np.copy(y_test)data = {'y_true': y_test,'y_predict': prediction}  # este dado esta no formato de dicionariodf = pd.DataFrame(data, columns=['y_true','y_predict'])confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])print(confusion_matrix)y_true = df['y_true']y_pred = df['y_predict']\n",
        "'''"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n#X =np.array(df.copy())/255.0 \\nX =np.array(df.copy())\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)\\nmodel = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh', solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  \\nprediction = model.predict(X_test)  \\ny =np.copy(y_test)\\ndata = {'y_true': y_test,'y_predict': prediction}  \\n# este dado esta no formato de dicionario\\ndf = pd.DataFrame(data, columns=['y_true','y_predict'])\\nconfusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\\nprint(confusion_matrix)\\ny_true = df['y_true']\\ny_pred = df['y_predict']  \\nMETRICS=sklearn.metrics.classification_report(y_true, y_pred)\\nprint(METRICS)\\n#X =np.array(df.copy())/255.0 X =np.array(df_all.copy())X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)model = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh',                       solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)  prediction = model.predict(X_test)  y =np.copy(y_test)data = {'y_true': y_test,'y_predict': prediction}  # este dado esta no formato de dicionariodf = pd.DataFrame(data, columns=['y_true','y_predict'])confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])print(confusion_matrix)y_true = df['y_true']y_pred = df['y_predict']\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iFNNrlWV9tH",
        "outputId": "a7625903-f889-4ca7-e0c5-f6d644156e5b"
      },
      "source": [
        "pred_test"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv5I61yhPQmk",
        "outputId": "4786be41-da46-49dc-8751-a289b3d6b3b2"
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[4] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  #prediction = model.predict_classes(result)\n",
        "  prediction= np.argmax(model.predict(result), axis=-1)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "3   114.0  105.443214  116.850418  ...  165.128967  114.773460  136.356720\n",
            "14  131.0   86.885376   81.719368  ...    9.864518    2.186644    1.964280\n",
            "11  122.0  123.400162  124.483734  ...  174.037079  172.800323  173.433487\n",
            "20  200.0  163.311203  154.356796  ...  152.120010  144.771622  139.509201\n",
            "25  130.0   59.711716   69.382256  ...   94.525681  108.068642  111.479767\n",
            "26  113.0  154.128830  157.977356  ...   33.516487    1.581565    1.002193\n",
            "37  111.0  247.895798  243.968994  ...   58.764061   59.435760   66.393311\n",
            "42  137.0  180.113205  190.783463  ...    2.309766    0.936331    2.006500\n",
            "0   139.0    0.242016    0.630506  ...  236.318176  232.765579  230.940826\n",
            "8   120.0    0.015556    0.872222  ...  116.352219  113.562225  110.703339\n",
            "19  140.0  216.440002  196.559998  ...  120.839996  129.279999  127.680000\n",
            "7   115.0  153.725433  153.156052  ...  167.855713  162.079453  124.198021\n",
            "45  135.0    1.213388    1.702277  ...   58.688778   59.477745   62.793083\n",
            "6   131.0  180.380386  184.765091  ...   15.485460   16.079308   16.993357\n",
            "22  197.0  165.323257  156.363113  ...  149.936798  141.214523  138.708603\n",
            "15  102.0    2.000000    1.173395  ...    0.549020    0.668589    1.226067\n",
            "18  151.0   36.989254   42.722511  ...   77.028557  145.027283  166.481339\n",
            "34  131.0  104.704147  111.959091  ...  164.984665  166.610962  146.270447\n",
            "36  120.0    1.821111    1.221111  ...   71.296669   79.424446   78.257782\n",
            "4   184.0  160.818512  146.035919  ...  173.051971  176.306213  173.830338\n",
            "32  114.0  177.752838  176.260086  ...    1.112958    0.341028    0.305940\n",
            "37  113.0    0.804762    0.004386  ...   53.436760   50.625580   47.417027\n",
            "49  110.0   72.751732   86.565948  ...   84.885948   85.273392   76.943474\n",
            "6   109.0  156.167404  173.221359  ...  104.872070  115.374710  128.898987\n",
            "26  137.0  151.263992  153.844635  ...  178.126755  180.866760  181.916443\n",
            "35  150.0  189.886398  188.200714  ...  252.323929  251.585251  248.230225\n",
            "45  152.0  134.152359  131.419662  ...  178.641968  165.754852  158.090027\n",
            "47  129.0    1.830058    1.132384  ...  252.450272  224.227448  152.820435\n",
            "\n",
            "[28 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRbWgmX_LFH",
        "outputId": "9793b09e-b4cf-4144-d47c-f2d07ce206f8"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "from GetBetterSegm import GetBetter"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marquesgabi_paper_fev_2021'...\n",
            "remote: Enumerating objects: 712, done.\u001b[K\n",
            "remote: Counting objects: 100% (473/473), done.\u001b[K\n",
            "remote: Compressing objects: 100% (471/471), done.\u001b[K\n",
            "remote: Total 712 (delta 300), reused 0 (delta 0), pack-reused 239\u001b[K\n",
            "Receiving objects: 100% (712/712), 5.78 MiB | 9.45 MiB/s, done.\n",
            "Resolving deltas: 100% (437/437), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAG_I6FwCvFr",
        "outputId": "4e968732-db78-4d2c-cdac-71ee967a4e50"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "#!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd marquesgabi_out_2020\n",
        "#%cd Doutorado\n",
        "#PSD_imageJ = 'Amostra7.csv' \n",
        "#PSD_new = pd.read_csv(PSD_imageJ,sep=';')\n",
        "#encoding='utf8'\n",
        "\n",
        "PSD_imageJ = 'Areas_ImageJ.csv'\n",
        "PSD_new = pd.read_csv(PSD_imageJ)\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marquesgabi_out_2020'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 146 (delta 75), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (146/146), 1.00 MiB | 1.51 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020\n",
            "   Juntas   Area\n",
            "0       1  2.001\n",
            "1       2  0.820\n",
            "2       3  1.270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tEPjIBnv_xM",
        "outputId": "4d190e1f-1848-4b0f-8e8b-5deab0c4ad3a"
      },
      "source": [
        "PSD_new.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(95, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "PekBHQOT_6CP",
        "outputId": "f2e482fd-0ea2-44a4-d928-b01c5f1b110f"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>114.0</td>\n",
              "      <td>105.443214</td>\n",
              "      <td>116.850418</td>\n",
              "      <td>133.907669</td>\n",
              "      <td>150.361343</td>\n",
              "      <td>173.200684</td>\n",
              "      <td>189.714066</td>\n",
              "      <td>197.461380</td>\n",
              "      <td>200.086182</td>\n",
              "      <td>198.012604</td>\n",
              "      <td>206.918747</td>\n",
              "      <td>234.902115</td>\n",
              "      <td>231.986786</td>\n",
              "      <td>142.039719</td>\n",
              "      <td>109.986771</td>\n",
              "      <td>149.568481</td>\n",
              "      <td>157.247147</td>\n",
              "      <td>155.447220</td>\n",
              "      <td>156.237015</td>\n",
              "      <td>151.957520</td>\n",
              "      <td>150.475830</td>\n",
              "      <td>150.816559</td>\n",
              "      <td>150.624496</td>\n",
              "      <td>150.570938</td>\n",
              "      <td>148.330261</td>\n",
              "      <td>145.720840</td>\n",
              "      <td>141.261627</td>\n",
              "      <td>139.066772</td>\n",
              "      <td>136.762070</td>\n",
              "      <td>108.884888</td>\n",
              "      <td>106.975990</td>\n",
              "      <td>107.631889</td>\n",
              "      <td>111.776550</td>\n",
              "      <td>125.824265</td>\n",
              "      <td>144.749451</td>\n",
              "      <td>168.413681</td>\n",
              "      <td>188.604492</td>\n",
              "      <td>196.231140</td>\n",
              "      <td>211.501083</td>\n",
              "      <td>220.546936</td>\n",
              "      <td>...</td>\n",
              "      <td>174.835327</td>\n",
              "      <td>173.208984</td>\n",
              "      <td>170.714981</td>\n",
              "      <td>170.223145</td>\n",
              "      <td>178.306244</td>\n",
              "      <td>186.640503</td>\n",
              "      <td>185.315186</td>\n",
              "      <td>192.924286</td>\n",
              "      <td>213.159744</td>\n",
              "      <td>155.926758</td>\n",
              "      <td>120.719299</td>\n",
              "      <td>141.835938</td>\n",
              "      <td>181.471832</td>\n",
              "      <td>186.201904</td>\n",
              "      <td>190.504166</td>\n",
              "      <td>199.288406</td>\n",
              "      <td>202.144669</td>\n",
              "      <td>202.635880</td>\n",
              "      <td>199.161591</td>\n",
              "      <td>187.648499</td>\n",
              "      <td>153.912582</td>\n",
              "      <td>176.520782</td>\n",
              "      <td>197.701767</td>\n",
              "      <td>200.343185</td>\n",
              "      <td>192.811935</td>\n",
              "      <td>173.657745</td>\n",
              "      <td>163.587265</td>\n",
              "      <td>173.787323</td>\n",
              "      <td>180.158813</td>\n",
              "      <td>182.381042</td>\n",
              "      <td>182.562943</td>\n",
              "      <td>183.800873</td>\n",
              "      <td>189.690674</td>\n",
              "      <td>195.517090</td>\n",
              "      <td>201.800873</td>\n",
              "      <td>215.856277</td>\n",
              "      <td>236.058487</td>\n",
              "      <td>165.128967</td>\n",
              "      <td>114.773460</td>\n",
              "      <td>136.356720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>131.0</td>\n",
              "      <td>86.885376</td>\n",
              "      <td>81.719368</td>\n",
              "      <td>84.174522</td>\n",
              "      <td>82.630096</td>\n",
              "      <td>85.645409</td>\n",
              "      <td>87.216179</td>\n",
              "      <td>86.158791</td>\n",
              "      <td>84.610626</td>\n",
              "      <td>79.944931</td>\n",
              "      <td>72.560974</td>\n",
              "      <td>73.094055</td>\n",
              "      <td>72.356339</td>\n",
              "      <td>76.872383</td>\n",
              "      <td>98.636620</td>\n",
              "      <td>108.697105</td>\n",
              "      <td>115.538429</td>\n",
              "      <td>116.395309</td>\n",
              "      <td>113.558762</td>\n",
              "      <td>109.171021</td>\n",
              "      <td>99.509003</td>\n",
              "      <td>99.828506</td>\n",
              "      <td>96.163208</td>\n",
              "      <td>93.924828</td>\n",
              "      <td>96.983276</td>\n",
              "      <td>103.427597</td>\n",
              "      <td>108.890564</td>\n",
              "      <td>105.622688</td>\n",
              "      <td>86.773094</td>\n",
              "      <td>78.291245</td>\n",
              "      <td>74.184601</td>\n",
              "      <td>73.404991</td>\n",
              "      <td>78.958275</td>\n",
              "      <td>82.902100</td>\n",
              "      <td>81.618950</td>\n",
              "      <td>80.847328</td>\n",
              "      <td>81.341934</td>\n",
              "      <td>81.495079</td>\n",
              "      <td>72.536156</td>\n",
              "      <td>75.227608</td>\n",
              "      <td>...</td>\n",
              "      <td>34.781013</td>\n",
              "      <td>31.013050</td>\n",
              "      <td>34.780724</td>\n",
              "      <td>37.334885</td>\n",
              "      <td>34.636734</td>\n",
              "      <td>28.860264</td>\n",
              "      <td>30.228424</td>\n",
              "      <td>31.799425</td>\n",
              "      <td>27.909618</td>\n",
              "      <td>21.876406</td>\n",
              "      <td>5.146494</td>\n",
              "      <td>9.379814</td>\n",
              "      <td>14.384360</td>\n",
              "      <td>18.205814</td>\n",
              "      <td>22.637434</td>\n",
              "      <td>22.796747</td>\n",
              "      <td>23.215546</td>\n",
              "      <td>27.821455</td>\n",
              "      <td>34.689529</td>\n",
              "      <td>33.292175</td>\n",
              "      <td>27.332500</td>\n",
              "      <td>25.300797</td>\n",
              "      <td>19.221548</td>\n",
              "      <td>9.467455</td>\n",
              "      <td>2.950702</td>\n",
              "      <td>2.344444</td>\n",
              "      <td>14.983159</td>\n",
              "      <td>33.905949</td>\n",
              "      <td>32.547695</td>\n",
              "      <td>33.446358</td>\n",
              "      <td>44.304642</td>\n",
              "      <td>42.147659</td>\n",
              "      <td>33.332382</td>\n",
              "      <td>31.422819</td>\n",
              "      <td>34.889809</td>\n",
              "      <td>34.479805</td>\n",
              "      <td>25.672222</td>\n",
              "      <td>9.864518</td>\n",
              "      <td>2.186644</td>\n",
              "      <td>1.964280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>122.0</td>\n",
              "      <td>123.400162</td>\n",
              "      <td>124.483734</td>\n",
              "      <td>120.857292</td>\n",
              "      <td>125.143242</td>\n",
              "      <td>128.000534</td>\n",
              "      <td>138.706802</td>\n",
              "      <td>144.707062</td>\n",
              "      <td>152.993011</td>\n",
              "      <td>163.595261</td>\n",
              "      <td>168.422440</td>\n",
              "      <td>172.867783</td>\n",
              "      <td>174.225464</td>\n",
              "      <td>175.094604</td>\n",
              "      <td>154.679108</td>\n",
              "      <td>103.385109</td>\n",
              "      <td>91.708130</td>\n",
              "      <td>89.773987</td>\n",
              "      <td>101.377846</td>\n",
              "      <td>131.314423</td>\n",
              "      <td>163.307968</td>\n",
              "      <td>178.273834</td>\n",
              "      <td>175.352600</td>\n",
              "      <td>171.756790</td>\n",
              "      <td>177.722382</td>\n",
              "      <td>177.242676</td>\n",
              "      <td>182.490723</td>\n",
              "      <td>186.671860</td>\n",
              "      <td>189.780701</td>\n",
              "      <td>127.542046</td>\n",
              "      <td>128.690140</td>\n",
              "      <td>120.986282</td>\n",
              "      <td>126.196716</td>\n",
              "      <td>141.809189</td>\n",
              "      <td>152.160706</td>\n",
              "      <td>156.355011</td>\n",
              "      <td>162.595261</td>\n",
              "      <td>171.939789</td>\n",
              "      <td>173.640945</td>\n",
              "      <td>170.327606</td>\n",
              "      <td>...</td>\n",
              "      <td>156.218231</td>\n",
              "      <td>180.255295</td>\n",
              "      <td>194.006989</td>\n",
              "      <td>200.271423</td>\n",
              "      <td>185.346146</td>\n",
              "      <td>177.414948</td>\n",
              "      <td>180.299103</td>\n",
              "      <td>181.450958</td>\n",
              "      <td>181.609772</td>\n",
              "      <td>178.915878</td>\n",
              "      <td>174.523514</td>\n",
              "      <td>172.194016</td>\n",
              "      <td>164.485901</td>\n",
              "      <td>171.800049</td>\n",
              "      <td>174.083298</td>\n",
              "      <td>174.331619</td>\n",
              "      <td>174.429703</td>\n",
              "      <td>174.874207</td>\n",
              "      <td>177.836060</td>\n",
              "      <td>181.402832</td>\n",
              "      <td>184.656525</td>\n",
              "      <td>184.754349</td>\n",
              "      <td>179.298294</td>\n",
              "      <td>166.141083</td>\n",
              "      <td>138.902710</td>\n",
              "      <td>111.124695</td>\n",
              "      <td>119.167152</td>\n",
              "      <td>155.565979</td>\n",
              "      <td>178.433731</td>\n",
              "      <td>190.407410</td>\n",
              "      <td>192.130615</td>\n",
              "      <td>179.867767</td>\n",
              "      <td>169.432678</td>\n",
              "      <td>169.649826</td>\n",
              "      <td>173.168228</td>\n",
              "      <td>172.951614</td>\n",
              "      <td>173.973663</td>\n",
              "      <td>174.037079</td>\n",
              "      <td>172.800323</td>\n",
              "      <td>173.433487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>200.0</td>\n",
              "      <td>163.311203</td>\n",
              "      <td>154.356796</td>\n",
              "      <td>175.943192</td>\n",
              "      <td>164.314011</td>\n",
              "      <td>152.010818</td>\n",
              "      <td>145.239609</td>\n",
              "      <td>135.483185</td>\n",
              "      <td>135.380798</td>\n",
              "      <td>145.652008</td>\n",
              "      <td>153.113190</td>\n",
              "      <td>162.573608</td>\n",
              "      <td>175.337585</td>\n",
              "      <td>178.690781</td>\n",
              "      <td>158.077209</td>\n",
              "      <td>104.029205</td>\n",
              "      <td>110.206001</td>\n",
              "      <td>109.088799</td>\n",
              "      <td>102.222397</td>\n",
              "      <td>95.389206</td>\n",
              "      <td>94.391602</td>\n",
              "      <td>93.996803</td>\n",
              "      <td>94.559601</td>\n",
              "      <td>107.915207</td>\n",
              "      <td>129.563202</td>\n",
              "      <td>159.199203</td>\n",
              "      <td>178.260010</td>\n",
              "      <td>183.127197</td>\n",
              "      <td>180.547623</td>\n",
              "      <td>163.152802</td>\n",
              "      <td>160.279999</td>\n",
              "      <td>146.557999</td>\n",
              "      <td>132.951996</td>\n",
              "      <td>126.831596</td>\n",
              "      <td>126.989594</td>\n",
              "      <td>123.266808</td>\n",
              "      <td>117.639198</td>\n",
              "      <td>140.495193</td>\n",
              "      <td>159.014008</td>\n",
              "      <td>167.802795</td>\n",
              "      <td>...</td>\n",
              "      <td>168.920013</td>\n",
              "      <td>170.106812</td>\n",
              "      <td>158.694809</td>\n",
              "      <td>137.566406</td>\n",
              "      <td>132.759995</td>\n",
              "      <td>129.907608</td>\n",
              "      <td>126.109192</td>\n",
              "      <td>121.960403</td>\n",
              "      <td>146.003601</td>\n",
              "      <td>156.335999</td>\n",
              "      <td>149.589996</td>\n",
              "      <td>141.298386</td>\n",
              "      <td>210.795197</td>\n",
              "      <td>194.146835</td>\n",
              "      <td>183.516785</td>\n",
              "      <td>177.357574</td>\n",
              "      <td>147.323212</td>\n",
              "      <td>135.304398</td>\n",
              "      <td>167.842392</td>\n",
              "      <td>178.950409</td>\n",
              "      <td>172.180008</td>\n",
              "      <td>164.500793</td>\n",
              "      <td>156.541992</td>\n",
              "      <td>167.556808</td>\n",
              "      <td>170.949203</td>\n",
              "      <td>171.318420</td>\n",
              "      <td>164.766800</td>\n",
              "      <td>160.703995</td>\n",
              "      <td>153.950394</td>\n",
              "      <td>147.480408</td>\n",
              "      <td>147.994019</td>\n",
              "      <td>148.225204</td>\n",
              "      <td>132.997208</td>\n",
              "      <td>129.938782</td>\n",
              "      <td>128.625595</td>\n",
              "      <td>127.014801</td>\n",
              "      <td>147.713196</td>\n",
              "      <td>152.120010</td>\n",
              "      <td>144.771622</td>\n",
              "      <td>139.509201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>130.0</td>\n",
              "      <td>59.711716</td>\n",
              "      <td>69.382256</td>\n",
              "      <td>71.189346</td>\n",
              "      <td>62.055153</td>\n",
              "      <td>54.573727</td>\n",
              "      <td>45.682842</td>\n",
              "      <td>45.789116</td>\n",
              "      <td>56.229824</td>\n",
              "      <td>96.186043</td>\n",
              "      <td>110.002846</td>\n",
              "      <td>113.864853</td>\n",
              "      <td>104.329239</td>\n",
              "      <td>92.165688</td>\n",
              "      <td>80.918823</td>\n",
              "      <td>71.573250</td>\n",
              "      <td>59.769230</td>\n",
              "      <td>64.275269</td>\n",
              "      <td>79.497047</td>\n",
              "      <td>87.575623</td>\n",
              "      <td>92.502487</td>\n",
              "      <td>97.672897</td>\n",
              "      <td>104.326149</td>\n",
              "      <td>114.377281</td>\n",
              "      <td>114.243309</td>\n",
              "      <td>114.327339</td>\n",
              "      <td>120.406868</td>\n",
              "      <td>128.421539</td>\n",
              "      <td>129.625793</td>\n",
              "      <td>59.667690</td>\n",
              "      <td>68.522133</td>\n",
              "      <td>71.492302</td>\n",
              "      <td>67.592667</td>\n",
              "      <td>63.025326</td>\n",
              "      <td>61.979172</td>\n",
              "      <td>68.539413</td>\n",
              "      <td>56.995743</td>\n",
              "      <td>93.790298</td>\n",
              "      <td>109.707932</td>\n",
              "      <td>114.692314</td>\n",
              "      <td>...</td>\n",
              "      <td>133.748413</td>\n",
              "      <td>133.111252</td>\n",
              "      <td>127.844734</td>\n",
              "      <td>120.231483</td>\n",
              "      <td>101.292305</td>\n",
              "      <td>76.048752</td>\n",
              "      <td>70.647568</td>\n",
              "      <td>74.673607</td>\n",
              "      <td>79.805206</td>\n",
              "      <td>101.831718</td>\n",
              "      <td>109.671013</td>\n",
              "      <td>112.149590</td>\n",
              "      <td>15.008995</td>\n",
              "      <td>13.306746</td>\n",
              "      <td>24.098225</td>\n",
              "      <td>43.848282</td>\n",
              "      <td>62.678108</td>\n",
              "      <td>55.033138</td>\n",
              "      <td>74.961662</td>\n",
              "      <td>141.112671</td>\n",
              "      <td>158.842606</td>\n",
              "      <td>165.581543</td>\n",
              "      <td>172.868408</td>\n",
              "      <td>166.202850</td>\n",
              "      <td>151.795502</td>\n",
              "      <td>146.748413</td>\n",
              "      <td>145.908173</td>\n",
              "      <td>140.184143</td>\n",
              "      <td>139.626740</td>\n",
              "      <td>136.643555</td>\n",
              "      <td>122.260834</td>\n",
              "      <td>92.346985</td>\n",
              "      <td>72.231483</td>\n",
              "      <td>61.084976</td>\n",
              "      <td>65.786507</td>\n",
              "      <td>71.467461</td>\n",
              "      <td>73.171364</td>\n",
              "      <td>94.525681</td>\n",
              "      <td>108.068642</td>\n",
              "      <td>111.479767</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Width           0           1  ...         781         782         783\n",
              "3   114.0  105.443214  116.850418  ...  165.128967  114.773460  136.356720\n",
              "14  131.0   86.885376   81.719368  ...    9.864518    2.186644    1.964280\n",
              "11  122.0  123.400162  124.483734  ...  174.037079  172.800323  173.433487\n",
              "20  200.0  163.311203  154.356796  ...  152.120010  144.771622  139.509201\n",
              "25  130.0   59.711716   69.382256  ...   94.525681  108.068642  111.479767\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "VaZPe_AxNBK9",
        "outputId": "e7fd50c3-33f0-4d0d-b30b-2a2d104c62dd"
      },
      "source": [
        "PSD_new.head()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Juntas</th>\n",
              "      <th>Area</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1.162</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Juntas   Area\n",
              "0       1  2.001\n",
              "1       2  0.820\n",
              "2       3  1.270\n",
              "3       4  0.958\n",
              "4       5  1.162"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "#lost_value = float(PSD_new.columns[1])\n",
        "\n",
        "# Area = np.array(PSD_new.iloc[:,1])\n",
        "Area = PSD_new['Area'].values\n",
        "# Area = np.concatenate( (Area, [lost_value] ) )\n",
        "# Area = np.concatenate( (Area, [lost_value] ) )\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfk_fNXGDK5_"
      },
      "source": [
        "wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        "wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        "X = pd.DataFrame([Diam1,Diameter_All])\n",
        "wts = pd.DataFrame([wt1,wt2])\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "OieAXw_by3nz",
        "outputId": "0859c428-2a80-45bf-b2fe-7bf5aebf2e2f"
      },
      "source": [
        "A = plt.hist(X,weights=wts,bins=7)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpdrvEySy8Ij"
      },
      "source": [
        "B = A[0][0]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUhGZHT8y9Or",
        "outputId": "abdc4237-b0c0-42a8-ea35-ec60c3ec3d24"
      },
      "source": [
        "Novo = []\n",
        "k = 0\n",
        "soma = 0\n",
        "for i in B:\n",
        "  if(k<4):\n",
        "    Novo.append(i)\n",
        "  else:\n",
        "    soma = soma + i\n",
        "  k = k + 1\n",
        "Novo.append(soma)\n",
        "print(Novo)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14.736842105263156, 24.2105263157895, 42.1052631578948, 14.736842105263179, 4.21052631578948]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "yMK89w-fzCVe",
        "outputId": "7361ee1f-be8e-44f0-af9d-62664cd4142f"
      },
      "source": [
        "# Freq1 = [19.12043703, 29.22484843, 19.35872174, 20.82190224, 11.47409056] # avarage 4 samples\n",
        "Freq1 = [20.69301557, 28.55598044, 18.50768331, 22.7106327, 8.905907357] # avarage 10 samples\n",
        "#Freq2 = [16.93792791, 31.38008965, 24.93810752, 18.56158392, 6.233810752, 0.4]\n",
        "Freq2 = [16.93792791, 31.38008965, 24.93810752, 18.56158392, 6.633810752]\n",
        "Freq3 = Novo\n",
        "barWidth = 0.25\n",
        "\n",
        "br1 = range(len(Freq1))\n",
        "# Set position of bar on X axis\n",
        "br2 = [x + barWidth for x in br1]\n",
        "br3 = [x + barWidth for x in br2]\n",
        "# labels = [0.8, 1.0, 1.2, 1.4, 1.6, 1.8]\n",
        "labels = [0.8, 1.0, 1.2, 1.4, 1.6]\n",
        "\n",
        "xx=[]\n",
        "for a in labels:\n",
        "  xx.append(str(a))\n",
        "plt.bar(br1, Freq1 , color=\"green\", align=\"center\", width=0.3, tick_label= xx) \n",
        "plt.bar(br2, Freq2 , color=\"red\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.bar(br3, Freq3 , color=\"blue\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.legend(['CNN 1','CNN 2','True'])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f2b1fef0210>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}