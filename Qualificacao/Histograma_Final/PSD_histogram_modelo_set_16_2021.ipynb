{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_modelo_set_16_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/Qualificacao/Histograma_Final/PSD_histogram_modelo_set_16_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from sklearn.metrics import r2_score\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZEvJvfoibE4",
        "outputId": "b89b8483-6bd9-4449-a045-352c9722bc24"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mahotas\n",
            "  Downloading mahotas-1.4.11-cp37-cp37m-manylinux2010_x86_64.whl (5.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7 MB 19.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.19.5)\n",
            "Installing collected packages: mahotas\n",
            "Successfully installed mahotas-1.4.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT"
      },
      "source": [
        "import mahotas.features.texture as mht\n",
        "import mahotas.features"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VcTdaNVh9EE",
        "outputId": "497fd941-9ec1-4ec6-fd8b-85f6d3ebeac0"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marquesgabi_fev_2020'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 73 (delta 37), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n",
            "/content/marquesgabi_fev_2020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v7SRrc8mH2N",
        "outputId": "adfb0795-46b0-4638-a6a8-656f42a9809f"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip' \n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Doutorado'...\n",
            "remote: Enumerating objects: 464, done.\u001b[K\n",
            "remote: Counting objects: 100% (214/214), done.\u001b[K\n",
            "remote: Compressing objects: 100% (213/213), done.\u001b[K\n",
            "remote: Total 464 (delta 102), reused 0 (delta 0), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (464/464), 166.11 MiB | 28.55 MiB/s, done.\n",
            "Resolving deltas: 100% (225/225), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqIYzUcnrdMp",
        "outputId": "e4911d73-3f6b-4053-9b78-4272a1d194d7"
      },
      "source": [
        "labels =[]\n",
        "with zipfile.ZipFile(Transfere, \"r\") as f:\n",
        "  for f in f.namelist():\n",
        "    labels.append(f)\n",
        "print(labels)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Fotos_Grandes-3cdAmostra/Q6-8-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-4.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[4] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHgqAnaFyCjp",
        "outputId": "a52b9855-3ff0-4d75-bed0-8370c8df9644"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (160/160), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 3 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 23.59 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from segment_filter_not_conclude import Segmenta  # got image provided segmented"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN5MN5a_v4np",
        "outputId": "79cf2281-42cf-4aa7-dc76-ba46713b8bf2"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     130  180.083099  170.937515  ...  123.247337  122.827461  123.262505\n",
            "1     165  169.908539  150.421600  ...    1.113756    0.069605    1.198494\n",
            "2     141  178.890167  181.855606  ...  112.517487  127.235909  126.649002\n",
            "3     160  136.851868  139.228119  ...  120.545624  119.625626  112.108749\n",
            "4     140  133.919998  143.839996  ...    1.000000    1.000000    1.000000\n",
            "5     150  131.547379  130.471100  ...  148.852798  148.707001  164.108261\n",
            "6     113  124.329788  131.803970  ...    0.006578    0.795912    1.556974\n",
            "7     122  154.918289  165.894913  ...    1.000000    1.000000    1.000000\n",
            "8     157  102.376007  105.307724  ...    0.534991    0.458558    1.388494\n",
            "9     127  104.461220   85.215385  ...   97.390656  107.601212  120.162193\n",
            "10    175  166.030396  168.292786  ...  148.793594  150.563202  141.174393\n",
            "11    190  168.370834  167.573517  ...  167.473114  175.338287  174.412628\n",
            "12    120  134.966675  136.560013  ...  123.374451  131.222229  134.295547\n",
            "13    140  170.279999  164.039993  ...  113.000000  109.799995  108.639999\n",
            "14    168  148.805557  144.805557  ...    2.166667    0.777778    1.000000\n",
            "15    165  149.655655  172.629456  ...  124.686035   23.849623    1.180312\n",
            "16    167  106.459946  102.269180  ...  153.562057  146.242188  145.883545\n",
            "17    176   98.650826   97.793907  ...   95.783562  155.018585  139.038742\n",
            "18    175  176.633591  180.470383  ...  166.395203  158.460800  161.052795\n",
            "19    154  149.694214  136.735550  ...    0.396694    1.487603    1.305785\n",
            "20    145  136.985336  143.950211  ...  148.140869  146.600189  152.471588\n",
            "21    141  133.371719  146.935410  ...  135.735275  138.523026  147.550827\n",
            "22    170  155.567764  160.808441  ...  203.070312  218.095932  228.282776\n",
            "23    169  204.404266  214.641891  ...    0.916845    0.189174    1.358811\n",
            "24    139  150.623367  144.738983  ...   59.344490   21.496611    3.889550\n",
            "25    173  184.024963  183.362869  ...  151.212357  126.699295  131.399475\n",
            "26    132   46.471992   43.755741  ...  125.836540  121.083572  116.361809\n",
            "27    200  109.812408  101.194412  ...    1.499600    0.216400    1.282400\n",
            "28    145  176.214325  170.780121  ...  137.742737  112.685944  114.526329\n",
            "29    181  124.678604  119.580788  ...    2.208968    1.425628    0.570953\n",
            "30    157  118.663757   59.590855  ...  132.100815  130.845062  130.620651\n",
            "31    106  183.466354  185.571030  ...  206.728729  210.380219  214.007828\n",
            "32    200  194.652008  184.758774  ...  164.635590  148.369598  136.363602\n",
            "33    142  140.025589  165.787552  ...  174.664948  172.212067  167.345963\n",
            "34    110    0.823802    0.004628  ...  169.261139  236.299820  251.050903\n",
            "35    156   88.911896   84.449051  ...    0.698882    0.288626    1.391190\n",
            "36    200  167.239197  171.084396  ...  103.221611  154.930008  230.045609\n",
            "37    170  176.743683  174.398758  ...  192.719193  196.912811  209.904938\n",
            "38    152    1.785319    0.890582  ...  134.605957  153.004837  161.412735\n",
            "39    189  142.342941  108.998627  ...  159.325104  143.986267  142.729767\n",
            "40    102  138.244919  140.229553  ...  166.271057  175.054993  156.642838\n",
            "41    133  165.795013  153.706375  ...  135.634354  141.631577  143.963989\n",
            "42    129   73.318611   81.984428  ...   12.578812   11.369629   17.471186\n",
            "43    164  113.139793  102.841766  ...  146.572281  146.459244  150.046997\n",
            "44    103  115.599953  121.040993  ...  152.768677  157.211334  156.479523\n",
            "45    103  169.603149  165.061554  ...  145.849182  153.426514  149.853699\n",
            "46    105  147.919998  134.320007  ...   73.520004   59.506672   10.555557\n",
            "47    114  112.253006  128.840256  ...  105.482307  107.098183  126.027084\n",
            "48    186  120.836861  117.621933  ...   97.665634  101.789124  102.051346\n",
            "49    113  187.620728  162.600906  ...  126.472549  119.198288  111.544441\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2emP4rNjQy",
        "outputId": "27fd8b13-b619-49db-cbad-98fa2175b1c8"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (160/160), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 3 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 23.59 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIFPGE_-vx3T"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "Description = '3 layers of Convolution: 64, 128, 256 '\n",
        "N1 = 20\n",
        "N2 = 20\n",
        "\n",
        "# make the CNN\n",
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=64, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=N1))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=N2))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbPQ1FSRG6A",
        "outputId": "19f5ff35-4415-435a-9386-235cacbfd3ff"
      },
      "source": [
        "\n",
        "# training the model\n",
        "r = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), \n",
        "              epochs=200, batch_size=32)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 3s 135ms/step - loss: 0.6138 - accuracy: 0.7201 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 1s 114ms/step - loss: 0.2280 - accuracy: 0.8950 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.1683 - accuracy: 0.9300 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1359 - accuracy: 0.9359 - val_loss: 0.6928 - val_accuracy: 0.5102\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 1s 115ms/step - loss: 0.0674 - accuracy: 0.9854 - val_loss: 0.6928 - val_accuracy: 0.5102\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0572 - accuracy: 0.9825 - val_loss: 0.6927 - val_accuracy: 0.5102\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 1s 114ms/step - loss: 0.0669 - accuracy: 0.9796 - val_loss: 0.6926 - val_accuracy: 0.5102\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0397 - accuracy: 0.9913 - val_loss: 0.6927 - val_accuracy: 0.5102\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0288 - accuracy: 0.9913 - val_loss: 0.6935 - val_accuracy: 0.5102\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 1s 115ms/step - loss: 0.0225 - accuracy: 0.9913 - val_loss: 0.6927 - val_accuracy: 0.5102\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.6926 - val_accuracy: 0.5102\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0128 - accuracy: 0.9971 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 1s 114ms/step - loss: 0.0051 - accuracy: 0.9971 - val_loss: 0.6934 - val_accuracy: 0.5102\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0075 - accuracy: 0.9971 - val_loss: 0.6954 - val_accuracy: 0.5102\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.6983 - val_accuracy: 0.5102\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0083 - accuracy: 0.9942 - val_loss: 0.6960 - val_accuracy: 0.5102\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 1s 115ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.6996 - val_accuracy: 0.5102\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.7032 - val_accuracy: 0.5102\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.7128 - val_accuracy: 0.5102\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.7168 - val_accuracy: 0.5102\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.7189 - val_accuracy: 0.5102\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 1s 115ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.7205 - val_accuracy: 0.5102\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 1s 115ms/step - loss: 9.2212e-04 - accuracy: 1.0000 - val_loss: 0.7235 - val_accuracy: 0.5102\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 1s 115ms/step - loss: 5.6605e-04 - accuracy: 1.0000 - val_loss: 0.7312 - val_accuracy: 0.5102\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 1s 115ms/step - loss: 7.7012e-04 - accuracy: 1.0000 - val_loss: 0.7391 - val_accuracy: 0.5102\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 1s 115ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.7710 - val_accuracy: 0.5102\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.7759 - val_accuracy: 0.5102\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7711 - val_accuracy: 0.5102\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7761 - val_accuracy: 0.5102\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 5.9738e-04 - accuracy: 1.0000 - val_loss: 0.7912 - val_accuracy: 0.5102\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 1s 115ms/step - loss: 3.9309e-04 - accuracy: 1.0000 - val_loss: 0.7965 - val_accuracy: 0.5102\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 6.5351e-04 - accuracy: 1.0000 - val_loss: 0.7713 - val_accuracy: 0.5102\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 1s 115ms/step - loss: 2.8244e-04 - accuracy: 1.0000 - val_loss: 0.7525 - val_accuracy: 0.5102\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0067 - accuracy: 0.9971 - val_loss: 0.8480 - val_accuracy: 0.5102\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.9113 - val_accuracy: 0.5102\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.7178 - val_accuracy: 0.5102\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 7.0412e-04 - accuracy: 1.0000 - val_loss: 0.6922 - val_accuracy: 0.5102\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.8169 - val_accuracy: 0.5102\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 4.5256e-04 - accuracy: 1.0000 - val_loss: 0.9045 - val_accuracy: 0.5102\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 1s 115ms/step - loss: 3.9352e-04 - accuracy: 1.0000 - val_loss: 0.9466 - val_accuracy: 0.5102\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.1052 - val_accuracy: 0.5102\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.9121 - val_accuracy: 0.5102\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.8859 - val_accuracy: 0.5102\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 7.6204e-04 - accuracy: 1.0000 - val_loss: 1.0265 - val_accuracy: 0.5102\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 1s 115ms/step - loss: 6.9834e-04 - accuracy: 1.0000 - val_loss: 1.1142 - val_accuracy: 0.5102\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 7.9453e-04 - accuracy: 1.0000 - val_loss: 1.1644 - val_accuracy: 0.5102\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 1s 115ms/step - loss: 2.1741e-04 - accuracy: 1.0000 - val_loss: 1.1792 - val_accuracy: 0.5102\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 1s 115ms/step - loss: 7.4411e-04 - accuracy: 1.0000 - val_loss: 1.2140 - val_accuracy: 0.5102\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 1.8350e-04 - accuracy: 1.0000 - val_loss: 1.2159 - val_accuracy: 0.5102\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 2.1036e-04 - accuracy: 1.0000 - val_loss: 1.1391 - val_accuracy: 0.5102\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 1.9788e-04 - accuracy: 1.0000 - val_loss: 1.0009 - val_accuracy: 0.5102\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 2.7778e-04 - accuracy: 1.0000 - val_loss: 1.0543 - val_accuracy: 0.5102\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 1s 115ms/step - loss: 2.6729e-04 - accuracy: 1.0000 - val_loss: 1.0319 - val_accuracy: 0.5102\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 9.0922e-05 - accuracy: 1.0000 - val_loss: 0.9635 - val_accuracy: 0.5238\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 1.7577e-04 - accuracy: 1.0000 - val_loss: 0.7059 - val_accuracy: 0.5986\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 1.5784e-04 - accuracy: 1.0000 - val_loss: 0.5003 - val_accuracy: 0.6871\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 1.0039e-04 - accuracy: 1.0000 - val_loss: 0.4221 - val_accuracy: 0.7483\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 3.5029e-04 - accuracy: 1.0000 - val_loss: 0.3505 - val_accuracy: 0.8163\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 3.6571e-04 - accuracy: 1.0000 - val_loss: 0.2563 - val_accuracy: 0.8980\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 1.4500e-04 - accuracy: 1.0000 - val_loss: 0.2394 - val_accuracy: 0.8912\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 8.9402e-05 - accuracy: 1.0000 - val_loss: 0.2191 - val_accuracy: 0.8980\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 1.1361e-04 - accuracy: 1.0000 - val_loss: 0.1958 - val_accuracy: 0.9048\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 1.7221e-04 - accuracy: 1.0000 - val_loss: 0.1786 - val_accuracy: 0.9252\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 9.5593e-05 - accuracy: 1.0000 - val_loss: 0.1605 - val_accuracy: 0.9456\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 1s 115ms/step - loss: 7.5578e-04 - accuracy: 1.0000 - val_loss: 0.3203 - val_accuracy: 0.8503\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 8.4296e-05 - accuracy: 1.0000 - val_loss: 0.4325 - val_accuracy: 0.8027\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 1.4532e-04 - accuracy: 1.0000 - val_loss: 0.4320 - val_accuracy: 0.7891\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 7.6479e-05 - accuracy: 1.0000 - val_loss: 0.3483 - val_accuracy: 0.8367\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 1s 115ms/step - loss: 1.5836e-04 - accuracy: 1.0000 - val_loss: 0.2239 - val_accuracy: 0.9116\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 6.5750e-04 - accuracy: 1.0000 - val_loss: 0.1930 - val_accuracy: 0.9456\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 3.8993e-04 - accuracy: 1.0000 - val_loss: 0.3613 - val_accuracy: 0.8435\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 2.8008e-04 - accuracy: 1.0000 - val_loss: 0.3295 - val_accuracy: 0.8367\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 1.4327e-04 - accuracy: 1.0000 - val_loss: 0.7903 - val_accuracy: 0.6190\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 5.2633e-04 - accuracy: 1.0000 - val_loss: 1.2197 - val_accuracy: 0.5374\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 4.1718e-04 - accuracy: 1.0000 - val_loss: 1.1082 - val_accuracy: 0.5714\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 1.7600e-04 - accuracy: 1.0000 - val_loss: 0.6168 - val_accuracy: 0.6803\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 1.0276e-04 - accuracy: 1.0000 - val_loss: 0.6229 - val_accuracy: 0.7143\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 1.3432e-04 - accuracy: 1.0000 - val_loss: 0.3700 - val_accuracy: 0.8299\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 1.4690e-04 - accuracy: 1.0000 - val_loss: 0.5017 - val_accuracy: 0.7891\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 7.6286e-05 - accuracy: 1.0000 - val_loss: 0.6623 - val_accuracy: 0.7551\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 1.0139e-04 - accuracy: 1.0000 - val_loss: 0.6050 - val_accuracy: 0.7755\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 7.8238e-05 - accuracy: 1.0000 - val_loss: 0.4686 - val_accuracy: 0.8095\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 1.7135e-04 - accuracy: 1.0000 - val_loss: 0.3441 - val_accuracy: 0.8435\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 4.2984e-05 - accuracy: 1.0000 - val_loss: 0.4017 - val_accuracy: 0.8776\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 1.1529e-04 - accuracy: 1.0000 - val_loss: 0.4111 - val_accuracy: 0.8912\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 8.4719e-05 - accuracy: 1.0000 - val_loss: 0.2950 - val_accuracy: 0.9184\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 1.3469 - val_accuracy: 0.7075\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 22.5476 - val_accuracy: 0.5102\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0388 - accuracy: 0.9796 - val_loss: 66.0602 - val_accuracy: 0.5102\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.1130 - accuracy: 0.9738 - val_loss: 211.1935 - val_accuracy: 0.5102\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0942 - accuracy: 0.9738 - val_loss: 474.6678 - val_accuracy: 0.5102\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 0.1418 - accuracy: 0.9592 - val_loss: 336.6022 - val_accuracy: 0.5102\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0497 - accuracy: 0.9767 - val_loss: 110.4854 - val_accuracy: 0.5102\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0676 - accuracy: 0.9738 - val_loss: 310.7746 - val_accuracy: 0.5102\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0325 - accuracy: 0.9913 - val_loss: 197.7213 - val_accuracy: 0.5102\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0196 - accuracy: 0.9942 - val_loss: 142.3019 - val_accuracy: 0.5102\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 1s 116ms/step - loss: 0.0107 - accuracy: 0.9971 - val_loss: 157.2850 - val_accuracy: 0.5102\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 157.7921 - val_accuracy: 0.5102\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 147.4789 - val_accuracy: 0.5102\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 134.3633 - val_accuracy: 0.5102\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 132.3550 - val_accuracy: 0.5102\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 5.9139e-04 - accuracy: 1.0000 - val_loss: 123.3683 - val_accuracy: 0.5102\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 9.4071e-04 - accuracy: 1.0000 - val_loss: 109.3367 - val_accuracy: 0.5102\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 91.5598 - val_accuracy: 0.5102\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 77.2577 - val_accuracy: 0.5102\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 6.1607e-04 - accuracy: 1.0000 - val_loss: 63.1300 - val_accuracy: 0.5102\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 3.1949e-04 - accuracy: 1.0000 - val_loss: 55.0220 - val_accuracy: 0.5102\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 39.0308 - val_accuracy: 0.5102\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 8.2844e-04 - accuracy: 1.0000 - val_loss: 30.6525 - val_accuracy: 0.5102\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 9.1177e-04 - accuracy: 1.0000 - val_loss: 19.8697 - val_accuracy: 0.5102\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 4.9318e-04 - accuracy: 1.0000 - val_loss: 17.7723 - val_accuracy: 0.5102\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 1.5882e-04 - accuracy: 1.0000 - val_loss: 16.7915 - val_accuracy: 0.5102\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 4.4295e-04 - accuracy: 1.0000 - val_loss: 14.4778 - val_accuracy: 0.5102\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 2.0786e-04 - accuracy: 1.0000 - val_loss: 12.1070 - val_accuracy: 0.5102\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 7.0009e-04 - accuracy: 1.0000 - val_loss: 9.0207 - val_accuracy: 0.5102\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 7.6836e-05 - accuracy: 1.0000 - val_loss: 7.4594 - val_accuracy: 0.5102\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 1.8824e-04 - accuracy: 1.0000 - val_loss: 6.8365 - val_accuracy: 0.5170\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 2.4343e-04 - accuracy: 1.0000 - val_loss: 6.2103 - val_accuracy: 0.5170\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 1.1944e-04 - accuracy: 1.0000 - val_loss: 5.5518 - val_accuracy: 0.5170\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 2.6281e-04 - accuracy: 1.0000 - val_loss: 4.6310 - val_accuracy: 0.5238\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 3.1795e-04 - accuracy: 1.0000 - val_loss: 3.4762 - val_accuracy: 0.5510\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 3.2415e-04 - accuracy: 1.0000 - val_loss: 2.6705 - val_accuracy: 0.5782\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 4.5050e-04 - accuracy: 1.0000 - val_loss: 2.3333 - val_accuracy: 0.6122\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 1.0576e-04 - accuracy: 1.0000 - val_loss: 2.0613 - val_accuracy: 0.6122\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 2.3521e-04 - accuracy: 1.0000 - val_loss: 1.9555 - val_accuracy: 0.6327\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 4.1729e-04 - accuracy: 1.0000 - val_loss: 2.1936 - val_accuracy: 0.6122\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 1.0532e-04 - accuracy: 1.0000 - val_loss: 3.1926 - val_accuracy: 0.5850\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 3.9826e-04 - accuracy: 1.0000 - val_loss: 6.5573 - val_accuracy: 0.5102\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 9.2856e-05 - accuracy: 1.0000 - val_loss: 10.1104 - val_accuracy: 0.5102\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 3.1644e-04 - accuracy: 1.0000 - val_loss: 11.6736 - val_accuracy: 0.5102\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 1.9924e-04 - accuracy: 1.0000 - val_loss: 11.4673 - val_accuracy: 0.5102\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 1.9023e-04 - accuracy: 1.0000 - val_loss: 9.7908 - val_accuracy: 0.5102\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 1.1718e-04 - accuracy: 1.0000 - val_loss: 8.5636 - val_accuracy: 0.5102\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 1.7372e-04 - accuracy: 1.0000 - val_loss: 8.0352 - val_accuracy: 0.5102\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 1.1695e-04 - accuracy: 1.0000 - val_loss: 7.2771 - val_accuracy: 0.5102\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 7.1401e-04 - accuracy: 1.0000 - val_loss: 10.6243 - val_accuracy: 0.5102\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 4.1547e-04 - accuracy: 1.0000 - val_loss: 11.0869 - val_accuracy: 0.5102\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 9.4712e-05 - accuracy: 1.0000 - val_loss: 10.0631 - val_accuracy: 0.5102\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 1.5949e-04 - accuracy: 1.0000 - val_loss: 9.2192 - val_accuracy: 0.5102\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 8.4815e-05 - accuracy: 1.0000 - val_loss: 7.7916 - val_accuracy: 0.5102\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 4.4026e-04 - accuracy: 1.0000 - val_loss: 6.2379 - val_accuracy: 0.5170\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 1.2400e-04 - accuracy: 1.0000 - val_loss: 4.3500 - val_accuracy: 0.5442\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 2.8907e-04 - accuracy: 1.0000 - val_loss: 1.3240 - val_accuracy: 0.7007\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 2.4693e-04 - accuracy: 1.0000 - val_loss: 0.6839 - val_accuracy: 0.7823\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 1s 117ms/step - loss: 3.3849e-05 - accuracy: 1.0000 - val_loss: 0.2154 - val_accuracy: 0.9048\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 7.6770e-05 - accuracy: 1.0000 - val_loss: 0.1872 - val_accuracy: 0.9320\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 7.1777e-05 - accuracy: 1.0000 - val_loss: 0.1597 - val_accuracy: 0.9456\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 8.8039e-05 - accuracy: 1.0000 - val_loss: 0.1345 - val_accuracy: 0.9524\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 1.0403e-04 - accuracy: 1.0000 - val_loss: 0.2384 - val_accuracy: 0.9524\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 1.4045e-04 - accuracy: 1.0000 - val_loss: 0.3299 - val_accuracy: 0.9524\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 2.3424e-04 - accuracy: 1.0000 - val_loss: 0.3004 - val_accuracy: 0.9524\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 8.6859e-05 - accuracy: 1.0000 - val_loss: 0.1886 - val_accuracy: 0.9592\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 3.6274e-05 - accuracy: 1.0000 - val_loss: 0.1695 - val_accuracy: 0.9524\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 3.7511e-04 - accuracy: 1.0000 - val_loss: 0.1853 - val_accuracy: 0.9524\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 1.3775e-04 - accuracy: 1.0000 - val_loss: 0.2167 - val_accuracy: 0.9524\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 1s 120ms/step - loss: 1.5255e-04 - accuracy: 1.0000 - val_loss: 0.2209 - val_accuracy: 0.9456\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 1.9180e-05 - accuracy: 1.0000 - val_loss: 0.2247 - val_accuracy: 0.9524\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 8.2903e-05 - accuracy: 1.0000 - val_loss: 0.2304 - val_accuracy: 0.9592\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 1s 118ms/step - loss: 4.1035e-05 - accuracy: 1.0000 - val_loss: 0.2434 - val_accuracy: 0.9524\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 1s 119ms/step - loss: 7.3225e-05 - accuracy: 1.0000 - val_loss: 0.2529 - val_accuracy: 0.9524\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 1.0226e-04 - accuracy: 1.0000 - val_loss: 0.2466 - val_accuracy: 0.9524\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 7.9650e-05 - accuracy: 1.0000 - val_loss: 0.3190 - val_accuracy: 0.9660\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 7.1038e-05 - accuracy: 1.0000 - val_loss: 0.4323 - val_accuracy: 0.9252\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 5.6112e-05 - accuracy: 1.0000 - val_loss: 0.4639 - val_accuracy: 0.9048\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 9.4477e-05 - accuracy: 1.0000 - val_loss: 0.4574 - val_accuracy: 0.9116\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 4.0880e-05 - accuracy: 1.0000 - val_loss: 0.3796 - val_accuracy: 0.9388\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 1.6735e-05 - accuracy: 1.0000 - val_loss: 0.3285 - val_accuracy: 0.9592\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 4.3589e-05 - accuracy: 1.0000 - val_loss: 0.3047 - val_accuracy: 0.9660\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 2.6814e-05 - accuracy: 1.0000 - val_loss: 0.2938 - val_accuracy: 0.9660\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 5.2598e-05 - accuracy: 1.0000 - val_loss: 0.2851 - val_accuracy: 0.9660\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 3.4975e-05 - accuracy: 1.0000 - val_loss: 0.2778 - val_accuracy: 0.9660\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 9.1707e-05 - accuracy: 1.0000 - val_loss: 0.2901 - val_accuracy: 0.9660\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 4.1290e-05 - accuracy: 1.0000 - val_loss: 0.2578 - val_accuracy: 0.9660\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 2.8501e-05 - accuracy: 1.0000 - val_loss: 0.2187 - val_accuracy: 0.9796\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 2.4523e-04 - accuracy: 1.0000 - val_loss: 1.2708 - val_accuracy: 0.7007\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 1.3391e-04 - accuracy: 1.0000 - val_loss: 3.2197 - val_accuracy: 0.5170\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 4.1632e-04 - accuracy: 1.0000 - val_loss: 4.0911 - val_accuracy: 0.5102\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 9.5545e-05 - accuracy: 1.0000 - val_loss: 4.2101 - val_accuracy: 0.5102\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 3.4035e-05 - accuracy: 1.0000 - val_loss: 3.9097 - val_accuracy: 0.5102\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 1.0757e-04 - accuracy: 1.0000 - val_loss: 3.1807 - val_accuracy: 0.5306\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 2.0129e-04 - accuracy: 1.0000 - val_loss: 4.8408 - val_accuracy: 0.5102\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 4.5135e-05 - accuracy: 1.0000 - val_loss: 6.0590 - val_accuracy: 0.5102\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 2.3652e-05 - accuracy: 1.0000 - val_loss: 5.5399 - val_accuracy: 0.5170\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 3.1076e-05 - accuracy: 1.0000 - val_loss: 4.5204 - val_accuracy: 0.5238\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 1.7490e-05 - accuracy: 1.0000 - val_loss: 3.3930 - val_accuracy: 0.5646\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 3.2674e-05 - accuracy: 1.0000 - val_loss: 2.2600 - val_accuracy: 0.6667\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 3.1847e-05 - accuracy: 1.0000 - val_loss: 1.4497 - val_accuracy: 0.7415\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 3.3529e-05 - accuracy: 1.0000 - val_loss: 0.8552 - val_accuracy: 0.8163\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 1s 122ms/step - loss: 1.5718e-05 - accuracy: 1.0000 - val_loss: 0.5515 - val_accuracy: 0.8639\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 2.8060e-05 - accuracy: 1.0000 - val_loss: 0.4873 - val_accuracy: 0.8844\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 1s 124ms/step - loss: 1.1474e-04 - accuracy: 1.0000 - val_loss: 0.9289 - val_accuracy: 0.8027\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 7.0893e-05 - accuracy: 1.0000 - val_loss: 1.0953 - val_accuracy: 0.7891\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 1s 125ms/step - loss: 2.2158e-05 - accuracy: 1.0000 - val_loss: 0.6204 - val_accuracy: 0.8707\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 1.4398e-05 - accuracy: 1.0000 - val_loss: 0.3662 - val_accuracy: 0.9184\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 1s 126ms/step - loss: 3.4846e-05 - accuracy: 1.0000 - val_loss: 0.2457 - val_accuracy: 0.9456\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 4.8451e-05 - accuracy: 1.0000 - val_loss: 0.2083 - val_accuracy: 0.9660\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 1s 127ms/step - loss: 1.4572e-04 - accuracy: 1.0000 - val_loss: 3.7241 - val_accuracy: 0.5714\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 2.7669e-05 - accuracy: 1.0000 - val_loss: 4.8808 - val_accuracy: 0.5238\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 3.4112e-05 - accuracy: 1.0000 - val_loss: 4.7160 - val_accuracy: 0.5306\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 2s 135ms/step - loss: 7.3629e-05 - accuracy: 1.0000 - val_loss: 3.9758 - val_accuracy: 0.5578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDVY6HbxMOlH",
        "outputId": "1eba6721-bca4-48b4-c596-6fb1ef15ecf0"
      },
      "source": [
        "# pred_test= model.predict_classes(X_test)\n",
        "pred_test = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict  0   1\n",
            "Actual        \n",
            "0        7  65\n",
            "1        0  75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv5I61yhPQmk",
        "outputId": "89800ea7-2874-44d3-9707-60d369a7e4bc"
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[4] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  #prediction = model.predict_classes(result)\n",
        "  prediction= np.argmax(model.predict(result), axis=-1)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "5   131.0    1.900647    5.351377  ...   69.508301   70.866028   71.179298\n",
            "27  138.0  160.164658  167.590836  ...  120.253098  126.169083  131.085281\n",
            "31  193.0  113.704910   96.307556  ...  146.392731  132.145630  145.295380\n",
            "39  169.0   38.455727   78.680008  ...  102.934311  111.168686  106.527809\n",
            "43  146.0  126.877838  124.163261  ...  193.833191  197.535751  200.228546\n",
            "46  146.0    0.228561    0.694314  ...    5.383937   25.614752   98.324448\n",
            "42  115.0    1.762873    1.223743  ...  166.174362  115.848305   52.776260\n",
            "3   125.0   83.288132   83.564613  ...   26.623362   21.204098    4.328320\n",
            "14  154.0   56.876030   58.842976  ...   91.041321   97.033066  118.884300\n",
            "32  158.0  113.792488  106.488861  ...  178.006714  170.581329  165.160721\n",
            "43  180.0   78.416801   78.205437  ...    0.513086    1.660247    0.723457\n",
            "12  132.0  228.003693  242.453644  ...   78.172646   77.562912   74.463730\n",
            "26  100.0  177.913605  179.644791  ...  201.460800  226.104004  250.089600\n",
            "30  132.0    0.032140    0.971534  ...   63.386597   60.416901   65.045921\n",
            "7   124.0    0.925078    0.029136  ...   75.720078   58.619141   63.680538\n",
            "28  139.0  197.543396  192.570450  ...   93.664825  101.941460  107.732040\n",
            "34  132.0  176.837479  177.958694  ...  250.704330  253.495880  252.970612\n",
            "44  129.0  174.898560  171.131958  ...  138.609756  124.656219  120.520515\n",
            "4   140.0  164.279999  182.720001  ...  153.919998  161.720001  172.800003\n",
            "20  171.0  195.808807  208.153015  ...  207.700378  206.799896  205.259796\n",
            "28  106.0    0.054824    0.958348  ...   34.108582   38.837311   45.339981\n",
            "33  108.0    0.740741    1.444444  ...  180.482849  189.705063  203.807953\n",
            "44  139.0  144.094925  154.285706  ...  210.681641  206.090118  174.593506\n",
            "12  102.0  201.193024  204.377167  ...  118.058838  119.558640  118.196083\n",
            "48  104.0  205.245590  199.008911  ...   96.875740  156.800308  230.838776\n",
            "\n",
            "[25 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRbWgmX_LFH",
        "outputId": "bc08ef59-4d52-471e-bcda-7dc61b0a4f22"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "from GetBetterSegm import GetBetter"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marquesgabi_paper_fev_2021'...\n",
            "remote: Enumerating objects: 707, done.\u001b[K\n",
            "remote: Counting objects: 100% (468/468), done.\u001b[K\n",
            "remote: Compressing objects: 100% (466/466), done.\u001b[K\n",
            "remote: Total 707 (delta 296), reused 0 (delta 0), pack-reused 239\u001b[K\n",
            "Receiving objects: 100% (707/707), 5.76 MiB | 12.03 MiB/s, done.\n",
            "Resolving deltas: 100% (433/433), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAG_I6FwCvFr",
        "outputId": "98b130eb-ccdd-4dbe-a05d-4a895072b01b"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "#!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd marquesgabi_out_2020\n",
        "#%cd Doutorado\n",
        "#PSD_imageJ = 'Amostra7.csv' \n",
        "#PSD_new = pd.read_csv(PSD_imageJ,sep=';')\n",
        "#encoding='utf8'\n",
        "\n",
        "PSD_imageJ = 'Areas_ImageJ.csv'\n",
        "PSD_new = pd.read_csv(PSD_imageJ)\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marquesgabi_out_2020'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 146 (delta 75), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (146/146), 1.00 MiB | 12.84 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020\n",
            "   Juntas   Area\n",
            "0       1  2.001\n",
            "1       2  0.820\n",
            "2       3  1.270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tEPjIBnv_xM",
        "outputId": "60ab3d89-bb55-407b-b732-a95af29b6ad0"
      },
      "source": [
        "PSD_new.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(95, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "PekBHQOT_6CP",
        "outputId": "47e99a5d-bee0-4075-b289-b104a879181e"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>131.0</td>\n",
              "      <td>1.900647</td>\n",
              "      <td>5.351377</td>\n",
              "      <td>39.328129</td>\n",
              "      <td>60.055359</td>\n",
              "      <td>66.521118</td>\n",
              "      <td>63.825588</td>\n",
              "      <td>64.221718</td>\n",
              "      <td>68.845581</td>\n",
              "      <td>68.285233</td>\n",
              "      <td>62.429173</td>\n",
              "      <td>67.306801</td>\n",
              "      <td>99.306915</td>\n",
              "      <td>114.747101</td>\n",
              "      <td>120.436737</td>\n",
              "      <td>124.337326</td>\n",
              "      <td>128.893707</td>\n",
              "      <td>138.824554</td>\n",
              "      <td>154.992126</td>\n",
              "      <td>166.247177</td>\n",
              "      <td>167.044342</td>\n",
              "      <td>170.399841</td>\n",
              "      <td>167.387329</td>\n",
              "      <td>162.154007</td>\n",
              "      <td>138.619949</td>\n",
              "      <td>115.333893</td>\n",
              "      <td>111.805077</td>\n",
              "      <td>125.813934</td>\n",
              "      <td>132.753983</td>\n",
              "      <td>0.952683</td>\n",
              "      <td>7.272944</td>\n",
              "      <td>37.893711</td>\n",
              "      <td>55.467045</td>\n",
              "      <td>57.585625</td>\n",
              "      <td>53.196724</td>\n",
              "      <td>46.302486</td>\n",
              "      <td>47.467800</td>\n",
              "      <td>51.626652</td>\n",
              "      <td>48.179302</td>\n",
              "      <td>77.657478</td>\n",
              "      <td>...</td>\n",
              "      <td>151.806808</td>\n",
              "      <td>151.269684</td>\n",
              "      <td>147.218048</td>\n",
              "      <td>143.840057</td>\n",
              "      <td>123.067657</td>\n",
              "      <td>88.214897</td>\n",
              "      <td>68.290359</td>\n",
              "      <td>65.113518</td>\n",
              "      <td>58.524090</td>\n",
              "      <td>69.659576</td>\n",
              "      <td>71.777748</td>\n",
              "      <td>76.621056</td>\n",
              "      <td>2.594079</td>\n",
              "      <td>28.328943</td>\n",
              "      <td>96.842308</td>\n",
              "      <td>103.419075</td>\n",
              "      <td>100.949295</td>\n",
              "      <td>92.172897</td>\n",
              "      <td>89.499733</td>\n",
              "      <td>81.840271</td>\n",
              "      <td>69.819061</td>\n",
              "      <td>58.686150</td>\n",
              "      <td>106.089386</td>\n",
              "      <td>151.639755</td>\n",
              "      <td>165.696106</td>\n",
              "      <td>169.915787</td>\n",
              "      <td>170.331909</td>\n",
              "      <td>165.536392</td>\n",
              "      <td>161.218048</td>\n",
              "      <td>161.174103</td>\n",
              "      <td>147.589584</td>\n",
              "      <td>121.061188</td>\n",
              "      <td>82.586555</td>\n",
              "      <td>67.474617</td>\n",
              "      <td>66.896271</td>\n",
              "      <td>65.190895</td>\n",
              "      <td>64.762192</td>\n",
              "      <td>69.508301</td>\n",
              "      <td>70.866028</td>\n",
              "      <td>71.179298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>138.0</td>\n",
              "      <td>160.164658</td>\n",
              "      <td>167.590836</td>\n",
              "      <td>165.876282</td>\n",
              "      <td>162.256866</td>\n",
              "      <td>145.253311</td>\n",
              "      <td>116.836578</td>\n",
              "      <td>83.458099</td>\n",
              "      <td>45.109848</td>\n",
              "      <td>40.819572</td>\n",
              "      <td>72.131058</td>\n",
              "      <td>80.276199</td>\n",
              "      <td>79.607857</td>\n",
              "      <td>78.968697</td>\n",
              "      <td>81.083382</td>\n",
              "      <td>78.143234</td>\n",
              "      <td>106.985916</td>\n",
              "      <td>142.495270</td>\n",
              "      <td>136.143234</td>\n",
              "      <td>104.771683</td>\n",
              "      <td>104.971214</td>\n",
              "      <td>112.697754</td>\n",
              "      <td>129.944138</td>\n",
              "      <td>144.452637</td>\n",
              "      <td>158.386688</td>\n",
              "      <td>170.542953</td>\n",
              "      <td>169.320099</td>\n",
              "      <td>164.925430</td>\n",
              "      <td>147.447159</td>\n",
              "      <td>190.178116</td>\n",
              "      <td>185.479095</td>\n",
              "      <td>168.849625</td>\n",
              "      <td>138.003143</td>\n",
              "      <td>68.994537</td>\n",
              "      <td>54.645451</td>\n",
              "      <td>47.891197</td>\n",
              "      <td>45.663933</td>\n",
              "      <td>54.261288</td>\n",
              "      <td>58.551559</td>\n",
              "      <td>64.408531</td>\n",
              "      <td>...</td>\n",
              "      <td>110.376816</td>\n",
              "      <td>110.651535</td>\n",
              "      <td>120.915985</td>\n",
              "      <td>118.482872</td>\n",
              "      <td>115.494217</td>\n",
              "      <td>116.595245</td>\n",
              "      <td>116.870407</td>\n",
              "      <td>117.610588</td>\n",
              "      <td>118.803818</td>\n",
              "      <td>123.102928</td>\n",
              "      <td>128.182098</td>\n",
              "      <td>131.338165</td>\n",
              "      <td>192.879852</td>\n",
              "      <td>206.334991</td>\n",
              "      <td>230.522568</td>\n",
              "      <td>249.312317</td>\n",
              "      <td>249.949158</td>\n",
              "      <td>244.589798</td>\n",
              "      <td>226.430984</td>\n",
              "      <td>180.130219</td>\n",
              "      <td>192.022461</td>\n",
              "      <td>177.772522</td>\n",
              "      <td>161.898758</td>\n",
              "      <td>149.254364</td>\n",
              "      <td>142.034241</td>\n",
              "      <td>139.006287</td>\n",
              "      <td>136.773987</td>\n",
              "      <td>134.624451</td>\n",
              "      <td>123.074562</td>\n",
              "      <td>114.031929</td>\n",
              "      <td>120.203941</td>\n",
              "      <td>123.738495</td>\n",
              "      <td>119.484146</td>\n",
              "      <td>115.198059</td>\n",
              "      <td>115.637474</td>\n",
              "      <td>117.884903</td>\n",
              "      <td>116.983185</td>\n",
              "      <td>120.253098</td>\n",
              "      <td>126.169083</td>\n",
              "      <td>131.085281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>193.0</td>\n",
              "      <td>113.704910</td>\n",
              "      <td>96.307556</td>\n",
              "      <td>98.142929</td>\n",
              "      <td>128.726669</td>\n",
              "      <td>166.090393</td>\n",
              "      <td>174.682220</td>\n",
              "      <td>179.704895</td>\n",
              "      <td>188.639694</td>\n",
              "      <td>202.856003</td>\n",
              "      <td>199.972931</td>\n",
              "      <td>172.575562</td>\n",
              "      <td>180.914368</td>\n",
              "      <td>191.954803</td>\n",
              "      <td>193.248260</td>\n",
              "      <td>195.302795</td>\n",
              "      <td>190.535370</td>\n",
              "      <td>184.109116</td>\n",
              "      <td>179.497330</td>\n",
              "      <td>174.218262</td>\n",
              "      <td>170.693436</td>\n",
              "      <td>170.004486</td>\n",
              "      <td>156.256897</td>\n",
              "      <td>144.662735</td>\n",
              "      <td>145.047760</td>\n",
              "      <td>140.756393</td>\n",
              "      <td>141.158875</td>\n",
              "      <td>142.282974</td>\n",
              "      <td>147.747971</td>\n",
              "      <td>141.044479</td>\n",
              "      <td>127.863831</td>\n",
              "      <td>125.950111</td>\n",
              "      <td>148.624283</td>\n",
              "      <td>161.197479</td>\n",
              "      <td>161.702667</td>\n",
              "      <td>158.823105</td>\n",
              "      <td>164.743591</td>\n",
              "      <td>153.454987</td>\n",
              "      <td>163.110992</td>\n",
              "      <td>196.577316</td>\n",
              "      <td>...</td>\n",
              "      <td>204.564255</td>\n",
              "      <td>214.262299</td>\n",
              "      <td>203.526779</td>\n",
              "      <td>152.168839</td>\n",
              "      <td>147.190735</td>\n",
              "      <td>148.166016</td>\n",
              "      <td>138.428192</td>\n",
              "      <td>105.249107</td>\n",
              "      <td>93.709755</td>\n",
              "      <td>105.335579</td>\n",
              "      <td>112.920189</td>\n",
              "      <td>134.819290</td>\n",
              "      <td>149.197708</td>\n",
              "      <td>149.042953</td>\n",
              "      <td>149.245728</td>\n",
              "      <td>151.378998</td>\n",
              "      <td>148.155655</td>\n",
              "      <td>192.776718</td>\n",
              "      <td>196.316422</td>\n",
              "      <td>198.130219</td>\n",
              "      <td>194.811371</td>\n",
              "      <td>192.511154</td>\n",
              "      <td>196.124115</td>\n",
              "      <td>184.972244</td>\n",
              "      <td>177.909607</td>\n",
              "      <td>177.142929</td>\n",
              "      <td>181.034897</td>\n",
              "      <td>199.181259</td>\n",
              "      <td>226.116364</td>\n",
              "      <td>213.232941</td>\n",
              "      <td>194.254471</td>\n",
              "      <td>99.812744</td>\n",
              "      <td>129.943466</td>\n",
              "      <td>143.968964</td>\n",
              "      <td>171.539001</td>\n",
              "      <td>179.270218</td>\n",
              "      <td>172.715317</td>\n",
              "      <td>146.392731</td>\n",
              "      <td>132.145630</td>\n",
              "      <td>145.295380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>169.0</td>\n",
              "      <td>38.455727</td>\n",
              "      <td>78.680008</td>\n",
              "      <td>126.235672</td>\n",
              "      <td>154.879471</td>\n",
              "      <td>170.686737</td>\n",
              "      <td>167.679550</td>\n",
              "      <td>99.030495</td>\n",
              "      <td>98.373375</td>\n",
              "      <td>114.388565</td>\n",
              "      <td>119.317795</td>\n",
              "      <td>113.786911</td>\n",
              "      <td>105.317184</td>\n",
              "      <td>100.490036</td>\n",
              "      <td>90.795700</td>\n",
              "      <td>84.927940</td>\n",
              "      <td>82.356819</td>\n",
              "      <td>77.200302</td>\n",
              "      <td>96.757607</td>\n",
              "      <td>115.541054</td>\n",
              "      <td>102.775948</td>\n",
              "      <td>85.229988</td>\n",
              "      <td>93.543564</td>\n",
              "      <td>100.595390</td>\n",
              "      <td>103.013023</td>\n",
              "      <td>103.011063</td>\n",
              "      <td>103.083733</td>\n",
              "      <td>120.073242</td>\n",
              "      <td>138.660614</td>\n",
              "      <td>37.581944</td>\n",
              "      <td>59.203381</td>\n",
              "      <td>73.163300</td>\n",
              "      <td>74.779900</td>\n",
              "      <td>79.886414</td>\n",
              "      <td>62.064770</td>\n",
              "      <td>49.900063</td>\n",
              "      <td>93.849785</td>\n",
              "      <td>111.796288</td>\n",
              "      <td>112.660820</td>\n",
              "      <td>100.653610</td>\n",
              "      <td>...</td>\n",
              "      <td>65.263008</td>\n",
              "      <td>67.564262</td>\n",
              "      <td>68.458450</td>\n",
              "      <td>72.024155</td>\n",
              "      <td>88.891113</td>\n",
              "      <td>100.325783</td>\n",
              "      <td>111.898361</td>\n",
              "      <td>116.073914</td>\n",
              "      <td>117.176453</td>\n",
              "      <td>116.713028</td>\n",
              "      <td>115.805145</td>\n",
              "      <td>117.157623</td>\n",
              "      <td>223.171844</td>\n",
              "      <td>182.721497</td>\n",
              "      <td>196.992081</td>\n",
              "      <td>251.033386</td>\n",
              "      <td>251.570358</td>\n",
              "      <td>252.675919</td>\n",
              "      <td>248.176407</td>\n",
              "      <td>190.987244</td>\n",
              "      <td>127.710304</td>\n",
              "      <td>146.999054</td>\n",
              "      <td>131.930588</td>\n",
              "      <td>114.874130</td>\n",
              "      <td>93.845520</td>\n",
              "      <td>72.880424</td>\n",
              "      <td>74.204300</td>\n",
              "      <td>71.269180</td>\n",
              "      <td>66.934242</td>\n",
              "      <td>70.747345</td>\n",
              "      <td>81.849014</td>\n",
              "      <td>95.931305</td>\n",
              "      <td>109.259079</td>\n",
              "      <td>107.632614</td>\n",
              "      <td>112.840752</td>\n",
              "      <td>110.298485</td>\n",
              "      <td>101.971184</td>\n",
              "      <td>102.934311</td>\n",
              "      <td>111.168686</td>\n",
              "      <td>106.527809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>146.0</td>\n",
              "      <td>126.877838</td>\n",
              "      <td>124.163261</td>\n",
              "      <td>117.725082</td>\n",
              "      <td>112.204735</td>\n",
              "      <td>126.188210</td>\n",
              "      <td>155.622620</td>\n",
              "      <td>165.107712</td>\n",
              "      <td>170.766174</td>\n",
              "      <td>172.043365</td>\n",
              "      <td>180.237381</td>\n",
              "      <td>177.038086</td>\n",
              "      <td>151.958725</td>\n",
              "      <td>119.235703</td>\n",
              "      <td>128.104156</td>\n",
              "      <td>123.244324</td>\n",
              "      <td>123.343399</td>\n",
              "      <td>166.616440</td>\n",
              "      <td>192.838608</td>\n",
              "      <td>196.036774</td>\n",
              "      <td>193.500107</td>\n",
              "      <td>190.911804</td>\n",
              "      <td>185.170380</td>\n",
              "      <td>172.849304</td>\n",
              "      <td>160.586227</td>\n",
              "      <td>150.942566</td>\n",
              "      <td>141.163254</td>\n",
              "      <td>153.134720</td>\n",
              "      <td>156.927185</td>\n",
              "      <td>120.512657</td>\n",
              "      <td>117.648712</td>\n",
              "      <td>111.537430</td>\n",
              "      <td>106.703125</td>\n",
              "      <td>109.160255</td>\n",
              "      <td>151.007492</td>\n",
              "      <td>170.562393</td>\n",
              "      <td>173.877472</td>\n",
              "      <td>174.433289</td>\n",
              "      <td>174.718140</td>\n",
              "      <td>166.478516</td>\n",
              "      <td>...</td>\n",
              "      <td>161.099640</td>\n",
              "      <td>162.483002</td>\n",
              "      <td>160.075989</td>\n",
              "      <td>173.671417</td>\n",
              "      <td>208.269836</td>\n",
              "      <td>207.133057</td>\n",
              "      <td>203.176010</td>\n",
              "      <td>198.435532</td>\n",
              "      <td>195.289734</td>\n",
              "      <td>195.883087</td>\n",
              "      <td>199.121033</td>\n",
              "      <td>201.316193</td>\n",
              "      <td>161.719833</td>\n",
              "      <td>172.404770</td>\n",
              "      <td>173.625427</td>\n",
              "      <td>172.567825</td>\n",
              "      <td>167.907303</td>\n",
              "      <td>162.172272</td>\n",
              "      <td>160.768631</td>\n",
              "      <td>162.003738</td>\n",
              "      <td>166.198349</td>\n",
              "      <td>167.961166</td>\n",
              "      <td>169.575150</td>\n",
              "      <td>170.948395</td>\n",
              "      <td>167.394257</td>\n",
              "      <td>149.128540</td>\n",
              "      <td>151.995499</td>\n",
              "      <td>155.231384</td>\n",
              "      <td>157.911804</td>\n",
              "      <td>159.249390</td>\n",
              "      <td>159.239441</td>\n",
              "      <td>175.084442</td>\n",
              "      <td>211.897171</td>\n",
              "      <td>213.601410</td>\n",
              "      <td>214.314697</td>\n",
              "      <td>207.902618</td>\n",
              "      <td>195.692078</td>\n",
              "      <td>193.833191</td>\n",
              "      <td>197.535751</td>\n",
              "      <td>200.228546</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Width           0           1  ...         781         782         783\n",
              "5   131.0    1.900647    5.351377  ...   69.508301   70.866028   71.179298\n",
              "27  138.0  160.164658  167.590836  ...  120.253098  126.169083  131.085281\n",
              "31  193.0  113.704910   96.307556  ...  146.392731  132.145630  145.295380\n",
              "39  169.0   38.455727   78.680008  ...  102.934311  111.168686  106.527809\n",
              "43  146.0  126.877838  124.163261  ...  193.833191  197.535751  200.228546\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "VaZPe_AxNBK9",
        "outputId": "06e2d71d-b74f-41d8-e11f-dd69ad1b6ec8"
      },
      "source": [
        "PSD_new.head()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Juntas</th>\n",
              "      <th>Area</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1.162</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Juntas   Area\n",
              "0       1  2.001\n",
              "1       2  0.820\n",
              "2       3  1.270\n",
              "3       4  0.958\n",
              "4       5  1.162"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "#lost_value = float(PSD_new.columns[1])\n",
        "\n",
        "# Area = np.array(PSD_new.iloc[:,1])\n",
        "Area = PSD_new['Area'].values\n",
        "# Area = np.concatenate( (Area, [lost_value] ) )\n",
        "# Area = np.concatenate( (Area, [lost_value] ) )\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aUb2_-jsY1Z",
        "outputId": "1cdc9b19-ac11-4f3c-c8e2-016b92ab4684"
      },
      "source": [
        "PSD_new.iloc[:,1].values"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.001, 0.82 , 1.27 , 0.958, 1.162, 2.014, 1.078, 1.234, 1.262,\n",
              "       1.347, 1.313, 2.449, 1.445, 1.209, 3.564, 1.59 , 0.891, 1.329,\n",
              "       1.403, 0.626, 1.65 , 1.551, 2.118, 1.194, 1.113, 1.072, 1.042,\n",
              "       0.725, 4.218, 0.881, 1.608, 0.446, 0.582, 1.282, 1.484, 1.246,\n",
              "       1.323, 1.21 , 2.013, 1.358, 1.579, 1.223, 0.96 , 0.718, 0.707,\n",
              "       0.992, 1.142, 1.287, 0.599, 0.664, 2.119, 0.926, 0.889, 0.929,\n",
              "       1.579, 1.888, 0.481, 1.695, 0.871, 1.262, 0.471, 1.493, 1.461,\n",
              "       1.326, 1.301, 0.982, 0.705, 1.819, 1.437, 1.049, 2.014, 1.276,\n",
              "       1.589, 1.412, 1.08 , 1.037, 1.672, 1.224, 1.403, 0.724, 1.736,\n",
              "       1.601, 1.432, 0.449, 1.245, 1.011, 2.151, 0.986, 0.981, 0.658,\n",
              "       1.064, 1.341, 1.044, 1.337, 1.341])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J705kDqsE8f",
        "outputId": "0f973d45-0013-47aa-d50b-b1b364ad0b0d"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(490, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "mK1GBUHWiIr4",
        "outputId": "e827ade3-b8e9-4f61-b2ea-e5282b09c871"
      },
      "source": [
        "Freq = [10.52631579, 24.21052632, 36.84210526, 14.73684211,  7.36842105, 0.]\n",
        "Freq2 = [12.90153, 28.11527, 27.66761, 20.21617, 10.34227, 0.]\n",
        "Freq3 = [22.22489, 30.15078, 25.10463, 19.30926, 2.810434, 0.]\n",
        "barWidth = 0.25\n",
        "\n",
        "br1 = range(len(Freq))\n",
        "# Set position of bar on X axis\n",
        "br2 = [x + barWidth for x in br1]\n",
        "br3 = [x + barWidth for x in br2]\n",
        "labels = [0.8, 1.0, 1.2, 1.4, 1.6, 1.8]\n",
        "\n",
        "xx=[]\n",
        "for a in labels:\n",
        "  xx.append(str(a))\n",
        "plt.bar(br1, Freq , color=\"green\", align=\"center\", width=0.3, tick_label= xx) \n",
        "plt.bar(br2, Freq2 , color=\"red\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.bar(br3, Freq3 , color=\"blue\", align=\"center\", width=0.3, tick_label= xx)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 6 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfk_fNXGDK5_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "7c9f42e2-4782-474c-e04c-3380c8df6558"
      },
      "source": [
        "wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        "wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        "X = pd.DataFrame([Diam1,Diameter_All])\n",
        "wts = pd.DataFrame([wt1,wt2])\n",
        "plt.hist(X,weights=wts,bins=7)\n",
        "plt.legend(['Image J','CNN'])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f7cc5947b10>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvAFjHQQeU0W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "1bf1c652-d26e-453a-8a01-f304b758f4d2"
      },
      "source": [
        "A = plt.hist(X,weights=wts,bins=7)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qQo3Yjdg6Ox"
      },
      "source": [
        "B = A[0][0]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIKwB7eHkS5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b57e3df-95bf-43b0-ffac-f69486399a17"
      },
      "source": [
        "Novo = []\n",
        "k = 0\n",
        "soma = 0\n",
        "for i in B:\n",
        "  if(k<4):\n",
        "    Novo.append(i)\n",
        "  else:\n",
        "    soma = soma + i\n",
        "  k = k + 1\n",
        "Novo.append(soma)\n",
        "print(Novo)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14.736842105263156, 24.2105263157895, 42.1052631578948, 14.736842105263179, 4.21052631578948]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAQafy99mXhn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "6d9cc5a4-7711-4c24-eabf-4fc09ae6eabc"
      },
      "source": [
        "Freq = [10.52631579, 24.21052632, 36.84210526, 14.73684211,  7.36842105]\n",
        "Freq2 = [12.90153, 28.11527, 27.66761, 20.21617, 10.34227]\n",
        "Freq3 = Novo\n",
        "barWidth = 0.25\n",
        "\n",
        "br1 = range(len(Freq))\n",
        "# Set position of bar on X axis\n",
        "br2 = [x + barWidth for x in br1]\n",
        "br3 = [x + barWidth for x in br2]\n",
        "# labels = [0.8, 1.0, 1.2, 1.4, 1.6, 1.8]\n",
        "labels = [0.8, 1.0, 1.2, 1.4, 1.6]\n",
        "\n",
        "xx=[]\n",
        "for a in labels:\n",
        "  xx.append(str(a))\n",
        "plt.bar(br1, Freq , color=\"green\", align=\"center\", width=0.3, tick_label= xx) \n",
        "plt.bar(br2, Freq2 , color=\"red\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.bar(br3, Freq3 , color=\"blue\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.legend(['CNN 1','CNN 2','True'])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f7cc0de0950>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Jv99FTAp9Qo"
      },
      "source": [
        ""
      ],
      "execution_count": 35,
      "outputs": []
    }
  ]
}