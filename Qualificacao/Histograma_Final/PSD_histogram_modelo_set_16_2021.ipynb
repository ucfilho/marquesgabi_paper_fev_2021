{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_modelo_set_16_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/Qualificacao/Histograma_Final/PSD_histogram_modelo_set_16_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from sklearn.metrics import r2_score\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZEvJvfoibE4",
        "outputId": "c7cd3924-d529-4492-b766-27b92e6ea959"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mahotas\n",
            "  Downloading mahotas-1.4.11-cp37-cp37m-manylinux2010_x86_64.whl (5.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7 MB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.19.5)\n",
            "Installing collected packages: mahotas\n",
            "Successfully installed mahotas-1.4.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT"
      },
      "source": [
        "import mahotas.features.texture as mht\n",
        "import mahotas.features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VcTdaNVh9EE",
        "outputId": "17d4a884-a38b-4606-fba0-a4c3cd02ad3f"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marquesgabi_fev_2020'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 73 (delta 37), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n",
            "/content/marquesgabi_fev_2020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v7SRrc8mH2N",
        "outputId": "d2111aa6-fce0-473e-8e87-bff3e0062e99"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip' \n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Doutorado'...\n",
            "remote: Enumerating objects: 464, done.\u001b[K\n",
            "remote: Counting objects: 100% (214/214), done.\u001b[K\n",
            "remote: Compressing objects: 100% (213/213), done.\u001b[K\n",
            "remote: Total 464 (delta 102), reused 0 (delta 0), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (464/464), 203.28 MiB | 28.96 MiB/s, done.\n",
            "Resolving deltas: 100% (219/219), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqIYzUcnrdMp",
        "outputId": "93e085f9-9529-4cd3-ce79-d54c23e3dd49"
      },
      "source": [
        "labels =[]\n",
        "with zipfile.ZipFile(Transfere, \"r\") as f:\n",
        "  for f in f.namelist():\n",
        "    labels.append(f)\n",
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Fotos_Grandes-3cdAmostra/Q6-8-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-8-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-4.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-7-3.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-9-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-1-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-6-5.jpg', 'Fotos_Grandes-3cdAmostra/Q6-2-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-2.jpg', 'Fotos_Grandes-3cdAmostra/Q6-4-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-3-1.jpg', 'Fotos_Grandes-3cdAmostra/Q6-5-4.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[4] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHgqAnaFyCjp",
        "outputId": "f741d550-636c-42ab-c494-af91a046fb53"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 21.23 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from segment_filter_not_conclude import Segmenta  # got image provided segmented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YN5MN5a_v4np",
        "outputId": "256dcee2-1acd-40fe-827e-6daee7e89705"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     142  118.606827  134.133896  ...   88.691330   92.896057   93.242020\n",
            "1     151  162.749786  167.421326  ...    0.623788    0.332398    1.405245\n",
            "2     133  133.033249  134.778397  ...    0.307479    0.518006    1.498615\n",
            "3     143    0.646731    1.667710  ...   24.050859   23.304808   25.018631\n",
            "4     162  133.092514  133.932922  ...  180.855972  187.058670  190.965698\n",
            "5     143   95.237953   94.233315  ...  184.873199  170.852448  154.543503\n",
            "6     134   96.420364   94.587448  ...    0.000000    0.104032    1.053019\n",
            "7     131  109.400909  125.040558  ...    0.997261    1.000000    1.000000\n",
            "8     101  126.672379  132.543976  ...    0.000000    0.586511    1.000000\n",
            "9     108  150.599457  151.393677  ...  152.572021  122.731133   81.039780\n",
            "10    191  111.770920  118.127647  ...  214.572983  217.643936  214.973816\n",
            "11    111  188.281952  192.990509  ...  144.572205  145.184479  131.731857\n",
            "12    181  172.136658  194.684479  ...  175.770920  168.718842  160.199203\n",
            "13    118  152.452454  159.895721  ...    0.208848    0.480034    1.293594\n",
            "14    155  157.716507  164.596176  ...    0.918668    0.810031    1.572237\n",
            "15    168  241.472229  248.138885  ...  139.861115  135.611115  137.111115\n",
            "16    117  217.806183  219.937820  ...   92.677116   91.424065   96.111771\n",
            "17    101  181.839050  187.600830  ...  222.941666  222.471054  215.583679\n",
            "18    184  156.688080  156.148865  ...  195.891281  195.698486  197.353973\n",
            "19    125  133.248138  124.043015  ...  112.506248  117.217537  126.687500\n",
            "20    176  131.869827  141.879135  ...    1.104339    0.153409    1.343492\n",
            "21    126  164.271606  153.876541  ...  103.469131  133.345688  148.382721\n",
            "22    174  163.539444  176.751755  ...  188.815170  183.950333  176.012970\n",
            "23    156   83.155823   92.365555  ...    0.717291    0.307035    1.409599\n",
            "24    194    1.473164    0.229249  ...  135.211807  133.397903  132.911789\n",
            "25    198  157.528305  133.887451  ...   65.814507  145.403214  151.953369\n",
            "26    146   82.737473   88.055168  ...  230.747421  199.300446  195.686234\n",
            "27    141   58.094360   57.474781  ...    0.457874    0.429506    1.436598\n",
            "28    108  116.997253  150.061722  ...  180.748962  185.637863  189.256531\n",
            "29    145  177.656082  148.459976  ...    0.526944    0.389013    1.423496\n",
            "30    185  167.706284  164.094147  ...  179.307800  175.613953  173.874527\n",
            "31    143  140.795135  160.160492  ...    1.000000    1.000000    0.982200\n",
            "32    100  108.948799  111.782394  ...    1.000000    1.000000    1.000000\n",
            "33    146    0.228561    0.710077  ...  147.909180  152.855133  157.931885\n",
            "34    118  162.609894  163.942245  ...  168.304779  177.415680  189.389236\n",
            "35    109  122.627716  118.190216  ...    1.000000    1.000000    1.447774\n",
            "36    122  152.765381  146.262817  ...   74.281914   77.323563   83.863739\n",
            "37    109  102.692780  108.558113  ...  153.965988  157.388351  156.556091\n",
            "38    194  152.980957  161.592499  ...  129.886047  140.950775  153.772980\n",
            "39    121  145.350525  132.318222  ...  157.942688  157.536774  159.454544\n",
            "40    197  177.979691  182.662842  ...  136.182556  127.865189  127.896011\n",
            "41    198  103.473106   98.908875  ...  136.517792  121.665733  121.048859\n",
            "42    172  126.750687  130.222839  ...  162.586273  139.140076  157.586807\n",
            "43    134  161.278000  154.857452  ...    0.317888    0.496993    1.452217\n",
            "44    168   92.972221   95.361115  ...    0.888889    0.194444    1.361111\n",
            "45    171  189.673416  186.956207  ...  132.565659  133.902374  140.925049\n",
            "46    111  170.222961  169.106155  ...  177.877441  184.359055  179.563675\n",
            "47    191  163.121719  160.807022  ...  100.881775  106.952065  103.508461\n",
            "48    159   81.990746  140.016220  ...  129.034805   75.803291   46.017677\n",
            "49    100  173.814407  163.971191  ...  136.846405  140.152008  137.503998\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "xzpQ1Pz0fX5L",
        "outputId": "9ccf23eb-ef67-4e68-cb33-8cf247521a17"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines\n",
        "# filename = 'model_ANN.pkl'\n",
        "filename = 'model_ANN_new.pkl'\n",
        "model = joblib.load(filename)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n!git clone https://github.com/ucfilho/MarquesGabi_Routines\\n%cd MarquesGabi_Routines\\n# filename = 'model_ANN.pkl'\\nfilename = 'model_ANN_new.pkl'\\nmodel = joblib.load(filename)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2emP4rNjQy",
        "outputId": "88c9c030-1935-4f8c-cce7-d715143697f8"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 22.24 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIFPGE_-vx3T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "Description = '3 layers of Convolution: 64, 128, 256 '\n",
        "N1 = 20\n",
        "N2 = 20\n",
        "\n",
        "# make the CNN\n",
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=64, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=N1))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=N2))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbPQ1FSRG6A",
        "outputId": "e9adc25e-ecc6-4d0d-ba88-c18daa7d738d"
      },
      "source": [
        "\n",
        "# training the model\n",
        "r = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), \n",
        "              epochs=200, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 3s 162ms/step - loss: 0.4839 - accuracy: 0.7609 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.2597 - accuracy: 0.8863 - val_loss: 0.6937 - val_accuracy: 0.5102\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.1646 - accuracy: 0.9504 - val_loss: 0.6950 - val_accuracy: 0.5102\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.1159 - accuracy: 0.9592 - val_loss: 0.6956 - val_accuracy: 0.5102\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0651 - accuracy: 0.9767 - val_loss: 0.6975 - val_accuracy: 0.5102\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0517 - accuracy: 0.9796 - val_loss: 0.6979 - val_accuracy: 0.5102\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0299 - accuracy: 0.9942 - val_loss: 0.7006 - val_accuracy: 0.5102\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.7016 - val_accuracy: 0.5102\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0243 - accuracy: 0.9913 - val_loss: 0.7042 - val_accuracy: 0.5102\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 1s 131ms/step - loss: 0.0243 - accuracy: 0.9942 - val_loss: 0.7058 - val_accuracy: 0.5102\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0233 - accuracy: 0.9942 - val_loss: 0.7033 - val_accuracy: 0.5102\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0140 - accuracy: 0.9971 - val_loss: 0.7034 - val_accuracy: 0.5102\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.7075 - val_accuracy: 0.5102\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.7097 - val_accuracy: 0.5102\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0088 - accuracy: 0.9971 - val_loss: 0.7203 - val_accuracy: 0.5102\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0187 - accuracy: 0.9942 - val_loss: 0.7087 - val_accuracy: 0.5102\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 1s 129ms/step - loss: 0.0099 - accuracy: 0.9971 - val_loss: 0.7232 - val_accuracy: 0.5102\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 1s 130ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.7407 - val_accuracy: 0.5102\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.7181 - val_accuracy: 0.5102\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 1s 132ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.7166 - val_accuracy: 0.5102\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.7175 - val_accuracy: 0.5102\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.7151 - val_accuracy: 0.5102\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7120 - val_accuracy: 0.5102\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 8.4814e-04 - accuracy: 1.0000 - val_loss: 0.7157 - val_accuracy: 0.5102\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 1s 133ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.7363 - val_accuracy: 0.5102\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 5.8296e-04 - accuracy: 1.0000 - val_loss: 0.7443 - val_accuracy: 0.5102\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 8.1495e-04 - accuracy: 1.0000 - val_loss: 0.7495 - val_accuracy: 0.5102\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7465 - val_accuracy: 0.5102\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 6.3766e-04 - accuracy: 1.0000 - val_loss: 0.7485 - val_accuracy: 0.5102\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 1s 134ms/step - loss: 7.1766e-04 - accuracy: 1.0000 - val_loss: 0.7396 - val_accuracy: 0.5102\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 3.7363e-04 - accuracy: 1.0000 - val_loss: 0.7286 - val_accuracy: 0.5102\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 7.1013e-04 - accuracy: 1.0000 - val_loss: 0.6985 - val_accuracy: 0.5102\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.0240 - accuracy: 0.9913 - val_loss: 0.9775 - val_accuracy: 0.5102\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.6479 - val_accuracy: 0.5918\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0096 - accuracy: 0.9971 - val_loss: 0.6528 - val_accuracy: 0.5306\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0244 - accuracy: 0.9942 - val_loss: 0.6774 - val_accuracy: 0.4898\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 0.0537 - accuracy: 0.9825 - val_loss: 2.4782 - val_accuracy: 0.5102\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 0.0278 - accuracy: 0.9825 - val_loss: 2.4867 - val_accuracy: 0.5102\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 0.0191 - accuracy: 0.9942 - val_loss: 1.3161 - val_accuracy: 0.5102\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 2.6616 - val_accuracy: 0.5102\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 2s 152ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 4.1684 - val_accuracy: 0.5102\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 4.3936 - val_accuracy: 0.5102\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 5.4061 - val_accuracy: 0.5102\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 7.6077e-04 - accuracy: 1.0000 - val_loss: 5.9667 - val_accuracy: 0.5102\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 3.1382e-04 - accuracy: 1.0000 - val_loss: 5.9896 - val_accuracy: 0.5102\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 6.3510e-04 - accuracy: 1.0000 - val_loss: 5.6317 - val_accuracy: 0.5102\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 5.7874e-04 - accuracy: 1.0000 - val_loss: 5.1646 - val_accuracy: 0.5102\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 9.8843e-04 - accuracy: 1.0000 - val_loss: 5.0781 - val_accuracy: 0.5102\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 5.8519e-04 - accuracy: 1.0000 - val_loss: 5.1025 - val_accuracy: 0.5102\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 4.6574 - val_accuracy: 0.5102\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 4.5587e-04 - accuracy: 1.0000 - val_loss: 4.5041 - val_accuracy: 0.5102\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 5.7216e-04 - accuracy: 1.0000 - val_loss: 4.6055 - val_accuracy: 0.5102\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 5.1081e-04 - accuracy: 1.0000 - val_loss: 4.6368 - val_accuracy: 0.5102\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 2.4356e-04 - accuracy: 1.0000 - val_loss: 4.5568 - val_accuracy: 0.5102\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 2s 157ms/step - loss: 1.7091e-04 - accuracy: 1.0000 - val_loss: 4.4548 - val_accuracy: 0.5102\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 4.8447e-04 - accuracy: 1.0000 - val_loss: 4.4861 - val_accuracy: 0.5102\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 1.9910e-04 - accuracy: 1.0000 - val_loss: 4.6113 - val_accuracy: 0.5102\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 1.0034e-04 - accuracy: 1.0000 - val_loss: 4.5467 - val_accuracy: 0.5102\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 7.0597e-04 - accuracy: 1.0000 - val_loss: 4.8134 - val_accuracy: 0.5102\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 3.6424e-04 - accuracy: 1.0000 - val_loss: 4.7873 - val_accuracy: 0.5102\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 1.8337e-04 - accuracy: 1.0000 - val_loss: 4.8144 - val_accuracy: 0.5102\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 2.3947e-04 - accuracy: 1.0000 - val_loss: 4.4999 - val_accuracy: 0.5102\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 9.1163e-05 - accuracy: 1.0000 - val_loss: 4.0934 - val_accuracy: 0.5102\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 2.4580e-04 - accuracy: 1.0000 - val_loss: 3.5284 - val_accuracy: 0.5102\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 2.5693e-04 - accuracy: 1.0000 - val_loss: 0.4362 - val_accuracy: 0.7959\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 2.3370e-04 - accuracy: 1.0000 - val_loss: 0.1556 - val_accuracy: 0.9524\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 2.4955e-04 - accuracy: 1.0000 - val_loss: 0.2333 - val_accuracy: 0.9048\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 9.7852e-05 - accuracy: 1.0000 - val_loss: 0.3085 - val_accuracy: 0.8844\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 7.9033e-05 - accuracy: 1.0000 - val_loss: 0.2987 - val_accuracy: 0.8844\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 2.0351e-04 - accuracy: 1.0000 - val_loss: 0.4925 - val_accuracy: 0.8231\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 3.3755e-04 - accuracy: 1.0000 - val_loss: 0.1633 - val_accuracy: 0.9456\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 1.0511e-04 - accuracy: 1.0000 - val_loss: 2.2706 - val_accuracy: 0.5306\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 2.0668e-04 - accuracy: 1.0000 - val_loss: 2.9549 - val_accuracy: 0.5170\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 2s 137ms/step - loss: 7.7960e-05 - accuracy: 1.0000 - val_loss: 2.9961 - val_accuracy: 0.5170\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 7.3218e-05 - accuracy: 1.0000 - val_loss: 2.9394 - val_accuracy: 0.5170\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 4.6749e-04 - accuracy: 1.0000 - val_loss: 1.4516 - val_accuracy: 0.6327\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 2.3500e-04 - accuracy: 1.0000 - val_loss: 0.2656 - val_accuracy: 0.9048\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 1.3001e-04 - accuracy: 1.0000 - val_loss: 0.4387 - val_accuracy: 0.8571\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 4.8576e-05 - accuracy: 1.0000 - val_loss: 0.4067 - val_accuracy: 0.8639\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 1.7874e-04 - accuracy: 1.0000 - val_loss: 0.4659 - val_accuracy: 0.8639\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 3.3818e-04 - accuracy: 1.0000 - val_loss: 0.4068 - val_accuracy: 0.8912\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 9.2517e-05 - accuracy: 1.0000 - val_loss: 0.2776 - val_accuracy: 0.8980\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 6.5876e-05 - accuracy: 1.0000 - val_loss: 0.2352 - val_accuracy: 0.9116\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 5.1293e-05 - accuracy: 1.0000 - val_loss: 0.2206 - val_accuracy: 0.9456\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4296 - val_accuracy: 0.9048\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 2s 157ms/step - loss: 0.0129 - accuracy: 0.9971 - val_loss: 95.6729 - val_accuracy: 0.5102\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 0.0445 - accuracy: 0.9854 - val_loss: 122.7414 - val_accuracy: 0.5102\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0709 - accuracy: 0.9738 - val_loss: 202.9820 - val_accuracy: 0.5102\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0662 - accuracy: 0.9796 - val_loss: 147.0618 - val_accuracy: 0.5102\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0507 - accuracy: 0.9854 - val_loss: 113.9005 - val_accuracy: 0.5102\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 0.0389 - accuracy: 0.9854 - val_loss: 511.1067 - val_accuracy: 0.5102\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0529 - accuracy: 0.9854 - val_loss: 28.9999 - val_accuracy: 0.4898\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 0.0525 - accuracy: 0.9854 - val_loss: 88.5553 - val_accuracy: 0.5102\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 137.8853 - val_accuracy: 0.5102\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.0078 - accuracy: 0.9971 - val_loss: 142.4583 - val_accuracy: 0.5102\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 123.8817 - val_accuracy: 0.5102\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 110.5945 - val_accuracy: 0.5102\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 2s 151ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 100.3772 - val_accuracy: 0.5102\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 6.9028e-04 - accuracy: 1.0000 - val_loss: 88.1011 - val_accuracy: 0.5102\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 2s 151ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 75.2164 - val_accuracy: 0.5102\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 3.0042e-04 - accuracy: 1.0000 - val_loss: 66.3919 - val_accuracy: 0.5102\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 2s 151ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 59.8592 - val_accuracy: 0.5102\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 49.1985 - val_accuracy: 0.5102\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 6.4597e-04 - accuracy: 1.0000 - val_loss: 41.6922 - val_accuracy: 0.5102\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 37.9283 - val_accuracy: 0.5102\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 7.4912e-04 - accuracy: 1.0000 - val_loss: 37.0326 - val_accuracy: 0.5102\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 1.9952e-04 - accuracy: 1.0000 - val_loss: 34.1511 - val_accuracy: 0.5102\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 2.6629e-04 - accuracy: 1.0000 - val_loss: 31.6825 - val_accuracy: 0.5102\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 4.0491e-04 - accuracy: 1.0000 - val_loss: 29.1831 - val_accuracy: 0.5102\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 2s 151ms/step - loss: 5.2022e-04 - accuracy: 1.0000 - val_loss: 25.4247 - val_accuracy: 0.5102\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 3.8683e-04 - accuracy: 1.0000 - val_loss: 21.7399 - val_accuracy: 0.5102\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 4.0343e-04 - accuracy: 1.0000 - val_loss: 18.5326 - val_accuracy: 0.5102\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 13.7233 - val_accuracy: 0.5102\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 2.7584e-04 - accuracy: 1.0000 - val_loss: 10.5164 - val_accuracy: 0.5102\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 8.3840 - val_accuracy: 0.5102\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 3.4411e-04 - accuracy: 1.0000 - val_loss: 5.6222 - val_accuracy: 0.5102\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 2s 152ms/step - loss: 8.7782e-05 - accuracy: 1.0000 - val_loss: 4.1587 - val_accuracy: 0.5102\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 1s 136ms/step - loss: 1.8151e-04 - accuracy: 1.0000 - val_loss: 2.4849 - val_accuracy: 0.5374\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.4581 - val_accuracy: 0.7959\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 6.6182e-05 - accuracy: 1.0000 - val_loss: 2.3022 - val_accuracy: 0.5714\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 1.9280e-04 - accuracy: 1.0000 - val_loss: 4.6657 - val_accuracy: 0.5238\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 2.8721e-04 - accuracy: 1.0000 - val_loss: 3.4738 - val_accuracy: 0.5374\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 4.6880e-04 - accuracy: 1.0000 - val_loss: 4.2743 - val_accuracy: 0.5306\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 2s 159ms/step - loss: 2.5184e-04 - accuracy: 1.0000 - val_loss: 9.5798 - val_accuracy: 0.5102\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 2s 153ms/step - loss: 9.3870e-04 - accuracy: 1.0000 - val_loss: 12.6021 - val_accuracy: 0.5102\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 5.1481e-05 - accuracy: 1.0000 - val_loss: 12.5851 - val_accuracy: 0.5102\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 0.0042 - accuracy: 0.9971 - val_loss: 29.3340 - val_accuracy: 0.5102\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 4.5454e-04 - accuracy: 1.0000 - val_loss: 42.9837 - val_accuracy: 0.5102\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 8.1787e-04 - accuracy: 1.0000 - val_loss: 35.8376 - val_accuracy: 0.5102\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 1.6394e-04 - accuracy: 1.0000 - val_loss: 32.0148 - val_accuracy: 0.5102\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 2s 151ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 39.5285 - val_accuracy: 0.5102\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 1.7927e-04 - accuracy: 1.0000 - val_loss: 40.0891 - val_accuracy: 0.5102\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 2s 155ms/step - loss: 7.3993e-05 - accuracy: 1.0000 - val_loss: 36.8661 - val_accuracy: 0.5102\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 2s 152ms/step - loss: 9.0796e-05 - accuracy: 1.0000 - val_loss: 32.6923 - val_accuracy: 0.5102\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 1.1377e-04 - accuracy: 1.0000 - val_loss: 28.6583 - val_accuracy: 0.5102\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 3.3246e-04 - accuracy: 1.0000 - val_loss: 24.6141 - val_accuracy: 0.5102\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 3.3218e-04 - accuracy: 1.0000 - val_loss: 21.9812 - val_accuracy: 0.5102\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 2.8947e-05 - accuracy: 1.0000 - val_loss: 19.7100 - val_accuracy: 0.5102\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 2s 156ms/step - loss: 8.9250e-05 - accuracy: 1.0000 - val_loss: 17.2914 - val_accuracy: 0.5102\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 2s 151ms/step - loss: 6.3664e-05 - accuracy: 1.0000 - val_loss: 15.0678 - val_accuracy: 0.5102\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 3.5147e-05 - accuracy: 1.0000 - val_loss: 13.0186 - val_accuracy: 0.5102\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 1.3463e-04 - accuracy: 1.0000 - val_loss: 10.7842 - val_accuracy: 0.5102\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 4.5961e-04 - accuracy: 1.0000 - val_loss: 8.3112 - val_accuracy: 0.5170\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 2.9723e-05 - accuracy: 1.0000 - val_loss: 6.6032 - val_accuracy: 0.5306\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 1.4058e-04 - accuracy: 1.0000 - val_loss: 5.4040 - val_accuracy: 0.5374\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 7.5073e-05 - accuracy: 1.0000 - val_loss: 4.3976 - val_accuracy: 0.5510\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 2s 152ms/step - loss: 1.1821e-04 - accuracy: 1.0000 - val_loss: 3.4460 - val_accuracy: 0.5986\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 3.1499e-05 - accuracy: 1.0000 - val_loss: 2.4585 - val_accuracy: 0.6599\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 4.1720e-05 - accuracy: 1.0000 - val_loss: 1.8015 - val_accuracy: 0.7211\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 5.3567e-05 - accuracy: 1.0000 - val_loss: 1.4252 - val_accuracy: 0.7347\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 2s 139ms/step - loss: 1.5888e-04 - accuracy: 1.0000 - val_loss: 0.9443 - val_accuracy: 0.8299\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 4.6069e-05 - accuracy: 1.0000 - val_loss: 0.7283 - val_accuracy: 0.8639\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 2.9966e-05 - accuracy: 1.0000 - val_loss: 0.5888 - val_accuracy: 0.8980\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 2s 157ms/step - loss: 4.9034e-05 - accuracy: 1.0000 - val_loss: 0.4859 - val_accuracy: 0.9048\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 2s 162ms/step - loss: 7.7348e-05 - accuracy: 1.0000 - val_loss: 0.4073 - val_accuracy: 0.9252\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 4.1640e-05 - accuracy: 1.0000 - val_loss: 0.3615 - val_accuracy: 0.9320\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 9.9291e-06 - accuracy: 1.0000 - val_loss: 0.3348 - val_accuracy: 0.9592\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 5.5566e-05 - accuracy: 1.0000 - val_loss: 0.3184 - val_accuracy: 0.9660\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 2.3236e-05 - accuracy: 1.0000 - val_loss: 0.3039 - val_accuracy: 0.9592\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 2s 145ms/step - loss: 2.2745e-05 - accuracy: 1.0000 - val_loss: 0.2898 - val_accuracy: 0.9660\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 2.1678e-05 - accuracy: 1.0000 - val_loss: 0.2790 - val_accuracy: 0.9660\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 6.9111e-05 - accuracy: 1.0000 - val_loss: 0.2709 - val_accuracy: 0.9660\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 9.9650e-05 - accuracy: 1.0000 - val_loss: 0.2635 - val_accuracy: 0.9660\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 1.8683e-04 - accuracy: 1.0000 - val_loss: 0.2644 - val_accuracy: 0.9592\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 9.8157e-05 - accuracy: 1.0000 - val_loss: 0.3315 - val_accuracy: 0.9592\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 6.6501e-05 - accuracy: 1.0000 - val_loss: 0.3764 - val_accuracy: 0.9524\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 3.9459e-04 - accuracy: 1.0000 - val_loss: 0.3839 - val_accuracy: 0.9252\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 1.7613e-04 - accuracy: 1.0000 - val_loss: 1.2570 - val_accuracy: 0.7211\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 4.2319e-05 - accuracy: 1.0000 - val_loss: 1.7994 - val_accuracy: 0.6531\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 2s 148ms/step - loss: 7.6881e-05 - accuracy: 1.0000 - val_loss: 1.7274 - val_accuracy: 0.7007\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 0.0037 - accuracy: 0.9971 - val_loss: 0.1869 - val_accuracy: 0.9592\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.7498 - val_accuracy: 0.8163\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 5.8585 - val_accuracy: 0.5238\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 4.3934e-05 - accuracy: 1.0000 - val_loss: 15.1687 - val_accuracy: 0.5102\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 0.0032 - accuracy: 0.9971 - val_loss: 15.4381 - val_accuracy: 0.5102\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 8.4503 - val_accuracy: 0.5170\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 21.4727 - val_accuracy: 0.5102\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 2s 143ms/step - loss: 4.6553e-04 - accuracy: 1.0000 - val_loss: 38.8650 - val_accuracy: 0.5102\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 4.1974e-04 - accuracy: 1.0000 - val_loss: 43.5847 - val_accuracy: 0.5102\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 6.5552e-04 - accuracy: 1.0000 - val_loss: 53.3130 - val_accuracy: 0.5102\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 7.8982e-05 - accuracy: 1.0000 - val_loss: 55.6422 - val_accuracy: 0.5102\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 3.2388e-05 - accuracy: 1.0000 - val_loss: 53.4299 - val_accuracy: 0.5102\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 2s 140ms/step - loss: 4.0140e-04 - accuracy: 1.0000 - val_loss: 49.0892 - val_accuracy: 0.5102\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 5.8966e-04 - accuracy: 1.0000 - val_loss: 44.2222 - val_accuracy: 0.5102\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 1.1768e-04 - accuracy: 1.0000 - val_loss: 38.1686 - val_accuracy: 0.5102\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 2.6610e-04 - accuracy: 1.0000 - val_loss: 33.3370 - val_accuracy: 0.5102\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 7.9794e-05 - accuracy: 1.0000 - val_loss: 28.2308 - val_accuracy: 0.5102\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 2s 166ms/step - loss: 7.8754e-05 - accuracy: 1.0000 - val_loss: 24.0865 - val_accuracy: 0.5102\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 2s 161ms/step - loss: 2.2138e-05 - accuracy: 1.0000 - val_loss: 20.6783 - val_accuracy: 0.5102\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 2s 161ms/step - loss: 4.5233e-05 - accuracy: 1.0000 - val_loss: 18.0694 - val_accuracy: 0.5102\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 2s 214ms/step - loss: 2.7553e-05 - accuracy: 1.0000 - val_loss: 15.7070 - val_accuracy: 0.5102\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 2s 190ms/step - loss: 3.5798e-05 - accuracy: 1.0000 - val_loss: 13.4888 - val_accuracy: 0.5102\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 2s 147ms/step - loss: 1.6891e-05 - accuracy: 1.0000 - val_loss: 11.4653 - val_accuracy: 0.5170\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 2s 150ms/step - loss: 2.2256e-04 - accuracy: 1.0000 - val_loss: 9.7158 - val_accuracy: 0.5170\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 6.3934e-04 - accuracy: 1.0000 - val_loss: 8.7768 - val_accuracy: 0.5170\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 5.1098e-05 - accuracy: 1.0000 - val_loss: 8.7464 - val_accuracy: 0.5170\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 2s 144ms/step - loss: 2.0922e-05 - accuracy: 1.0000 - val_loss: 7.7132 - val_accuracy: 0.5238\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 2s 149ms/step - loss: 4.2936e-05 - accuracy: 1.0000 - val_loss: 6.0472 - val_accuracy: 0.5374\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 2s 142ms/step - loss: 3.3492e-05 - accuracy: 1.0000 - val_loss: 4.5825 - val_accuracy: 0.5918\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 2s 146ms/step - loss: 3.0516e-04 - accuracy: 1.0000 - val_loss: 3.1744 - val_accuracy: 0.6463\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDVY6HbxMOlH",
        "outputId": "ae2a9a3a-75d8-4cd8-b68d-79c443ef021f"
      },
      "source": [
        "# pred_test= model.predict_classes(X_test)\n",
        "pred_test = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict   0   1\n",
            "Actual         \n",
            "0        20  52\n",
            "1         0  75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iFNNrlWV9tH",
        "outputId": "ed059575-e57e-4d42-a3bc-06876590b90c"
      },
      "source": [
        "pred_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
              "       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv5I61yhPQmk",
        "outputId": "aa25b322-04fc-413c-fa45-be3b6990743b"
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[4] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  #prediction = model.predict_classes(result)\n",
        "  prediction= np.argmax(model.predict(result), axis=-1)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0   115.0    0.085369    1.012703  ...    0.737543    1.520151    1.572325\n",
            "14  183.0   97.252716  101.316254  ...  152.839798  163.009750  136.953903\n",
            "15  175.0  140.063995  139.724808  ...  177.483200  169.412796  159.147186\n",
            "24  118.0    1.361965    3.064924  ...   57.721634   83.546394  103.239006\n",
            "25  194.0  108.462418  128.326385  ...   67.848114   73.981712   70.858322\n",
            "30  187.0  148.714203  136.653107  ...  139.900146  138.282684  125.662827\n",
            "33  126.0  145.975311  148.938278  ...  160.024689  161.271606  179.370377\n",
            "34  113.0  113.879082  114.362595  ...  102.976898  101.992096  127.265808\n",
            "37  144.0  163.788559  168.153549  ...  144.577179  151.406647  145.686737\n",
            "41  142.0  183.354691  182.631821  ...  217.006744  201.838928  187.961914\n",
            "42  155.0  129.126968  122.404549  ...  195.825531  203.731247  201.898819\n",
            "49  187.0   90.606476   63.991768  ...  140.206207  161.868393  206.757629\n",
            "0   115.0    0.980265    0.052930  ...  131.990173  123.889977  129.326050\n",
            "1   120.0  181.048889  186.928894  ...  193.101120  192.836670  192.866669\n",
            "3   166.0  112.088394  106.791534  ...    0.109450    1.339382    0.886195\n",
            "12  200.0  133.428818  141.702408  ...  148.545197  140.970398  134.617203\n",
            "13  121.0  186.831284  189.667023  ...   64.159958   63.806641   63.870159\n",
            "14  128.0  186.629883  168.027344  ...  170.542969  171.818359  167.272461\n",
            "15  159.0  150.381226  148.846405  ...  105.074516  110.974213  144.156677\n",
            "23  188.0   78.720230   84.488007  ...  151.415558  145.167496  146.929840\n",
            "25  113.0  197.441071  221.152893  ...  149.426117  145.965134  144.355133\n",
            "33  116.0  178.583817  186.430435  ...  166.592148  165.130798  158.614746\n",
            "36  102.0    0.000000    0.702807  ...   80.801620  102.943871  130.302582\n",
            "37  111.0  137.205017  150.558640  ...  165.177246  165.227661  165.436417\n",
            "39  161.0  193.765594  197.319489  ...  145.489609  150.939499  146.451797\n",
            "\n",
            "[25 rows x 785 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRbWgmX_LFH",
        "outputId": "a9d3de23-7fd9-4bac-bd2e-563c0be1cc25"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "from GetBetterSegm import GetBetter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marquesgabi_paper_fev_2021'...\n",
            "remote: Enumerating objects: 702, done.\u001b[K\n",
            "remote: Counting objects: 100% (463/463), done.\u001b[K\n",
            "remote: Compressing objects: 100% (461/461), done.\u001b[K\n",
            "remote: Total 702 (delta 292), reused 0 (delta 0), pack-reused 239\u001b[K\n",
            "Receiving objects: 100% (702/702), 5.73 MiB | 12.07 MiB/s, done.\n",
            "Resolving deltas: 100% (429/429), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAG_I6FwCvFr",
        "outputId": "98b1c840-fa4b-41b1-f256-9f12ebb22521"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "#!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd marquesgabi_out_2020\n",
        "#%cd Doutorado\n",
        "#PSD_imageJ = 'Amostra7.csv' \n",
        "#PSD_new = pd.read_csv(PSD_imageJ,sep=';')\n",
        "#encoding='utf8'\n",
        "\n",
        "PSD_imageJ = 'Areas_ImageJ.csv'\n",
        "PSD_new = pd.read_csv(PSD_imageJ)\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'marquesgabi_out_2020'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 146 (delta 75), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (146/146), 1.00 MiB | 7.08 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020\n",
            "   Juntas   Area\n",
            "0       1  2.001\n",
            "1       2  0.820\n",
            "2       3  1.270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tEPjIBnv_xM",
        "outputId": "8fc80ff7-cd70-4598-f5ce-eb0951d41ce0"
      },
      "source": [
        "PSD_new.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(95, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfagXc-Mv3oa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "PekBHQOT_6CP",
        "outputId": "ee4bcae4-0c08-42d8-821c-874670250bad"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>115.0</td>\n",
              "      <td>0.085369</td>\n",
              "      <td>1.012703</td>\n",
              "      <td>1.855274</td>\n",
              "      <td>1.224499</td>\n",
              "      <td>0.441890</td>\n",
              "      <td>0.215803</td>\n",
              "      <td>1.044461</td>\n",
              "      <td>1.953951</td>\n",
              "      <td>1.094064</td>\n",
              "      <td>0.311456</td>\n",
              "      <td>0.346238</td>\n",
              "      <td>1.128847</td>\n",
              "      <td>1.681663</td>\n",
              "      <td>0.345104</td>\n",
              "      <td>0.463667</td>\n",
              "      <td>0.247713</td>\n",
              "      <td>0.712363</td>\n",
              "      <td>3.916597</td>\n",
              "      <td>17.113874</td>\n",
              "      <td>21.650436</td>\n",
              "      <td>24.740944</td>\n",
              "      <td>20.876747</td>\n",
              "      <td>35.838787</td>\n",
              "      <td>54.718033</td>\n",
              "      <td>68.845444</td>\n",
              "      <td>78.982758</td>\n",
              "      <td>75.422676</td>\n",
              "      <td>66.239014</td>\n",
              "      <td>0.085369</td>\n",
              "      <td>1.012703</td>\n",
              "      <td>1.855274</td>\n",
              "      <td>1.224499</td>\n",
              "      <td>0.441890</td>\n",
              "      <td>0.215803</td>\n",
              "      <td>1.044461</td>\n",
              "      <td>1.953951</td>\n",
              "      <td>1.094064</td>\n",
              "      <td>0.311456</td>\n",
              "      <td>0.346238</td>\n",
              "      <td>...</td>\n",
              "      <td>1.259282</td>\n",
              "      <td>1.828960</td>\n",
              "      <td>0.997656</td>\n",
              "      <td>0.057164</td>\n",
              "      <td>0.607108</td>\n",
              "      <td>1.389716</td>\n",
              "      <td>1.702760</td>\n",
              "      <td>0.894745</td>\n",
              "      <td>0.025406</td>\n",
              "      <td>0.737543</td>\n",
              "      <td>1.520151</td>\n",
              "      <td>1.572325</td>\n",
              "      <td>0.085369</td>\n",
              "      <td>1.012703</td>\n",
              "      <td>1.855274</td>\n",
              "      <td>1.224499</td>\n",
              "      <td>0.441890</td>\n",
              "      <td>0.215803</td>\n",
              "      <td>1.044461</td>\n",
              "      <td>1.953951</td>\n",
              "      <td>1.094064</td>\n",
              "      <td>0.311456</td>\n",
              "      <td>0.346238</td>\n",
              "      <td>1.128847</td>\n",
              "      <td>1.927637</td>\n",
              "      <td>1.035992</td>\n",
              "      <td>0.181021</td>\n",
              "      <td>0.476673</td>\n",
              "      <td>1.259282</td>\n",
              "      <td>1.828960</td>\n",
              "      <td>0.997656</td>\n",
              "      <td>0.057164</td>\n",
              "      <td>0.607108</td>\n",
              "      <td>1.389716</td>\n",
              "      <td>1.702760</td>\n",
              "      <td>0.894745</td>\n",
              "      <td>0.025406</td>\n",
              "      <td>0.737543</td>\n",
              "      <td>1.520151</td>\n",
              "      <td>1.572325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>183.0</td>\n",
              "      <td>97.252716</td>\n",
              "      <td>101.316254</td>\n",
              "      <td>111.686790</td>\n",
              "      <td>112.070740</td>\n",
              "      <td>112.082718</td>\n",
              "      <td>118.793747</td>\n",
              "      <td>129.495865</td>\n",
              "      <td>142.412567</td>\n",
              "      <td>136.249268</td>\n",
              "      <td>107.887634</td>\n",
              "      <td>110.886017</td>\n",
              "      <td>110.491623</td>\n",
              "      <td>116.429985</td>\n",
              "      <td>138.332260</td>\n",
              "      <td>141.752197</td>\n",
              "      <td>143.584961</td>\n",
              "      <td>137.500198</td>\n",
              "      <td>126.865128</td>\n",
              "      <td>114.216644</td>\n",
              "      <td>112.716324</td>\n",
              "      <td>119.975128</td>\n",
              "      <td>129.886627</td>\n",
              "      <td>138.997818</td>\n",
              "      <td>144.847733</td>\n",
              "      <td>139.796341</td>\n",
              "      <td>141.643463</td>\n",
              "      <td>148.213837</td>\n",
              "      <td>153.575821</td>\n",
              "      <td>106.575821</td>\n",
              "      <td>118.725609</td>\n",
              "      <td>125.837349</td>\n",
              "      <td>123.291740</td>\n",
              "      <td>130.908783</td>\n",
              "      <td>135.057541</td>\n",
              "      <td>136.669983</td>\n",
              "      <td>144.394302</td>\n",
              "      <td>159.249023</td>\n",
              "      <td>136.347488</td>\n",
              "      <td>113.950424</td>\n",
              "      <td>...</td>\n",
              "      <td>237.632172</td>\n",
              "      <td>248.179855</td>\n",
              "      <td>214.773331</td>\n",
              "      <td>185.517471</td>\n",
              "      <td>177.264420</td>\n",
              "      <td>175.716446</td>\n",
              "      <td>160.573578</td>\n",
              "      <td>155.566071</td>\n",
              "      <td>151.354004</td>\n",
              "      <td>146.713379</td>\n",
              "      <td>155.829590</td>\n",
              "      <td>160.155426</td>\n",
              "      <td>140.739044</td>\n",
              "      <td>139.225647</td>\n",
              "      <td>132.321594</td>\n",
              "      <td>130.457520</td>\n",
              "      <td>129.131927</td>\n",
              "      <td>131.544647</td>\n",
              "      <td>133.395477</td>\n",
              "      <td>134.187256</td>\n",
              "      <td>132.749023</td>\n",
              "      <td>178.260437</td>\n",
              "      <td>229.479416</td>\n",
              "      <td>234.976624</td>\n",
              "      <td>242.104004</td>\n",
              "      <td>250.686539</td>\n",
              "      <td>250.927872</td>\n",
              "      <td>252.296448</td>\n",
              "      <td>250.704620</td>\n",
              "      <td>229.143890</td>\n",
              "      <td>196.769150</td>\n",
              "      <td>182.939590</td>\n",
              "      <td>178.815002</td>\n",
              "      <td>171.155060</td>\n",
              "      <td>158.661301</td>\n",
              "      <td>151.277740</td>\n",
              "      <td>148.954742</td>\n",
              "      <td>152.839798</td>\n",
              "      <td>163.009750</td>\n",
              "      <td>136.953903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>175.0</td>\n",
              "      <td>140.063995</td>\n",
              "      <td>139.724808</td>\n",
              "      <td>138.281586</td>\n",
              "      <td>148.009598</td>\n",
              "      <td>161.267181</td>\n",
              "      <td>188.575989</td>\n",
              "      <td>201.584000</td>\n",
              "      <td>206.044785</td>\n",
              "      <td>157.182404</td>\n",
              "      <td>128.129593</td>\n",
              "      <td>164.718369</td>\n",
              "      <td>164.343979</td>\n",
              "      <td>150.910400</td>\n",
              "      <td>161.889603</td>\n",
              "      <td>171.756790</td>\n",
              "      <td>179.219193</td>\n",
              "      <td>175.403183</td>\n",
              "      <td>165.582397</td>\n",
              "      <td>149.087997</td>\n",
              "      <td>146.035187</td>\n",
              "      <td>143.900787</td>\n",
              "      <td>158.912003</td>\n",
              "      <td>174.060791</td>\n",
              "      <td>160.931183</td>\n",
              "      <td>111.091194</td>\n",
              "      <td>101.731194</td>\n",
              "      <td>97.574394</td>\n",
              "      <td>112.636787</td>\n",
              "      <td>148.947189</td>\n",
              "      <td>151.212799</td>\n",
              "      <td>153.001602</td>\n",
              "      <td>156.092804</td>\n",
              "      <td>165.731186</td>\n",
              "      <td>187.407990</td>\n",
              "      <td>210.137604</td>\n",
              "      <td>202.438385</td>\n",
              "      <td>180.958389</td>\n",
              "      <td>176.552002</td>\n",
              "      <td>167.470398</td>\n",
              "      <td>...</td>\n",
              "      <td>253.215988</td>\n",
              "      <td>253.263977</td>\n",
              "      <td>252.796783</td>\n",
              "      <td>235.641586</td>\n",
              "      <td>189.732788</td>\n",
              "      <td>203.908798</td>\n",
              "      <td>212.500793</td>\n",
              "      <td>204.999985</td>\n",
              "      <td>179.447983</td>\n",
              "      <td>169.297592</td>\n",
              "      <td>167.278397</td>\n",
              "      <td>159.585587</td>\n",
              "      <td>172.734390</td>\n",
              "      <td>174.932785</td>\n",
              "      <td>173.977585</td>\n",
              "      <td>146.809586</td>\n",
              "      <td>145.235199</td>\n",
              "      <td>153.971207</td>\n",
              "      <td>168.929596</td>\n",
              "      <td>182.481583</td>\n",
              "      <td>195.257568</td>\n",
              "      <td>205.273605</td>\n",
              "      <td>208.484787</td>\n",
              "      <td>208.046387</td>\n",
              "      <td>210.555191</td>\n",
              "      <td>215.297607</td>\n",
              "      <td>216.756775</td>\n",
              "      <td>231.387192</td>\n",
              "      <td>232.463989</td>\n",
              "      <td>227.039963</td>\n",
              "      <td>206.028793</td>\n",
              "      <td>154.524780</td>\n",
              "      <td>122.644798</td>\n",
              "      <td>158.964798</td>\n",
              "      <td>194.779175</td>\n",
              "      <td>205.678391</td>\n",
              "      <td>189.239990</td>\n",
              "      <td>177.483200</td>\n",
              "      <td>169.412796</td>\n",
              "      <td>159.147186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>118.0</td>\n",
              "      <td>1.361965</td>\n",
              "      <td>3.064924</td>\n",
              "      <td>4.746050</td>\n",
              "      <td>31.273485</td>\n",
              "      <td>92.091919</td>\n",
              "      <td>107.227806</td>\n",
              "      <td>123.803505</td>\n",
              "      <td>135.558167</td>\n",
              "      <td>143.440964</td>\n",
              "      <td>144.785416</td>\n",
              "      <td>154.636887</td>\n",
              "      <td>153.017227</td>\n",
              "      <td>151.334961</td>\n",
              "      <td>149.864120</td>\n",
              "      <td>148.196198</td>\n",
              "      <td>136.498413</td>\n",
              "      <td>128.947998</td>\n",
              "      <td>126.502724</td>\n",
              "      <td>126.562195</td>\n",
              "      <td>138.089340</td>\n",
              "      <td>154.056305</td>\n",
              "      <td>143.914963</td>\n",
              "      <td>64.315140</td>\n",
              "      <td>60.296181</td>\n",
              "      <td>87.445274</td>\n",
              "      <td>96.157715</td>\n",
              "      <td>101.933350</td>\n",
              "      <td>109.648376</td>\n",
              "      <td>1.733123</td>\n",
              "      <td>2.051422</td>\n",
              "      <td>2.260557</td>\n",
              "      <td>51.195347</td>\n",
              "      <td>95.041077</td>\n",
              "      <td>111.019249</td>\n",
              "      <td>110.283249</td>\n",
              "      <td>117.259987</td>\n",
              "      <td>135.781662</td>\n",
              "      <td>147.981598</td>\n",
              "      <td>154.115204</td>\n",
              "      <td>...</td>\n",
              "      <td>126.275490</td>\n",
              "      <td>126.358803</td>\n",
              "      <td>132.593781</td>\n",
              "      <td>139.052292</td>\n",
              "      <td>138.730804</td>\n",
              "      <td>124.371155</td>\n",
              "      <td>102.785690</td>\n",
              "      <td>82.742599</td>\n",
              "      <td>34.944267</td>\n",
              "      <td>67.121223</td>\n",
              "      <td>90.993103</td>\n",
              "      <td>102.742027</td>\n",
              "      <td>1.851192</td>\n",
              "      <td>1.440965</td>\n",
              "      <td>1.963803</td>\n",
              "      <td>24.698648</td>\n",
              "      <td>66.176384</td>\n",
              "      <td>78.713013</td>\n",
              "      <td>83.812119</td>\n",
              "      <td>105.289856</td>\n",
              "      <td>120.173225</td>\n",
              "      <td>125.513062</td>\n",
              "      <td>136.168915</td>\n",
              "      <td>143.293304</td>\n",
              "      <td>148.925293</td>\n",
              "      <td>148.991959</td>\n",
              "      <td>143.918976</td>\n",
              "      <td>132.530304</td>\n",
              "      <td>126.900589</td>\n",
              "      <td>127.846878</td>\n",
              "      <td>133.849167</td>\n",
              "      <td>135.311401</td>\n",
              "      <td>129.960938</td>\n",
              "      <td>124.088478</td>\n",
              "      <td>108.118073</td>\n",
              "      <td>73.397583</td>\n",
              "      <td>21.186153</td>\n",
              "      <td>57.721634</td>\n",
              "      <td>83.546394</td>\n",
              "      <td>103.239006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>194.0</td>\n",
              "      <td>108.462418</td>\n",
              "      <td>128.326385</td>\n",
              "      <td>165.922302</td>\n",
              "      <td>159.590897</td>\n",
              "      <td>150.199036</td>\n",
              "      <td>167.950134</td>\n",
              "      <td>168.905182</td>\n",
              "      <td>169.433838</td>\n",
              "      <td>171.579849</td>\n",
              "      <td>168.063431</td>\n",
              "      <td>162.925690</td>\n",
              "      <td>171.597610</td>\n",
              "      <td>181.255386</td>\n",
              "      <td>170.069916</td>\n",
              "      <td>164.289825</td>\n",
              "      <td>162.778717</td>\n",
              "      <td>105.065460</td>\n",
              "      <td>103.174927</td>\n",
              "      <td>102.801880</td>\n",
              "      <td>99.745239</td>\n",
              "      <td>103.930161</td>\n",
              "      <td>114.642464</td>\n",
              "      <td>120.478371</td>\n",
              "      <td>120.821648</td>\n",
              "      <td>99.894562</td>\n",
              "      <td>68.929527</td>\n",
              "      <td>77.869484</td>\n",
              "      <td>72.069710</td>\n",
              "      <td>114.513428</td>\n",
              "      <td>108.999573</td>\n",
              "      <td>106.734291</td>\n",
              "      <td>143.156860</td>\n",
              "      <td>160.562958</td>\n",
              "      <td>155.407471</td>\n",
              "      <td>149.377502</td>\n",
              "      <td>154.574020</td>\n",
              "      <td>155.908264</td>\n",
              "      <td>161.018051</td>\n",
              "      <td>160.553925</td>\n",
              "      <td>...</td>\n",
              "      <td>190.513443</td>\n",
              "      <td>195.103287</td>\n",
              "      <td>189.800903</td>\n",
              "      <td>180.050049</td>\n",
              "      <td>174.559021</td>\n",
              "      <td>166.081421</td>\n",
              "      <td>147.803711</td>\n",
              "      <td>81.843231</td>\n",
              "      <td>49.760754</td>\n",
              "      <td>49.394939</td>\n",
              "      <td>56.815605</td>\n",
              "      <td>64.523537</td>\n",
              "      <td>192.729385</td>\n",
              "      <td>194.425003</td>\n",
              "      <td>191.957367</td>\n",
              "      <td>182.679001</td>\n",
              "      <td>150.607056</td>\n",
              "      <td>108.969055</td>\n",
              "      <td>105.930588</td>\n",
              "      <td>106.291946</td>\n",
              "      <td>115.144424</td>\n",
              "      <td>125.743637</td>\n",
              "      <td>125.935043</td>\n",
              "      <td>113.963959</td>\n",
              "      <td>112.436386</td>\n",
              "      <td>143.214157</td>\n",
              "      <td>168.099670</td>\n",
              "      <td>176.961197</td>\n",
              "      <td>183.667862</td>\n",
              "      <td>188.586761</td>\n",
              "      <td>184.410553</td>\n",
              "      <td>184.631821</td>\n",
              "      <td>167.968521</td>\n",
              "      <td>104.013809</td>\n",
              "      <td>50.745556</td>\n",
              "      <td>37.665531</td>\n",
              "      <td>62.166107</td>\n",
              "      <td>67.848114</td>\n",
              "      <td>73.981712</td>\n",
              "      <td>70.858322</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Width           0           1  ...         781         782         783\n",
              "0   115.0    0.085369    1.012703  ...    0.737543    1.520151    1.572325\n",
              "14  183.0   97.252716  101.316254  ...  152.839798  163.009750  136.953903\n",
              "15  175.0  140.063995  139.724808  ...  177.483200  169.412796  159.147186\n",
              "24  118.0    1.361965    3.064924  ...   57.721634   83.546394  103.239006\n",
              "25  194.0  108.462418  128.326385  ...   67.848114   73.981712   70.858322\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "VaZPe_AxNBK9",
        "outputId": "761bbfe8-ffe9-48fa-86bd-ac047b99be47"
      },
      "source": [
        "PSD_new.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Juntas</th>\n",
              "      <th>Area</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1.162</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Juntas   Area\n",
              "0       1  2.001\n",
              "1       2  0.820\n",
              "2       3  1.270\n",
              "3       4  0.958\n",
              "4       5  1.162"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "#lost_value = float(PSD_new.columns[1])\n",
        "\n",
        "# Area = np.array(PSD_new.iloc[:,1])\n",
        "Area = PSD_new['Area'].values\n",
        "# Area = np.concatenate( (Area, [lost_value] ) )\n",
        "# Area = np.concatenate( (Area, [lost_value] ) )\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aUb2_-jsY1Z",
        "outputId": "59374625-8c07-4e4b-ef50-543eff32ca26"
      },
      "source": [
        "PSD_new.iloc[:,1].values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.001, 0.82 , 1.27 , 0.958, 1.162, 2.014, 1.078, 1.234, 1.262,\n",
              "       1.347, 1.313, 2.449, 1.445, 1.209, 3.564, 1.59 , 0.891, 1.329,\n",
              "       1.403, 0.626, 1.65 , 1.551, 2.118, 1.194, 1.113, 1.072, 1.042,\n",
              "       0.725, 4.218, 0.881, 1.608, 0.446, 0.582, 1.282, 1.484, 1.246,\n",
              "       1.323, 1.21 , 2.013, 1.358, 1.579, 1.223, 0.96 , 0.718, 0.707,\n",
              "       0.992, 1.142, 1.287, 0.599, 0.664, 2.119, 0.926, 0.889, 0.929,\n",
              "       1.579, 1.888, 0.481, 1.695, 0.871, 1.262, 0.471, 1.493, 1.461,\n",
              "       1.326, 1.301, 0.982, 0.705, 1.819, 1.437, 1.049, 2.014, 1.276,\n",
              "       1.589, 1.412, 1.08 , 1.037, 1.672, 1.224, 1.403, 0.724, 1.736,\n",
              "       1.601, 1.432, 0.449, 1.245, 1.011, 2.151, 0.986, 0.981, 0.658,\n",
              "       1.064, 1.341, 1.044, 1.337, 1.341])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J705kDqsE8f",
        "outputId": "e331c36b-2c67-4ad6-d7ac-8fd6fff32ae7"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(490, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "mK1GBUHWiIr4",
        "outputId": "b37dbdde-e31e-4bc5-82f0-e70289eca2a1"
      },
      "source": [
        "Freq = [10.52631579, 24.21052632, 36.84210526, 14.73684211,  7.36842105, 0.]\n",
        "Freq2 = [12.90153, 28.11527, 27.66761, 20.21617, 10.34227, 0.]\n",
        "Freq3 = [22.22489, 30.15078, 25.10463, 19.30926, 2.810434, 0.]\n",
        "barWidth = 0.25\n",
        "\n",
        "br1 = range(len(Freq))\n",
        "# Set position of bar on X axis\n",
        "br2 = [x + barWidth for x in br1]\n",
        "br3 = [x + barWidth for x in br2]\n",
        "labels = [0.8, 1.0, 1.2, 1.4, 1.6, 1.8]\n",
        "\n",
        "xx=[]\n",
        "for a in labels:\n",
        "  xx.append(str(a))\n",
        "plt.bar(br1, Freq , color=\"green\", align=\"center\", width=0.3, tick_label= xx) \n",
        "plt.bar(br2, Freq2 , color=\"red\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.bar(br3, Freq3 , color=\"blue\", align=\"center\", width=0.3, tick_label= xx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 6 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vfk_fNXGDK5_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "60198404-887a-4daa-ce9f-08f07f9855c3"
      },
      "source": [
        "wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        "wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        "X = pd.DataFrame([Diam1,Diameter_All])\n",
        "wts = pd.DataFrame([wt1,wt2])\n",
        "plt.hist(X,weights=wts,bins=7)\n",
        "plt.legend(['Image J','CNN'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f679e48c0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvAFjHQQeU0W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "5e1cf3d3-800b-414c-b676-bd9e78d8223e"
      },
      "source": [
        "A = plt.hist(X,weights=wts,bins=7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-jGknHyeU89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b541588-725a-4936-ba1a-eaf535e62b0d"
      },
      "source": [
        "print(A[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([[14.73684211, 24.21052632, 42.10526316, 14.73684211,  2.10526316,\n",
            "         0.        ,  2.10526316],\n",
            "       [16.        , 32.        , 24.        , 20.        ,  8.        ,\n",
            "         0.        ,  0.        ]]), array([0.75356807, 0.97697823, 1.20038839, 1.42379855, 1.64720871,\n",
            "       1.87061887, 2.09402903, 2.31743919]), <a list of 2 Lists of Patches objects>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qQo3Yjdg6Ox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ebda385-6ba7-498b-ef2c-771dcc2c064a"
      },
      "source": [
        "print(A[0][0])\n",
        "B = A[0][0]\n",
        "print(B[6])\n",
        "print(A[0][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14.73684211 24.21052632 42.10526316 14.73684211  2.10526316  0.\n",
            "  2.10526316]\n",
            "2.10526315789474\n",
            "[16. 32. 24. 20.  8.  0.  0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIKwB7eHkS5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f92d1de9-27b1-4d8f-fae0-b8b92b5c38fe"
      },
      "source": [
        "Novo = []\n",
        "k = 0\n",
        "soma = 0\n",
        "for i in B:\n",
        "  if(k<4):\n",
        "    Novo.append(i)\n",
        "  else:\n",
        "    soma = soma + i\n",
        "  k = k + 1\n",
        "Novo.append(soma)\n",
        "print(Novo)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14.736842105263156, 24.2105263157895, 42.1052631578948, 14.736842105263179, 4.21052631578948]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAQafy99mXhn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "c1a69763-fe0d-4fa6-915b-40046975dfd7"
      },
      "source": [
        "Freq = [10.52631579, 24.21052632, 36.84210526, 14.73684211,  7.36842105]\n",
        "Freq2 = [12.90153, 28.11527, 27.66761, 20.21617, 10.34227]\n",
        "Freq3 = Novo\n",
        "barWidth = 0.25\n",
        "\n",
        "br1 = range(len(Freq))\n",
        "# Set position of bar on X axis\n",
        "br2 = [x + barWidth for x in br1]\n",
        "br3 = [x + barWidth for x in br2]\n",
        "# labels = [0.8, 1.0, 1.2, 1.4, 1.6, 1.8]\n",
        "labels = [0.8, 1.0, 1.2, 1.4, 1.6]\n",
        "\n",
        "xx=[]\n",
        "for a in labels:\n",
        "  xx.append(str(a))\n",
        "plt.bar(br1, Freq , color=\"green\", align=\"center\", width=0.3, tick_label= xx) \n",
        "plt.bar(br2, Freq2 , color=\"red\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.bar(br3, Freq3 , color=\"blue\", align=\"center\", width=0.3, tick_label= xx)\n",
        "plt.legend(['CNN 1','CNN 2','True'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f679a396a50>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Jv99FTAp9Qo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}