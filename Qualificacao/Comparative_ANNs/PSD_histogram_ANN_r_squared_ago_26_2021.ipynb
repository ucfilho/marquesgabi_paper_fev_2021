{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_ANN_r_squared_ago_26_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/Qualificacao/Comparative_ANNs/PSD_histogram_ANN_r_squared_ago_26_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from sklearn.metrics import r2_score\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZEvJvfoibE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49b4b325-db3e-4a2d-dad9-27baa80465a9"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mahotas in /usr/local/lib/python3.7/dist-packages (1.4.11)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT"
      },
      "source": [
        "import mahotas.features.texture as mht\n",
        "import mahotas.features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VcTdaNVh9EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be1b3891-0faf-4c0d-923d-78cf8aaceb33"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'marquesgabi_fev_2020' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v7SRrc8mH2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7295cc27-1a9b-436d-9d6f-6d57f25da457"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip'\n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Doutorado' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[4] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHgqAnaFyCjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f8703a3-7cad-4e49-9fd9-9fdf78394fc0"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'MarquesGabi_Routines' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from segment_filter_not_conclude import Segmenta  # got image provided segmented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5MN5a_v4np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "077f3c3e-cabf-4a91-ab32-a483afd4b9e9"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     184  253.114349  250.017471  ...  163.355377  166.816162  162.337418\n",
            "1     164  159.576431  132.502090  ...  135.886368  138.710892  145.581787\n",
            "2     183  183.755264  185.328629  ...    1.269312    0.128789    1.329422\n",
            "3     133  170.711899  166.612198  ...  247.842117  251.880890  254.085876\n",
            "4     178  113.979431  111.269165  ...  155.007706  148.450317  132.045334\n",
            "5     143  100.795967   98.406723  ...    0.492885    0.408969    1.429948\n",
            "6     115   68.954254   65.572472  ...  152.954636  150.113251  142.234100\n",
            "7     183  139.351395  129.314438  ...    1.269312    0.128789    1.329422\n",
            "8     130  142.942490  138.786285  ...  248.254669  248.856827  248.514099\n",
            "9     126  193.567902  192.456802  ...  252.728394  253.074081  252.259277\n",
            "10    129  121.867561  126.214409  ...  133.104980  128.920441  128.351837\n",
            "11    190  187.408417  180.439102  ...  161.992905  152.040100  148.743271\n",
            "12    114   90.450912  100.566643  ...  153.569412  150.045868  142.927353\n",
            "13    117  168.661774  183.655121  ...  128.851410  116.573303  106.564835\n",
            "14    120  214.832230  218.675568  ...  184.608902  186.214447  184.712219\n",
            "15    127  217.050095  229.765076  ...   94.202492  128.721558  126.684357\n",
            "16    174   91.398613  141.702484  ...  166.536545  151.541290  148.515274\n",
            "17    101  113.672287  121.211159  ...   65.957954   63.848251   63.892952\n",
            "18    114  179.709457  178.302872  ...  122.191757  118.653122  113.323486\n",
            "19    175  156.337585  151.067200  ...  229.043182  240.747192  242.203186\n",
            "20    110  168.481659  153.593063  ...  169.570908  123.108429  110.639664\n",
            "21    100  135.862411  132.577591  ...    0.000000    0.960000    1.560000\n",
            "22    119  136.799301  145.152252  ...  132.892731  142.557098  150.370239\n",
            "23    131  129.250153  116.841202  ...  101.185936  116.979828  134.230927\n",
            "24    168  193.944443  188.500000  ...  151.083328  140.083328  135.750000\n",
            "25    121  151.159546  148.116867  ...  177.616974  176.221634  174.272446\n",
            "26    140  106.159996  135.479996  ...  252.159988  253.919998  250.959991\n",
            "27    106  238.899612  246.810608  ...  156.154510  157.106094  155.551086\n",
            "28    109  139.691940  142.895538  ...  206.319000  201.722321  194.399704\n",
            "29    166  115.847435  121.234566  ...  136.344742  131.035126  109.287987\n",
            "30    113  119.012680  116.443336  ...    1.000000    1.486726    1.072441\n",
            "31    118   82.811546   82.072098  ...  132.158859  137.678528  141.424591\n",
            "32    128  157.295898  154.856445  ...  100.884766  102.500000   97.536133\n",
            "33    185  181.607529  180.449585  ...    1.312491    0.165522    1.339518\n",
            "34    181    0.964989    1.570892  ...  122.407509  127.699829  128.042740\n",
            "35    130  140.454910  148.602142  ...   80.681183  117.918114  125.769478\n",
            "36    153  199.550186  194.477264  ...    0.654406    0.314537    1.399504\n",
            "37    174  152.077576  162.496109  ...  207.094223  190.438232  182.585144\n",
            "38    114  164.858719  162.304077  ...  242.047714  247.859039  245.546936\n",
            "39    115  221.025482  216.838104  ...  170.404984  167.938293  153.478256\n",
            "40    126    0.716049    0.049383  ...   88.617287  104.555557  121.641983\n",
            "41    176    0.229855    1.434401  ...    0.000000    0.598141    1.039773\n",
            "42    117  189.509399  202.952438  ...  203.246048  206.575867  207.743378\n",
            "43    131  233.051163  226.063339  ...    0.676942    1.000000    0.693258\n",
            "44    160    0.730625    0.280625  ...   99.275620  104.009995  111.548119\n",
            "45    190  230.239548  220.760101  ...  136.989685  140.524200  155.161774\n",
            "46    131   81.839691   81.079712  ...    1.023367    1.885962    1.031000\n",
            "47    105  129.546661  132.702225  ...    0.431111    0.213333    0.400000\n",
            "48    169   59.098492   65.269974  ...  130.102890  130.302856  130.204910\n",
            "49    108  125.707825  121.978050  ...    1.061728    0.640604    1.750343\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzpQ1Pz0fX5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ab216061-3a8b-41b2-c32c-f6917c049d19"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines\n",
        "# filename = 'model_ANN.pkl'\n",
        "filename = 'model_ANN_new.pkl'\n",
        "model = joblib.load(filename)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n!git clone https://github.com/ucfilho/MarquesGabi_Routines\\n%cd MarquesGabi_Routines\\n# filename = 'model_ANN.pkl'\\nfilename = 'model_ANN_new.pkl'\\nmodel = joblib.load(filename)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2emP4rNjQy",
        "outputId": "9e36c9e6-578f-49e0-d832-e05d960bd31c"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'MarquesGabi_Routines' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "Description = 'ANN without convolution '\n",
        "N1 = 200\n",
        "N2 = 10\n",
        "\n",
        "# make the CNN\n",
        "model = Sequential()\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=N1))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=N2))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbPQ1FSRG6A",
        "outputId": "981fe658-4074-4412-9da6-226212a8a456"
      },
      "source": [
        "\n",
        "# gives us back a <keras.callbacks.History object at 0x112e61a90>\n",
        "r = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), \n",
        "              epochs=200, batch_size=32)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 24s 81ms/step - loss: 0.6931 - accuracy: 0.4847 - val_loss: 0.6899 - val_accuracy: 0.4898\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.6892 - accuracy: 0.4995 - val_loss: 0.6860 - val_accuracy: 0.4898\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.6854 - accuracy: 0.5168 - val_loss: 0.6795 - val_accuracy: 0.5510\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.6790 - accuracy: 0.5525 - val_loss: 0.6726 - val_accuracy: 0.5578\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.6704 - accuracy: 0.5781 - val_loss: 0.6649 - val_accuracy: 0.5578\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.6625 - accuracy: 0.6159 - val_loss: 0.6549 - val_accuracy: 0.6735\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.6681 - accuracy: 0.6576 - val_loss: 0.6497 - val_accuracy: 0.5850\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.6574 - accuracy: 0.6195 - val_loss: 0.6346 - val_accuracy: 0.6939\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.6450 - accuracy: 0.7140 - val_loss: 0.6222 - val_accuracy: 0.7075\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.6163 - accuracy: 0.6867 - val_loss: 0.6091 - val_accuracy: 0.7347\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.6288 - accuracy: 0.7617 - val_loss: 0.5950 - val_accuracy: 0.7959\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.6048 - accuracy: 0.7186 - val_loss: 0.5821 - val_accuracy: 0.7687\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.5926 - accuracy: 0.7748 - val_loss: 0.5655 - val_accuracy: 0.8027\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.5501 - accuracy: 0.7807 - val_loss: 0.5464 - val_accuracy: 0.8231\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.5773 - accuracy: 0.8318 - val_loss: 0.5303 - val_accuracy: 0.8027\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.5349 - accuracy: 0.8024 - val_loss: 0.5114 - val_accuracy: 0.8776\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.5383 - accuracy: 0.8440 - val_loss: 0.5015 - val_accuracy: 0.8027\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.5370 - accuracy: 0.7832 - val_loss: 0.4748 - val_accuracy: 0.8776\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.4870 - accuracy: 0.8612 - val_loss: 0.4552 - val_accuracy: 0.9116\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.4785 - accuracy: 0.8461 - val_loss: 0.4367 - val_accuracy: 0.9048\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.4678 - accuracy: 0.8633 - val_loss: 0.4188 - val_accuracy: 0.9184\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.4641 - accuracy: 0.8429 - val_loss: 0.3999 - val_accuracy: 0.9388\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.4282 - accuracy: 0.8752 - val_loss: 0.3839 - val_accuracy: 0.9388\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.4430 - accuracy: 0.8680 - val_loss: 0.3958 - val_accuracy: 0.8435\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.4186 - accuracy: 0.8888 - val_loss: 0.3597 - val_accuracy: 0.9116\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.3753 - accuracy: 0.8705 - val_loss: 0.3416 - val_accuracy: 0.9524\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3875 - accuracy: 0.8812 - val_loss: 0.3411 - val_accuracy: 0.9184\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.3762 - accuracy: 0.8700 - val_loss: 0.3197 - val_accuracy: 0.9320\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.3477 - accuracy: 0.8965 - val_loss: 0.3077 - val_accuracy: 0.9388\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3519 - accuracy: 0.8857 - val_loss: 0.3044 - val_accuracy: 0.9320\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.3864 - accuracy: 0.8592 - val_loss: 0.3437 - val_accuracy: 0.8639\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3730 - accuracy: 0.8586 - val_loss: 0.2838 - val_accuracy: 0.9456\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3602 - accuracy: 0.8788 - val_loss: 0.2932 - val_accuracy: 0.9252\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3477 - accuracy: 0.8476 - val_loss: 0.2994 - val_accuracy: 0.9252\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.3435 - accuracy: 0.8584 - val_loss: 0.2709 - val_accuracy: 0.9320\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.3018 - accuracy: 0.9033 - val_loss: 0.2590 - val_accuracy: 0.9456\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3405 - accuracy: 0.8796 - val_loss: 0.2540 - val_accuracy: 0.9456\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.3234 - accuracy: 0.8741 - val_loss: 0.2482 - val_accuracy: 0.9524\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2763 - accuracy: 0.8972 - val_loss: 0.2448 - val_accuracy: 0.9524\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2560 - accuracy: 0.9164 - val_loss: 0.2385 - val_accuracy: 0.9524\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3027 - accuracy: 0.8910 - val_loss: 0.2364 - val_accuracy: 0.9456\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2765 - accuracy: 0.9309 - val_loss: 0.2301 - val_accuracy: 0.9524\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.3257 - accuracy: 0.8915 - val_loss: 0.2493 - val_accuracy: 0.9388\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2736 - accuracy: 0.9126 - val_loss: 0.2301 - val_accuracy: 0.9320\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2748 - accuracy: 0.9152 - val_loss: 0.2255 - val_accuracy: 0.9388\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2793 - accuracy: 0.9096 - val_loss: 0.2185 - val_accuracy: 0.9456\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2500 - accuracy: 0.9139 - val_loss: 0.2136 - val_accuracy: 0.9524\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2870 - accuracy: 0.9002 - val_loss: 0.2087 - val_accuracy: 0.9524\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2546 - accuracy: 0.9218 - val_loss: 0.2156 - val_accuracy: 0.9456\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2830 - accuracy: 0.9002 - val_loss: 0.2024 - val_accuracy: 0.9524\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2832 - accuracy: 0.8976 - val_loss: 0.2225 - val_accuracy: 0.9388\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2233 - accuracy: 0.9282 - val_loss: 0.1994 - val_accuracy: 0.9456\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2961 - accuracy: 0.8700 - val_loss: 0.2217 - val_accuracy: 0.9388\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2495 - accuracy: 0.9181 - val_loss: 0.2122 - val_accuracy: 0.9252\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2857 - accuracy: 0.9024 - val_loss: 0.2285 - val_accuracy: 0.9388\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2355 - accuracy: 0.9046 - val_loss: 0.1915 - val_accuracy: 0.9524\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2737 - accuracy: 0.9235 - val_loss: 0.2049 - val_accuracy: 0.9388\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2281 - accuracy: 0.9216 - val_loss: 0.1889 - val_accuracy: 0.9524\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2786 - accuracy: 0.8885 - val_loss: 0.1929 - val_accuracy: 0.9524\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2883 - accuracy: 0.8876 - val_loss: 0.1890 - val_accuracy: 0.9592\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2537 - accuracy: 0.9220 - val_loss: 0.1843 - val_accuracy: 0.9524\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2436 - accuracy: 0.9258 - val_loss: 0.1834 - val_accuracy: 0.9592\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2087 - accuracy: 0.9313 - val_loss: 0.1800 - val_accuracy: 0.9592\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2483 - accuracy: 0.9126 - val_loss: 0.1945 - val_accuracy: 0.9524\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2160 - accuracy: 0.9370 - val_loss: 0.1770 - val_accuracy: 0.9592\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.2392 - accuracy: 0.9063 - val_loss: 0.2085 - val_accuracy: 0.9456\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.2098 - accuracy: 0.9331 - val_loss: 0.1805 - val_accuracy: 0.9388\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2650 - accuracy: 0.8908 - val_loss: 0.1835 - val_accuracy: 0.9524\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1954 - accuracy: 0.9448 - val_loss: 0.1698 - val_accuracy: 0.9592\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2327 - accuracy: 0.9095 - val_loss: 0.1685 - val_accuracy: 0.9592\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2506 - accuracy: 0.9077 - val_loss: 0.1675 - val_accuracy: 0.9592\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2224 - accuracy: 0.9228 - val_loss: 0.1797 - val_accuracy: 0.9524\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2380 - accuracy: 0.9124 - val_loss: 0.1638 - val_accuracy: 0.9660\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2341 - accuracy: 0.9316 - val_loss: 0.1628 - val_accuracy: 0.9660\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1896 - accuracy: 0.9368 - val_loss: 0.1609 - val_accuracy: 0.9660\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2011 - accuracy: 0.9421 - val_loss: 0.1586 - val_accuracy: 0.9660\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2340 - accuracy: 0.9197 - val_loss: 0.1672 - val_accuracy: 0.9524\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1964 - accuracy: 0.9297 - val_loss: 0.1548 - val_accuracy: 0.9660\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1965 - accuracy: 0.9376 - val_loss: 0.1547 - val_accuracy: 0.9660\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1966 - accuracy: 0.9225 - val_loss: 0.1634 - val_accuracy: 0.9524\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2090 - accuracy: 0.9448 - val_loss: 0.1526 - val_accuracy: 0.9660\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2117 - accuracy: 0.9207 - val_loss: 0.1536 - val_accuracy: 0.9592\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2194 - accuracy: 0.9290 - val_loss: 0.1485 - val_accuracy: 0.9660\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2162 - accuracy: 0.9158 - val_loss: 0.1689 - val_accuracy: 0.9524\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2202 - accuracy: 0.9289 - val_loss: 0.1726 - val_accuracy: 0.9252\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2385 - accuracy: 0.9108 - val_loss: 0.1440 - val_accuracy: 0.9660\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2124 - accuracy: 0.9189 - val_loss: 0.1595 - val_accuracy: 0.9524\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2610 - accuracy: 0.9033 - val_loss: 0.1477 - val_accuracy: 0.9388\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1807 - accuracy: 0.9382 - val_loss: 0.1503 - val_accuracy: 0.9524\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1484 - accuracy: 0.9579 - val_loss: 0.1435 - val_accuracy: 0.9388\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1700 - accuracy: 0.9372 - val_loss: 0.1717 - val_accuracy: 0.9524\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1835 - accuracy: 0.9421 - val_loss: 0.1318 - val_accuracy: 0.9660\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1888 - accuracy: 0.9178 - val_loss: 0.1387 - val_accuracy: 0.9592\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1776 - accuracy: 0.9346 - val_loss: 0.1428 - val_accuracy: 0.9524\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.2037 - accuracy: 0.9191 - val_loss: 0.1318 - val_accuracy: 0.9592\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1702 - accuracy: 0.9573 - val_loss: 0.1385 - val_accuracy: 0.9592\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1771 - accuracy: 0.9405 - val_loss: 0.1336 - val_accuracy: 0.9456\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1929 - accuracy: 0.9401 - val_loss: 0.1539 - val_accuracy: 0.9592\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1695 - accuracy: 0.9417 - val_loss: 0.1293 - val_accuracy: 0.9592\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1576 - accuracy: 0.9455 - val_loss: 0.1384 - val_accuracy: 0.9592\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1643 - accuracy: 0.9503 - val_loss: 0.1313 - val_accuracy: 0.9524\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1676 - accuracy: 0.9438 - val_loss: 0.1222 - val_accuracy: 0.9660\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1514 - accuracy: 0.9454 - val_loss: 0.1204 - val_accuracy: 0.9660\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1715 - accuracy: 0.9310 - val_loss: 0.1178 - val_accuracy: 0.9660\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1502 - accuracy: 0.9496 - val_loss: 0.1171 - val_accuracy: 0.9660\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1648 - accuracy: 0.9504 - val_loss: 0.1154 - val_accuracy: 0.9660\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1579 - accuracy: 0.9481 - val_loss: 0.1165 - val_accuracy: 0.9592\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1593 - accuracy: 0.9376 - val_loss: 0.1406 - val_accuracy: 0.9388\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1912 - accuracy: 0.9257 - val_loss: 0.1209 - val_accuracy: 0.9592\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1616 - accuracy: 0.9489 - val_loss: 0.1247 - val_accuracy: 0.9660\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1779 - accuracy: 0.9404 - val_loss: 0.1514 - val_accuracy: 0.9252\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1962 - accuracy: 0.9202 - val_loss: 0.1286 - val_accuracy: 0.9660\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1753 - accuracy: 0.9493 - val_loss: 0.1159 - val_accuracy: 0.9524\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1573 - accuracy: 0.9446 - val_loss: 0.1131 - val_accuracy: 0.9660\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1780 - accuracy: 0.9416 - val_loss: 0.1097 - val_accuracy: 0.9592\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1522 - accuracy: 0.9569 - val_loss: 0.1088 - val_accuracy: 0.9592\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1438 - accuracy: 0.9584 - val_loss: 0.1071 - val_accuracy: 0.9592\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1562 - accuracy: 0.9410 - val_loss: 0.1058 - val_accuracy: 0.9592\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1624 - accuracy: 0.9482 - val_loss: 0.1052 - val_accuracy: 0.9660\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1372 - accuracy: 0.9495 - val_loss: 0.1049 - val_accuracy: 0.9592\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1387 - accuracy: 0.9649 - val_loss: 0.1051 - val_accuracy: 0.9592\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1514 - accuracy: 0.9644 - val_loss: 0.1048 - val_accuracy: 0.9592\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1575 - accuracy: 0.9350 - val_loss: 0.1200 - val_accuracy: 0.9524\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1713 - accuracy: 0.9506 - val_loss: 0.1273 - val_accuracy: 0.9728\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1775 - accuracy: 0.9471 - val_loss: 0.1030 - val_accuracy: 0.9592\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1397 - accuracy: 0.9453 - val_loss: 0.1051 - val_accuracy: 0.9660\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1635 - accuracy: 0.9470 - val_loss: 0.1058 - val_accuracy: 0.9660\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1279 - accuracy: 0.9556 - val_loss: 0.1151 - val_accuracy: 0.9728\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1119 - accuracy: 0.9718 - val_loss: 0.0988 - val_accuracy: 0.9592\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1326 - accuracy: 0.9597 - val_loss: 0.0981 - val_accuracy: 0.9592\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1436 - accuracy: 0.9514 - val_loss: 0.1126 - val_accuracy: 0.9592\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1163 - accuracy: 0.9595 - val_loss: 0.1149 - val_accuracy: 0.9728\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1458 - accuracy: 0.9436 - val_loss: 0.0996 - val_accuracy: 0.9660\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1262 - accuracy: 0.9518 - val_loss: 0.1054 - val_accuracy: 0.9524\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1307 - accuracy: 0.9546 - val_loss: 0.1172 - val_accuracy: 0.9524\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1536 - accuracy: 0.9557 - val_loss: 0.1109 - val_accuracy: 0.9728\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1432 - accuracy: 0.9549 - val_loss: 0.0971 - val_accuracy: 0.9660\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1206 - accuracy: 0.9643 - val_loss: 0.0960 - val_accuracy: 0.9660\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1374 - accuracy: 0.9446 - val_loss: 0.0952 - val_accuracy: 0.9660\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1189 - accuracy: 0.9604 - val_loss: 0.1032 - val_accuracy: 0.9660\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1213 - accuracy: 0.9723 - val_loss: 0.1004 - val_accuracy: 0.9660\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1321 - accuracy: 0.9582 - val_loss: 0.0957 - val_accuracy: 0.9592\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1217 - accuracy: 0.9591 - val_loss: 0.0930 - val_accuracy: 0.9660\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1214 - accuracy: 0.9638 - val_loss: 0.1041 - val_accuracy: 0.9592\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1188 - accuracy: 0.9744 - val_loss: 0.1008 - val_accuracy: 0.9728\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1381 - accuracy: 0.9544 - val_loss: 0.0919 - val_accuracy: 0.9660\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1104 - accuracy: 0.9712 - val_loss: 0.0937 - val_accuracy: 0.9592\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1001 - accuracy: 0.9795 - val_loss: 0.0970 - val_accuracy: 0.9728\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1293 - accuracy: 0.9546 - val_loss: 0.1161 - val_accuracy: 0.9524\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1348 - accuracy: 0.9643 - val_loss: 0.1501 - val_accuracy: 0.9388\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1360 - accuracy: 0.9384 - val_loss: 0.0918 - val_accuracy: 0.9660\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1069 - accuracy: 0.9707 - val_loss: 0.1018 - val_accuracy: 0.9524\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1235 - accuracy: 0.9612 - val_loss: 0.1374 - val_accuracy: 0.9388\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1339 - accuracy: 0.9528 - val_loss: 0.1132 - val_accuracy: 0.9592\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0999 - accuracy: 0.9699 - val_loss: 0.1153 - val_accuracy: 0.9660\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1383 - accuracy: 0.9431 - val_loss: 0.0995 - val_accuracy: 0.9728\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1600 - accuracy: 0.9419 - val_loss: 0.1061 - val_accuracy: 0.9524\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1152 - accuracy: 0.9701 - val_loss: 0.0997 - val_accuracy: 0.9728\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1285 - accuracy: 0.9648 - val_loss: 0.0878 - val_accuracy: 0.9660\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1108 - accuracy: 0.9540 - val_loss: 0.0889 - val_accuracy: 0.9660\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1154 - accuracy: 0.9650 - val_loss: 0.0895 - val_accuracy: 0.9728\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0856 - accuracy: 0.9746 - val_loss: 0.0890 - val_accuracy: 0.9660\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1006 - accuracy: 0.9759 - val_loss: 0.0960 - val_accuracy: 0.9728\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1160 - accuracy: 0.9489 - val_loss: 0.0891 - val_accuracy: 0.9728\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0890 - accuracy: 0.9807 - val_loss: 0.0968 - val_accuracy: 0.9524\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1130 - accuracy: 0.9621 - val_loss: 0.0937 - val_accuracy: 0.9728\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1038 - accuracy: 0.9638 - val_loss: 0.0947 - val_accuracy: 0.9524\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0957 - accuracy: 0.9783 - val_loss: 0.0861 - val_accuracy: 0.9660\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1119 - accuracy: 0.9706 - val_loss: 0.1295 - val_accuracy: 0.9456\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1147 - accuracy: 0.9541 - val_loss: 0.0935 - val_accuracy: 0.9524\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.1139 - accuracy: 0.9700 - val_loss: 0.0887 - val_accuracy: 0.9728\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1378 - accuracy: 0.9541 - val_loss: 0.0872 - val_accuracy: 0.9660\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1132 - accuracy: 0.9593 - val_loss: 0.0867 - val_accuracy: 0.9660\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0863 - accuracy: 0.9849 - val_loss: 0.0870 - val_accuracy: 0.9660\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0975 - accuracy: 0.9666 - val_loss: 0.0898 - val_accuracy: 0.9796\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1220 - accuracy: 0.9582 - val_loss: 0.0856 - val_accuracy: 0.9660\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1040 - accuracy: 0.9676 - val_loss: 0.0866 - val_accuracy: 0.9660\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1104 - accuracy: 0.9718 - val_loss: 0.0857 - val_accuracy: 0.9660\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0879 - accuracy: 0.9705 - val_loss: 0.0893 - val_accuracy: 0.9728\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0842 - accuracy: 0.9769 - val_loss: 0.0855 - val_accuracy: 0.9660\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0925 - accuracy: 0.9669 - val_loss: 0.0921 - val_accuracy: 0.9524\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1046 - accuracy: 0.9608 - val_loss: 0.0874 - val_accuracy: 0.9660\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1194 - accuracy: 0.9561 - val_loss: 0.0973 - val_accuracy: 0.9796\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1054 - accuracy: 0.9560 - val_loss: 0.0864 - val_accuracy: 0.9660\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0813 - accuracy: 0.9772 - val_loss: 0.0923 - val_accuracy: 0.9796\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1243 - accuracy: 0.9675 - val_loss: 0.0879 - val_accuracy: 0.9592\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0966 - accuracy: 0.9618 - val_loss: 0.0873 - val_accuracy: 0.9660\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0808 - accuracy: 0.9744 - val_loss: 0.0882 - val_accuracy: 0.9728\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0635 - accuracy: 0.9818 - val_loss: 0.0855 - val_accuracy: 0.9660\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0834 - accuracy: 0.9748 - val_loss: 0.0906 - val_accuracy: 0.9524\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1176 - accuracy: 0.9595 - val_loss: 0.0906 - val_accuracy: 0.9728\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0743 - accuracy: 0.9785 - val_loss: 0.0862 - val_accuracy: 0.9728\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0891 - accuracy: 0.9731 - val_loss: 0.1142 - val_accuracy: 0.9524\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1051 - accuracy: 0.9563 - val_loss: 0.1108 - val_accuracy: 0.9592\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.1088 - accuracy: 0.9614 - val_loss: 0.0874 - val_accuracy: 0.9660\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0812 - accuracy: 0.9824 - val_loss: 0.0875 - val_accuracy: 0.9660\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0803 - accuracy: 0.9785 - val_loss: 0.0869 - val_accuracy: 0.9660\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0870 - accuracy: 0.9740 - val_loss: 0.0928 - val_accuracy: 0.9728\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0781 - accuracy: 0.9844 - val_loss: 0.0863 - val_accuracy: 0.9660\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0925 - accuracy: 0.9659 - val_loss: 0.0938 - val_accuracy: 0.9524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpf0XlSARX78",
        "outputId": "5faba846-3f9e-4a1c-a4fe-72402e5b8460"
      },
      "source": [
        "pred_test= model.predict_classes(X_test)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predict   0   1\n",
            "Actual         \n",
            "0        67   5\n",
            "1         2  73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iFNNrlWV9tH",
        "outputId": "9e9e54e3-78dd-4fa5-b952-c58248a6e326"
      },
      "source": [
        "pred_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QISvYcJBgWbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd3216f8-f025-404d-ece8-475295d931bd"
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[0] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  prediction = model.predict_classes(result)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0   124.0   49.781475   50.079082  ...   68.232048   75.217468   94.662842\n",
            "1   109.0   71.474960   78.456192  ...   63.436237   70.593552   76.998566\n",
            "2   188.0   35.418285   42.421005  ...    6.247623    6.215935    5.620190\n",
            "6   127.0   60.390476   58.422722  ...   34.564884    8.062310    1.000000\n",
            "8   183.0   78.908005   81.991821  ...  145.432678  151.349609  141.707230\n",
            "10  198.0   52.067234   54.035198  ...   54.159576   60.785637   65.905518\n",
            "11  107.0   92.065765   95.124725  ...   52.210670   36.727226   36.155994\n",
            "13  163.0   56.860668   58.676464  ...   79.720200   70.884529   64.323723\n",
            "15  155.0    0.085078    0.123621  ...    0.462601    0.088658    0.000000\n",
            "18  125.0   89.957253   91.822350  ...   55.176262   57.910088   61.496517\n",
            "19  178.0  207.535553  125.215141  ...   77.365494   55.221062   28.750160\n",
            "21  128.0  118.627930  123.080078  ...   99.916992   98.119141   94.929688\n",
            "24  192.0  162.530807   61.984367  ...   66.585938   63.724388   62.315529\n",
            "25  164.0    1.049970    0.750149  ...   48.493156   51.063652   52.709099\n",
            "27  121.0   91.671677   90.433914  ...   67.456390   77.217537   76.553375\n",
            "29  112.0   92.125000   91.000000  ...   95.062500   97.750000   97.062500\n",
            "30  130.0   15.955503   16.183668  ...    1.978698    1.015858    0.000000\n",
            "32  107.0   76.542404   79.620407  ...   85.388939   75.950035   71.267357\n",
            "35  131.0   79.629211   76.806648  ...   98.014740  203.747971  191.227020\n",
            "36  128.0   66.967773   78.233398  ...   83.856445   86.542969   89.374023\n",
            "38  126.0   56.580250   62.703705  ...   56.654320   57.012344   65.086426\n",
            "39  137.0   75.957321   68.703339  ...   49.606262   36.585434   23.339441\n",
            "40  197.0   63.525318   65.548508  ...   65.162415   67.733574   68.187637\n",
            "42  186.0  102.222351  103.659042  ...    0.000000    0.000000    0.000000\n",
            "45  105.0   62.577785   61.875561  ...   66.191116   87.631119   85.631119\n",
            "46  187.0   67.776688   72.021706  ...    0.511024    1.430867    1.513169\n",
            "47  152.0  103.851105  107.092789  ...   50.000690   37.193214   25.864960\n",
            "49  114.0   44.683289   51.066788  ...    6.325639    2.704525    1.017236\n",
            "\n",
            "[28 rows x 785 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRbWgmX_LFH",
        "outputId": "b65a022c-d1cb-459f-ae6a-756a796a4572"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "from GetBetterSegm import GetBetter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'marquesgabi_paper_fev_2021' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAG_I6FwCvFr",
        "outputId": "882302dc-19ef-4800-e8b8-e85692a2146f"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "%cd marquesgabi_out_2020\n",
        "PSD_imageJ = 'Areas_ImageJ.csv'\n",
        "PSD_new = pd.read_csv(PSD_imageJ)\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'marquesgabi_out_2020' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020\n",
            "   Juntas   Area\n",
            "0       1  2.001\n",
            "1       2  0.820\n",
            "2       3  1.270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "PekBHQOT_6CP",
        "outputId": "392a15c2-da68-43ea-c9b9-6067111741dc"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>124.0</td>\n",
              "      <td>49.781475</td>\n",
              "      <td>50.079082</td>\n",
              "      <td>47.880333</td>\n",
              "      <td>43.773151</td>\n",
              "      <td>44.623306</td>\n",
              "      <td>46.336105</td>\n",
              "      <td>43.642036</td>\n",
              "      <td>43.623310</td>\n",
              "      <td>43.433922</td>\n",
              "      <td>44.044743</td>\n",
              "      <td>43.894897</td>\n",
              "      <td>42.920918</td>\n",
              "      <td>41.712795</td>\n",
              "      <td>41.551506</td>\n",
              "      <td>43.403744</td>\n",
              "      <td>45.159210</td>\n",
              "      <td>46.854317</td>\n",
              "      <td>50.423515</td>\n",
              "      <td>53.717999</td>\n",
              "      <td>61.404785</td>\n",
              "      <td>68.790840</td>\n",
              "      <td>73.082207</td>\n",
              "      <td>77.326744</td>\n",
              "      <td>82.018723</td>\n",
              "      <td>85.514053</td>\n",
              "      <td>86.508835</td>\n",
              "      <td>93.852234</td>\n",
              "      <td>96.668045</td>\n",
              "      <td>50.444324</td>\n",
              "      <td>49.459938</td>\n",
              "      <td>46.105095</td>\n",
              "      <td>44.928192</td>\n",
              "      <td>47.048901</td>\n",
              "      <td>46.498432</td>\n",
              "      <td>43.844952</td>\n",
              "      <td>44.354836</td>\n",
              "      <td>43.751297</td>\n",
              "      <td>44.651402</td>\n",
              "      <td>45.597290</td>\n",
              "      <td>...</td>\n",
              "      <td>21.676380</td>\n",
              "      <td>20.097815</td>\n",
              "      <td>29.744015</td>\n",
              "      <td>48.724243</td>\n",
              "      <td>57.673252</td>\n",
              "      <td>61.519249</td>\n",
              "      <td>66.454727</td>\n",
              "      <td>69.828300</td>\n",
              "      <td>71.337135</td>\n",
              "      <td>67.426636</td>\n",
              "      <td>69.586884</td>\n",
              "      <td>91.735687</td>\n",
              "      <td>94.009361</td>\n",
              "      <td>90.024971</td>\n",
              "      <td>86.914665</td>\n",
              "      <td>83.914673</td>\n",
              "      <td>81.844940</td>\n",
              "      <td>80.833496</td>\n",
              "      <td>78.798126</td>\n",
              "      <td>75.226845</td>\n",
              "      <td>75.122787</td>\n",
              "      <td>74.055145</td>\n",
              "      <td>74.126945</td>\n",
              "      <td>72.491142</td>\n",
              "      <td>63.625385</td>\n",
              "      <td>43.027054</td>\n",
              "      <td>27.045784</td>\n",
              "      <td>23.604576</td>\n",
              "      <td>19.913631</td>\n",
              "      <td>20.095732</td>\n",
              "      <td>35.847031</td>\n",
              "      <td>56.123829</td>\n",
              "      <td>62.260143</td>\n",
              "      <td>65.521332</td>\n",
              "      <td>68.232048</td>\n",
              "      <td>69.002083</td>\n",
              "      <td>70.896973</td>\n",
              "      <td>68.232048</td>\n",
              "      <td>75.217468</td>\n",
              "      <td>94.662842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>109.0</td>\n",
              "      <td>71.474960</td>\n",
              "      <td>78.456192</td>\n",
              "      <td>77.404762</td>\n",
              "      <td>68.784271</td>\n",
              "      <td>65.971046</td>\n",
              "      <td>69.445335</td>\n",
              "      <td>67.268661</td>\n",
              "      <td>63.274384</td>\n",
              "      <td>57.632187</td>\n",
              "      <td>56.582108</td>\n",
              "      <td>53.114132</td>\n",
              "      <td>53.565441</td>\n",
              "      <td>55.840672</td>\n",
              "      <td>57.815079</td>\n",
              "      <td>59.149651</td>\n",
              "      <td>57.745560</td>\n",
              "      <td>52.744465</td>\n",
              "      <td>47.435314</td>\n",
              "      <td>43.184326</td>\n",
              "      <td>45.685974</td>\n",
              "      <td>49.960606</td>\n",
              "      <td>53.473610</td>\n",
              "      <td>56.262772</td>\n",
              "      <td>57.344666</td>\n",
              "      <td>57.179276</td>\n",
              "      <td>58.641281</td>\n",
              "      <td>59.057400</td>\n",
              "      <td>60.020451</td>\n",
              "      <td>84.795563</td>\n",
              "      <td>85.467888</td>\n",
              "      <td>74.432617</td>\n",
              "      <td>67.694801</td>\n",
              "      <td>68.772491</td>\n",
              "      <td>71.676453</td>\n",
              "      <td>65.960861</td>\n",
              "      <td>59.278931</td>\n",
              "      <td>56.890499</td>\n",
              "      <td>55.759956</td>\n",
              "      <td>53.381279</td>\n",
              "      <td>...</td>\n",
              "      <td>90.492805</td>\n",
              "      <td>99.850937</td>\n",
              "      <td>104.321846</td>\n",
              "      <td>109.962975</td>\n",
              "      <td>116.324966</td>\n",
              "      <td>112.302238</td>\n",
              "      <td>85.895966</td>\n",
              "      <td>61.332718</td>\n",
              "      <td>58.603477</td>\n",
              "      <td>60.736130</td>\n",
              "      <td>64.318489</td>\n",
              "      <td>69.908417</td>\n",
              "      <td>77.839737</td>\n",
              "      <td>79.023483</td>\n",
              "      <td>78.468056</td>\n",
              "      <td>79.381958</td>\n",
              "      <td>78.624771</td>\n",
              "      <td>77.777878</td>\n",
              "      <td>76.454842</td>\n",
              "      <td>74.932076</td>\n",
              "      <td>74.014397</td>\n",
              "      <td>74.170776</td>\n",
              "      <td>73.057991</td>\n",
              "      <td>73.503746</td>\n",
              "      <td>72.099403</td>\n",
              "      <td>71.350304</td>\n",
              "      <td>73.827454</td>\n",
              "      <td>81.368744</td>\n",
              "      <td>90.609123</td>\n",
              "      <td>99.855812</td>\n",
              "      <td>104.900253</td>\n",
              "      <td>110.465958</td>\n",
              "      <td>114.345673</td>\n",
              "      <td>109.487000</td>\n",
              "      <td>95.521919</td>\n",
              "      <td>71.636055</td>\n",
              "      <td>60.960861</td>\n",
              "      <td>63.436237</td>\n",
              "      <td>70.593552</td>\n",
              "      <td>76.998566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>188.0</td>\n",
              "      <td>35.418285</td>\n",
              "      <td>42.421005</td>\n",
              "      <td>69.515167</td>\n",
              "      <td>96.645988</td>\n",
              "      <td>105.017654</td>\n",
              "      <td>105.245354</td>\n",
              "      <td>97.218651</td>\n",
              "      <td>99.826622</td>\n",
              "      <td>105.066994</td>\n",
              "      <td>99.149384</td>\n",
              "      <td>97.942520</td>\n",
              "      <td>101.666359</td>\n",
              "      <td>97.837021</td>\n",
              "      <td>96.540062</td>\n",
              "      <td>99.660019</td>\n",
              "      <td>105.187866</td>\n",
              "      <td>110.454956</td>\n",
              "      <td>113.416916</td>\n",
              "      <td>116.504295</td>\n",
              "      <td>117.066093</td>\n",
              "      <td>112.846535</td>\n",
              "      <td>81.926666</td>\n",
              "      <td>62.426888</td>\n",
              "      <td>61.039829</td>\n",
              "      <td>60.563602</td>\n",
              "      <td>60.584431</td>\n",
              "      <td>57.992760</td>\n",
              "      <td>56.399273</td>\n",
              "      <td>35.999996</td>\n",
              "      <td>35.679947</td>\n",
              "      <td>35.282028</td>\n",
              "      <td>55.449520</td>\n",
              "      <td>99.235847</td>\n",
              "      <td>116.694435</td>\n",
              "      <td>124.270248</td>\n",
              "      <td>119.385231</td>\n",
              "      <td>103.420105</td>\n",
              "      <td>100.074234</td>\n",
              "      <td>100.200089</td>\n",
              "      <td>...</td>\n",
              "      <td>0.315075</td>\n",
              "      <td>0.340425</td>\n",
              "      <td>0.340425</td>\n",
              "      <td>0.061566</td>\n",
              "      <td>0.042553</td>\n",
              "      <td>0.042553</td>\n",
              "      <td>0.305568</td>\n",
              "      <td>0.258941</td>\n",
              "      <td>0.244454</td>\n",
              "      <td>0.191489</td>\n",
              "      <td>0.191489</td>\n",
              "      <td>0.169307</td>\n",
              "      <td>5.594839</td>\n",
              "      <td>5.778633</td>\n",
              "      <td>5.946582</td>\n",
              "      <td>5.807152</td>\n",
              "      <td>5.861023</td>\n",
              "      <td>6.149388</td>\n",
              "      <td>6.488456</td>\n",
              "      <td>6.086012</td>\n",
              "      <td>6.143051</td>\n",
              "      <td>6.133545</td>\n",
              "      <td>6.517881</td>\n",
              "      <td>8.272974</td>\n",
              "      <td>9.579901</td>\n",
              "      <td>6.783160</td>\n",
              "      <td>5.918062</td>\n",
              "      <td>5.810321</td>\n",
              "      <td>6.333182</td>\n",
              "      <td>6.190584</td>\n",
              "      <td>6.231779</td>\n",
              "      <td>5.965595</td>\n",
              "      <td>5.639203</td>\n",
              "      <td>6.032141</td>\n",
              "      <td>7.353553</td>\n",
              "      <td>7.276595</td>\n",
              "      <td>6.277048</td>\n",
              "      <td>6.247623</td>\n",
              "      <td>6.215935</td>\n",
              "      <td>5.620190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>127.0</td>\n",
              "      <td>60.390476</td>\n",
              "      <td>58.422722</td>\n",
              "      <td>36.294750</td>\n",
              "      <td>6.571269</td>\n",
              "      <td>6.485026</td>\n",
              "      <td>13.973898</td>\n",
              "      <td>28.969866</td>\n",
              "      <td>34.830364</td>\n",
              "      <td>37.471512</td>\n",
              "      <td>40.284084</td>\n",
              "      <td>40.434376</td>\n",
              "      <td>39.734264</td>\n",
              "      <td>37.954185</td>\n",
              "      <td>35.002602</td>\n",
              "      <td>32.942650</td>\n",
              "      <td>30.083206</td>\n",
              "      <td>30.011469</td>\n",
              "      <td>30.776678</td>\n",
              "      <td>31.514975</td>\n",
              "      <td>31.447392</td>\n",
              "      <td>32.794655</td>\n",
              "      <td>33.852005</td>\n",
              "      <td>36.889206</td>\n",
              "      <td>39.670532</td>\n",
              "      <td>42.036705</td>\n",
              "      <td>42.262138</td>\n",
              "      <td>37.875565</td>\n",
              "      <td>32.571457</td>\n",
              "      <td>59.031059</td>\n",
              "      <td>56.146194</td>\n",
              "      <td>39.566742</td>\n",
              "      <td>9.679087</td>\n",
              "      <td>8.750264</td>\n",
              "      <td>12.813504</td>\n",
              "      <td>25.628059</td>\n",
              "      <td>33.931118</td>\n",
              "      <td>36.628433</td>\n",
              "      <td>37.231319</td>\n",
              "      <td>36.972408</td>\n",
              "      <td>...</td>\n",
              "      <td>64.490173</td>\n",
              "      <td>63.216194</td>\n",
              "      <td>61.443546</td>\n",
              "      <td>60.390163</td>\n",
              "      <td>58.755035</td>\n",
              "      <td>57.454273</td>\n",
              "      <td>55.228279</td>\n",
              "      <td>54.686771</td>\n",
              "      <td>51.120777</td>\n",
              "      <td>41.828445</td>\n",
              "      <td>14.402071</td>\n",
              "      <td>0.998388</td>\n",
              "      <td>21.802467</td>\n",
              "      <td>42.028645</td>\n",
              "      <td>61.978737</td>\n",
              "      <td>63.735199</td>\n",
              "      <td>61.215263</td>\n",
              "      <td>62.666935</td>\n",
              "      <td>61.234856</td>\n",
              "      <td>66.218796</td>\n",
              "      <td>67.006325</td>\n",
              "      <td>67.274292</td>\n",
              "      <td>64.454956</td>\n",
              "      <td>64.945503</td>\n",
              "      <td>66.482300</td>\n",
              "      <td>65.878670</td>\n",
              "      <td>66.040794</td>\n",
              "      <td>67.039871</td>\n",
              "      <td>64.989273</td>\n",
              "      <td>62.769917</td>\n",
              "      <td>60.626141</td>\n",
              "      <td>58.015556</td>\n",
              "      <td>56.282227</td>\n",
              "      <td>53.789074</td>\n",
              "      <td>53.070927</td>\n",
              "      <td>52.565132</td>\n",
              "      <td>47.104034</td>\n",
              "      <td>34.564884</td>\n",
              "      <td>8.062310</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>183.0</td>\n",
              "      <td>78.908005</td>\n",
              "      <td>81.991821</td>\n",
              "      <td>85.015175</td>\n",
              "      <td>86.614502</td>\n",
              "      <td>92.178276</td>\n",
              "      <td>90.299324</td>\n",
              "      <td>87.696289</td>\n",
              "      <td>98.299950</td>\n",
              "      <td>100.549789</td>\n",
              "      <td>100.848877</td>\n",
              "      <td>96.918770</td>\n",
              "      <td>93.094299</td>\n",
              "      <td>91.574425</td>\n",
              "      <td>95.063454</td>\n",
              "      <td>100.035118</td>\n",
              "      <td>103.804001</td>\n",
              "      <td>103.784081</td>\n",
              "      <td>100.818039</td>\n",
              "      <td>96.558868</td>\n",
              "      <td>91.355843</td>\n",
              "      <td>79.118248</td>\n",
              "      <td>81.576065</td>\n",
              "      <td>85.917984</td>\n",
              "      <td>87.988083</td>\n",
              "      <td>92.592674</td>\n",
              "      <td>93.498878</td>\n",
              "      <td>95.918869</td>\n",
              "      <td>101.296700</td>\n",
              "      <td>77.964172</td>\n",
              "      <td>80.332176</td>\n",
              "      <td>84.778259</td>\n",
              "      <td>84.710182</td>\n",
              "      <td>78.482552</td>\n",
              "      <td>77.918297</td>\n",
              "      <td>93.395599</td>\n",
              "      <td>99.537727</td>\n",
              "      <td>100.248741</td>\n",
              "      <td>97.771378</td>\n",
              "      <td>97.135330</td>\n",
              "      <td>...</td>\n",
              "      <td>96.970528</td>\n",
              "      <td>89.538841</td>\n",
              "      <td>85.321625</td>\n",
              "      <td>97.727730</td>\n",
              "      <td>101.887871</td>\n",
              "      <td>105.612495</td>\n",
              "      <td>112.334198</td>\n",
              "      <td>117.585960</td>\n",
              "      <td>127.362747</td>\n",
              "      <td>127.995605</td>\n",
              "      <td>108.032150</td>\n",
              "      <td>55.451256</td>\n",
              "      <td>94.374039</td>\n",
              "      <td>62.657890</td>\n",
              "      <td>32.807789</td>\n",
              "      <td>37.602974</td>\n",
              "      <td>41.475136</td>\n",
              "      <td>44.112785</td>\n",
              "      <td>44.205795</td>\n",
              "      <td>46.047894</td>\n",
              "      <td>69.455704</td>\n",
              "      <td>87.644119</td>\n",
              "      <td>87.907204</td>\n",
              "      <td>86.087379</td>\n",
              "      <td>87.723488</td>\n",
              "      <td>97.205650</td>\n",
              "      <td>100.517090</td>\n",
              "      <td>97.639915</td>\n",
              "      <td>92.111801</td>\n",
              "      <td>91.176804</td>\n",
              "      <td>96.747589</td>\n",
              "      <td>103.815231</td>\n",
              "      <td>105.554871</td>\n",
              "      <td>103.954292</td>\n",
              "      <td>108.930153</td>\n",
              "      <td>117.208694</td>\n",
              "      <td>131.184601</td>\n",
              "      <td>145.432678</td>\n",
              "      <td>151.349609</td>\n",
              "      <td>141.707230</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Width          0          1  ...         781         782         783\n",
              "0  124.0  49.781475  50.079082  ...   68.232048   75.217468   94.662842\n",
              "1  109.0  71.474960  78.456192  ...   63.436237   70.593552   76.998566\n",
              "2  188.0  35.418285  42.421005  ...    6.247623    6.215935    5.620190\n",
              "6  127.0  60.390476  58.422722  ...   34.564884    8.062310    1.000000\n",
              "8  183.0  78.908005  81.991821  ...  145.432678  151.349609  141.707230\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "Area = np.array(PSD_new['Area'])\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J705kDqsE8f",
        "outputId": "7062621d-1958-4834-9cad-00d2fbaa2505"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(490, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wCFDX8esLoQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn-F050Hr9Ui"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Vfk_fNXGDK5_",
        "outputId": "0e308609-fb3c-4a0c-c6c3-9d44e4e3b6f7"
      },
      "source": [
        " wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        " wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        " X = pd.DataFrame([Diam1,Diameter_All])\n",
        " wts = pd.DataFrame([wt1,wt2])\n",
        "plt.hist(X,weights=wts)\n",
        "plt.legend(['Image J','CNN'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f9969d27d10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nGDbBEeiUij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "4050eb19-3ebf-467b-800c-8139c5f6e096"
      },
      "source": [
        "# plt.hist(x, bins=bins, density=True, histtype='step', cumulative=-1,label='Reversed emp.')\n",
        "plt.hist(X, density=True, histtype='step', cumulative=True,label='Reversed emp.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.07368421, 0.2       , 0.41052632, 0.75789474, 0.88421053,\n",
              "         0.96842105, 0.97894737, 0.97894737, 0.98947368, 1.        ],\n",
              "        [0.17857143, 0.46428571, 0.64285714, 0.71428571, 0.78571429,\n",
              "         0.96428571, 1.        , 1.        , 1.        , 1.        ]]),\n",
              " array([0.75356807, 0.90995518, 1.06634229, 1.22272941, 1.37911652,\n",
              "        1.53550363, 1.69189074, 1.84827785, 2.00466497, 2.16105208,\n",
              "        2.31743919]),\n",
              " <a list of 2 Lists of Patches objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "9xENlBUUxfTu",
        "outputId": "e54d0c94-70c2-4f27-f12e-6c4aa9cd4ede"
      },
      "source": [
        "Obj = plt.hist(X, density=True, histtype='step', cumulative=True,label='Reversed emp.')\n",
        "Y1, Y2 = Obj[0]\n",
        "Rsquared = r2_score(Y1, Y2)\n",
        "print('r_squared =',Rsquared)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r_squared = 0.8727793322958461\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KukfpGTTKlj"
      },
      "source": [
        "#df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bT8GFymJAII"
      },
      "source": [
        "# r.history['accuracy']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uPdRxL2VwLR"
      },
      "source": [
        "acc_train = r.history['accuracy'][-1]\n",
        "acc_test = r.history['val_accuracy'][-1]\n",
        "loss_train = r.history['loss'][-1]\n",
        "loss_test = r.history['val_loss'][-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euTd_-CYN1v0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "3b57e19b-0d10-43b9-c245-31910c4308c2"
      },
      "source": [
        "\n",
        "df = pd.DataFrame({'N1':N1, 'N2':N2,'R^2':Rsquared,\n",
        "                   'acc train':acc_train,'acc test':acc_test,\n",
        "                   'loss train':loss_train,'loss test':loss_test,\n",
        "                   'Details':Description},\n",
        "                  index= [0])\n",
        "Arq = \"output.xlsx\"\n",
        "df.to_excel(Arq)\n",
        "files.download(Arq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_0bd554b8-ae19-4a9c-8d7f-0e5285d5924f\", \"output.xlsx\", 5142)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "ZZHa1j4HT9Dq",
        "outputId": "321e0c0f-0eb8-46bd-d42d-9652ff8df7ee"
      },
      "source": [
        "counts, bins, bars = plt.hist(X,weights=wts)\n",
        "print(bars)\n",
        "print(bins)\n",
        "print(counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<a list of 2 Lists of Patches objects>\n",
            "[0.75356807 0.90995518 1.06634229 1.22272941 1.37911652 1.53550363\n",
            " 1.69189074 1.84827785 2.00466497 2.16105208 2.31743919]\n",
            "[[ 7.36842105 12.63157895 21.05263158 34.73684211 12.63157895  8.42105263\n",
            "   1.05263158  0.          1.05263158  1.05263158]\n",
            " [17.85714286 28.57142857 17.85714286  7.14285714  7.14285714 17.85714286\n",
            "   3.57142857  0.          0.          0.        ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o_vDGeWUwIZ",
        "outputId": "2209e0bc-546d-442e-9ee0-36a94b9d1307"
      },
      "source": [
        "print(counts.sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200.00000000000006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "KcH52-6iJQ8t",
        "outputId": "e2600be9-1cfb-4bba-9c72-462d3d919ae5"
      },
      "source": [
        "\n",
        "plt.hist([Diam1,Diameter_All])\n",
        "plt.legend(['Image J','CNN'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f99605ff9d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r11AxFK_JIii",
        "outputId": "b40d5ac1-156c-4a2e-fdc2-3c2f165d09f1"
      },
      "source": [
        "[Diam1,Diameter_All]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.59616801403081,\n",
              "  1.0217907939900581,\n",
              "  1.2716187407449044,\n",
              "  1.104429030701514,\n",
              "  1.2163487785097904,\n",
              "  1.6013445735058454,\n",
              "  1.1715597420637607,\n",
              "  1.2534662333717612,\n",
              "  1.2676073151634049,\n",
              "  1.309600575274104,\n",
              "  1.292966945531582,\n",
              "  1.7658322811231006,\n",
              "  1.3564037533648712,\n",
              "  1.2407040781688483,\n",
              "  2.130217298173151,\n",
              "  1.4228319915327,\n",
              "  1.0651086490865755,\n",
              "  1.3008210311003705,\n",
              "  1.336545951796433,\n",
              "  0.8927754224911278,\n",
              "  1.4494292838262302,\n",
              "  1.4052738287907582,\n",
              "  1.6421697097891788,\n",
              "  1.2329833804288621,\n",
              "  1.19042665178928,\n",
              "  1.1682948223612457,\n",
              "  1.1518314137121108,\n",
              "  0.9607802401865855,\n",
              "  2.317439190074449,\n",
              "  1.0591147430338594,\n",
              "  1.4308630919602832,\n",
              "  0.7535680705496237,\n",
              "  0.8608283307581511,\n",
              "  1.2776122636975893,\n",
              "  1.3745862957220916,\n",
              "  1.259546137598783,\n",
              "  1.2978813187979172,\n",
              "  1.2412170838050638,\n",
              "  1.6009469708743893,\n",
              "  1.3149369953539032,\n",
              "  1.417901703622935,\n",
              "  1.2478669653497139,\n",
              "  1.1055812783082735,\n",
              "  0.9561307405997607,\n",
              "  0.9487783503683882,\n",
              "  1.1238565871041026,\n",
              "  1.2058356273089446,\n",
              "  1.2801012827406097,\n",
              "  0.8733100751144249,\n",
              "  0.9194732501297403,\n",
              "  1.6425573339441792,\n",
              "  1.085826790250066,\n",
              "  1.0639125693728595,\n",
              "  1.0875842666474016,\n",
              "  1.417901703622935,\n",
              "  1.550443891425932,\n",
              "  0.7825779328716171,\n",
              "  1.4690612745308145,\n",
              "  1.053086721720641,\n",
              "  1.2676073151634049,\n",
              "  0.7744003006005755,\n",
              "  1.3787482149724068,\n",
              "  1.363892581861956,\n",
              "  1.299352006316543,\n",
              "  1.2870449283923413,\n",
              "  1.11817763925502,\n",
              "  0.9474354220939228,\n",
              "  1.5218484589055707,\n",
              "  1.3526437911676632,\n",
              "  1.1556938532445284,\n",
              "  1.6013445735058454,\n",
              "  1.274619025074578,\n",
              "  1.422384489715834,\n",
              "  1.3408259533459403,\n",
              "  1.172646028567008,\n",
              "  1.1490645795125545,\n",
              "  1.459060149136146,\n",
              "  1.2483770274864237,\n",
              "  1.336545951796433,\n",
              "  0.9601174044814821,\n",
              "  1.4867225193896279,\n",
              "  1.4277452542806772,\n",
              "  1.35028849808504,\n",
              "  0.7560982446653928,\n",
              "  1.259040600296622,\n",
              "  1.13456827900627,\n",
              "  1.6549133695530214,\n",
              "  1.1204526724091788,\n",
              "  1.1176081573544434,\n",
              "  0.9153095762832032,\n",
              "  1.1639273497938836,\n",
              "  1.3066806149514323,\n",
              "  1.1529362882239027,\n",
              "  1.3047303442899274,\n",
              "  1.3066806149514323],\n",
              " [1.0539494635242008,\n",
              "  1.0416553924281398,\n",
              "  1.463016388064762,\n",
              "  1.0318456014714243,\n",
              "  1.2768275393402608,\n",
              "  1.6369291992821535,\n",
              "  0.7554659720728326,\n",
              "  1.5560433005811647,\n",
              "  1.2574803157639014,\n",
              "  1.0200799540768852,\n",
              "  1.4923983163261216,\n",
              "  1.1154585078878958,\n",
              "  1.6602671996537957,\n",
              "  1.1669215741295502,\n",
              "  1.1637061306467669,\n",
              "  0.7783143007204917,\n",
              "  0.9718415162664953,\n",
              "  0.8393410544312058,\n",
              "  1.181872380700041,\n",
              "  0.9263972491744885,\n",
              "  0.9757915445140601,\n",
              "  0.9933758471401505,\n",
              "  1.6585480771153789,\n",
              "  1.7411265910577098,\n",
              "  0.7998500536260826,\n",
              "  1.5808322448720415,\n",
              "  1.19044685495041,\n",
              "  0.7711696507097084]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    }
  ]
}