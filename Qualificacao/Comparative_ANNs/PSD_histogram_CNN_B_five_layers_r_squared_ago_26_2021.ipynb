{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_CNN_B_five_layers_r_squared_ago_26_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/Qualificacao/Comparative_ANNs/PSD_histogram_CNN_B_five_layers_r_squared_ago_26_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from sklearn.metrics import r2_score\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZEvJvfoibE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "920e19e0-26e3-4e0b-9bf4-3940798596b6"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mahotas\n",
            "  Downloading mahotas-1.4.11-cp37-cp37m-manylinux2010_x86_64.whl (5.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7 MB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.19.5)\n",
            "Installing collected packages: mahotas\n",
            "Successfully installed mahotas-1.4.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT"
      },
      "source": [
        "import mahotas.features.texture as mht\n",
        "import mahotas.features"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VcTdaNVh9EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a98377e4-4542-40c5-99bd-02ddaa2e55e4"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_fev_2020'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 73 (delta 37), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n",
            "/content/marquesgabi_fev_2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v7SRrc8mH2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea96ee3f-770f-47da-aac8-966e3278d73e"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip'\n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Doutorado'...\n",
            "remote: Enumerating objects: 408, done.\u001b[K\n",
            "remote: Counting objects: 100% (158/158), done.\u001b[K\n",
            "remote: Compressing objects: 100% (157/157), done.\u001b[K\n",
            "remote: Total 408 (delta 66), reused 0 (delta 0), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (408/408), 165.55 MiB | 25.03 MiB/s, done.\n",
            "Resolving deltas: 100% (189/189), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[4] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHgqAnaFyCjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d988eec8-1d2f-45ff-9943-7add63bffc2f"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (160/160), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 3 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 14.14 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from segment_filter_not_conclude import Segmenta  # got image provided segmented"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5MN5a_v4np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4612a464-0dae-4b5d-c144-7ad3fc9cf621"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     117  171.303452  176.847183  ...   76.137405   73.075096   96.487335\n",
            "1     135  153.019196  174.301178  ...   21.045870    5.038024    5.751331\n",
            "2     112  146.687500  147.312500  ...  164.500000  164.687500  159.687500\n",
            "3     136    0.424740    1.395329  ...  131.438599  129.035477  126.452423\n",
            "4     105  155.133347  156.920029  ...  118.791122  112.506676  111.200012\n",
            "5     168  164.333328  168.194443  ...  193.805557  164.666672  126.777779\n",
            "6     144  253.924377  253.729172  ...  172.255402  172.145065  171.179016\n",
            "7     166  252.528793  250.503845  ...  181.678894  189.903595  187.581635\n",
            "8     133   69.747925   72.739616  ...  223.534622  180.310242  160.349030\n",
            "9     102  120.074211  122.750870  ...    0.745098    1.372549    1.725490\n",
            "10    176  190.601761  192.121384  ...   64.683365   82.294418   89.597618\n",
            "11    145  151.087128  143.431580  ...  110.264305  109.602089  108.112923\n",
            "12    196  184.387756  175.163269  ...  111.000000  140.204086  138.081635\n",
            "13    195   61.173660   59.703568  ...  161.264740  132.108353  134.166336\n",
            "14    123  133.792130  124.500961  ...   49.899666   41.389584   21.720867\n",
            "15    108  143.167343  147.308624  ...  166.868317  173.702316  172.190674\n",
            "16    120  133.502213  128.238892  ...  202.128891  204.145554  205.955551\n",
            "17    124    0.373569    1.244537  ...   98.511955   95.709671   96.307999\n",
            "18    131  108.765213   87.427414  ...    0.267059    0.541868    1.473166\n",
            "19    193   55.963917   56.603504  ...   99.087280  100.964722  107.463531\n",
            "20    101  210.257629  218.389969  ...  137.342819  133.009811  120.686897\n",
            "21    134  179.274017  195.738708  ...  212.029861  197.681900  185.599014\n",
            "22    154  125.132240  134.404984  ...    0.685950    0.322314    1.413223\n",
            "23    137   53.274445   57.536041  ...    0.380361    0.467952    1.446055\n",
            "24    152  168.567871  178.475067  ...  115.110809  125.146812  126.736839\n",
            "25    185  149.843475  165.918625  ...  143.127762  165.314667  187.130661\n",
            "26    107   15.638571   11.629837  ...  160.705994  151.696030  148.707840\n",
            "27    185  163.906982  161.104828  ...    0.601286    0.536421    1.651220\n",
            "28    176  121.098648  106.232956  ...    1.140496    0.193182    1.368802\n",
            "29    184    1.645085    0.706994  ...  252.295837  245.771255  252.545837\n",
            "30    116  156.260422  168.065399  ...  195.323425  204.003555  197.571930\n",
            "31    117  196.563446  190.887787  ...  148.628021  144.882965  140.417206\n",
            "32    118   97.376038   73.894287  ...   80.952316   79.382645  108.137604\n",
            "33    110  152.435364  142.013550  ...  201.133881  198.212891  198.932236\n",
            "34    127  112.793480  114.839226  ...  171.214157  179.101685  177.474182\n",
            "35    120    0.121111    1.031111  ...   93.091110  110.046669  114.054443\n",
            "36    135  134.507874  138.880432  ...  121.551765  125.265068  122.383751\n",
            "37    123  160.627945  161.057587  ...  137.286469  143.836731  153.711548\n",
            "38    115  241.580246  226.486877  ...  140.602036  144.488480  147.106537\n",
            "39    154  144.487610  180.867798  ...    0.669422    0.305785    1.396694\n",
            "40    134  118.434616   99.741371  ...    0.317888    0.496993    1.452217\n",
            "41    132   87.469238   91.069794  ...  152.036743  158.715332  171.281921\n",
            "42    133  173.628815  181.828262  ...  120.781166  129.199448  134.279785\n",
            "43    115  179.541306  192.914322  ...    1.381021    0.598412    0.059282\n",
            "44    140    1.680000    2.920000  ...  101.239998  103.199997  111.119995\n",
            "45    109  153.088287  164.959167  ...  212.027252  213.889816  205.921295\n",
            "46    126   72.530869   70.061729  ...  183.839508  163.753098  144.716049\n",
            "47    177  252.947433  248.089767  ...  155.222809  150.768829  146.399979\n",
            "48    175  167.051178  160.503998  ...  124.409592  130.195190  134.495987\n",
            "49    110  190.889252  198.720642  ...  178.003296  187.008911  193.804306\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzpQ1Pz0fX5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "e29218a1-260a-4004-baaa-696bcd9076a9"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines\n",
        "# filename = 'model_ANN.pkl'\n",
        "filename = 'model_ANN_new.pkl'\n",
        "model = joblib.load(filename)\n",
        "'''"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n!git clone https://github.com/ucfilho/MarquesGabi_Routines\\n%cd MarquesGabi_Routines\\n# filename = 'model_ANN.pkl'\\nfilename = 'model_ANN_new.pkl'\\nmodel = joblib.load(filename)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2emP4rNjQy",
        "outputId": "73f611c7-edae-4cfa-bd18-4a6889806834"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (160/160), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 3 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 23.74 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "Description = '5 layers of Convolution: 32, 64, 128, 256, 512 '\n",
        "N1 = 200\n",
        "N2 = 10\n",
        "\n",
        "# make the CNN\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=32, kernel_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=64, kernel_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=512, kernel_size=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(MaxPooling2D())\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=N1))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=N2))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbPQ1FSRG6A",
        "outputId": "1da26d47-22f9-4615-9d88-fc25e69dad6d"
      },
      "source": [
        "\n",
        "# training the model\n",
        "r = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), \n",
        "              epochs=200, batch_size=32)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 32s 197ms/step - loss: 25.7714 - accuracy: 0.5656 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 1.4343 - accuracy: 0.5160 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.7579 - accuracy: 0.4985 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.6913 - accuracy: 0.4956 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 1s 98ms/step - loss: 0.6933 - accuracy: 0.4956 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 1s 99ms/step - loss: 0.6933 - accuracy: 0.4956 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6933 - accuracy: 0.4956 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 1s 113ms/step - loss: 0.6912 - accuracy: 0.4985 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.6932 - accuracy: 0.4956 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.6932 - accuracy: 0.4956 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6932 - accuracy: 0.4956 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.6932 - accuracy: 0.4956 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.7383 - accuracy: 0.4985 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 1s 104ms/step - loss: 0.6911 - accuracy: 0.4985 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.6932 - accuracy: 0.4956 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.6932 - accuracy: 0.4956 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.6932 - accuracy: 0.4956 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 1s 99ms/step - loss: 0.6932 - accuracy: 0.4956 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.6911 - accuracy: 0.4985 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.6932 - accuracy: 0.4927 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 1s 110ms/step - loss: 0.6932 - accuracy: 0.4548 - val_loss: 0.6931 - val_accuracy: 0.4898\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.6911 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6911 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.6912 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.6911 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6851 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 1s 104ms/step - loss: 4.7280 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6932 - accuracy: 0.4898 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.6932 - accuracy: 0.4956 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 1s 104ms/step - loss: 0.6932 - accuracy: 0.4956 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6932 - accuracy: 0.4956 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 1s 114ms/step - loss: 1.2277 - accuracy: 0.4840 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6892 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6879 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 1s 99ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 1s 111ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 1s 106ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6911 - accuracy: 0.5073 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 1s 105ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6911 - accuracy: 0.5073 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 1s 105ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 1s 112ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6911 - accuracy: 0.5073 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 1s 106ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.9428 - accuracy: 0.5015 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 1s 110ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 1s 105ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 1s 104ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 1s 104ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 1s 108ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 1s 104ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 1s 104ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 1s 112ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 1s 121ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 1s 105ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 1s 105ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 1s 105ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 1s 105ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 1s 106ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 1s 123ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 1s 104ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 1s 104ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6912 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 1s 112ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 1s 106ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 1s 112ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 1s 105ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 1s 105ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 1s 113ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 1s 104ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 1s 100ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 1s 104ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 1s 104ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 1s 112ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 1s 108ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 1s 106ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 1s 104ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 1s 106ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 1s 104ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 1s 105ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 1s 104ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 1s 105ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 1s 103ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6932 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 1s 105ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 1s 101ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 1s 102ms/step - loss: 0.6931 - accuracy: 0.5044 - val_loss: 0.6934 - val_accuracy: 0.4898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpf0XlSARX78",
        "outputId": "0e963570-8dcc-4a07-9271-a673ffd45969"
      },
      "source": [
        "# pred_test= model.predict_classes(X_test)\n",
        "pred_test = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predict   0\n",
            "Actual     \n",
            "0        72\n",
            "1        75\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iFNNrlWV9tH",
        "outputId": "3b26e2f5-2fa8-46bb-b03d-c04bef6d1971"
      },
      "source": [
        "pred_test"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QISvYcJBgWbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e857cf5-10eb-4305-f5f0-833dc42dda04"
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[0] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  #prediction = model.predict_classes(result)\n",
        "  prediction = np.argmax(model.predict(result), axis=-1)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0   122.0   53.482124   44.769955  ...   48.417629   45.946789   39.860249\n",
            "1   132.0  127.720856  126.203857  ...   53.087234   53.450878   53.274567\n",
            "2   181.0   96.559853   96.366013  ...    5.817802    5.798999    5.845151\n",
            "3   119.0   82.861595   83.034599  ...   61.584774   59.840832   57.342560\n",
            "4   181.0   55.821743   76.831665  ...   63.831818   64.485519   56.518978\n",
            "5   162.0   39.229080   38.320683  ...   59.909466   59.210335   55.357567\n",
            "6   158.0   77.604866   78.754684  ...    0.747797    0.161513    0.000000\n",
            "7   149.0   83.639160   77.228371  ...  114.717628  117.006012  120.807457\n",
            "8   123.0  142.025330  138.465134  ...   54.474586   53.271931   32.200081\n",
            "9   133.0   83.216072   76.202225  ...   67.700836   66.867035   67.049866\n",
            "10  125.0  102.025665  106.798790  ...   79.216644   79.602180   77.942665\n",
            "11  109.0   34.079033   46.051929  ...  124.137100  119.893272  122.674011\n",
            "12  132.0   57.524338   41.370983  ...    2.162535    1.699725    1.424242\n",
            "13  143.0   84.670937   87.399574  ...   58.770355   69.695587   77.680626\n",
            "14  176.0   79.033051   66.681816  ...   51.195763   45.098141   28.289253\n",
            "15  146.0   65.037155   61.036781  ...   40.761116   53.274162   57.565022\n",
            "16  108.0   26.087791   27.928669  ...   49.481480   46.707817   38.510288\n",
            "17  174.0  105.543678  106.430183  ...   26.629545   18.706434    2.910160\n",
            "18  156.0   90.053253   93.158463  ...   67.532547   50.523346   45.048653\n",
            "19  129.0   41.982094   41.439816  ...   74.521126   78.220413   82.883842\n",
            "20  185.0   70.659370   76.313354  ...  102.652206  108.658905  100.980362\n",
            "21  144.0   37.968369   42.332565  ...   82.135033   85.590279   85.641975\n",
            "22  140.0   93.519997   98.680000  ...  114.639999  117.559998  112.919998\n",
            "23  186.0   71.891670   69.871780  ...  109.047989  109.841263  100.037346\n",
            "24  106.0   80.221085   79.192245  ...    2.384122    2.427198    3.386971\n",
            "25  114.0   81.725761   87.549103  ...  103.640816  110.012001  110.929832\n",
            "26  117.0  100.830956   99.283447  ...  120.219376   82.039810   25.722916\n",
            "27  136.0   61.935986   60.993946  ...   74.293259   78.482697   84.277679\n",
            "28  121.0   74.082848   70.463020  ...   85.550850   88.230728   86.967834\n",
            "29  102.0   71.372559   70.124573  ...    0.000000    0.000000    0.000000\n",
            "30  177.0   58.693184   32.812439  ...   31.160454   37.758942   62.848347\n",
            "31  184.0  103.859161  112.229195  ...    5.091681    3.093573    1.862004\n",
            "32  193.0   85.288139   60.653549  ...   97.448021   97.629196   94.011276\n",
            "33  102.0    0.853518    2.377548  ...   68.004242   71.972702   74.630920\n",
            "34  174.0  145.328339  110.891281  ...   88.442993   92.321976   95.742241\n",
            "35  159.0  154.248520  126.547646  ...  149.318146   62.710533   23.575293\n",
            "36  128.0    0.658203    1.654297  ...   72.408203   68.989258   71.139648\n",
            "37  114.0   90.044632   95.173599  ...  100.993851  102.535240  103.000000\n",
            "38  170.0   68.383675   66.140907  ...   35.912529   37.154881   40.474464\n",
            "39  126.0   96.716057   90.000000  ...   56.641972   47.851852   39.703705\n",
            "40  179.0   92.286064   94.685432  ...   88.627541   86.248444   84.641685\n",
            "41  101.0   29.215960   28.123518  ...    0.000000    0.000000    0.000000\n",
            "42  143.0   72.596603  102.937843  ...   55.459091   56.958920   58.531807\n",
            "43  177.0   94.301781  100.199211  ...   80.158661   89.575409   92.645790\n",
            "44  125.0   59.478535   58.901253  ...    0.000000    0.000000    0.000000\n",
            "45  177.0    0.975869    0.615692  ...   32.609081   48.443260   49.846558\n",
            "46  156.0   86.838928   84.583168  ...   70.884949   64.608154   60.659439\n",
            "47  170.0   64.507683   62.287067  ...   97.766792   98.679871  101.788788\n",
            "48  194.0  103.693687  104.862778  ...   76.464973   81.478477   80.628014\n",
            "49  193.0   82.275848   96.892715  ...    6.009315    6.342318    5.951435\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRbWgmX_LFH",
        "outputId": "0496a5fb-7c34-4d20-fb3c-932ba6f3a23f"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "from GetBetterSegm import GetBetter"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_paper_fev_2021'...\n",
            "remote: Enumerating objects: 682, done.\u001b[K\n",
            "remote: Counting objects: 100% (443/443), done.\u001b[K\n",
            "remote: Compressing objects: 100% (441/441), done.\u001b[K\n",
            "remote: Total 682 (delta 278), reused 0 (delta 0), pack-reused 239\u001b[K\n",
            "Receiving objects: 100% (682/682), 5.56 MiB | 6.59 MiB/s, done.\n",
            "Resolving deltas: 100% (415/415), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAG_I6FwCvFr",
        "outputId": "1ac768c0-51a6-45c6-99bb-7c76dc658a9b"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "%cd marquesgabi_out_2020\n",
        "PSD_imageJ = 'Areas_ImageJ.csv'\n",
        "PSD_new = pd.read_csv(PSD_imageJ)\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_out_2020'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 146 (delta 75), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (146/146), 1.00 MiB | 5.74 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020\n",
            "   Juntas   Area\n",
            "0       1  2.001\n",
            "1       2  0.820\n",
            "2       3  1.270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "PekBHQOT_6CP",
        "outputId": "c07df203-6aff-400f-e164-1425cc84d234"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>122.0</td>\n",
              "      <td>53.482124</td>\n",
              "      <td>44.769955</td>\n",
              "      <td>22.670517</td>\n",
              "      <td>5.144048</td>\n",
              "      <td>2.806772</td>\n",
              "      <td>2.212846</td>\n",
              "      <td>3.311475</td>\n",
              "      <td>5.098092</td>\n",
              "      <td>17.637463</td>\n",
              "      <td>49.059658</td>\n",
              "      <td>74.334587</td>\n",
              "      <td>86.313080</td>\n",
              "      <td>89.438583</td>\n",
              "      <td>91.763771</td>\n",
              "      <td>91.882011</td>\n",
              "      <td>87.397743</td>\n",
              "      <td>80.131683</td>\n",
              "      <td>66.517059</td>\n",
              "      <td>63.714588</td>\n",
              "      <td>62.592846</td>\n",
              "      <td>66.040581</td>\n",
              "      <td>70.229782</td>\n",
              "      <td>71.796562</td>\n",
              "      <td>70.048096</td>\n",
              "      <td>74.081154</td>\n",
              "      <td>75.185165</td>\n",
              "      <td>74.744148</td>\n",
              "      <td>76.348564</td>\n",
              "      <td>59.198334</td>\n",
              "      <td>57.264980</td>\n",
              "      <td>47.080086</td>\n",
              "      <td>18.249126</td>\n",
              "      <td>5.516797</td>\n",
              "      <td>4.789573</td>\n",
              "      <td>6.489653</td>\n",
              "      <td>21.435099</td>\n",
              "      <td>49.149422</td>\n",
              "      <td>69.045685</td>\n",
              "      <td>82.995697</td>\n",
              "      <td>...</td>\n",
              "      <td>54.140816</td>\n",
              "      <td>54.647137</td>\n",
              "      <td>53.196182</td>\n",
              "      <td>55.595535</td>\n",
              "      <td>54.669170</td>\n",
              "      <td>52.398815</td>\n",
              "      <td>49.556572</td>\n",
              "      <td>46.878258</td>\n",
              "      <td>42.500942</td>\n",
              "      <td>36.282719</td>\n",
              "      <td>31.840363</td>\n",
              "      <td>28.827198</td>\n",
              "      <td>144.720230</td>\n",
              "      <td>94.359581</td>\n",
              "      <td>76.048370</td>\n",
              "      <td>82.133293</td>\n",
              "      <td>83.640945</td>\n",
              "      <td>86.078468</td>\n",
              "      <td>87.664879</td>\n",
              "      <td>87.663254</td>\n",
              "      <td>85.314964</td>\n",
              "      <td>74.298035</td>\n",
              "      <td>56.538563</td>\n",
              "      <td>55.111526</td>\n",
              "      <td>56.992470</td>\n",
              "      <td>54.823162</td>\n",
              "      <td>54.097824</td>\n",
              "      <td>52.213387</td>\n",
              "      <td>52.536949</td>\n",
              "      <td>53.263638</td>\n",
              "      <td>54.151840</td>\n",
              "      <td>55.786884</td>\n",
              "      <td>54.568661</td>\n",
              "      <td>54.938454</td>\n",
              "      <td>53.470573</td>\n",
              "      <td>52.664871</td>\n",
              "      <td>50.239182</td>\n",
              "      <td>48.417629</td>\n",
              "      <td>45.946789</td>\n",
              "      <td>39.860249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>132.0</td>\n",
              "      <td>127.720856</td>\n",
              "      <td>126.203857</td>\n",
              "      <td>122.465569</td>\n",
              "      <td>113.584023</td>\n",
              "      <td>100.452713</td>\n",
              "      <td>90.086327</td>\n",
              "      <td>88.509651</td>\n",
              "      <td>87.606987</td>\n",
              "      <td>83.869614</td>\n",
              "      <td>76.831963</td>\n",
              "      <td>68.561989</td>\n",
              "      <td>67.572090</td>\n",
              "      <td>65.154274</td>\n",
              "      <td>62.521580</td>\n",
              "      <td>64.620758</td>\n",
              "      <td>65.568413</td>\n",
              "      <td>65.703400</td>\n",
              "      <td>74.712585</td>\n",
              "      <td>94.993584</td>\n",
              "      <td>101.613411</td>\n",
              "      <td>91.187340</td>\n",
              "      <td>74.704315</td>\n",
              "      <td>68.561989</td>\n",
              "      <td>69.736465</td>\n",
              "      <td>68.593208</td>\n",
              "      <td>71.479340</td>\n",
              "      <td>73.528931</td>\n",
              "      <td>70.505058</td>\n",
              "      <td>126.780540</td>\n",
              "      <td>130.760330</td>\n",
              "      <td>130.973389</td>\n",
              "      <td>128.630859</td>\n",
              "      <td>122.882469</td>\n",
              "      <td>116.583115</td>\n",
              "      <td>105.102852</td>\n",
              "      <td>95.785133</td>\n",
              "      <td>90.497704</td>\n",
              "      <td>83.548210</td>\n",
              "      <td>77.678604</td>\n",
              "      <td>...</td>\n",
              "      <td>68.398529</td>\n",
              "      <td>71.391190</td>\n",
              "      <td>65.953171</td>\n",
              "      <td>56.343441</td>\n",
              "      <td>54.974293</td>\n",
              "      <td>56.526176</td>\n",
              "      <td>56.157028</td>\n",
              "      <td>54.211208</td>\n",
              "      <td>55.213959</td>\n",
              "      <td>53.897156</td>\n",
              "      <td>52.577595</td>\n",
              "      <td>51.833794</td>\n",
              "      <td>81.390266</td>\n",
              "      <td>88.848495</td>\n",
              "      <td>92.976128</td>\n",
              "      <td>90.654732</td>\n",
              "      <td>83.722687</td>\n",
              "      <td>65.519745</td>\n",
              "      <td>59.619839</td>\n",
              "      <td>61.235081</td>\n",
              "      <td>66.422409</td>\n",
              "      <td>77.359047</td>\n",
              "      <td>88.037659</td>\n",
              "      <td>86.406799</td>\n",
              "      <td>81.441696</td>\n",
              "      <td>70.988068</td>\n",
              "      <td>65.079895</td>\n",
              "      <td>63.144173</td>\n",
              "      <td>61.074387</td>\n",
              "      <td>58.532600</td>\n",
              "      <td>56.393944</td>\n",
              "      <td>56.918274</td>\n",
              "      <td>54.620754</td>\n",
              "      <td>54.164375</td>\n",
              "      <td>53.435261</td>\n",
              "      <td>52.214878</td>\n",
              "      <td>52.163452</td>\n",
              "      <td>53.087234</td>\n",
              "      <td>53.450878</td>\n",
              "      <td>53.274567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>181.0</td>\n",
              "      <td>96.559853</td>\n",
              "      <td>96.366013</td>\n",
              "      <td>98.867462</td>\n",
              "      <td>100.981995</td>\n",
              "      <td>102.374168</td>\n",
              "      <td>107.345268</td>\n",
              "      <td>111.229843</td>\n",
              "      <td>112.375854</td>\n",
              "      <td>106.991913</td>\n",
              "      <td>79.610451</td>\n",
              "      <td>59.832150</td>\n",
              "      <td>59.745762</td>\n",
              "      <td>58.729435</td>\n",
              "      <td>58.670948</td>\n",
              "      <td>57.172642</td>\n",
              "      <td>55.854858</td>\n",
              "      <td>56.052898</td>\n",
              "      <td>57.635815</td>\n",
              "      <td>59.480362</td>\n",
              "      <td>61.374439</td>\n",
              "      <td>71.769608</td>\n",
              "      <td>85.969284</td>\n",
              "      <td>95.512932</td>\n",
              "      <td>97.762985</td>\n",
              "      <td>93.125801</td>\n",
              "      <td>87.429810</td>\n",
              "      <td>85.515121</td>\n",
              "      <td>82.846100</td>\n",
              "      <td>95.220451</td>\n",
              "      <td>96.612526</td>\n",
              "      <td>99.524467</td>\n",
              "      <td>99.841125</td>\n",
              "      <td>103.898232</td>\n",
              "      <td>106.266960</td>\n",
              "      <td>104.489273</td>\n",
              "      <td>104.716499</td>\n",
              "      <td>83.872467</td>\n",
              "      <td>57.325661</td>\n",
              "      <td>57.373192</td>\n",
              "      <td>...</td>\n",
              "      <td>0.216202</td>\n",
              "      <td>0.506792</td>\n",
              "      <td>0.459785</td>\n",
              "      <td>0.239278</td>\n",
              "      <td>0.358078</td>\n",
              "      <td>0.392265</td>\n",
              "      <td>0.392265</td>\n",
              "      <td>0.392265</td>\n",
              "      <td>0.392265</td>\n",
              "      <td>0.465767</td>\n",
              "      <td>0.425597</td>\n",
              "      <td>0.425598</td>\n",
              "      <td>10.707488</td>\n",
              "      <td>7.448490</td>\n",
              "      <td>6.415250</td>\n",
              "      <td>5.960442</td>\n",
              "      <td>6.216935</td>\n",
              "      <td>6.526327</td>\n",
              "      <td>6.300693</td>\n",
              "      <td>6.415219</td>\n",
              "      <td>6.031470</td>\n",
              "      <td>5.819511</td>\n",
              "      <td>6.541193</td>\n",
              "      <td>7.737920</td>\n",
              "      <td>7.182260</td>\n",
              "      <td>6.546443</td>\n",
              "      <td>6.461372</td>\n",
              "      <td>6.404963</td>\n",
              "      <td>5.798999</td>\n",
              "      <td>5.434907</td>\n",
              "      <td>6.093862</td>\n",
              "      <td>5.919508</td>\n",
              "      <td>6.034889</td>\n",
              "      <td>6.164800</td>\n",
              "      <td>5.840878</td>\n",
              "      <td>5.555417</td>\n",
              "      <td>5.932328</td>\n",
              "      <td>5.817802</td>\n",
              "      <td>5.798999</td>\n",
              "      <td>5.845151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>119.0</td>\n",
              "      <td>82.861595</td>\n",
              "      <td>83.034599</td>\n",
              "      <td>81.837364</td>\n",
              "      <td>79.214539</td>\n",
              "      <td>79.581314</td>\n",
              "      <td>81.214539</td>\n",
              "      <td>82.757782</td>\n",
              "      <td>81.982704</td>\n",
              "      <td>81.896194</td>\n",
              "      <td>80.968857</td>\n",
              "      <td>75.875435</td>\n",
              "      <td>66.283737</td>\n",
              "      <td>100.453293</td>\n",
              "      <td>105.532883</td>\n",
              "      <td>108.595154</td>\n",
              "      <td>110.885811</td>\n",
              "      <td>115.823532</td>\n",
              "      <td>123.069214</td>\n",
              "      <td>130.339111</td>\n",
              "      <td>139.487885</td>\n",
              "      <td>145.560562</td>\n",
              "      <td>144.418701</td>\n",
              "      <td>140.609009</td>\n",
              "      <td>142.588226</td>\n",
              "      <td>155.173004</td>\n",
              "      <td>157.221466</td>\n",
              "      <td>156.899643</td>\n",
              "      <td>124.325256</td>\n",
              "      <td>85.726646</td>\n",
              "      <td>83.242218</td>\n",
              "      <td>78.446365</td>\n",
              "      <td>75.131485</td>\n",
              "      <td>76.432526</td>\n",
              "      <td>78.923874</td>\n",
              "      <td>79.927338</td>\n",
              "      <td>79.373703</td>\n",
              "      <td>79.681656</td>\n",
              "      <td>77.010376</td>\n",
              "      <td>50.083046</td>\n",
              "      <td>...</td>\n",
              "      <td>108.605545</td>\n",
              "      <td>86.778549</td>\n",
              "      <td>60.093430</td>\n",
              "      <td>53.408306</td>\n",
              "      <td>55.716263</td>\n",
              "      <td>57.702427</td>\n",
              "      <td>56.844292</td>\n",
              "      <td>54.110725</td>\n",
              "      <td>57.695499</td>\n",
              "      <td>60.595154</td>\n",
              "      <td>59.750870</td>\n",
              "      <td>59.979237</td>\n",
              "      <td>111.287201</td>\n",
              "      <td>113.418686</td>\n",
              "      <td>110.294128</td>\n",
              "      <td>105.802765</td>\n",
              "      <td>105.193764</td>\n",
              "      <td>103.325264</td>\n",
              "      <td>104.484436</td>\n",
              "      <td>103.730103</td>\n",
              "      <td>105.006912</td>\n",
              "      <td>105.622833</td>\n",
              "      <td>108.006912</td>\n",
              "      <td>112.754326</td>\n",
              "      <td>116.712799</td>\n",
              "      <td>117.640137</td>\n",
              "      <td>117.806229</td>\n",
              "      <td>118.539787</td>\n",
              "      <td>117.173004</td>\n",
              "      <td>103.692039</td>\n",
              "      <td>67.823532</td>\n",
              "      <td>51.228371</td>\n",
              "      <td>52.470589</td>\n",
              "      <td>54.498272</td>\n",
              "      <td>55.875435</td>\n",
              "      <td>56.906578</td>\n",
              "      <td>59.557098</td>\n",
              "      <td>61.584774</td>\n",
              "      <td>59.840832</td>\n",
              "      <td>57.342560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>181.0</td>\n",
              "      <td>55.821743</td>\n",
              "      <td>76.831665</td>\n",
              "      <td>96.225792</td>\n",
              "      <td>104.837372</td>\n",
              "      <td>106.403267</td>\n",
              "      <td>117.628258</td>\n",
              "      <td>119.225525</td>\n",
              "      <td>115.049759</td>\n",
              "      <td>110.612747</td>\n",
              "      <td>96.214478</td>\n",
              "      <td>82.137054</td>\n",
              "      <td>72.926292</td>\n",
              "      <td>73.575989</td>\n",
              "      <td>74.001190</td>\n",
              "      <td>79.128059</td>\n",
              "      <td>85.421021</td>\n",
              "      <td>82.300903</td>\n",
              "      <td>25.815210</td>\n",
              "      <td>46.157967</td>\n",
              "      <td>65.850708</td>\n",
              "      <td>66.724983</td>\n",
              "      <td>62.892006</td>\n",
              "      <td>66.512993</td>\n",
              "      <td>65.698059</td>\n",
              "      <td>64.721649</td>\n",
              "      <td>64.865913</td>\n",
              "      <td>64.695770</td>\n",
              "      <td>64.001717</td>\n",
              "      <td>60.049206</td>\n",
              "      <td>78.092789</td>\n",
              "      <td>94.202896</td>\n",
              "      <td>100.495499</td>\n",
              "      <td>104.751907</td>\n",
              "      <td>112.854523</td>\n",
              "      <td>114.119019</td>\n",
              "      <td>115.260071</td>\n",
              "      <td>109.046555</td>\n",
              "      <td>95.053757</td>\n",
              "      <td>81.656609</td>\n",
              "      <td>...</td>\n",
              "      <td>24.261837</td>\n",
              "      <td>34.696621</td>\n",
              "      <td>45.716866</td>\n",
              "      <td>52.520252</td>\n",
              "      <td>59.990601</td>\n",
              "      <td>65.987488</td>\n",
              "      <td>66.173012</td>\n",
              "      <td>69.759102</td>\n",
              "      <td>66.911942</td>\n",
              "      <td>65.426514</td>\n",
              "      <td>61.924236</td>\n",
              "      <td>52.456154</td>\n",
              "      <td>58.449528</td>\n",
              "      <td>59.054821</td>\n",
              "      <td>86.217857</td>\n",
              "      <td>102.797867</td>\n",
              "      <td>71.754524</td>\n",
              "      <td>68.421967</td>\n",
              "      <td>60.914871</td>\n",
              "      <td>60.133392</td>\n",
              "      <td>71.046341</td>\n",
              "      <td>67.670799</td>\n",
              "      <td>54.821159</td>\n",
              "      <td>51.561062</td>\n",
              "      <td>54.656364</td>\n",
              "      <td>55.895214</td>\n",
              "      <td>53.067947</td>\n",
              "      <td>43.519707</td>\n",
              "      <td>17.901958</td>\n",
              "      <td>24.792772</td>\n",
              "      <td>42.219105</td>\n",
              "      <td>54.651020</td>\n",
              "      <td>66.672455</td>\n",
              "      <td>68.523430</td>\n",
              "      <td>67.049232</td>\n",
              "      <td>67.154976</td>\n",
              "      <td>64.457314</td>\n",
              "      <td>63.831818</td>\n",
              "      <td>64.485519</td>\n",
              "      <td>56.518978</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Width           0           1  ...        781        782        783\n",
              "0  122.0   53.482124   44.769955  ...  48.417629  45.946789  39.860249\n",
              "1  132.0  127.720856  126.203857  ...  53.087234  53.450878  53.274567\n",
              "2  181.0   96.559853   96.366013  ...   5.817802   5.798999   5.845151\n",
              "3  119.0   82.861595   83.034599  ...  61.584774  59.840832  57.342560\n",
              "4  181.0   55.821743   76.831665  ...  63.831818  64.485519  56.518978\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "Area = np.array(PSD_new['Area'])\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J705kDqsE8f",
        "outputId": "dde1f808-687e-4afe-fe23-c5399e9a36c5"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(490, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wCFDX8esLoQ"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn-F050Hr9Ui"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "Vfk_fNXGDK5_",
        "outputId": "429094d4-d7ee-4f4a-c3f4-b4d391b12a97"
      },
      "source": [
        " wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        " wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        " X = pd.DataFrame([Diam1,Diameter_All])\n",
        " wts = pd.DataFrame([wt1,wt2])\n",
        "plt.hist(X,weights=wts)\n",
        "plt.legend(['Image J','CNN'])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f252928d790>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nGDbBEeiUij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "9e7d62bf-b89e-4367-bd54-96a3d9ceb7ee"
      },
      "source": [
        "# plt.hist(x, bins=bins, density=True, histtype='step', cumulative=-1,label='Reversed emp.')\n",
        "plt.hist(X, density=True, histtype='step', cumulative=True,label='Reversed emp.')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.04210526, 0.14736842, 0.29473684, 0.66315789, 0.86315789,\n",
              "         0.95789474, 0.97894737, 0.97894737, 0.98947368, 1.        ],\n",
              "        [0.08      , 0.2       , 0.36      , 0.56      , 0.76      ,\n",
              "         0.98      , 1.        , 1.        , 1.        , 1.        ]]),\n",
              " array([0.64758457, 0.81457003, 0.98155549, 1.14854096, 1.31552642,\n",
              "        1.48251188, 1.64949734, 1.8164828 , 1.98346827, 2.15045373,\n",
              "        2.31743919]),\n",
              " <a list of 2 Lists of Patches objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "9xENlBUUxfTu",
        "outputId": "ebb25d06-1082-4038-9aa8-8f373cf1f51d"
      },
      "source": [
        "Obj = plt.hist(X, density=True, histtype='step', cumulative=True,label='Reversed emp.')\n",
        "Y1, Y2 = Obj[0]\n",
        "Rsquared = r2_score(Y1, Y2)\n",
        "print('r_squared =',Rsquared)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r_squared = 0.9764386790481524\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2XboMiFbkaa"
      },
      "source": [
        "acc_train = r.history['accuracy'][-1]\n",
        "acc_test = r.history['val_accuracy'][-1]\n",
        "loss_train = r.history['loss'][-1]\n",
        "loss_test = r.history['val_loss'][-1]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euTd_-CYN1v0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f5adbdfb-ffe4-483e-bfc7-fb01f78856c2"
      },
      "source": [
        "df = pd.DataFrame({'N1':N1, 'N2':N2,'R^2':Rsquared,\n",
        "                   'acc train':acc_train,'acc test':acc_test,\n",
        "                   'loss train':loss_train,'loss test':loss_test,\n",
        "                   'Details':Description},\n",
        "                  index= [0])\n",
        "Arq = \"output.xlsx\"\n",
        "df.to_excel(Arq)\n",
        "files.download(Arq)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_3c649349-dc0b-47e9-b0e5-7d903b41d759\", \"output.xlsx\", 5168)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "-KukfpGTTKlj",
        "outputId": "6d26cdcf-282f-4409-d96f-759a88f46dd5"
      },
      "source": [
        "df"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>N1</th>\n",
              "      <th>N2</th>\n",
              "      <th>R^2</th>\n",
              "      <th>acc train</th>\n",
              "      <th>acc test</th>\n",
              "      <th>loss train</th>\n",
              "      <th>loss test</th>\n",
              "      <th>Details</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>200</td>\n",
              "      <td>10</td>\n",
              "      <td>0.976439</td>\n",
              "      <td>0.504373</td>\n",
              "      <td>0.489796</td>\n",
              "      <td>0.693115</td>\n",
              "      <td>0.693358</td>\n",
              "      <td>5 layers of Convolution: 32, 64, 128, 256, 512</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    N1  N2  ...  loss test                                          Details\n",
              "0  200  10  ...   0.693358  5 layers of Convolution: 32, 64, 128, 256, 512 \n",
              "\n",
              "[1 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "ZZHa1j4HT9Dq",
        "outputId": "cb3cabe3-592b-45db-c820-26fd436c675c"
      },
      "source": [
        "counts, bins, bars = plt.hist(X,weights=wts)\n",
        "print(bars)\n",
        "print(bins)\n",
        "print(counts)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<a list of 2 Lists of Patches objects>\n",
            "[0.64758457 0.81457003 0.98155549 1.14854096 1.31552642 1.48251188\n",
            " 1.64949734 1.8164828  1.98346827 2.15045373 2.31743919]\n",
            "[[ 4.21052632 10.52631579 14.73684211 36.84210526 20.          9.47368421\n",
            "   2.10526316  0.          1.05263158  1.05263158]\n",
            " [ 8.         12.         16.         20.         20.         22.\n",
            "   2.          0.          0.          0.        ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o_vDGeWUwIZ",
        "outputId": "580257e9-6f85-4ce3-e278-5c425eb1166b"
      },
      "source": [
        "print(counts.sum())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200.0000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "KcH52-6iJQ8t",
        "outputId": "24f21746-185c-40c7-f35c-3d1fca67bc8d"
      },
      "source": [
        "\n",
        "plt.hist([Diam1,Diameter_All])\n",
        "plt.legend(['Image J','CNN'])\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f24ddf30390>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUiklEQVR4nO3df4xddZ3/8eebMjD7lQYKHWqllCnYgHRZWhzKspC1W4RlJSuSoMLukvKNpIgLsXGzsUIixa/fANpdUMPKlq986daiEqRfRXS/NFhiUAGnUKBldvlZsaS0QxG0+5Vi2/f3j3uKwzDTe+fHnTsf+nwkN3Pv55wz9zUnp6+eOefcM5GZSJLKs1+rA0iShscCl6RCWeCSVCgLXJIKZYFLUqH2H8s3mzx5cnZ2do7lW0pS8dauXftyZnb0Hx/TAu/s7KS7u3ss31KSihcRvxxo3EMoklQoC1ySCmWBS1KhxvQYuKR92+9//3s2bdrE66+/3uoo41J7ezvTpk2jra2tofktcEljZtOmTUycOJHOzk4iotVxxpXMZNu2bWzatIkZM2Y0tIyHUCSNmddff53DDjvM8h5ARHDYYYcN6beTugUeEe0R8XBEPBYRGyLimmr8toh4PiLWVY/ZI8guaR9heQ9uqOumkUMoO4D5mbk9ItqAByLiR9W0f8zMO4eYUZI0CuoWeNZuGL69etlWPbyJuKQR61x8z6h+v43XnVN3noMOOojt27fXna/Z5s2bx9KlS+nq6hr292joJGZETADWAu8FbsrMhyLiMuB/RsTngfuAxZm5Y4BlFwILAaZPnz7soBp7w/nH1cg/IEmjo6GTmJm5KzNnA9OAuRHxx8DngOOAk4FDgc8OsuyyzOzKzK6Ojrd9lF+SWuL+++/nAx/4AOeeey5HH300ixcvZuXKlcydO5cTTjiBZ599FoC7776bU045hTlz5vDBD36QLVu2ANDb28uZZ57JrFmzuOSSSzjqqKN4+eWXAfjmN7/J3LlzmT17Npdeeim7du1qys8wpKtQMvNVYA1wdmZuzpodwP8G5jYjoCQ1y2OPPcbNN99MT08PK1as4KmnnuLhhx/mkksu4Wtf+xoAp59+Og8++CCPPvooF1xwAV/60pcAuOaaa5g/fz4bNmzg/PPP54UXXgCgp6eH73znO/z0pz9l3bp1TJgwgZUrVzYlf91DKBHRAfw+M1+NiD8CzgSuj4ipmbk5aqdNPwKsb0pCSWqSk08+malTpwJwzDHHcNZZZwFwwgknsGbNGqB27frHP/5xNm/ezBtvvPHmNdoPPPAAq1atAuDss89m0qRJANx3332sXbuWk08+GYDf/e53HH744U3J38gx8KnA8uo4+H7AHZn5g4j4cVXuAawDPtmUhJLUJAceeOCbz/fbb783X++3337s3LkTgCuuuILPfOYzfPjDH+b+++9nyZIle/2emcmCBQu49tprm5Z7j7qHUDLz8cyck5l/kpl/nJlfqMbnZ+YJ1djfZWbrT+tK0ih77bXXOOKIIwBYvnz5m+OnnXYad9xxBwD33nsvv/71rwE444wzuPPOO9m6dSsAr7zyCr/85YB3gx0xP0ovqWVKuGppyZIlfPSjH2XSpEnMnz+f559/HoCrr76aCy+8kBUrVnDqqafy7ne/m4kTJzJ58mS++MUvctZZZ7F7927a2tq46aabOOqoo97yfXfu3PmW3wCGI2qXeY+Nrq6u9A86lMPLCDXaenp6eN/73tfqGKNix44dTJgwgf3335+f//znXHbZZaxbt67hZd/73veyfv16Dj744LdMG2gdRcTazHzbBePugUvSMLzwwgt87GMfY/fu3RxwwAHccsstDS3X3d3NRRddxKc+9am3lfdQWeCSNAwzZ87k0UcfHfJyXV1d9PT0jEoG70YoSYWywCWpUBa4JBXKApekQnkSU1LrLBnZVRhv/36v1Z3lpZdeYtGiRfziF7/gkEMOYcqUKdx4440ce+yxfPWrX+WKK64A4PLLL6erq4uLL76Yiy++mNWrV/Pcc89x4IEH8vLLL9PV1cXGjRtHN/8QuQcuaZ+RmZx33nnMmzePZ599lrVr13LttdeyZcsWDj/8cL7yla/wxhtvDLjshAkTuPXWW8c48d5Z4JL2GWvWrKGtrY1PfvIPt2468cQTOfLII+no6OCMM854y8fl+1q0aBE33HDDm/dIGQ8scEn7jPXr1/P+979/0Omf/exnWbp06YD3754+fTqnn346K1asaGbEIbHAJaly9NFHc8opp3D77bcPOP1zn/scX/7yl9m9e/cYJxuYBS5pnzFr1izWrl2713muvPJKrr/+ega6T9TMmTOZPXv2m3chbDULXNI+Y/78+ezYsYNly5a9Ofb444/zq1/96s3Xxx13HMcffzx33333gN/jqquuYunSpU3P2ggvI5TUOg1c9jeaIoJVq1axaNEirr/+etrb2+ns7OTGG298y3xXXXUVc+bMGfB7zJo1i5NOOolHHnlkLCLvlQUuaZ/ynve8Z8BDIOvX/+GvQp544olvOc592223vWXeu+66q2n5hsJDKJJUKAtckgplgUsaU2P5V8BKM9R1U7fAI6I9Ih6OiMciYkNEXFONz4iIhyLimYj4TkQcMMzMkvYR7e3tbNu2zRIfQGaybds22tvbG16mkZOYO4D5mbk9ItqAByLiR8BngBsy89sRcTPwCeDrwwkuad8wbdo0Nm3aRG9vb6ujjEvt7e1Mmzat4fnrFnjW/qvcXr1sqx4JzAf+phpfDizBApe0F21tbcyYMaPVMd4xGjoGHhETImIdsBVYDTwLvJqZe+7qsgk4YpBlF0ZEd0R0+7+uJI2ehgo8M3dl5mxgGjAXOK7RN8jMZZnZlZldHR0dw4wpSepvSFehZOarwBrgVOCQiNhzCGYa8OIoZ5Mk7UUjV6F0RMQh1fM/As4EeqgV+fnVbAuA7zUrpCTp7Rq5CmUqsDwiJlAr/Dsy8wcR8STw7Yj4IvAo8I0m5pQk9dPIVSiPA2+7q0tmPkfteLgkqQX8JKYkFcoCl6RCWeCSVCgLXJIKZYFLUqEscEkqlAUuSYWywCWpUBa4JBXKApekQlngklQoC1ySCmWBS1KhLHBJKpQFLkmFssAlqVAWuCQVygKXpEJZ4JJUKAtckgplgUtSoeoWeEQcGRFrIuLJiNgQEZ+uxpdExIsRsa56fKj5cSVJe+zfwDw7gX/IzEciYiKwNiJWV9NuyMylzYsnSRpM3QLPzM3A5ur5byOiBzii2cEkSXs3pGPgEdEJzAEeqoYuj4jHI+LWiJg0yDILI6I7Irp7e3tHFFaS9AcNF3hEHAR8F1iUmb8Bvg4cA8ymtof+TwMtl5nLMrMrM7s6OjpGIbIkCRos8Ihoo1beKzPzLoDM3JKZuzJzN3ALMLd5MSVJ/TVyFUoA3wB6MvOf+4xP7TPbecD60Y8nSRpMI1ehnAZcBDwREeuqsSuBCyNiNpDARuDSpiSUJA2okatQHgBigEk/HP04kqRG+UlMSSqUBS5JhbLAJalQFrgkFcoCl6RCWeCSVCgLXJIKZYFLUqEscEkqlAUuSYWywCWpUBa4JBXKApekQlngklQoC1ySCmWBS1KhLHBJKpQFLkmFssAlqVAWuCQVqm6BR8SREbEmIp6MiA0R8elq/NCIWB0RT1dfJzU/riRpj0b2wHcC/5CZxwN/Cvx9RBwPLAbuy8yZwH3Va0nSGKlb4Jm5OTMfqZ7/FugBjgDOBZZXsy0HPtKskJKktxvSMfCI6ATmAA8BUzJzczXpJWDKIMssjIjuiOju7e0dQVRJUl8NF3hEHAR8F1iUmb/pOy0zE8iBlsvMZZnZlZldHR0dIworSfqDhgo8ItqolffKzLyrGt4SEVOr6VOBrc2JKEkaSCNXoQTwDaAnM/+5z6TvAwuq5wuA741+PEnSYPZvYJ7TgIuAJyJiXTV2JXAdcEdEfAL4JfCx5kSUJA2kboFn5gNADDL5jNGNI0lqVCN74NKY61x8z5CX2XjdOU1IIo1ffpRekgplgUtSoSxwSSqUBS5JhbLAJalQFrgkFcoCl6RCWeCSVCgLXJIKZYFLUqEscEkqlAUuSYWywCWpUBa4JBXKApekQlngklQoC1ySCmWBS1KhLHBJKpQFLkmFqlvgEXFrRGyNiPV9xpZExIsRsa56fKi5MSVJ/TWyB34bcPYA4zdk5uzq8cPRjSVJqqdugWfmT4BXxiCLJGkIRnIM/PKIeLw6xDJpsJkiYmFEdEdEd29v7wjeTpLU13AL/OvAMcBsYDPwT4PNmJnLMrMrM7s6OjqG+XaSpP6GVeCZuSUzd2XmbuAWYO7oxpIk1TOsAo+IqX1engesH2xeSVJz7F9vhoj4FjAPmBwRm4CrgXkRMRtIYCNwaRMzSpIGULfAM/PCAYa/0YQskqQhqFvgaq3OxfcMeZmN153ThCSSxhs/Si9JhbLAJalQFrgkFcoCl6RCWeCSVCgLXJIKZYFLUqEscEkqlAUuSYWywCWpUBa4JBXKApekQlngklQoC1ySCmWBS1KhLHBJKpQFLkmF8i/ySKNhycFDnP+15uTQPsU9cEkqVN0Cj4hbI2JrRKzvM3ZoRKyOiKerr5OaG1OS1F8je+C3AWf3G1sM3JeZM4H7qteSpDFUt8Az8yfAK/2GzwWWV8+XAx8Z5VySpDqGewx8SmZurp6/BEwZpTySpAaN+CqUzMyIyMGmR8RCYCHA9OnTR/p2LdG5+J4hL7PxunOakKQArbwawytBtI8Z7h74loiYClB93TrYjJm5LDO7MrOro6NjmG8nSepvuAX+fWBB9XwB8L3RiSNJalQjlxF+C/g5cGxEbIqITwDXAWdGxNPAB6vXkqQxVPcYeGZeOMikM0Y5iyRpCPwkpiQVynuhSP0M66qj9iYEkepwD1ySCmWBS1KhLHBJKpQFLkmF8iTmO5EfKZf2Ce6BS1KhLHBJKpQFLkmFssAlqVAWuCQVygKXpEJZ4JJUKAtckgplgUtSoSxwSSqUBS5JhfJeKM3i/UgkNZl74JJUKAtckgo1okMoEbER+C2wC9iZmV2jEUqSVN9oHAP/i8x8eRS+jyRpCDyEIkmFGmmBJ3BvRKyNiIUDzRARCyOiOyK6e3t7R/h2kqQ9Rlrgp2fmScBfAX8fEX/ef4bMXJaZXZnZ1dHRMcK3kyTtMaICz8wXq69bgVXA3NEIJUmqb9gFHhHvioiJe54DZwHrRyuYJGnvRnIVyhRgVUTs+T63Z+a/j0oqSVJdwy7wzHwOOHEUs0iShqCYe6F0Lr5nyMtsbP+boS3g/UgkFcTrwCWpUBa4JBXKApekQlngklQoC1ySCmWBS1KhLHBJKpQFLkmFssAlqVAWuCQVygKXpEIVcy8UaV8x1Pv+bLzunCYl0XjnHrgkFcoCl6RCWeCSVCgLXJIKZYFLUqG8CkUq3ZKDhzj/+PzLU628+qbUK3/cA5ekQlngklSoERV4RJwdEf8ZEc9ExOLRCiVJqm/YBR4RE4CbgL8CjgcujIjjRyuYJGnvRrIHPhd4JjOfy8w3gG8D545OLElSPZGZw1sw4nzg7My8pHp9EXBKZl7eb76FwMLq5bHAfw4/7ohMBl5u0XuPVKnZS80N5WYvNTeUm30sch+VmR39B5t+GWFmLgOWNft96omI7szsanWO4Sg1e6m5odzspeaGcrO3MvdIDqG8CBzZ5/W0akySNAZGUuC/AGZGxIyIOAC4APj+6MSSJNUz7EMombkzIi4H/i8wAbg1MzeMWrLR1/LDOCNQavZSc0O52UvNDeVmb1nuYZ/ElCS1lp/ElKRCWeCSVKh3RIHX+0h/RNwQEeuqx1MR8Wqfabv6TBvTk7ARcWtEbI2I9YNMj4j4avVzPR4RJ/WZtiAinq4eC8YudUO5/7bK+0RE/CwiTuwzbWM1vi4iuscu9ZvvXy/7vIh4rc828fk+01p264gGcv9jn8zrq+360Gpay9Z5RBwZEWsi4smI2BARnx5gnvG6nTeSvbXbemYW/aB2AvVZ4GjgAOAx4Pi9zH8FtROue15vb2H2PwdOAtYPMv1DwI+AAP4UeKgaPxR4rvo6qXo+aRzl/rM9eajdauGhPtM2ApPH8TqfB/xgpNvZWOfuN+9fAz8eD+scmAqcVD2fCDzVf72N4+28kewt3dbfCXvgQ/1I/4XAt8YkWR2Z+RPglb3Mci7wb1nzIHBIREwF/hJYnZmvZOavgdXA2c1PXFMvd2b+rMoF8CC1zwiMCw2s88G09NYRQ8w9nrbxzZn5SPX8t0APcES/2cbrdl43e6u39XdCgR8B/KrP6028fQMBICKOAmYAP+4z3B4R3RHxYER8pHkxh2Wwn63hn3kc+AS1vas9Erg3ItZWt1kYj06NiMci4kcRMasaK2KdR8R/o1Zy3+0zPC7WeUR0AnOAh/pNGvfb+V6y9zXm2/q+9hd5LgDuzMxdfcaOyswXI+Jo4McR8URmPtuifO8oEfEX1Dbq0/sMn16t78OB1RHxH9Xe5XjxCLVtYntEfAj4P8DMFmcair8GfpqZfffWW77OI+Igav+pLMrM34zle49UI9lbta2/E/bAh/KR/gvo96tlZr5YfX0OuJ/a/7LjxWA/27i/jUFE/Anwv4BzM3PbnvE+63srsIraoYlxIzN/k5nbq+c/BNoiYjIFrPPK3rbxlqzziGijVoArM/OuAWYZt9t5A9lbu62P1QmBZj2o/RbxHLVDI3tOLs0aYL7jqJ1UiD5jk4ADq+eTgacZwxNT1ft2MvgJtXN468mdh6vxQ4Hnq/yTqueHjqPc04FngD/rN/4uYGKf5z+jdkfLsd5m9pb93Xu2EWr/4F6o1n9D21mrclfTD6Z2nPxd42WdV+vu34Ab9zLPuNzOG8ze0m29+EMoOchH+iPiC0B3Zu65NPAC4NtZrdHK+4B/jYjd1H4buS4znxyr7BHxLWpXPUyOiE3A1UAbQGbeDPyQ2hn6Z4D/B/z3atorEfE/qN2PBuAL+dZfmVud+/PAYcC/RATAzqzdrW0KsKoa2x+4PTP/faxyN5j9fOCyiNgJ/A64oNpmWnrriAZyA5wH3JuZ/9Vn0Vav89OAi4AnImJdNXYlteIb19t5g9lbuq37UXpJKtQ74Ri4JO2TLHBJKpQFLkmFssAlqVAWuCQVygKXpEJZ4JJUqP8Pvk4/2EE6PqoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r11AxFK_JIii",
        "outputId": "a37bdcbd-aa31-4a06-dacc-ccd896722d45"
      },
      "source": [
        "[Diam1,Diameter_All]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.59616801403081,\n",
              "  1.0217907939900581,\n",
              "  1.2716187407449044,\n",
              "  1.104429030701514,\n",
              "  1.2163487785097904,\n",
              "  1.6013445735058454,\n",
              "  1.1715597420637607,\n",
              "  1.2534662333717612,\n",
              "  1.2676073151634049,\n",
              "  1.309600575274104,\n",
              "  1.292966945531582,\n",
              "  1.7658322811231006,\n",
              "  1.3564037533648712,\n",
              "  1.2407040781688483,\n",
              "  2.130217298173151,\n",
              "  1.4228319915327,\n",
              "  1.0651086490865755,\n",
              "  1.3008210311003705,\n",
              "  1.336545951796433,\n",
              "  0.8927754224911278,\n",
              "  1.4494292838262302,\n",
              "  1.4052738287907582,\n",
              "  1.6421697097891788,\n",
              "  1.2329833804288621,\n",
              "  1.19042665178928,\n",
              "  1.1682948223612457,\n",
              "  1.1518314137121108,\n",
              "  0.9607802401865855,\n",
              "  2.317439190074449,\n",
              "  1.0591147430338594,\n",
              "  1.4308630919602832,\n",
              "  0.7535680705496237,\n",
              "  0.8608283307581511,\n",
              "  1.2776122636975893,\n",
              "  1.3745862957220916,\n",
              "  1.259546137598783,\n",
              "  1.2978813187979172,\n",
              "  1.2412170838050638,\n",
              "  1.6009469708743893,\n",
              "  1.3149369953539032,\n",
              "  1.417901703622935,\n",
              "  1.2478669653497139,\n",
              "  1.1055812783082735,\n",
              "  0.9561307405997607,\n",
              "  0.9487783503683882,\n",
              "  1.1238565871041026,\n",
              "  1.2058356273089446,\n",
              "  1.2801012827406097,\n",
              "  0.8733100751144249,\n",
              "  0.9194732501297403,\n",
              "  1.6425573339441792,\n",
              "  1.085826790250066,\n",
              "  1.0639125693728595,\n",
              "  1.0875842666474016,\n",
              "  1.417901703622935,\n",
              "  1.550443891425932,\n",
              "  0.7825779328716171,\n",
              "  1.4690612745308145,\n",
              "  1.053086721720641,\n",
              "  1.2676073151634049,\n",
              "  0.7744003006005755,\n",
              "  1.3787482149724068,\n",
              "  1.363892581861956,\n",
              "  1.299352006316543,\n",
              "  1.2870449283923413,\n",
              "  1.11817763925502,\n",
              "  0.9474354220939228,\n",
              "  1.5218484589055707,\n",
              "  1.3526437911676632,\n",
              "  1.1556938532445284,\n",
              "  1.6013445735058454,\n",
              "  1.274619025074578,\n",
              "  1.422384489715834,\n",
              "  1.3408259533459403,\n",
              "  1.172646028567008,\n",
              "  1.1490645795125545,\n",
              "  1.459060149136146,\n",
              "  1.2483770274864237,\n",
              "  1.336545951796433,\n",
              "  0.9601174044814821,\n",
              "  1.4867225193896279,\n",
              "  1.4277452542806772,\n",
              "  1.35028849808504,\n",
              "  0.7560982446653928,\n",
              "  1.259040600296622,\n",
              "  1.13456827900627,\n",
              "  1.6549133695530214,\n",
              "  1.1204526724091788,\n",
              "  1.1176081573544434,\n",
              "  0.9153095762832032,\n",
              "  1.1639273497938836,\n",
              "  1.3066806149514323,\n",
              "  1.1529362882239027,\n",
              "  1.3047303442899274,\n",
              "  1.3066806149514323],\n",
              " [0.9735199463355776,\n",
              "  1.30507940696517,\n",
              "  1.5404981885089797,\n",
              "  0.9852114267968886,\n",
              "  1.4727505057301888,\n",
              "  1.3162124995147306,\n",
              "  1.346550513979066,\n",
              "  1.3665933056489297,\n",
              "  1.2281495093912447,\n",
              "  1.3214987838182324,\n",
              "  0.8704039108513574,\n",
              "  0.7740829952153113,\n",
              "  1.2694975970692006,\n",
              "  1.2429768708234377,\n",
              "  1.434171224433886,\n",
              "  1.1434555317286834,\n",
              "  1.1197071243593166,\n",
              "  1.6892077926955629,\n",
              "  1.3133680204568483,\n",
              "  0.977714070907902,\n",
              "  1.4986417456532357,\n",
              "  1.1578243558665238,\n",
              "  1.0207526271328173,\n",
              "  1.4842189345771921,\n",
              "  0.7321499795772001,\n",
              "  0.9055001020405682,\n",
              "  0.8608339901838006,\n",
              "  1.1698814475982857,\n",
              "  0.9931950967963896,\n",
              "  0.9590455738600041,\n",
              "  1.368527728293161,\n",
              "  1.470502922665121,\n",
              "  1.584187220509944,\n",
              "  0.647584569064857,\n",
              "  1.496760087368395,\n",
              "  1.4883824084848165,\n",
              "  1.0245167561889006,\n",
              "  0.9959992972502169,\n",
              "  1.640113928525532,\n",
              "  1.2469977928666496,\n",
              "  1.579771133189685,\n",
              "  0.7654970632689775,\n",
              "  1.4785409482820147,\n",
              "  1.427439291506829,\n",
              "  0.9882655951162601,\n",
              "  1.2024266315613603,\n",
              "  1.3061317779480701,\n",
              "  1.542873259995711,\n",
              "  1.6444703979130595,\n",
              "  1.6359937463774252]]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}