{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_CNN_B_r_squared_jun_23_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/Qualificacao/Comparative_ANNs/PSD_histogram_CNN_B_r_squared_jun_23_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from sklearn.metrics import r2_score\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZEvJvfoibE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3be2da34-37e6-4cf3-843e-ca452102de8f"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mahotas\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/ad/553b246b0a35dccc3ed58dc8889a67124bf5ab858e9c6b7255d56086e70c/mahotas-1.4.11-cp37-cp37m-manylinux2010_x86_64.whl (5.7MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7MB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.19.5)\n",
            "Installing collected packages: mahotas\n",
            "Successfully installed mahotas-1.4.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT"
      },
      "source": [
        "import mahotas.features.texture as mht\n",
        "import mahotas.features"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VcTdaNVh9EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "346eab46-3a5e-48df-c984-1feaa5321bb8"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_fev_2020'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 73 (delta 37), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n",
            "/content/marquesgabi_fev_2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v7SRrc8mH2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fab0a3c-dcb3-4a84-cb6e-6554cd51852f"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip'\n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Doutorado'...\n",
            "remote: Enumerating objects: 361, done.\u001b[K\n",
            "remote: Counting objects: 100% (111/111), done.\u001b[K\n",
            "remote: Compressing objects: 100% (110/110), done.\u001b[K\n",
            "remote: Total 361 (delta 38), reused 0 (delta 0), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (361/361), 202.49 MiB | 29.42 MiB/s, done.\n",
            "Resolving deltas: 100% (155/155), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[4] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHgqAnaFyCjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "859e682d-1525-4b1d-d785-2732b2d2c4ac"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 22.15 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from segment_filter_not_conclude import Segmenta  # got image provided segmented"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5MN5a_v4np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42705fd6-ceae-4c5a-e474-a9c34ba04e16"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     130  128.476929  135.798111  ...  162.551483  174.763077  192.380600\n",
            "1     169  218.139496  216.005295  ...    1.183432    0.612794    0.773502\n",
            "2     194  135.596649  139.299271  ...  199.255280  197.183838  152.034958\n",
            "3     141  119.244408  116.487411  ...  145.617828  130.322861  124.465012\n",
            "4     197  152.838974  138.385208  ...  169.878799  218.168442  229.160919\n",
            "5     126  123.432106  108.493820  ...    1.000000    1.000000    0.802469\n",
            "6     196  150.061218  188.673462  ...    1.448980    0.183673    1.306122\n",
            "7     166  112.043243  107.135719  ...   80.328926   96.132950   95.035561\n",
            "8     185   51.852795   52.599037  ...    0.293177    1.536421    0.986355\n",
            "9     121   99.786629  108.390007  ...    0.025271    0.682535    1.000000\n",
            "10    139   95.318825   84.306442  ...    1.024740    0.039128    0.844884\n",
            "11    113  150.933823  137.092178  ...  183.742584  184.280518  184.094604\n",
            "12    177   94.348549  107.176605  ...   33.130390   21.517761    4.712375\n",
            "13    170  133.853149  141.275986  ...  172.487076  175.942566  166.239594\n",
            "14    169   97.136375  108.335625  ...    0.918735    0.190224    1.359791\n",
            "15    155  252.568832  254.056961  ...  164.977753  164.858994  158.938751\n",
            "16    113  126.626678   98.807114  ...  172.660492  161.701920  142.392029\n",
            "17    174  125.101746  113.900269  ...  170.443802  165.087341  161.301651\n",
            "18    164  237.176102  214.756699  ...  214.055313  211.340256  211.519928\n",
            "19    176  167.037186  162.177689  ...  190.200943  190.395142  180.533554\n",
            "20    143  171.515228  172.527206  ...  202.281555  198.322266  161.637329\n",
            "21    170   77.930107   88.057457  ...    0.944498    0.183945    1.356540\n",
            "22    192  252.623688  241.093292  ...  146.966995  148.204422  170.332901\n",
            "23    128   92.369141   93.980469  ...    1.000000    1.000000    1.000000\n",
            "24    144  105.523918   92.987656  ...    0.043210    0.987654    1.820988\n",
            "25    141  156.205490  155.667969  ...  223.768784  249.825256  250.554993\n",
            "26    137  136.743347  140.290802  ...  129.555328  132.770081  122.749748\n",
            "27    164  166.686508  171.370605  ...  160.129700  157.684113  161.239151\n",
            "28    159  252.044312  251.682785  ...  134.445755  137.414215  139.593002\n",
            "29    145   59.913910   59.583447  ...    1.000000    1.000000    1.000000\n",
            "30    199  247.148438  253.320007  ...  178.485916  193.148163  191.900574\n",
            "31    135  129.163162  124.709625  ...  110.690460  114.788139  137.392044\n",
            "32    171  153.188950  154.535690  ...   97.556648   79.790604   77.960571\n",
            "33    141  154.723907  149.355865  ...  209.467697  176.464325  175.311157\n",
            "34    189    1.039781    2.016461  ...   11.551440   38.362141   53.267490\n",
            "35    120  174.910004  177.556686  ...  197.024429  238.917786  252.854431\n",
            "36    112    1.062500    2.062500  ...   83.437500   77.625000   74.437500\n",
            "37    199   80.310394   90.177162  ...  149.346970  154.130112  148.057495\n",
            "38    116  196.428070  194.612366  ...  214.501770  232.120087  237.812134\n",
            "39    133    3.670360   11.238227  ...  128.994461  146.038788  152.005539\n",
            "40    126  136.246918  140.839508  ...  173.876556  175.469131  184.407410\n",
            "41    189  134.862839  103.680382  ...    1.355281    0.155007    1.318244\n",
            "42    164  112.022606  133.097565  ...  124.778702  138.849487  140.606186\n",
            "43    139  158.636292  167.766144  ...  200.294296  205.194244  211.265869\n",
            "44    186  122.525856  123.858032  ...   97.369873   88.327904   87.015152\n",
            "45    101  164.984421  164.528992  ...  194.061462  190.614746  186.653946\n",
            "46    106    0.318975    0.205767  ...   90.233185  103.171951  100.056252\n",
            "47    170    0.932042    0.242491  ...  113.085403  131.391556  125.497307\n",
            "48    182   11.213018   95.532555  ...  147.000015  165.112457  224.307709\n",
            "49    199  172.155930  186.842270  ...  162.290237  161.532379  160.009674\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzpQ1Pz0fX5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d47f7688-e047-4a01-a159-0faaa4999d4a"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines\n",
        "# filename = 'model_ANN.pkl'\n",
        "filename = 'model_ANN_new.pkl'\n",
        "model = joblib.load(filename)\n",
        "'''"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n!git clone https://github.com/ucfilho/MarquesGabi_Routines\\n%cd MarquesGabi_Routines\\n# filename = 'model_ANN.pkl'\\nfilename = 'model_ANN_new.pkl'\\nmodel = joblib.load(filename)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2emP4rNjQy",
        "outputId": "7d992aa5-50f3-4870-ca99-d0f0431c47c9"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 21.93 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "Description = '3 layers of Convolution: 32, 64, 128 '\n",
        "N1 = 200\n",
        "N2 = 10\n",
        "\n",
        "# make the CNN\n",
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=32, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=N1))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=N2))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbPQ1FSRG6A",
        "outputId": "9a782457-5899-426c-b1ee-e732c260368b"
      },
      "source": [
        "\n",
        "# training the model\n",
        "r = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), \n",
        "              epochs=200, batch_size=32)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 20s 147ms/step - loss: 0.5884 - accuracy: 0.6638 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.3293 - accuracy: 0.8284 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.2131 - accuracy: 0.9011 - val_loss: 0.6931 - val_accuracy: 0.4898\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.1031 - accuracy: 0.9780 - val_loss: 0.6931 - val_accuracy: 0.4898\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.0686 - accuracy: 0.9845 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0329 - accuracy: 0.9958 - val_loss: 0.6929 - val_accuracy: 0.4898\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.0255 - accuracy: 0.9945 - val_loss: 0.6925 - val_accuracy: 0.5102\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.0105 - accuracy: 0.9995 - val_loss: 0.6924 - val_accuracy: 0.5102\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0267 - accuracy: 0.9930 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0351 - accuracy: 0.9811 - val_loss: 0.6930 - val_accuracy: 0.4898\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0338 - accuracy: 0.9871 - val_loss: 0.6968 - val_accuracy: 0.4898\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0278 - accuracy: 0.9856 - val_loss: 0.6914 - val_accuracy: 0.5102\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0520 - accuracy: 0.9958 - val_loss: 0.6910 - val_accuracy: 0.5102\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.0248 - accuracy: 0.9868 - val_loss: 0.6911 - val_accuracy: 0.5102\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0168 - accuracy: 0.9913 - val_loss: 0.6932 - val_accuracy: 0.5102\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.0123 - accuracy: 0.9983 - val_loss: 0.6900 - val_accuracy: 0.5102\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 0.6893 - val_accuracy: 0.5782\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.6905 - val_accuracy: 0.4966\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.6903 - val_accuracy: 0.6599\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6885 - val_accuracy: 0.5102\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6884 - val_accuracy: 0.5102\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.6862 - val_accuracy: 0.5102\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6864 - val_accuracy: 0.5102\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6890 - val_accuracy: 0.5102\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 3.9854e-04 - accuracy: 1.0000 - val_loss: 0.6934 - val_accuracy: 0.5102\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 9.3304e-04 - accuracy: 1.0000 - val_loss: 0.6999 - val_accuracy: 0.5102\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 6.5498e-04 - accuracy: 1.0000 - val_loss: 0.7081 - val_accuracy: 0.5102\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 4.3456e-04 - accuracy: 1.0000 - val_loss: 0.7166 - val_accuracy: 0.5102\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 3.1851e-04 - accuracy: 1.0000 - val_loss: 0.7259 - val_accuracy: 0.5102\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 2.7731e-04 - accuracy: 1.0000 - val_loss: 0.7371 - val_accuracy: 0.5102\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 4.1392e-04 - accuracy: 1.0000 - val_loss: 0.7509 - val_accuracy: 0.5102\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.8961e-04 - accuracy: 1.0000 - val_loss: 0.7663 - val_accuracy: 0.5102\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 3.5480e-04 - accuracy: 1.0000 - val_loss: 0.7734 - val_accuracy: 0.5102\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 2.5783e-04 - accuracy: 1.0000 - val_loss: 0.7821 - val_accuracy: 0.5102\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 3.4326e-04 - accuracy: 1.0000 - val_loss: 0.7805 - val_accuracy: 0.5102\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 4.5203e-04 - accuracy: 1.0000 - val_loss: 0.7721 - val_accuracy: 0.5102\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 4.3314e-04 - accuracy: 1.0000 - val_loss: 0.8689 - val_accuracy: 0.5102\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.7978 - val_accuracy: 0.5102\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 7.6844e-04 - accuracy: 1.0000 - val_loss: 0.7407 - val_accuracy: 0.5102\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 6.9287e-04 - accuracy: 1.0000 - val_loss: 0.8210 - val_accuracy: 0.5102\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.8093 - val_accuracy: 0.5102\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 7.5250e-04 - accuracy: 1.0000 - val_loss: 2.5208 - val_accuracy: 0.5102\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 2.8140 - val_accuracy: 0.5102\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0155 - accuracy: 0.9933 - val_loss: 0.9485 - val_accuracy: 0.5102\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0606 - accuracy: 0.9808 - val_loss: 4.9473 - val_accuracy: 0.5102\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0331 - accuracy: 0.9850 - val_loss: 9.9641 - val_accuracy: 0.5102\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.0385 - accuracy: 0.9914 - val_loss: 29.6145 - val_accuracy: 0.5102\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0091 - accuracy: 0.9959 - val_loss: 39.0557 - val_accuracy: 0.5102\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 31.0946 - val_accuracy: 0.5102\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 28.9232 - val_accuracy: 0.5102\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 26.3151 - val_accuracy: 0.5102\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 29.6296 - val_accuracy: 0.5102\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 3.7216e-04 - accuracy: 1.0000 - val_loss: 32.3727 - val_accuracy: 0.5102\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 3.6342e-04 - accuracy: 1.0000 - val_loss: 32.9385 - val_accuracy: 0.5102\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 3.6924e-04 - accuracy: 1.0000 - val_loss: 32.3553 - val_accuracy: 0.5102\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 3.2318e-04 - accuracy: 1.0000 - val_loss: 31.5045 - val_accuracy: 0.5102\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 3.9398e-04 - accuracy: 1.0000 - val_loss: 30.9985 - val_accuracy: 0.5102\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 3.8958e-04 - accuracy: 1.0000 - val_loss: 29.9212 - val_accuracy: 0.5102\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 5.9422e-04 - accuracy: 1.0000 - val_loss: 28.1722 - val_accuracy: 0.5102\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.9305e-04 - accuracy: 1.0000 - val_loss: 26.7075 - val_accuracy: 0.5102\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 4.5351e-04 - accuracy: 1.0000 - val_loss: 25.2623 - val_accuracy: 0.5102\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.4785e-04 - accuracy: 1.0000 - val_loss: 22.6199 - val_accuracy: 0.5102\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 4.3813e-04 - accuracy: 1.0000 - val_loss: 21.0701 - val_accuracy: 0.5102\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.7349e-04 - accuracy: 1.0000 - val_loss: 19.7695 - val_accuracy: 0.5102\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 1.9116e-04 - accuracy: 1.0000 - val_loss: 18.4622 - val_accuracy: 0.5102\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 4.1411e-04 - accuracy: 1.0000 - val_loss: 18.2123 - val_accuracy: 0.5102\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.2458e-04 - accuracy: 1.0000 - val_loss: 18.1553 - val_accuracy: 0.5102\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 1.5903e-04 - accuracy: 1.0000 - val_loss: 17.0943 - val_accuracy: 0.5102\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.9383e-04 - accuracy: 1.0000 - val_loss: 15.6380 - val_accuracy: 0.5102\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 2.2189e-04 - accuracy: 1.0000 - val_loss: 14.4262 - val_accuracy: 0.5102\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 8.0699e-05 - accuracy: 1.0000 - val_loss: 13.1860 - val_accuracy: 0.5102\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.6512e-04 - accuracy: 1.0000 - val_loss: 11.9255 - val_accuracy: 0.5102\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.4713e-04 - accuracy: 1.0000 - val_loss: 11.0607 - val_accuracy: 0.5102\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 8.1780e-05 - accuracy: 1.0000 - val_loss: 10.0933 - val_accuracy: 0.5102\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 2.0833e-04 - accuracy: 1.0000 - val_loss: 8.8203 - val_accuracy: 0.5102\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.0613e-04 - accuracy: 1.0000 - val_loss: 7.5835 - val_accuracy: 0.5102\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 7.0863e-05 - accuracy: 1.0000 - val_loss: 6.5569 - val_accuracy: 0.5102\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 5.3365e-05 - accuracy: 1.0000 - val_loss: 5.5878 - val_accuracy: 0.5170\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 5.0158e-05 - accuracy: 1.0000 - val_loss: 4.6566 - val_accuracy: 0.5238\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 9.0841e-05 - accuracy: 1.0000 - val_loss: 3.9296 - val_accuracy: 0.5510\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 1.3404e-04 - accuracy: 1.0000 - val_loss: 3.0919 - val_accuracy: 0.6190\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 8.9266e-05 - accuracy: 1.0000 - val_loss: 2.4581 - val_accuracy: 0.6667\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 4.5030e-05 - accuracy: 1.0000 - val_loss: 1.9309 - val_accuracy: 0.7075\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 3.4162e-04 - accuracy: 1.0000 - val_loss: 1.1429 - val_accuracy: 0.7687\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 7.6016e-04 - accuracy: 0.9995 - val_loss: 4.5042 - val_accuracy: 0.5578\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 3.9227e-04 - accuracy: 1.0000 - val_loss: 13.0628 - val_accuracy: 0.5102\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0203 - accuracy: 0.9876 - val_loss: 43.2502 - val_accuracy: 0.5102\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.0242 - accuracy: 0.9933 - val_loss: 52.4500 - val_accuracy: 0.5102\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.0551 - accuracy: 0.9821 - val_loss: 54.9908 - val_accuracy: 0.5102\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0200 - accuracy: 0.9976 - val_loss: 58.5210 - val_accuracy: 0.5102\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0419 - accuracy: 0.9876 - val_loss: 30.4019 - val_accuracy: 0.4898\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.1412 - accuracy: 0.9555 - val_loss: 55.5674 - val_accuracy: 0.5102\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.0656 - accuracy: 0.9881 - val_loss: 51.0268 - val_accuracy: 0.5102\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0474 - accuracy: 0.9931 - val_loss: 62.2744 - val_accuracy: 0.5102\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0682 - accuracy: 0.9743 - val_loss: 26.5772 - val_accuracy: 0.5102\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0427 - accuracy: 0.9811 - val_loss: 13.9135 - val_accuracy: 0.5102\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0089 - accuracy: 0.9990 - val_loss: 28.3921 - val_accuracy: 0.5102\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 32.5903 - val_accuracy: 0.5102\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.0024 - accuracy: 0.9990 - val_loss: 34.5764 - val_accuracy: 0.5102\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 3.9519e-04 - accuracy: 1.0000 - val_loss: 34.3287 - val_accuracy: 0.5102\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 32.2075 - val_accuracy: 0.5102\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 7.2523e-04 - accuracy: 1.0000 - val_loss: 31.3574 - val_accuracy: 0.5102\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 5.9761e-04 - accuracy: 1.0000 - val_loss: 29.0783 - val_accuracy: 0.5102\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 17.9800 - val_accuracy: 0.5102\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 14.7344 - val_accuracy: 0.5102\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 14.1880 - val_accuracy: 0.5102\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 4.9130e-04 - accuracy: 1.0000 - val_loss: 12.6564 - val_accuracy: 0.5102\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 3.2841e-04 - accuracy: 1.0000 - val_loss: 10.1948 - val_accuracy: 0.5102\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 7.1393 - val_accuracy: 0.5102\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 5.3539e-04 - accuracy: 1.0000 - val_loss: 3.2472 - val_accuracy: 0.5646\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 7.9301e-04 - accuracy: 1.0000 - val_loss: 3.2608 - val_accuracy: 0.5646\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 2.3268e-04 - accuracy: 1.0000 - val_loss: 3.3802 - val_accuracy: 0.5510\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 7.4431e-05 - accuracy: 1.0000 - val_loss: 3.2004 - val_accuracy: 0.5510\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 6.8465e-05 - accuracy: 1.0000 - val_loss: 2.6399 - val_accuracy: 0.5646\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 2.0869e-04 - accuracy: 1.0000 - val_loss: 2.2275 - val_accuracy: 0.5918\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 5.1250e-04 - accuracy: 1.0000 - val_loss: 1.9618 - val_accuracy: 0.6054\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 4.8295e-05 - accuracy: 1.0000 - val_loss: 1.3920 - val_accuracy: 0.6395\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 4.7110e-04 - accuracy: 1.0000 - val_loss: 1.3452 - val_accuracy: 0.6599\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 2.3933e-05 - accuracy: 1.0000 - val_loss: 0.9116 - val_accuracy: 0.6871\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 1.6395e-04 - accuracy: 1.0000 - val_loss: 0.5872 - val_accuracy: 0.7619\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 6.5713e-05 - accuracy: 1.0000 - val_loss: 0.4413 - val_accuracy: 0.8299\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.1286e-04 - accuracy: 1.0000 - val_loss: 0.2738 - val_accuracy: 0.8776\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 4.7115e-05 - accuracy: 1.0000 - val_loss: 0.2409 - val_accuracy: 0.9320\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 3.3324e-05 - accuracy: 1.0000 - val_loss: 0.2485 - val_accuracy: 0.8980\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.1389e-04 - accuracy: 1.0000 - val_loss: 0.2404 - val_accuracy: 0.8912\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.9778e-04 - accuracy: 1.0000 - val_loss: 0.1976 - val_accuracy: 0.9388\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 3.1214e-05 - accuracy: 1.0000 - val_loss: 0.2113 - val_accuracy: 0.9456\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 2.7198e-05 - accuracy: 1.0000 - val_loss: 0.2107 - val_accuracy: 0.9456\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.0415e-04 - accuracy: 1.0000 - val_loss: 0.2405 - val_accuracy: 0.9116\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 7.8084e-05 - accuracy: 1.0000 - val_loss: 0.2777 - val_accuracy: 0.8844\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 3.7318e-05 - accuracy: 1.0000 - val_loss: 0.3165 - val_accuracy: 0.8639\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 6.2608e-05 - accuracy: 1.0000 - val_loss: 0.3300 - val_accuracy: 0.8571\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 8.3859e-05 - accuracy: 1.0000 - val_loss: 0.5260 - val_accuracy: 0.8027\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 4.9321e-05 - accuracy: 1.0000 - val_loss: 0.8664 - val_accuracy: 0.7823\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 2.1717e-05 - accuracy: 1.0000 - val_loss: 0.9321 - val_accuracy: 0.7483\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.9160e-05 - accuracy: 1.0000 - val_loss: 0.9590 - val_accuracy: 0.7347\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 4.9165e-04 - accuracy: 1.0000 - val_loss: 0.2961 - val_accuracy: 0.9388\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 2.5567e-05 - accuracy: 1.0000 - val_loss: 1.3123 - val_accuracy: 0.7823\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 3.4950e-05 - accuracy: 1.0000 - val_loss: 1.6979 - val_accuracy: 0.7483\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.7986e-05 - accuracy: 1.0000 - val_loss: 1.5668 - val_accuracy: 0.7823\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 9.6274e-05 - accuracy: 1.0000 - val_loss: 1.1575 - val_accuracy: 0.7959\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 2.4393e-05 - accuracy: 1.0000 - val_loss: 0.9215 - val_accuracy: 0.8435\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 2.4341e-05 - accuracy: 1.0000 - val_loss: 0.7908 - val_accuracy: 0.8776\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 6.3762e-05 - accuracy: 1.0000 - val_loss: 0.6573 - val_accuracy: 0.8844\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.9171e-05 - accuracy: 1.0000 - val_loss: 0.5249 - val_accuracy: 0.8912\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 3.3127e-05 - accuracy: 1.0000 - val_loss: 0.4338 - val_accuracy: 0.9184\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 6.2796e-05 - accuracy: 1.0000 - val_loss: 0.2070 - val_accuracy: 0.9592\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 1.6597e-05 - accuracy: 1.0000 - val_loss: 0.1818 - val_accuracy: 0.9592\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 3.2341e-05 - accuracy: 1.0000 - val_loss: 0.1851 - val_accuracy: 0.9660\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 2.4443e-05 - accuracy: 1.0000 - val_loss: 0.1964 - val_accuracy: 0.9592\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.1242e-05 - accuracy: 1.0000 - val_loss: 0.2098 - val_accuracy: 0.9592\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 3.8984e-05 - accuracy: 1.0000 - val_loss: 0.2111 - val_accuracy: 0.9592\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 2.2594e-05 - accuracy: 1.0000 - val_loss: 0.2174 - val_accuracy: 0.9592\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 2.1110e-05 - accuracy: 1.0000 - val_loss: 0.2194 - val_accuracy: 0.9592\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 2.5144e-04 - accuracy: 1.0000 - val_loss: 0.2559 - val_accuracy: 0.9592\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 1.0160e-05 - accuracy: 1.0000 - val_loss: 0.3791 - val_accuracy: 0.9388\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 2.7882e-05 - accuracy: 1.0000 - val_loss: 0.3678 - val_accuracy: 0.9456\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 9.5196e-06 - accuracy: 1.0000 - val_loss: 0.3307 - val_accuracy: 0.9456\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 6.8404e-05 - accuracy: 1.0000 - val_loss: 0.2873 - val_accuracy: 0.9524\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 2.8741e-05 - accuracy: 1.0000 - val_loss: 0.2489 - val_accuracy: 0.9660\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.1206e-05 - accuracy: 1.0000 - val_loss: 0.2251 - val_accuracy: 0.9660\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.2314e-04 - accuracy: 1.0000 - val_loss: 0.1996 - val_accuracy: 0.9592\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.7592e-05 - accuracy: 1.0000 - val_loss: 0.1959 - val_accuracy: 0.9592\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 9.8712e-06 - accuracy: 1.0000 - val_loss: 0.1952 - val_accuracy: 0.9660\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 5.3977e-05 - accuracy: 1.0000 - val_loss: 0.2007 - val_accuracy: 0.9660\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 6.9226e-06 - accuracy: 1.0000 - val_loss: 0.2270 - val_accuracy: 0.9660\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 1.6990e-05 - accuracy: 1.0000 - val_loss: 0.2151 - val_accuracy: 0.9660\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 2.9182e-05 - accuracy: 1.0000 - val_loss: 0.2110 - val_accuracy: 0.9660\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 1.0814e-05 - accuracy: 1.0000 - val_loss: 0.2067 - val_accuracy: 0.9660\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 3.0099e-05 - accuracy: 1.0000 - val_loss: 0.2001 - val_accuracy: 0.9660\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 2.0282e-05 - accuracy: 1.0000 - val_loss: 0.1954 - val_accuracy: 0.9660\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 1.6009e-05 - accuracy: 1.0000 - val_loss: 0.1894 - val_accuracy: 0.9728\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 6.9775e-06 - accuracy: 1.0000 - val_loss: 0.1902 - val_accuracy: 0.9728\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 1.3737e-05 - accuracy: 1.0000 - val_loss: 0.1965 - val_accuracy: 0.9660\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 1.3589e-04 - accuracy: 1.0000 - val_loss: 0.1911 - val_accuracy: 0.9796\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 8.4243e-05 - accuracy: 1.0000 - val_loss: 0.1991 - val_accuracy: 0.9660\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 8.8030e-05 - accuracy: 1.0000 - val_loss: 0.2236 - val_accuracy: 0.9592\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 1.1462e-05 - accuracy: 1.0000 - val_loss: 0.2441 - val_accuracy: 0.9592\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 1.2351e-05 - accuracy: 1.0000 - val_loss: 0.2525 - val_accuracy: 0.9592\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 1s 59ms/step - loss: 2.0980e-05 - accuracy: 1.0000 - val_loss: 0.2538 - val_accuracy: 0.9592\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 3.6413e-05 - accuracy: 1.0000 - val_loss: 0.2373 - val_accuracy: 0.9592\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 2.1863e-05 - accuracy: 1.0000 - val_loss: 0.2194 - val_accuracy: 0.9660\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.1604e-05 - accuracy: 1.0000 - val_loss: 0.2129 - val_accuracy: 0.9660\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 8.3232e-06 - accuracy: 1.0000 - val_loss: 0.2082 - val_accuracy: 0.9660\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 2.9054e-05 - accuracy: 1.0000 - val_loss: 0.1989 - val_accuracy: 0.9660\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 1.2697e-05 - accuracy: 1.0000 - val_loss: 0.2076 - val_accuracy: 0.9660\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 1.2566e-05 - accuracy: 1.0000 - val_loss: 0.2153 - val_accuracy: 0.9728\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 1.6736e-05 - accuracy: 1.0000 - val_loss: 0.2269 - val_accuracy: 0.9728\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 4.4802e-05 - accuracy: 1.0000 - val_loss: 0.3218 - val_accuracy: 0.9456\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 1.5369e-05 - accuracy: 1.0000 - val_loss: 0.3509 - val_accuracy: 0.9252\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 1.1486e-05 - accuracy: 1.0000 - val_loss: 0.3396 - val_accuracy: 0.9320\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 1.7072e-05 - accuracy: 1.0000 - val_loss: 0.3240 - val_accuracy: 0.9524\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 1.3971e-05 - accuracy: 1.0000 - val_loss: 0.3050 - val_accuracy: 0.9524\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 1s 57ms/step - loss: 9.1477e-05 - accuracy: 1.0000 - val_loss: 0.2113 - val_accuracy: 0.9728\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 5.9586e-06 - accuracy: 1.0000 - val_loss: 0.1957 - val_accuracy: 0.9864\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 9.4477e-06 - accuracy: 1.0000 - val_loss: 0.2041 - val_accuracy: 0.9796\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 2.9933e-05 - accuracy: 1.0000 - val_loss: 0.2208 - val_accuracy: 0.9728\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 2.9387e-05 - accuracy: 1.0000 - val_loss: 0.2236 - val_accuracy: 0.9728\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 1s 58ms/step - loss: 5.1731e-05 - accuracy: 1.0000 - val_loss: 0.3914 - val_accuracy: 0.9252\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 5.6386e-06 - accuracy: 1.0000 - val_loss: 0.3908 - val_accuracy: 0.9252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpf0XlSARX78",
        "outputId": "ad35da97-3874-4616-b079-e838639f9508"
      },
      "source": [
        "pred_test= model.predict_classes(X_test)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predict   0   1\n",
            "Actual         \n",
            "0        62  10\n",
            "1         1  74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iFNNrlWV9tH",
        "outputId": "1d2f178d-01e4-4347-938d-93857e669e51"
      },
      "source": [
        "pred_test"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
              "       1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
              "       0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QISvYcJBgWbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e526130-fdc5-4f16-c7a5-3969e3d90578"
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[0] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  prediction = model.predict_classes(result)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "3   123.0  126.321640  149.682999  ...   16.398506    9.290436    9.901316\n",
            "7   167.0   97.423691   87.658974  ...   52.425552   61.184086   70.082153\n",
            "15  108.0  218.030182  193.765427  ...    0.567901    0.587106    1.115226\n",
            "23  102.0   94.346413   97.222237  ...   78.223381   59.680515   47.980396\n",
            "26  122.0   59.502548   61.048370  ...    5.804622    0.637194    0.943563\n",
            "30  106.0   90.440369   92.909576  ...  122.928802  123.701324  125.370239\n",
            "31  135.0   80.397202   79.047409  ...   99.931244   92.554176   90.774536\n",
            "44  150.0   73.120888   75.500626  ...   62.993423   61.666309   57.215466\n",
            "17  151.0   82.716721   80.792862  ...   48.814579   48.426605   61.250690\n",
            "21  183.0   87.842781   86.051872  ...   45.935799   49.151276   51.449821\n",
            "28  167.0    0.916670    2.521317  ...    0.000000    0.000000    0.000000\n",
            "40  116.0   85.135551   84.946487  ...   91.442329  104.586212  112.882278\n",
            "6   169.0   84.213684   84.582008  ...    6.121389    5.610622    6.245894\n",
            "9   184.0    0.797259    0.492911  ...   81.352547   59.504246   57.748104\n",
            "13  191.0   95.360420   92.241592  ...   55.237606   60.445690   64.564934\n",
            "30  104.0    7.368343    8.924557  ...    0.000000    0.000000    0.000000\n",
            "36  130.0   76.548882  102.647354  ...  116.590057  110.959534   91.156929\n",
            "37  143.0  122.194916  122.018379  ...   37.029537   37.027435   35.694164\n",
            "40  144.0   51.272377   36.206791  ...   54.276237   48.660496   51.581020\n",
            "42  121.0  110.850151  102.764091  ...   97.365143   98.391228   90.756912\n",
            "43  151.0   86.926399   83.486473  ...  109.654099   80.291130   84.541115\n",
            "49  161.0  100.531189  107.321365  ...  107.013237   70.181473   44.725899\n",
            "16  114.0  107.587563  109.278549  ...   90.183136   90.341339   87.065865\n",
            "19  121.0   75.741074   88.446487  ...  121.609116  122.951576  123.912033\n",
            "26  151.0    0.206307    0.259111  ...    0.000000    0.000000    0.000000\n",
            "30  112.0   87.125000   26.062500  ...    0.000000    0.000000    0.000000\n",
            "31  114.0   65.012009   62.855034  ...    8.721760    8.445984    8.372730\n",
            "44  118.0  129.956619  127.432625  ...   88.891121   89.749214   88.467972\n",
            "49  120.0   84.902229   85.516670  ...   49.107780   47.165558   44.186668\n",
            "\n",
            "[29 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRbWgmX_LFH",
        "outputId": "c61c9439-eb8e-4649-e063-ae36955bba4d"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "from GetBetterSegm import GetBetter"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_paper_fev_2021'...\n",
            "remote: Enumerating objects: 645, done.\u001b[K\n",
            "remote: Counting objects: 100% (406/406), done.\u001b[K\n",
            "remote: Compressing objects: 100% (404/404), done.\u001b[K\n",
            "remote: Total 645 (delta 252), reused 0 (delta 0), pack-reused 239\u001b[K\n",
            "Receiving objects: 100% (645/645), 5.39 MiB | 14.47 MiB/s, done.\n",
            "Resolving deltas: 100% (389/389), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAG_I6FwCvFr",
        "outputId": "bbeb5fef-72e7-429a-dfee-8a2cfe688602"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "%cd marquesgabi_out_2020\n",
        "PSD_imageJ = 'Areas_ImageJ.csv'\n",
        "PSD_new = pd.read_csv(PSD_imageJ)\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_out_2020'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 146 (delta 75), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (146/146), 1.00 MiB | 7.13 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020\n",
            "   Juntas   Area\n",
            "0       1  2.001\n",
            "1       2  0.820\n",
            "2       3  1.270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "PekBHQOT_6CP",
        "outputId": "60fd0b27-0ca5-459b-8e1d-199b76b3fd59"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>123.0</td>\n",
              "      <td>126.321640</td>\n",
              "      <td>149.682999</td>\n",
              "      <td>164.413239</td>\n",
              "      <td>49.738255</td>\n",
              "      <td>33.192875</td>\n",
              "      <td>49.254215</td>\n",
              "      <td>45.334988</td>\n",
              "      <td>43.010448</td>\n",
              "      <td>40.823055</td>\n",
              "      <td>45.392693</td>\n",
              "      <td>50.675259</td>\n",
              "      <td>52.933571</td>\n",
              "      <td>53.862915</td>\n",
              "      <td>54.293079</td>\n",
              "      <td>53.039135</td>\n",
              "      <td>47.661777</td>\n",
              "      <td>36.027431</td>\n",
              "      <td>18.198757</td>\n",
              "      <td>5.274639</td>\n",
              "      <td>2.333532</td>\n",
              "      <td>0.804085</td>\n",
              "      <td>0.576046</td>\n",
              "      <td>0.434926</td>\n",
              "      <td>0.153612</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>130.997681</td>\n",
              "      <td>155.305573</td>\n",
              "      <td>166.455811</td>\n",
              "      <td>55.335587</td>\n",
              "      <td>27.947454</td>\n",
              "      <td>42.469498</td>\n",
              "      <td>39.772686</td>\n",
              "      <td>38.032719</td>\n",
              "      <td>42.111443</td>\n",
              "      <td>48.200012</td>\n",
              "      <td>52.153481</td>\n",
              "      <td>...</td>\n",
              "      <td>56.452248</td>\n",
              "      <td>50.422501</td>\n",
              "      <td>49.113426</td>\n",
              "      <td>46.690399</td>\n",
              "      <td>43.202133</td>\n",
              "      <td>42.096836</td>\n",
              "      <td>41.532951</td>\n",
              "      <td>37.530575</td>\n",
              "      <td>29.926565</td>\n",
              "      <td>20.155790</td>\n",
              "      <td>11.573336</td>\n",
              "      <td>10.151033</td>\n",
              "      <td>59.376560</td>\n",
              "      <td>48.787495</td>\n",
              "      <td>35.002579</td>\n",
              "      <td>28.184547</td>\n",
              "      <td>27.038139</td>\n",
              "      <td>27.883009</td>\n",
              "      <td>29.120102</td>\n",
              "      <td>30.610086</td>\n",
              "      <td>36.366646</td>\n",
              "      <td>44.798138</td>\n",
              "      <td>50.030670</td>\n",
              "      <td>53.636330</td>\n",
              "      <td>53.939190</td>\n",
              "      <td>56.924713</td>\n",
              "      <td>58.507641</td>\n",
              "      <td>56.714790</td>\n",
              "      <td>53.686298</td>\n",
              "      <td>51.976738</td>\n",
              "      <td>47.356403</td>\n",
              "      <td>43.366055</td>\n",
              "      <td>41.351910</td>\n",
              "      <td>39.652523</td>\n",
              "      <td>39.981495</td>\n",
              "      <td>35.904823</td>\n",
              "      <td>24.799988</td>\n",
              "      <td>16.398506</td>\n",
              "      <td>9.290436</td>\n",
              "      <td>9.901316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>167.0</td>\n",
              "      <td>97.423691</td>\n",
              "      <td>87.658974</td>\n",
              "      <td>76.917931</td>\n",
              "      <td>71.742767</td>\n",
              "      <td>73.053711</td>\n",
              "      <td>81.083908</td>\n",
              "      <td>86.514359</td>\n",
              "      <td>94.943138</td>\n",
              "      <td>109.098030</td>\n",
              "      <td>117.098175</td>\n",
              "      <td>116.671165</td>\n",
              "      <td>118.585686</td>\n",
              "      <td>121.194786</td>\n",
              "      <td>119.774300</td>\n",
              "      <td>120.165306</td>\n",
              "      <td>125.201630</td>\n",
              "      <td>125.296509</td>\n",
              "      <td>123.500175</td>\n",
              "      <td>123.093384</td>\n",
              "      <td>122.895599</td>\n",
              "      <td>126.977280</td>\n",
              "      <td>130.982162</td>\n",
              "      <td>129.934357</td>\n",
              "      <td>123.833305</td>\n",
              "      <td>115.457214</td>\n",
              "      <td>101.210518</td>\n",
              "      <td>92.874939</td>\n",
              "      <td>84.684898</td>\n",
              "      <td>97.289803</td>\n",
              "      <td>88.071686</td>\n",
              "      <td>76.544876</td>\n",
              "      <td>71.917610</td>\n",
              "      <td>74.739113</td>\n",
              "      <td>89.128082</td>\n",
              "      <td>105.299545</td>\n",
              "      <td>112.015961</td>\n",
              "      <td>120.726288</td>\n",
              "      <td>122.380699</td>\n",
              "      <td>123.050529</td>\n",
              "      <td>...</td>\n",
              "      <td>131.762756</td>\n",
              "      <td>209.716125</td>\n",
              "      <td>184.262344</td>\n",
              "      <td>57.778625</td>\n",
              "      <td>89.728546</td>\n",
              "      <td>93.389839</td>\n",
              "      <td>106.122665</td>\n",
              "      <td>86.259293</td>\n",
              "      <td>39.189075</td>\n",
              "      <td>55.030876</td>\n",
              "      <td>62.505760</td>\n",
              "      <td>70.034111</td>\n",
              "      <td>84.135605</td>\n",
              "      <td>84.168526</td>\n",
              "      <td>85.057159</td>\n",
              "      <td>85.625412</td>\n",
              "      <td>87.072502</td>\n",
              "      <td>87.679199</td>\n",
              "      <td>84.295395</td>\n",
              "      <td>85.037971</td>\n",
              "      <td>84.173798</td>\n",
              "      <td>77.764107</td>\n",
              "      <td>64.769699</td>\n",
              "      <td>60.940159</td>\n",
              "      <td>103.215675</td>\n",
              "      <td>234.335190</td>\n",
              "      <td>209.339142</td>\n",
              "      <td>192.290482</td>\n",
              "      <td>154.007858</td>\n",
              "      <td>145.467941</td>\n",
              "      <td>116.190910</td>\n",
              "      <td>73.543549</td>\n",
              "      <td>95.629074</td>\n",
              "      <td>97.699783</td>\n",
              "      <td>116.377754</td>\n",
              "      <td>88.860878</td>\n",
              "      <td>33.451328</td>\n",
              "      <td>52.425552</td>\n",
              "      <td>61.184086</td>\n",
              "      <td>70.082153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>108.0</td>\n",
              "      <td>218.030182</td>\n",
              "      <td>193.765427</td>\n",
              "      <td>93.824417</td>\n",
              "      <td>105.836761</td>\n",
              "      <td>119.075439</td>\n",
              "      <td>117.146774</td>\n",
              "      <td>105.367622</td>\n",
              "      <td>94.094650</td>\n",
              "      <td>89.596710</td>\n",
              "      <td>88.747597</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>89.371735</td>\n",
              "      <td>84.485596</td>\n",
              "      <td>76.112480</td>\n",
              "      <td>68.941017</td>\n",
              "      <td>63.094643</td>\n",
              "      <td>62.178326</td>\n",
              "      <td>62.595333</td>\n",
              "      <td>61.554184</td>\n",
              "      <td>60.814812</td>\n",
              "      <td>55.916321</td>\n",
              "      <td>51.038406</td>\n",
              "      <td>50.347050</td>\n",
              "      <td>51.204388</td>\n",
              "      <td>53.203018</td>\n",
              "      <td>59.846367</td>\n",
              "      <td>67.248283</td>\n",
              "      <td>73.640602</td>\n",
              "      <td>215.584351</td>\n",
              "      <td>233.042511</td>\n",
              "      <td>183.179688</td>\n",
              "      <td>90.348419</td>\n",
              "      <td>112.336075</td>\n",
              "      <td>115.396431</td>\n",
              "      <td>108.735245</td>\n",
              "      <td>96.001373</td>\n",
              "      <td>88.485588</td>\n",
              "      <td>88.004112</td>\n",
              "      <td>85.545952</td>\n",
              "      <td>...</td>\n",
              "      <td>0.868313</td>\n",
              "      <td>1.181070</td>\n",
              "      <td>1.834019</td>\n",
              "      <td>1.426612</td>\n",
              "      <td>1.482853</td>\n",
              "      <td>1.042524</td>\n",
              "      <td>1.160494</td>\n",
              "      <td>1.241427</td>\n",
              "      <td>1.200274</td>\n",
              "      <td>1.271605</td>\n",
              "      <td>1.554184</td>\n",
              "      <td>2.679012</td>\n",
              "      <td>1.935528</td>\n",
              "      <td>0.705075</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.067215</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.057613</td>\n",
              "      <td>0.633745</td>\n",
              "      <td>0.086420</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016461</td>\n",
              "      <td>0.644719</td>\n",
              "      <td>0.673525</td>\n",
              "      <td>0.259259</td>\n",
              "      <td>0.441701</td>\n",
              "      <td>0.469136</td>\n",
              "      <td>0.780521</td>\n",
              "      <td>0.876543</td>\n",
              "      <td>0.474623</td>\n",
              "      <td>0.289438</td>\n",
              "      <td>0.172840</td>\n",
              "      <td>0.271605</td>\n",
              "      <td>0.035665</td>\n",
              "      <td>0.533608</td>\n",
              "      <td>0.567901</td>\n",
              "      <td>0.587106</td>\n",
              "      <td>1.115226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>102.0</td>\n",
              "      <td>94.346413</td>\n",
              "      <td>97.222237</td>\n",
              "      <td>99.785477</td>\n",
              "      <td>99.092667</td>\n",
              "      <td>98.594780</td>\n",
              "      <td>99.333725</td>\n",
              "      <td>97.554794</td>\n",
              "      <td>98.869675</td>\n",
              "      <td>99.618233</td>\n",
              "      <td>100.347183</td>\n",
              "      <td>93.888519</td>\n",
              "      <td>92.098427</td>\n",
              "      <td>91.931190</td>\n",
              "      <td>92.921577</td>\n",
              "      <td>92.049606</td>\n",
              "      <td>90.458672</td>\n",
              "      <td>88.937721</td>\n",
              "      <td>88.640915</td>\n",
              "      <td>90.892357</td>\n",
              "      <td>94.359489</td>\n",
              "      <td>99.519806</td>\n",
              "      <td>102.694748</td>\n",
              "      <td>107.710503</td>\n",
              "      <td>110.543266</td>\n",
              "      <td>111.032303</td>\n",
              "      <td>111.828156</td>\n",
              "      <td>112.424072</td>\n",
              "      <td>109.356033</td>\n",
              "      <td>97.905823</td>\n",
              "      <td>101.250679</td>\n",
              "      <td>104.000786</td>\n",
              "      <td>104.350639</td>\n",
              "      <td>103.011543</td>\n",
              "      <td>100.245682</td>\n",
              "      <td>98.492897</td>\n",
              "      <td>97.188782</td>\n",
              "      <td>99.068451</td>\n",
              "      <td>97.916580</td>\n",
              "      <td>93.567871</td>\n",
              "      <td>...</td>\n",
              "      <td>50.549793</td>\n",
              "      <td>46.350639</td>\n",
              "      <td>55.567863</td>\n",
              "      <td>76.697426</td>\n",
              "      <td>97.567490</td>\n",
              "      <td>109.421394</td>\n",
              "      <td>114.580559</td>\n",
              "      <td>114.019234</td>\n",
              "      <td>111.801239</td>\n",
              "      <td>104.554802</td>\n",
              "      <td>94.151871</td>\n",
              "      <td>72.158791</td>\n",
              "      <td>94.672829</td>\n",
              "      <td>95.124954</td>\n",
              "      <td>95.435226</td>\n",
              "      <td>94.863907</td>\n",
              "      <td>94.826996</td>\n",
              "      <td>95.225311</td>\n",
              "      <td>99.288742</td>\n",
              "      <td>100.658218</td>\n",
              "      <td>100.181091</td>\n",
              "      <td>102.277206</td>\n",
              "      <td>101.772789</td>\n",
              "      <td>99.500587</td>\n",
              "      <td>98.396782</td>\n",
              "      <td>96.416771</td>\n",
              "      <td>91.871216</td>\n",
              "      <td>81.794319</td>\n",
              "      <td>57.606308</td>\n",
              "      <td>44.361404</td>\n",
              "      <td>48.639759</td>\n",
              "      <td>51.341026</td>\n",
              "      <td>60.805470</td>\n",
              "      <td>78.973862</td>\n",
              "      <td>90.958870</td>\n",
              "      <td>93.934647</td>\n",
              "      <td>89.603622</td>\n",
              "      <td>78.223381</td>\n",
              "      <td>59.680515</td>\n",
              "      <td>47.980396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>122.0</td>\n",
              "      <td>59.502548</td>\n",
              "      <td>61.048370</td>\n",
              "      <td>60.705727</td>\n",
              "      <td>62.353127</td>\n",
              "      <td>63.745224</td>\n",
              "      <td>64.448799</td>\n",
              "      <td>67.382690</td>\n",
              "      <td>65.985756</td>\n",
              "      <td>62.854336</td>\n",
              "      <td>62.263634</td>\n",
              "      <td>61.737972</td>\n",
              "      <td>59.830421</td>\n",
              "      <td>60.321419</td>\n",
              "      <td>59.653858</td>\n",
              "      <td>59.345329</td>\n",
              "      <td>59.810265</td>\n",
              "      <td>60.091640</td>\n",
              "      <td>57.084923</td>\n",
              "      <td>53.662186</td>\n",
              "      <td>43.755707</td>\n",
              "      <td>34.008598</td>\n",
              "      <td>16.520288</td>\n",
              "      <td>7.384573</td>\n",
              "      <td>4.740661</td>\n",
              "      <td>1.719699</td>\n",
              "      <td>1.270895</td>\n",
              "      <td>1.052674</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>59.541252</td>\n",
              "      <td>59.131683</td>\n",
              "      <td>57.181400</td>\n",
              "      <td>59.163124</td>\n",
              "      <td>61.310398</td>\n",
              "      <td>61.558178</td>\n",
              "      <td>62.936035</td>\n",
              "      <td>63.368717</td>\n",
              "      <td>60.409836</td>\n",
              "      <td>59.476212</td>\n",
              "      <td>60.169037</td>\n",
              "      <td>...</td>\n",
              "      <td>24.588013</td>\n",
              "      <td>22.142433</td>\n",
              "      <td>20.885782</td>\n",
              "      <td>21.580219</td>\n",
              "      <td>21.020155</td>\n",
              "      <td>16.038698</td>\n",
              "      <td>14.990593</td>\n",
              "      <td>15.334049</td>\n",
              "      <td>12.862402</td>\n",
              "      <td>7.694437</td>\n",
              "      <td>1.575652</td>\n",
              "      <td>0.311475</td>\n",
              "      <td>105.037613</td>\n",
              "      <td>104.572693</td>\n",
              "      <td>90.728035</td>\n",
              "      <td>11.607901</td>\n",
              "      <td>10.884171</td>\n",
              "      <td>17.423273</td>\n",
              "      <td>18.204247</td>\n",
              "      <td>20.803009</td>\n",
              "      <td>18.124161</td>\n",
              "      <td>16.436977</td>\n",
              "      <td>59.911316</td>\n",
              "      <td>93.336189</td>\n",
              "      <td>94.113678</td>\n",
              "      <td>92.465195</td>\n",
              "      <td>84.793327</td>\n",
              "      <td>72.632088</td>\n",
              "      <td>41.500130</td>\n",
              "      <td>25.492609</td>\n",
              "      <td>22.899757</td>\n",
              "      <td>23.123352</td>\n",
              "      <td>24.134373</td>\n",
              "      <td>19.158291</td>\n",
              "      <td>16.378391</td>\n",
              "      <td>15.020156</td>\n",
              "      <td>12.452835</td>\n",
              "      <td>5.804622</td>\n",
              "      <td>0.637194</td>\n",
              "      <td>0.943563</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Width           0           1  ...        781        782        783\n",
              "3   123.0  126.321640  149.682999  ...  16.398506   9.290436   9.901316\n",
              "7   167.0   97.423691   87.658974  ...  52.425552  61.184086  70.082153\n",
              "15  108.0  218.030182  193.765427  ...   0.567901   0.587106   1.115226\n",
              "23  102.0   94.346413   97.222237  ...  78.223381  59.680515  47.980396\n",
              "26  122.0   59.502548   61.048370  ...   5.804622   0.637194   0.943563\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "Area = np.array(PSD_new['Area'])\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J705kDqsE8f",
        "outputId": "2508efc9-f7f4-428e-9ae4-d50cf9bb7965"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(490, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wCFDX8esLoQ"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn-F050Hr9Ui"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "Vfk_fNXGDK5_",
        "outputId": "383d0a3b-a693-4a9c-d62e-1a0f40d447bc"
      },
      "source": [
        " wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        " wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        " X = pd.DataFrame([Diam1,Diameter_All])\n",
        " wts = pd.DataFrame([wt1,wt2])\n",
        "plt.hist(X,weights=wts)\n",
        "plt.legend(['Image J','CNN'])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7febc7b97d90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD6CAYAAAC4RRw1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUfElEQVR4nO3dfZBddZ3n8feX0NDOkoVIGoxE6PAwPMkmwSbIhhozQdgIWyJViDI7DEyBQR0os1pbRKhawq5bJBoFdVitMLDEGB0ohBFGZ1aKCePi8NSBEIJdw/AQMWxImoAoswKGfPePexM7oTv3dvd9+pH3q6qLe8/5nXs/aTgffjn3nHMjM5EklWevdgeQJI2NBS5JhbLAJalQFrgkFcoCl6RCWeCSVKiaBR4R3RHxcEQ8HhFPRsQ11eW3RMRzEbGm+jOj+XElSdvtXceYN4C5mflaRHQB90fE31XX/ZfMvL3eN5s8eXL29vaOIaYk7blWr179Umb27Lq8ZoFn5Uqf16pPu6o/Y7r6p7e3l/7+/rFsKkl7rIj4xXDL6zoGHhETImINsBm4JzMfqq76HxGxNiKui4h9G5RVklSHugo8M9/KzBnAVGBWRLwf+CJwDHAS8G7giuG2jYj5EdEfEf2Dg4MNii1JGtVZKJn5K2AVMC8zN2bFG8D/AmaNsM2yzOzLzL6enrcdwpEkjVHNY+AR0QP8LjN/FRHvAk4HlkTElMzcGBEBfAxY1+Sskgr3u9/9jg0bNvD666+3O0pH6u7uZurUqXR1ddU1vp6zUKYAyyNiApUZ+22Z+bcR8Q/Vcg9gDfDpsYaWtGfYsGEDEydOpLe3l8rcT9tlJlu2bGHDhg1Mmzatrm3qOQtlLTBzmOVzRx9R0p7s9ddft7xHEBEceOCBjOazQq/ElNRSlvfIRvu7scAlqVD1HAOXpKboXfijhr7e+sVn1Ryz33778dprr9Uc12xz5sxh6dKl9PX1jfk1LHCNaCw7Vz07kKTG8BCKpD3Sfffdx4c+9CHOPvtsDj/8cBYuXMjKlSuZNWsWJ5xwAs888wwAd999NyeffDIzZ87kwx/+MJs2bQJgcHCQ008/neOPP55LLrmEww47jJdeegmA7373u8yaNYsZM2Zw6aWX8tZbbzXlz2CBS9pjPf7443z7299mYGCAFStW8NRTT/Hwww9zySWX8M1vfhOAU089lQcffJDHHnuMT37yk3z5y18G4JprrmHu3Lk8+eSTnHvuuTz//PMADAwMcOutt/Kzn/2MNWvWMGHCBFauXNmU/B5CkbTHOumkk5gyZQoARxxxBGeccQYAJ5xwAqtWrQIq565/4hOfYOPGjbz55ps7ztG+//77ufPOOwGYN28ekyZNAuDee+9l9erVnHTSSQD89re/5aCDDmpKfgtc0h5r331/fw++vfbaa8fzvfbai61btwJw+eWX8/nPf56PfvSj3HfffSxatGi3r5mZXHjhhVx77bVNy72dh1AkaTdeffVVDjnkEACWL1++Y/ns2bO57bbbAPjJT37CK6+8AsBpp53G7bffzubNmwF4+eWX+cUvhr0b7Lg5A5fUNiWctbRo0SI+/vGPM2nSJObOnctzzz0HwNVXX83555/PihUrOOWUU3jPe97DxIkTmTx5Ml/60pc444wz2LZtG11dXdxwww0cdthhO73u1q1bd/obwFhE5fsaWqOvry/9QodyeBqhGm1gYIBjjz223TEa4o033mDChAnsvffePPDAA3zmM59hzZo1dW975JFHsm7dOvbff/+d1g33O4qI1Zn5thPGnYFL0hg8//zznHfeeWzbto199tmHG2+8sa7t+vv7ueCCC/jsZz/7tvIeLQtcksbgqKOO4rHHHhv1dn19fQwMDDQkgx9iSlKhLHBJKpQFLkmFssAlqVB+iCmpfRaN7yyMt7/eqzWHvPjiiyxYsIBHHnmEAw44gIMPPpjrr7+eo48+mm984xtcfvnlAFx22WX09fVx0UUXcdFFF3HPPffw7LPPsu+++/LSSy/R19fH+vXrG5t/lJyBS9pjZCbnnHMOc+bM4ZlnnmH16tVce+21bNq0iYMOOoivf/3rvPnmm8NuO2HCBG6++eYWJ949Z+DNMtqZRR0zB0njs2rVKrq6uvj0p3//HezTp09n/fr19PT0MHv2bJYvX86nPvWpt227YMECrrvuumHXtYszcEl7jHXr1vGBD3xgxPVXXHEFS5cuHfb+3YceeiinnnoqK1asaGbEUbHAJanq8MMP5+STT+Z73/vesOu/+MUv8pWvfIVt27a1ONnwahZ4RHRHxMMR8XhEPBkR11SXT4uIhyLi6Yi4NSL2aX5cSRq7448/ntWrV+92zJVXXsmSJUsY7j5RRx11FDNmzNhxF8J2q2cG/gYwNzOnAzOAeRHxQWAJcF1mHgm8AlzcvJiSNH5z587ljTfeYNmyZTuWrV27ll/+8pc7nh9zzDEcd9xx3H333cO+xlVXXcXSpUubnrUeNT/EzMr/hrZ/hXNX9SeBucCfVJcvBxYB32p8REnvWC3+8D4iuPPOO1mwYAFLliyhu7ub3t5err/++p3GXXXVVcycOXPY1zj++OM58cQTefTRR1sRebfqOgslIiYAq4EjgRuAZ4BfZebW6pANwCEjbDsfmA+VDwEkqZ3e+973DnsIZN26dTseT58+fafj3LfccstOY++4446m5RuNuj7EzMy3MnMGMBWYBRxT7xtk5rLM7MvMvp6enjHGlCTtalRnoWTmr4BVwCnAARGxfQY/FXihwdkkSbtRz1koPRFxQPXxu4DTgQEqRX5uddiFwA+bFVLSO0crvwWsNKP93dQzA58CrIqItcAjwD2Z+bfAFcDnI+Jp4EDgplFmlbSH6e7uZsuWLZb4MDKTLVu20N3dXfc29ZyFshZ428exmfkslePhklSXqVOnsmHDBgYHB9sdpSN1d3czderUusd7LxRJLdPV1cW0adPaHeMdw0vpJalQFrgkFcoCl6RCWeCSVCgLXJIKZYFLUqEscEkqlAUuSYWywCWpUBa4JBXKApekQlngklQoC1ySCmWBS1KhLHBJKpQFLkmFssAlqVAWuCQVygKXpEJZ4JJUKAtckgpVs8Aj4n0RsSoifh4RT0bE56rLF0XECxGxpvpzZvPjSpK227uOMVuBL2TmoxExEVgdEfdU112XmUubF0+SNJKaBZ6ZG4GN1ce/iYgB4JBmB5Mk7d6ojoFHRC8wE3iouuiyiFgbETdHxKQRtpkfEf0R0T84ODiusJKk36u7wCNiP+AHwILM/DXwLeAIYAaVGfpXh9suM5dlZl9m9vX09DQgsiQJ6izwiOiiUt4rM/MOgMzclJlvZeY24EZgVvNiSpJ2Vc9ZKAHcBAxk5teGLJ8yZNg5wLrGx5MkjaSes1BmAxcAT0TEmuqyK4HzI2IGkMB64NKmJJQkDaues1DuB2KYVT9ufBxJUr28ElOSCmWBS1KhLHBJKpQFLkmFssAlqVD1nEaoNupd+KNRb7N+8VlNSCKp0zgDl6RCWeCSVCgLXJIKZYFLUqEscEkqlAUuSYWywCWpUBa4JBXKApekQlngklQoC1ySCmWBS1KhLHBJKpQFLkmFssAlqVA17wceEe8DvgMcDCSwLDO/HhHvBm4FeoH1wHmZ+Urzoo7Bov1HOf7V5uRotT31zy3tYeqZgW8FvpCZxwEfBP4iIo4DFgL3ZuZRwL3V55KkFqlZ4Jm5MTMfrT7+DTAAHAKcDSyvDlsOfKxZISVJbzeqY+AR0QvMBB4CDs7MjdVVL1I5xCJJapG6Czwi9gN+ACzIzF8PXZeZSeX4+HDbzY+I/ojoHxwcHFdYSdLv1VXgEdFFpbxXZuYd1cWbImJKdf0UYPNw22bmsszsy8y+np6eRmSWJFFHgUdEADcBA5n5tSGr7gIurD6+EPhh4+NJkkZS8zRCYDZwAfBERKypLrsSWAzcFhEXA78AzmtOREnScGoWeGbeD8QIq09rbBxJUr28ElOSCmWBS1KhLHBJKpQFLkmFssAlqVAWuCQVygKXpEJZ4JJUKAtckgplgUtSoSxwSSqUBS5JhbLAJalQFrgkFcoCl6RCWeCSVCgLXJIKZYFLUqEscEkqlAUuSYWywCWpUBa4JBWqZoFHxM0RsTki1g1ZtigiXoiINdWfM5sbU5K0q3pm4LcA84ZZfl1mzqj+/LixsSRJtdQs8Mz8KfByC7JIkkZhPMfAL4uItdVDLJMalkiSVJexFvi3gCOAGcBG4KsjDYyI+RHRHxH9g4ODY3w7SdKuxlTgmbkpM9/KzG3AjcCs3Yxdlpl9mdnX09Mz1pySpF2MqcAjYsqQp+cA60YaK0lqjr1rDYiI7wNzgMkRsQG4GpgTETOABNYDlzYxoyRpGDULPDPPH2bxTU3IIkkaBa/ElKRCWeCSVKiah1CkUVm0/yjHv9qcHNIewAJXR+pd+KNRb7N+8VlNSCJ1Lg+hSFKhLHBJKpQFLkmFssAlqVAWuCQVygKXpEJZ4JJUKAtckgplgUtSoSxwSSqUBS5JhbLAJalQFrgkFcoCl6RCWeCSVCgLXJIKZYFLUqEscEkqVM2vVIuIm4H/CGzOzPdXl70buBXoBdYD52XmK82L2V5j+nqv7iYEkaQh6vlOzFuAvwS+M2TZQuDezFwcEQurz69ofDxpFPxCZe1hah5CycyfAi/vsvhsYHn18XLgYw3OJUmqYazHwA/OzI3Vxy8CB480MCLmR0R/RPQPDg6O8e0kSbsa94eYmZlA7mb9sszsy8y+np6e8b6dJKlqrAW+KSKmAFT/ublxkSRJ9Rhrgd8FXFh9fCHww8bEkSTVq2aBR8T3gQeAoyNiQ0RcDCwGTo+IfwE+XH0uSWqhmqcRZub5I6w6rcFZJEmj4JWYklQoC1ySCmWBS1KhLHBJKlQ990KR9ihjunnZ4rOakETaPWfgklQoC1ySCmWBS1KhLHBJKpQFLkmFssAlqVDFnEbo91JK0s6cgUtSoSxwSSqUBS5JhbLAJalQFrgkFcoCl6RCWeCSVCgLXJIKZYFLUqEscEkq1LgupY+I9cBvgLeArZnZ14hQkqTaGnEvlD/OzJca8DqSpFHwEIokFWq8BZ7ATyJidUTMH25ARMyPiP6I6B8cHBzn20mSthtvgZ+amScCHwH+IiL+aNcBmbksM/sys6+np2ecbydJ2m5cBZ6ZL1T/uRm4E5jViFCSpNrGXOAR8W8iYuL2x8AZwLpGBZMk7d54zkI5GLgzIra/zvcy8+8bkkqSVNOYCzwznwWmNzCLJGkUPI1QkgplgUtSoSxwSSqUBS5JhWrEvVAktdOi/Uc5/tXm5FDLOQOXpEJZ4JJUKAtckgplgUtSoSxwSSqUBS5JhbLAJalQFrgkFcoCl6RCWeCSVCgLXJIKZYFLUqEscEkqlHcjlDpM78IfjWr8+u4mBWmxUf+5F5/1jnjv8XAGLkmFssAlqVDjKvCImBcR/xwRT0fEwkaFkiTVNuYCj4gJwA3AR4DjgPMj4rhGBZMk7d54ZuCzgKcz89nMfBP4a+DsxsSSJNUyngI/BPjlkOcbqsskSS0QmTm2DSPOBeZl5iXV5xcAJ2fmZbuMmw/Mrz49GvjnscfdrcnAS0167fHq5GzQ2fk6ORt0dr5Ozgadna/Tsh2WmT27LhzPeeAvAO8b8nxqddlOMnMZsGwc71OXiOjPzL5mv89YdHI26Ox8nZwNOjtfJ2eDzs7XydmGGs8hlEeAoyJiWkTsA3wSuKsxsSRJtYx5Bp6ZWyPiMuB/AxOAmzPzyYYlkyTt1rgupc/MHwM/blCW8Wr6YZpx6ORs0Nn5OjkbdHa+Ts4GnZ2vk7PtMOYPMSVJ7eWl9JJUqKIKvNal+xFxaESsiojHImJtRJzZwmw3R8TmiFg3wvqIiG9Us6+NiBNbla3OfP+pmuuJiPiniJjeKdmGjDspIrZWT2FtmXryRcSciFgTEU9GxD92SraI2D8i7o6Ix6vZ/rxV2arv/77qPvnz6vt/bpgxbdk36szWtv2iLplZxA+VD0qfAQ4H9gEeB47bZcwy4DPVx8cB61uY74+AE4F1I6w/E/g7IIAPAg+1+PdXK9+/ByZVH3+klflqZRvy7/8fqHzmcm6H/e4OAH4OHFp9flAHZbsSWFJ93AO8DOzTwnxTgBOrjycCTw2z37Zl36gzW9v2i3p+SpqB13PpfgL/tvp4f+D/tipcZv6Uys4xkrOB72TFg8ABETGlNelq58vMf8rMV6pPH6RyXn9L1PG7A7gc+AGwufmJdlZHvj8B7sjM56vjW5axjmwJTIyIAParjt3aimwAmbkxMx+tPv4NMMDbr9huy75RT7Z27hf1KKnA67l0fxHwpxGxgcpM7fLWRKtLSbceuJjKjKgjRMQhwDnAt9qdZQR/CEyKiPsiYnVE/Fm7Aw3xl8CxVCYzTwCfy8xt7QgSEb3ATOChXVa1fd/YTbahOmq/gHfeN/KcD9ySmV+NiFOAFRHx/nb9B1uiiPhjKv+hntruLENcD1yRmdsqE8mOszfwAeA04F3AAxHxYGY+1d5YAPwHYA0wFzgCuCci/k9m/rqVISJiPyp/g1rQ6veupZ5sHbpfFFXg9Vy6fzEwDyAzH4iIbir3NGj5X7uHUdetB9opIv4d8FfARzJzS7vzDNEH/HW1vCcDZ0bE1sz8m/bG2mEDsCUz/xX414j4KTCdyjHVdvtzYHFWDuI+HRHPAccAD7cqQER0USnIlZl5xzBD2rZv1JGtk/eLog6h1HPp/vNUZkFExLFANzDY0pQjuwv4s+on7h8EXs3Mje0OtV1EHArcAVzQITPHHTJzWmb2ZmYvcDvw2Q4qb4AfAqdGxN4R8QfAyVSOp3aCofvEwVRuKPdsq968euz9JmAgM782wrC27Bv1ZOvk/QIKmoHnCJfuR8R/A/oz8y7gC8CNEfGfqXx4c1F15tF0EfF9YA4wuXoM/mqgq5r921SOyZ8JPA38Pyozo5apI99/BQ4E/md1prs1W3QznzqytVWtfJk5EBF/D6wFtgF/lZm7PSWyVdmA/w7cEhFPUDnL44rMbOVd9mYDFwBPRMSa6rIrgUOHZGzXvlFPtrbtF/XwSkxJKlRJh1AkSUNY4JJUKAtckgplgUtSoSxwSSqUBS5JhbLAJalQFrgkFer/A2GFpafCpPRtAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nGDbBEeiUij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "282c68e4-d0ba-4fee-ae34-8a0ea084aac0"
      },
      "source": [
        "# plt.hist(x, bins=bins, density=True, histtype='step', cumulative=-1,label='Reversed emp.')\n",
        "plt.hist(X, density=True, histtype='step', cumulative=True,label='Reversed emp.')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.07368421, 0.17894737, 0.41052632, 0.74736842, 0.88421053,\n",
              "         0.96842105, 0.97894737, 0.97894737, 0.98947368, 1.        ],\n",
              "        [0.20689655, 0.51724138, 0.72413793, 0.86206897, 0.96551724,\n",
              "         0.96551724, 1.        , 1.        , 1.        , 1.        ]]),\n",
              " array([0.74706594, 0.90410327, 1.06114059, 1.21817792, 1.37521524,\n",
              "        1.53225257, 1.68928989, 1.84632722, 2.00336454, 2.16040187,\n",
              "        2.31743919]),\n",
              " <a list of 2 Lists of Patches objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "9xENlBUUxfTu",
        "outputId": "0bdd028b-2142-47fc-cb9b-cc74c954f104"
      },
      "source": [
        "Obj = plt.hist(X, density=True, histtype='step', cumulative=True,label='Reversed emp.')\n",
        "Y1, Y2 = Obj[0]\n",
        "Rsquared = r2_score(Y1, Y2)\n",
        "print('r_squared =',Rsquared)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r_squared = 0.7871625525064307\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2XboMiFbkaa"
      },
      "source": [
        "acc_train = r.history['accuracy'][-1]\n",
        "acc_test = r.history['val_accuracy'][-1]\n",
        "loss_train = r.history['loss'][-1]\n",
        "loss_test = r.history['val_loss'][-1]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euTd_-CYN1v0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f78d3774-e997-4f05-e891-4a879d9ba1ad"
      },
      "source": [
        "df = pd.DataFrame({'N1':N1, 'N2':N2,'R^2':Rsquared,\n",
        "                   'acc train':acc_train,'acc test':acc_test,\n",
        "                   'loss train':loss_train,'loss test':loss_test,\n",
        "                   'Details':Description},\n",
        "                  index= [0])\n",
        "Arq = \"output.xlsx\"\n",
        "df.to_excel(Arq)\n",
        "files.download(Arq)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_b8d52a59-e478-48e5-be1b-ffbf46ff8374\", \"output.xlsx\", 5153)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "-KukfpGTTKlj",
        "outputId": "46ac3b5d-f7a7-4b5a-e9e2-473ad57ea340"
      },
      "source": [
        "df"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>N1</th>\n",
              "      <th>N2</th>\n",
              "      <th>R^2</th>\n",
              "      <th>acc train</th>\n",
              "      <th>acc test</th>\n",
              "      <th>loss train</th>\n",
              "      <th>loss test</th>\n",
              "      <th>Details</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>200</td>\n",
              "      <td>10</td>\n",
              "      <td>0.787163</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.92517</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.39081</td>\n",
              "      <td>3 layers of Convolution: 32, 64, 128</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    N1  N2  ...  loss test                                Details\n",
              "0  200  10  ...    0.39081  3 layers of Convolution: 32, 64, 128 \n",
              "\n",
              "[1 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "ZZHa1j4HT9Dq",
        "outputId": "36dff135-a46a-420d-f91f-a2aebd7f954b"
      },
      "source": [
        "counts, bins, bars = plt.hist(X,weights=wts)\n",
        "print(bars)\n",
        "print(bins)\n",
        "print(counts)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<a list of 2 Lists of Patches objects>\n",
            "[0.74706594 0.90410327 1.06114059 1.21817792 1.37521524 1.53225257\n",
            " 1.68928989 1.84632722 2.00336454 2.16040187 2.31743919]\n",
            "[[ 7.36842105 10.52631579 23.15789474 33.68421053 13.68421053  8.42105263\n",
            "   1.05263158  0.          1.05263158  1.05263158]\n",
            " [20.68965517 31.03448276 20.68965517 13.79310345 10.34482759  0.\n",
            "   3.44827586  0.          0.          0.        ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o_vDGeWUwIZ",
        "outputId": "edb1b971-b220-4e58-da51-b7eadc4717dc"
      },
      "source": [
        "print(counts.sum())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200.0000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "KcH52-6iJQ8t",
        "outputId": "5802e5b1-01ef-41f2-95c9-f7395ffa45a4"
      },
      "source": [
        "\n",
        "plt.hist([Diam1,Diameter_All])\n",
        "plt.legend(['Image J','CNN'])\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7febbdbd1210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r11AxFK_JIii",
        "outputId": "82809be8-628b-4914-9290-8d54dfea33bf"
      },
      "source": [
        "[Diam1,Diameter_All]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.59616801403081,\n",
              "  1.0217907939900581,\n",
              "  1.2716187407449044,\n",
              "  1.104429030701514,\n",
              "  1.2163487785097904,\n",
              "  1.6013445735058454,\n",
              "  1.1715597420637607,\n",
              "  1.2534662333717612,\n",
              "  1.2676073151634049,\n",
              "  1.309600575274104,\n",
              "  1.292966945531582,\n",
              "  1.7658322811231006,\n",
              "  1.3564037533648712,\n",
              "  1.2407040781688483,\n",
              "  2.130217298173151,\n",
              "  1.4228319915327,\n",
              "  1.0651086490865755,\n",
              "  1.3008210311003705,\n",
              "  1.336545951796433,\n",
              "  0.8927754224911278,\n",
              "  1.4494292838262302,\n",
              "  1.4052738287907582,\n",
              "  1.6421697097891788,\n",
              "  1.2329833804288621,\n",
              "  1.19042665178928,\n",
              "  1.1682948223612457,\n",
              "  1.1518314137121108,\n",
              "  0.9607802401865855,\n",
              "  2.317439190074449,\n",
              "  1.0591147430338594,\n",
              "  1.4308630919602832,\n",
              "  0.7535680705496237,\n",
              "  0.8608283307581511,\n",
              "  1.2776122636975893,\n",
              "  1.3745862957220916,\n",
              "  1.259546137598783,\n",
              "  1.2978813187979172,\n",
              "  1.2412170838050638,\n",
              "  1.6009469708743893,\n",
              "  1.3149369953539032,\n",
              "  1.417901703622935,\n",
              "  1.2478669653497139,\n",
              "  1.1055812783082735,\n",
              "  0.9561307405997607,\n",
              "  0.9487783503683882,\n",
              "  1.1238565871041026,\n",
              "  1.2058356273089446,\n",
              "  1.2801012827406097,\n",
              "  0.8733100751144249,\n",
              "  0.9194732501297403,\n",
              "  1.6425573339441792,\n",
              "  1.085826790250066,\n",
              "  1.0639125693728595,\n",
              "  1.0875842666474016,\n",
              "  1.417901703622935,\n",
              "  1.550443891425932,\n",
              "  0.7825779328716171,\n",
              "  1.4690612745308145,\n",
              "  1.053086721720641,\n",
              "  1.2676073151634049,\n",
              "  0.7744003006005755,\n",
              "  1.3787482149724068,\n",
              "  1.363892581861956,\n",
              "  1.299352006316543,\n",
              "  1.2870449283923413,\n",
              "  1.11817763925502,\n",
              "  0.9474354220939228,\n",
              "  1.5218484589055707,\n",
              "  1.3526437911676632,\n",
              "  1.1556938532445284,\n",
              "  1.6013445735058454,\n",
              "  1.274619025074578,\n",
              "  1.422384489715834,\n",
              "  1.3408259533459403,\n",
              "  1.172646028567008,\n",
              "  1.1490645795125545,\n",
              "  1.459060149136146,\n",
              "  1.2483770274864237,\n",
              "  1.336545951796433,\n",
              "  0.9601174044814821,\n",
              "  1.4867225193896279,\n",
              "  1.4277452542806772,\n",
              "  1.35028849808504,\n",
              "  0.7560982446653928,\n",
              "  1.259040600296622,\n",
              "  1.13456827900627,\n",
              "  1.6549133695530214,\n",
              "  1.1204526724091788,\n",
              "  1.1176081573544434,\n",
              "  0.9153095762832032,\n",
              "  1.1639273497938836,\n",
              "  1.3066806149514323,\n",
              "  1.1529362882239027,\n",
              "  1.3047303442899274,\n",
              "  1.3066806149514323],\n",
              " [0.9274866865263769,\n",
              "  1.477544755879403,\n",
              "  0.9067533899788442,\n",
              "  0.8176515961023421,\n",
              "  1.0618074752209081,\n",
              "  0.9733319076362458,\n",
              "  1.0127303772822145,\n",
              "  1.117441602613488,\n",
              "  1.3209438876874395,\n",
              "  1.3244585671791516,\n",
              "  1.3203228350753233,\n",
              "  0.7470659425058697,\n",
              "  1.3547364680519196,\n",
              "  1.4364851440556485,\n",
              "  1.7003632853695085,\n",
              "  0.7975317067519331,\n",
              "  1.0793346772936472,\n",
              "  1.1737760234866423,\n",
              "  0.927392204490045,\n",
              "  0.9431348969245509,\n",
              "  1.3945212923063488,\n",
              "  1.0537761465669933,\n",
              "  0.8281340542567222,\n",
              "  0.885447598483792,\n",
              "  1.1694009779493897,\n",
              "  0.9285740068983243,\n",
              "  1.1548386386364586,\n",
              "  0.9167987265357388,\n",
              "  0.8538450542411343]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}