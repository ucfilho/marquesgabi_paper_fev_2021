{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_CNN_B_r_squared_jun_23_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/Qualificacao/Comparative_ANNs/PSD_histogram_CNN_B_r_squared_jun_23_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from sklearn.metrics import r2_score\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZEvJvfoibE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c7cb250-b8fb-4ad9-c11b-21ea1f69cdde"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mahotas in /usr/local/lib/python3.7/dist-packages (1.4.11)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT"
      },
      "source": [
        "import mahotas.features.texture as mht\n",
        "import mahotas.features"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VcTdaNVh9EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8321235a-e3e2-4d22-9ab9-3b28eba1d578"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'marquesgabi_fev_2020' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v7SRrc8mH2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "364e952f-bd91-4503-9188-24c3459e5a81"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip'\n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Doutorado' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[4] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHgqAnaFyCjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d29448f-775b-4b67-ca28-06b45ba1d3a5"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'MarquesGabi_Routines' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from segment_filter_not_conclude import Segmenta  # got image provided segmented"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5MN5a_v4np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d98638-26a2-4139-8e4e-9d676eba8d98"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     139  126.105331  122.764290  ...  154.385895  156.548416  155.888611\n",
            "1     111  139.434143  143.914444  ...    1.081649    1.993264    1.061359\n",
            "2     139    1.039128    0.046323  ...   51.998341   59.529060   62.908493\n",
            "3     150  149.484970  149.852615  ...    0.797333    0.154844    1.221511\n",
            "4     125  134.426880  145.701004  ...    1.066176    1.906880    1.023296\n",
            "5     123  146.642349  146.999405  ...  197.372864  201.677444  200.553925\n",
            "6     129  124.999512  115.061646  ...  176.193375  174.143326  173.152023\n",
            "7     102  107.123810  113.821999  ...  117.173409  101.717819   85.078827\n",
            "8     103  160.928360  137.314819  ...  131.743332  132.550369  133.046936\n",
            "9     100  105.643204  106.697594  ...    0.067200    0.971200    1.638400\n",
            "10    167  118.047157  122.942703  ...  115.248451  114.527008  111.432831\n",
            "11    168  142.833328  167.305557  ...  124.805557  118.944443  118.027779\n",
            "12    144   96.564049   92.886574  ...  104.193672  128.679779  133.043991\n",
            "13    197    1.329743    1.312325  ...   90.839371   82.041695   75.088776\n",
            "14    117  111.271614  112.611084  ...    0.030682    0.732486    1.535905\n",
            "15    126  100.283958  106.160492  ...    0.160494    0.604938    1.493827\n",
            "16    183  129.859741  140.642929  ...  156.074203  149.287766  130.800156\n",
            "17    154  135.487610  131.917374  ...   77.528931  119.388435  157.983475\n",
            "18    164  146.486023  149.803696  ...    1.000000    1.000000    1.000000\n",
            "19    135  139.013046  150.323883  ...  109.399826  142.432373  166.989731\n",
            "20    167  175.951813  172.547897  ...    1.345477    0.184804    0.893542\n",
            "21    147  123.975067  126.990936  ...  110.700684  136.521545  149.365082\n",
            "22    101   85.234390   57.975105  ...  150.634949  148.362427  146.432816\n",
            "23    193    1.095224    0.305673  ...   31.143385   20.081720   12.972159\n",
            "24    194    0.751514    2.025401  ...  149.735443  153.380676  148.187592\n",
            "25    133  137.961212  135.728531  ...  143.421051  143.432144  146.623276\n",
            "26    158  126.202370   75.740585  ...    0.178177    1.292101    1.519949\n",
            "27    126  106.074081  105.950615  ...  119.320984  116.061729  112.407410\n",
            "28    189  159.702332  158.572021  ...  196.685883  177.083694  171.519882\n",
            "29    161   99.510391   90.720230  ...  201.451782  221.644638  234.741013\n",
            "30    148  159.454361  163.141739  ...  165.933548  168.766983  169.515732\n",
            "31    139   80.094971   75.727699  ...  140.615280  144.392303  146.860718\n",
            "32    102   87.408707   72.685898  ...    1.857747    1.021530    0.251826\n",
            "33    130  164.649948  164.273605  ...  190.524048  163.764023  164.504852\n",
            "34    104    1.051775    0.397929  ...   41.437874   26.872784   10.829883\n",
            "35    172  173.128738  165.589508  ...  217.843170  216.752304  176.790161\n",
            "36    137  237.085190  244.973190  ...    0.867281    1.226277    1.261548\n",
            "37    118  164.436066  163.901184  ...  180.903748  168.300201  150.337524\n",
            "38    198  119.682877  106.751953  ...    1.474543    0.200184    1.294153\n",
            "39    161  164.432907  166.534973  ...   80.455582   86.550095   79.317581\n",
            "40    122    1.924751    1.012093  ...    0.069067    0.659231    1.511690\n",
            "41    113  159.917145  207.356567  ...  150.325546  131.098114  136.547882\n",
            "42    128  123.604492  126.221680  ...  127.413086  120.180664   73.167969\n",
            "43    196  223.081635  217.836731  ...  161.897949  170.408157  174.102036\n",
            "44    150  155.671844  100.636795  ...    1.000000    1.000000    1.000000\n",
            "45    118  109.301346  129.748337  ...    1.152542    1.000000    1.000000\n",
            "46    144  253.119598  254.000000  ...  184.037048  175.689041  167.464508\n",
            "47    144  156.995392  143.011581  ...  125.679779  103.488434  105.783188\n",
            "48    126  202.654327  210.950623  ...  172.037048  169.209885  165.259262\n",
            "49    187  168.198090  171.315292  ...  145.649918  164.310989  174.091080\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzpQ1Pz0fX5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "4b9dadd7-9b87-4426-f3dd-1a95854556bb"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines\n",
        "# filename = 'model_ANN.pkl'\n",
        "filename = 'model_ANN_new.pkl'\n",
        "model = joblib.load(filename)\n",
        "'''"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n!git clone https://github.com/ucfilho/MarquesGabi_Routines\\n%cd MarquesGabi_Routines\\n# filename = 'model_ANN.pkl'\\nfilename = 'model_ANN_new.pkl'\\nmodel = joblib.load(filename)\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2emP4rNjQy",
        "outputId": "e64b902c-b582-495d-f6ae-9ddb36c16a20"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'MarquesGabi_Routines' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "Description = '3 layers of Convolution: 32, 64, 128 '\n",
        "N1 = 200\n",
        "N2 = 10\n",
        "\n",
        "# make the CNN\n",
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=32, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=N1))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=N2))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbPQ1FSRG6A",
        "outputId": "a0f43213-0d1f-4f27-9869-1666544e38a3"
      },
      "source": [
        "\n",
        "# training the model\n",
        "r = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), \n",
        "              epochs=200, batch_size=32)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 2s 77ms/step - loss: 0.6616 - accuracy: 0.6356 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.4035 - accuracy: 0.8163 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.2579 - accuracy: 0.8892 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.1489 - accuracy: 0.9388 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0762 - accuracy: 0.9767 - val_loss: 0.6928 - val_accuracy: 0.7347\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0473 - accuracy: 0.9767 - val_loss: 0.6930 - val_accuracy: 0.4898\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0316 - accuracy: 0.9913 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0175 - accuracy: 0.9971 - val_loss: 0.6930 - val_accuracy: 0.4898\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0195 - accuracy: 0.9942 - val_loss: 0.6929 - val_accuracy: 0.4898\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0201 - accuracy: 0.9942 - val_loss: 0.6928 - val_accuracy: 0.4898\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.0205 - accuracy: 0.9942 - val_loss: 0.6923 - val_accuracy: 0.4898\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.6928 - val_accuracy: 0.4898\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.0219 - accuracy: 0.9913 - val_loss: 0.6943 - val_accuracy: 0.4898\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0089 - accuracy: 0.9971 - val_loss: 0.6920 - val_accuracy: 0.4898\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.0113 - accuracy: 0.9942 - val_loss: 0.6912 - val_accuracy: 0.4966\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.0153 - accuracy: 0.9971 - val_loss: 0.6917 - val_accuracy: 0.4898\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0275 - accuracy: 0.9854 - val_loss: 0.6920 - val_accuracy: 0.4898\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0091 - accuracy: 0.9971 - val_loss: 0.6909 - val_accuracy: 0.5102\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 0.0109 - accuracy: 0.9942 - val_loss: 0.6914 - val_accuracy: 0.6871\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.0467 - accuracy: 0.9913 - val_loss: 0.7024 - val_accuracy: 0.5102\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0394 - accuracy: 0.9883 - val_loss: 0.6950 - val_accuracy: 0.5102\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0334 - accuracy: 0.9883 - val_loss: 0.7078 - val_accuracy: 0.5102\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0160 - accuracy: 0.9913 - val_loss: 0.7119 - val_accuracy: 0.5102\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0310 - accuracy: 0.9825 - val_loss: 0.7246 - val_accuracy: 0.5102\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0120 - accuracy: 0.9971 - val_loss: 0.7356 - val_accuracy: 0.5102\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0221 - accuracy: 0.9942 - val_loss: 0.7065 - val_accuracy: 0.5102\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.0519 - accuracy: 0.9854 - val_loss: 0.7451 - val_accuracy: 0.5102\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0413 - accuracy: 0.9796 - val_loss: 0.7474 - val_accuracy: 0.5102\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.0222 - accuracy: 0.9942 - val_loss: 0.8649 - val_accuracy: 0.5102\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0064 - accuracy: 0.9971 - val_loss: 1.1305 - val_accuracy: 0.5102\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 1.2872 - val_accuracy: 0.5102\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.3479 - val_accuracy: 0.5102\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.0032 - accuracy: 0.9971 - val_loss: 1.5281 - val_accuracy: 0.5102\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 8.8603e-04 - accuracy: 1.0000 - val_loss: 1.5940 - val_accuracy: 0.5102\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 1.6806 - val_accuracy: 0.5102\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.0099 - accuracy: 0.9942 - val_loss: 1.2346 - val_accuracy: 0.5102\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 1s 56ms/step - loss: 0.0031 - accuracy: 0.9971 - val_loss: 0.7307 - val_accuracy: 0.5102\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.0053 - accuracy: 0.9971 - val_loss: 0.6504 - val_accuracy: 0.5238\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.7313 - val_accuracy: 0.5102\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.8844 - val_accuracy: 0.5102\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 3.7824e-04 - accuracy: 1.0000 - val_loss: 0.9686 - val_accuracy: 0.5102\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 1.0925 - val_accuracy: 0.5102\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 7.2659e-04 - accuracy: 1.0000 - val_loss: 1.1908 - val_accuracy: 0.5102\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 5.0281e-04 - accuracy: 1.0000 - val_loss: 1.2657 - val_accuracy: 0.5102\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 3.9533e-04 - accuracy: 1.0000 - val_loss: 1.3262 - val_accuracy: 0.5102\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 4.9301e-04 - accuracy: 1.0000 - val_loss: 1.3676 - val_accuracy: 0.5102\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 2.8637e-04 - accuracy: 1.0000 - val_loss: 1.3943 - val_accuracy: 0.5102\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 4.1888e-04 - accuracy: 1.0000 - val_loss: 1.4477 - val_accuracy: 0.5102\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 2.8058e-04 - accuracy: 1.0000 - val_loss: 1.5086 - val_accuracy: 0.5102\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.6629e-04 - accuracy: 1.0000 - val_loss: 1.5166 - val_accuracy: 0.5102\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 2.7996e-04 - accuracy: 1.0000 - val_loss: 1.4910 - val_accuracy: 0.5102\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 3.5439e-04 - accuracy: 1.0000 - val_loss: 1.3469 - val_accuracy: 0.5170\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.8927e-04 - accuracy: 1.0000 - val_loss: 1.2549 - val_accuracy: 0.5238\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.6802e-04 - accuracy: 1.0000 - val_loss: 1.1857 - val_accuracy: 0.5238\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 4.8822e-04 - accuracy: 1.0000 - val_loss: 1.0967 - val_accuracy: 0.5374\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.6258e-04 - accuracy: 1.0000 - val_loss: 0.9021 - val_accuracy: 0.5986\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6671 - val_accuracy: 0.6735\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 3.4896e-04 - accuracy: 1.0000 - val_loss: 2.7907 - val_accuracy: 0.5102\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0039 - accuracy: 0.9971 - val_loss: 1.6557 - val_accuracy: 0.5442\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 3.5364e-04 - accuracy: 1.0000 - val_loss: 3.3678 - val_accuracy: 0.5102\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 2.9316 - val_accuracy: 0.5102\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 8.4085e-04 - accuracy: 1.0000 - val_loss: 5.0538 - val_accuracy: 0.5102\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 2.5362e-04 - accuracy: 1.0000 - val_loss: 4.5670 - val_accuracy: 0.5102\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 7.5278e-04 - accuracy: 1.0000 - val_loss: 3.8537 - val_accuracy: 0.5102\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 2.3924e-04 - accuracy: 1.0000 - val_loss: 5.2507 - val_accuracy: 0.5102\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 6.6941e-04 - accuracy: 1.0000 - val_loss: 5.3093 - val_accuracy: 0.5102\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.4103e-04 - accuracy: 1.0000 - val_loss: 4.5484 - val_accuracy: 0.5102\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 5.2211e-04 - accuracy: 1.0000 - val_loss: 3.6214 - val_accuracy: 0.5102\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.5106e-04 - accuracy: 1.0000 - val_loss: 2.6288 - val_accuracy: 0.5102\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 9.7152e-05 - accuracy: 1.0000 - val_loss: 1.9641 - val_accuracy: 0.5170\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 4.9192e-04 - accuracy: 1.0000 - val_loss: 1.2177 - val_accuracy: 0.5850\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.5842e-04 - accuracy: 1.0000 - val_loss: 0.6717 - val_accuracy: 0.7007\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 1.2943e-04 - accuracy: 1.0000 - val_loss: 0.5227 - val_accuracy: 0.7347\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.0207e-04 - accuracy: 1.0000 - val_loss: 0.3831 - val_accuracy: 0.8095\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.1445e-04 - accuracy: 1.0000 - val_loss: 0.2761 - val_accuracy: 0.8503\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.3763e-04 - accuracy: 1.0000 - val_loss: 0.2556 - val_accuracy: 0.8844\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.3035e-04 - accuracy: 1.0000 - val_loss: 0.3290 - val_accuracy: 0.8571\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.7588e-04 - accuracy: 1.0000 - val_loss: 0.2750 - val_accuracy: 0.8707\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 4.6059e-04 - accuracy: 1.0000 - val_loss: 0.4021 - val_accuracy: 0.8367\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 2.4660e-04 - accuracy: 1.0000 - val_loss: 0.4589 - val_accuracy: 0.8639\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 6.2731e-05 - accuracy: 1.0000 - val_loss: 0.6192 - val_accuracy: 0.8299\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.0022e-04 - accuracy: 1.0000 - val_loss: 0.6131 - val_accuracy: 0.8299\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 2.8720e-04 - accuracy: 1.0000 - val_loss: 0.3209 - val_accuracy: 0.8776\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 2.1530e-04 - accuracy: 1.0000 - val_loss: 1.0721 - val_accuracy: 0.7007\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 2.7691e-04 - accuracy: 1.0000 - val_loss: 0.4514 - val_accuracy: 0.8571\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.3609e-04 - accuracy: 1.0000 - val_loss: 0.2496 - val_accuracy: 0.8912\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 9.1428e-05 - accuracy: 1.0000 - val_loss: 0.2507 - val_accuracy: 0.8912\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 6.0647e-05 - accuracy: 1.0000 - val_loss: 0.2941 - val_accuracy: 0.8912\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 7.2171e-05 - accuracy: 1.0000 - val_loss: 0.3217 - val_accuracy: 0.8912\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 8.0263e-05 - accuracy: 1.0000 - val_loss: 0.3760 - val_accuracy: 0.8912\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 3.1437e-05 - accuracy: 1.0000 - val_loss: 0.3976 - val_accuracy: 0.8912\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.5941e-04 - accuracy: 1.0000 - val_loss: 0.2575 - val_accuracy: 0.9048\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.3149e-04 - accuracy: 1.0000 - val_loss: 0.1718 - val_accuracy: 0.9320\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.3144e-04 - accuracy: 1.0000 - val_loss: 0.1466 - val_accuracy: 0.9320\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 5.1352e-05 - accuracy: 1.0000 - val_loss: 0.1267 - val_accuracy: 0.9592\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 6.1591e-05 - accuracy: 1.0000 - val_loss: 0.1325 - val_accuracy: 0.9524\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 4.5652e-05 - accuracy: 1.0000 - val_loss: 0.1447 - val_accuracy: 0.9456\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.0364e-04 - accuracy: 1.0000 - val_loss: 0.1822 - val_accuracy: 0.9524\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 3.7038e-05 - accuracy: 1.0000 - val_loss: 0.1997 - val_accuracy: 0.9456\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 1.1097e-04 - accuracy: 1.0000 - val_loss: 0.1471 - val_accuracy: 0.9592\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 5.4364e-05 - accuracy: 1.0000 - val_loss: 0.2114 - val_accuracy: 0.9456\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 4.8381e-05 - accuracy: 1.0000 - val_loss: 0.2353 - val_accuracy: 0.9456\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.4719e-04 - accuracy: 1.0000 - val_loss: 0.2089 - val_accuracy: 0.9456\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 3.9952e-05 - accuracy: 1.0000 - val_loss: 0.1790 - val_accuracy: 0.9524\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 5.7733e-05 - accuracy: 1.0000 - val_loss: 0.1647 - val_accuracy: 0.9592\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 4.2049e-05 - accuracy: 1.0000 - val_loss: 0.1805 - val_accuracy: 0.9524\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 6.4940e-05 - accuracy: 1.0000 - val_loss: 0.1213 - val_accuracy: 0.9728\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 7.3301e-05 - accuracy: 1.0000 - val_loss: 0.1196 - val_accuracy: 0.9728\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.0053 - accuracy: 0.9971 - val_loss: 41.5955 - val_accuracy: 0.5102\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.0124 - accuracy: 0.9913 - val_loss: 91.7939 - val_accuracy: 0.5102\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 37.5431 - val_accuracy: 0.5102\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 4.4794e-04 - accuracy: 1.0000 - val_loss: 43.9022 - val_accuracy: 0.5102\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 3.2433e-04 - accuracy: 1.0000 - val_loss: 42.5742 - val_accuracy: 0.5102\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.9950e-04 - accuracy: 1.0000 - val_loss: 37.0419 - val_accuracy: 0.5102\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 9.7457e-05 - accuracy: 1.0000 - val_loss: 32.9354 - val_accuracy: 0.5102\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 2.9055e-04 - accuracy: 1.0000 - val_loss: 29.2853 - val_accuracy: 0.5102\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.1389e-04 - accuracy: 1.0000 - val_loss: 25.8269 - val_accuracy: 0.5102\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 7.6593e-05 - accuracy: 1.0000 - val_loss: 22.8901 - val_accuracy: 0.5102\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.9940e-04 - accuracy: 1.0000 - val_loss: 20.2813 - val_accuracy: 0.5102\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 9.5689e-05 - accuracy: 1.0000 - val_loss: 17.8087 - val_accuracy: 0.5102\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.3778e-04 - accuracy: 1.0000 - val_loss: 17.5081 - val_accuracy: 0.5102\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 1.7154e-04 - accuracy: 1.0000 - val_loss: 16.2427 - val_accuracy: 0.5102\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 2.4924e-04 - accuracy: 1.0000 - val_loss: 15.0488 - val_accuracy: 0.5102\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 5.4611e-05 - accuracy: 1.0000 - val_loss: 15.4152 - val_accuracy: 0.5102\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 2.2413e-04 - accuracy: 1.0000 - val_loss: 14.3636 - val_accuracy: 0.5102\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 5.4670e-05 - accuracy: 1.0000 - val_loss: 15.4000 - val_accuracy: 0.5102\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 3.4830e-05 - accuracy: 1.0000 - val_loss: 13.9671 - val_accuracy: 0.5102\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 7.0588e-05 - accuracy: 1.0000 - val_loss: 11.6327 - val_accuracy: 0.5102\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 3.0387e-05 - accuracy: 1.0000 - val_loss: 8.6013 - val_accuracy: 0.5102\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 6.3272e-05 - accuracy: 1.0000 - val_loss: 6.5039 - val_accuracy: 0.5238\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 3.7575e-05 - accuracy: 1.0000 - val_loss: 4.6096 - val_accuracy: 0.5374\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 3.3755e-05 - accuracy: 1.0000 - val_loss: 3.0979 - val_accuracy: 0.6259\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 3.0860e-05 - accuracy: 1.0000 - val_loss: 1.9629 - val_accuracy: 0.7279\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.7499e-05 - accuracy: 1.0000 - val_loss: 1.1465 - val_accuracy: 0.7755\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.8808e-05 - accuracy: 1.0000 - val_loss: 0.6566 - val_accuracy: 0.8571\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 4.3411e-05 - accuracy: 1.0000 - val_loss: 0.4210 - val_accuracy: 0.8980\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 6.6153e-05 - accuracy: 1.0000 - val_loss: 0.3350 - val_accuracy: 0.9048\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 2.6360e-05 - accuracy: 1.0000 - val_loss: 0.3103 - val_accuracy: 0.9048\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 3.2729e-05 - accuracy: 1.0000 - val_loss: 0.2617 - val_accuracy: 0.9048\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 8.9764e-04 - accuracy: 1.0000 - val_loss: 2.3117 - val_accuracy: 0.7211\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 4.5477e-04 - accuracy: 1.0000 - val_loss: 6.0196 - val_accuracy: 0.5442\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 3.9155e-05 - accuracy: 1.0000 - val_loss: 7.8255 - val_accuracy: 0.5306\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 9.4062e-05 - accuracy: 1.0000 - val_loss: 7.4238 - val_accuracy: 0.5306\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 8.2425e-05 - accuracy: 1.0000 - val_loss: 6.0578 - val_accuracy: 0.5442\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 2.8949e-05 - accuracy: 1.0000 - val_loss: 4.5837 - val_accuracy: 0.5986\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 2.7113e-05 - accuracy: 1.0000 - val_loss: 3.4516 - val_accuracy: 0.6531\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 3.2023e-05 - accuracy: 1.0000 - val_loss: 2.6564 - val_accuracy: 0.6939\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 8.4637e-05 - accuracy: 1.0000 - val_loss: 2.3502 - val_accuracy: 0.7279\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 2.3005e-04 - accuracy: 1.0000 - val_loss: 4.5221 - val_accuracy: 0.5510\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 7.1776e-05 - accuracy: 1.0000 - val_loss: 5.3897 - val_accuracy: 0.5306\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 3.5332e-05 - accuracy: 1.0000 - val_loss: 4.9717 - val_accuracy: 0.5374\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.3137e-05 - accuracy: 1.0000 - val_loss: 4.2584 - val_accuracy: 0.5510\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 1.4679e-05 - accuracy: 1.0000 - val_loss: 3.5259 - val_accuracy: 0.5986\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.6403e-05 - accuracy: 1.0000 - val_loss: 2.7989 - val_accuracy: 0.6259\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.2587e-05 - accuracy: 1.0000 - val_loss: 2.1893 - val_accuracy: 0.6939\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 2.1892e-05 - accuracy: 1.0000 - val_loss: 1.7576 - val_accuracy: 0.7619\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 2.1570e-05 - accuracy: 1.0000 - val_loss: 1.4821 - val_accuracy: 0.7687\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.3098e-05 - accuracy: 1.0000 - val_loss: 1.2448 - val_accuracy: 0.8027\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 8.6703e-04 - accuracy: 1.0000 - val_loss: 14.9197 - val_accuracy: 0.5102\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 3.1223e-04 - accuracy: 1.0000 - val_loss: 23.2618 - val_accuracy: 0.5102\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 8.7051e-05 - accuracy: 1.0000 - val_loss: 21.3338 - val_accuracy: 0.5102\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.7909e-04 - accuracy: 1.0000 - val_loss: 17.2485 - val_accuracy: 0.5102\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 8.6643e-05 - accuracy: 1.0000 - val_loss: 17.7650 - val_accuracy: 0.5102\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 3.7678e-05 - accuracy: 1.0000 - val_loss: 15.8011 - val_accuracy: 0.5102\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.5377e-05 - accuracy: 1.0000 - val_loss: 13.2010 - val_accuracy: 0.5102\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 2.7862e-05 - accuracy: 1.0000 - val_loss: 10.9358 - val_accuracy: 0.5102\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.3477e-05 - accuracy: 1.0000 - val_loss: 9.0745 - val_accuracy: 0.5170\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 1.2934e-05 - accuracy: 1.0000 - val_loss: 7.8491 - val_accuracy: 0.5170\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.2216e-05 - accuracy: 1.0000 - val_loss: 6.7108 - val_accuracy: 0.5170\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 5.8048e-06 - accuracy: 1.0000 - val_loss: 5.6523 - val_accuracy: 0.5306\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 1.0534e-05 - accuracy: 1.0000 - val_loss: 4.6623 - val_accuracy: 0.5850\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 2.1040e-05 - accuracy: 1.0000 - val_loss: 3.7920 - val_accuracy: 0.6259\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 8.6397e-06 - accuracy: 1.0000 - val_loss: 2.8781 - val_accuracy: 0.6803\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 2.8519e-05 - accuracy: 1.0000 - val_loss: 2.0661 - val_accuracy: 0.7415\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.9577e-05 - accuracy: 1.0000 - val_loss: 1.5295 - val_accuracy: 0.7687\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 2.1031e-05 - accuracy: 1.0000 - val_loss: 1.0314 - val_accuracy: 0.7959\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 2.5906e-05 - accuracy: 1.0000 - val_loss: 0.7458 - val_accuracy: 0.8707\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.7452e-05 - accuracy: 1.0000 - val_loss: 0.6719 - val_accuracy: 0.8844\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 1.9774e-04 - accuracy: 1.0000 - val_loss: 0.8909 - val_accuracy: 0.7959\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 1.0278e-05 - accuracy: 1.0000 - val_loss: 1.8872 - val_accuracy: 0.6735\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 5.7485e-05 - accuracy: 1.0000 - val_loss: 2.2778 - val_accuracy: 0.6395\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 7.3061e-06 - accuracy: 1.0000 - val_loss: 2.3372 - val_accuracy: 0.6395\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 2.5358e-05 - accuracy: 1.0000 - val_loss: 2.1367 - val_accuracy: 0.6531\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 7.5723e-05 - accuracy: 1.0000 - val_loss: 2.2786 - val_accuracy: 0.6803\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 1s 54ms/step - loss: 3.4565e-05 - accuracy: 1.0000 - val_loss: 1.8307 - val_accuracy: 0.7007\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 2.2482e-05 - accuracy: 1.0000 - val_loss: 1.5405 - val_accuracy: 0.7551\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 2.2941e-05 - accuracy: 1.0000 - val_loss: 0.9586 - val_accuracy: 0.7959\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.1770e-05 - accuracy: 1.0000 - val_loss: 0.6331 - val_accuracy: 0.8503\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 7.6581e-06 - accuracy: 1.0000 - val_loss: 0.4750 - val_accuracy: 0.8980\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 6.1804e-06 - accuracy: 1.0000 - val_loss: 0.3715 - val_accuracy: 0.9048\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 1s 55ms/step - loss: 8.5788e-06 - accuracy: 1.0000 - val_loss: 0.2776 - val_accuracy: 0.9320\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 9.2394e-06 - accuracy: 1.0000 - val_loss: 0.2151 - val_accuracy: 0.9660\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 1.0434e-05 - accuracy: 1.0000 - val_loss: 0.1916 - val_accuracy: 0.9796\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 5.0280e-05 - accuracy: 1.0000 - val_loss: 0.1875 - val_accuracy: 0.9796\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 8.2554e-06 - accuracy: 1.0000 - val_loss: 0.2039 - val_accuracy: 0.9796\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.5083e-05 - accuracy: 1.0000 - val_loss: 0.2125 - val_accuracy: 0.9728\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 9.4363e-06 - accuracy: 1.0000 - val_loss: 0.2151 - val_accuracy: 0.9728\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 5.4028e-06 - accuracy: 1.0000 - val_loss: 0.2162 - val_accuracy: 0.9728\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 5.9152e-06 - accuracy: 1.0000 - val_loss: 0.2157 - val_accuracy: 0.9728\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 4.3403e-06 - accuracy: 1.0000 - val_loss: 0.2135 - val_accuracy: 0.9728\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpf0XlSARX78",
        "outputId": "20a678d9-2e11-4b82-da20-d7fafd666845"
      },
      "source": [
        "#pred_test= model.predict_classes(X_test)\n",
        "pred_test= np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predict   0   1\n",
            "Actual         \n",
            "0        69   3\n",
            "1         1  74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iFNNrlWV9tH",
        "outputId": "b47be7a3-9a9e-4a7b-a7d1-3dee5848652f"
      },
      "source": [
        "pred_test"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QISvYcJBgWbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e93cc0aa-1fba-4595-a903-a092568f6117"
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[0] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  #prediction = model.predict_classes(result)\n",
        "  prediction= np.argmax(model.predict(result), axis=-1)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0   152.0   50.378117   51.056786  ...  157.300552  108.574799   70.511772\n",
            "3   151.0   57.209335   50.073242  ...  112.237625  106.178108   81.611908\n",
            "4   120.0   87.367783   44.252228  ...    0.511111    0.000000    0.000000\n",
            "5   113.0   54.331192   55.017700  ...   88.006737   85.108070   72.290154\n",
            "8   153.0   56.294254   53.946178  ...   38.478706   33.101116   19.170235\n",
            "9   158.0   47.477806   44.613201  ...   64.979492   73.500725   80.170486\n",
            "11  137.0  113.008957  111.042938  ...   34.074966   26.182323   16.610260\n",
            "13  178.0  113.346802  149.910889  ...   88.080048   90.381523   93.937141\n",
            "14  199.0  101.856262   98.023331  ...    3.135047    1.654504    1.110073\n",
            "17  146.0   54.607803   53.600113  ...   10.720773    3.477388    0.054232\n",
            "19  145.0   37.973267   54.564281  ...   94.842667   95.878326   96.706879\n",
            "20  118.0   45.513359   45.783970  ...   71.779663   62.908646   45.383797\n",
            "22  142.0   51.672485   50.790516  ...   45.668522   34.331284   29.728228\n",
            "23  107.0   46.069004   45.911434  ...    9.639620    8.894488   10.014586\n",
            "24  108.0   29.082304   27.231825  ...   51.514404   49.031555   45.001373\n",
            "25  193.0   66.115997   69.813499  ...  101.460579   19.800880   31.891514\n",
            "26  145.0   68.908249   65.301308  ...   68.293701   73.010696   80.749207\n",
            "27  139.0   70.958336   75.989960  ...   43.775269   44.656380   46.756580\n",
            "29  184.0   37.928635   35.768898  ...  108.491020  105.148865  105.672493\n",
            "31  157.0   51.549480   46.366791  ...  105.001915  108.767174  106.452110\n",
            "32  154.0   44.859512   51.685955  ...   81.165298   83.297523   85.694221\n",
            "34  103.0   88.192665   88.301437  ...   96.002075   95.248657   95.635498\n",
            "35  182.0   84.704147   63.213024  ...   32.260357   31.331364   35.307693\n",
            "36  141.0  111.908463  117.362816  ...   96.081284   98.113976   99.549217\n",
            "37  109.0   61.585472   54.146530  ...   37.717445   38.769882   37.670818\n",
            "38  104.0   89.798828   90.979294  ...   76.187881   78.724861   88.167160\n",
            "39  113.0   19.533243   18.796459  ...    0.000000    0.000000    0.000000\n",
            "40  197.0   68.536171   68.161827  ...   90.707405   86.060097   69.239464\n",
            "41  177.0  107.557693  110.524712  ...    5.937022    6.263238    6.461648\n",
            "42  123.0   42.302998   39.283234  ...   81.778107   78.213097   62.855576\n",
            "44  183.0   36.255516   38.307240  ...   48.527817   47.939709   50.675533\n",
            "45  160.0   53.543121   53.239380  ...  137.471237  143.020630  145.337494\n",
            "46  164.0    8.080309    2.526472  ...   88.447952   91.274834   92.581795\n",
            "47  176.0  103.791840  106.115181  ...    3.080062    1.627583    1.211260\n",
            "48  165.0  106.084702  105.574585  ...    0.092562    0.051423    0.000000\n",
            "49  139.0   57.599968   63.330933  ...  139.522537  143.975098  149.296875\n",
            "\n",
            "[36 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRbWgmX_LFH",
        "outputId": "ef298ed5-62cd-4ded-aef7-b69b8d2c9bef"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "from GetBetterSegm import GetBetter"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_paper_fev_2021'...\n",
            "remote: Enumerating objects: 671, done.\u001b[K\n",
            "remote: Counting objects: 100% (432/432), done.\u001b[K\n",
            "remote: Compressing objects: 100% (430/430), done.\u001b[K\n",
            "remote: Total 671 (delta 270), reused 0 (delta 0), pack-reused 239\u001b[K\n",
            "Receiving objects: 100% (671/671), 5.50 MiB | 13.67 MiB/s, done.\n",
            "Resolving deltas: 100% (407/407), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAG_I6FwCvFr",
        "outputId": "e4a11a72-23ce-41a9-ded3-56307b9e092e"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "%cd marquesgabi_out_2020\n",
        "PSD_imageJ = 'Areas_ImageJ.csv'\n",
        "PSD_new = pd.read_csv(PSD_imageJ)\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_out_2020'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 146 (delta 75), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (146/146), 1.00 MiB | 16.57 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020\n",
            "   Juntas   Area\n",
            "0       1  2.001\n",
            "1       2  0.820\n",
            "2       3  1.270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "PekBHQOT_6CP",
        "outputId": "61a19a64-792c-4a31-9b56-72e3f6f979ed"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>152.0</td>\n",
              "      <td>50.378117</td>\n",
              "      <td>51.056786</td>\n",
              "      <td>53.599720</td>\n",
              "      <td>54.993073</td>\n",
              "      <td>54.824093</td>\n",
              "      <td>43.374653</td>\n",
              "      <td>89.959129</td>\n",
              "      <td>119.245148</td>\n",
              "      <td>130.838654</td>\n",
              "      <td>139.754852</td>\n",
              "      <td>136.808853</td>\n",
              "      <td>132.174515</td>\n",
              "      <td>131.139893</td>\n",
              "      <td>131.774231</td>\n",
              "      <td>133.163437</td>\n",
              "      <td>138.943222</td>\n",
              "      <td>130.218826</td>\n",
              "      <td>129.759705</td>\n",
              "      <td>130.369812</td>\n",
              "      <td>135.012466</td>\n",
              "      <td>143.615646</td>\n",
              "      <td>167.376740</td>\n",
              "      <td>181.464676</td>\n",
              "      <td>192.331024</td>\n",
              "      <td>216.565109</td>\n",
              "      <td>159.371887</td>\n",
              "      <td>76.616341</td>\n",
              "      <td>99.650970</td>\n",
              "      <td>49.146816</td>\n",
              "      <td>50.198753</td>\n",
              "      <td>51.954292</td>\n",
              "      <td>51.669670</td>\n",
              "      <td>48.443909</td>\n",
              "      <td>35.801937</td>\n",
              "      <td>76.663437</td>\n",
              "      <td>117.980606</td>\n",
              "      <td>129.882263</td>\n",
              "      <td>141.573395</td>\n",
              "      <td>139.981995</td>\n",
              "      <td>...</td>\n",
              "      <td>152.447372</td>\n",
              "      <td>151.438354</td>\n",
              "      <td>152.367035</td>\n",
              "      <td>155.186966</td>\n",
              "      <td>155.751389</td>\n",
              "      <td>152.950836</td>\n",
              "      <td>150.298477</td>\n",
              "      <td>142.874649</td>\n",
              "      <td>127.283241</td>\n",
              "      <td>100.755539</td>\n",
              "      <td>66.259697</td>\n",
              "      <td>55.333099</td>\n",
              "      <td>74.232681</td>\n",
              "      <td>72.773544</td>\n",
              "      <td>68.925201</td>\n",
              "      <td>59.313019</td>\n",
              "      <td>55.855270</td>\n",
              "      <td>57.656509</td>\n",
              "      <td>57.484070</td>\n",
              "      <td>58.023544</td>\n",
              "      <td>63.713989</td>\n",
              "      <td>105.945290</td>\n",
              "      <td>134.265228</td>\n",
              "      <td>138.015228</td>\n",
              "      <td>134.078934</td>\n",
              "      <td>134.823410</td>\n",
              "      <td>142.497925</td>\n",
              "      <td>146.860107</td>\n",
              "      <td>149.909973</td>\n",
              "      <td>154.552628</td>\n",
              "      <td>158.274231</td>\n",
              "      <td>166.363556</td>\n",
              "      <td>171.117722</td>\n",
              "      <td>173.623260</td>\n",
              "      <td>172.274933</td>\n",
              "      <td>169.632965</td>\n",
              "      <td>164.477844</td>\n",
              "      <td>157.300552</td>\n",
              "      <td>108.574799</td>\n",
              "      <td>70.511772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>151.0</td>\n",
              "      <td>57.209335</td>\n",
              "      <td>50.073242</td>\n",
              "      <td>55.100433</td>\n",
              "      <td>61.088593</td>\n",
              "      <td>66.445244</td>\n",
              "      <td>69.921417</td>\n",
              "      <td>69.333801</td>\n",
              "      <td>64.871719</td>\n",
              "      <td>62.102058</td>\n",
              "      <td>61.686199</td>\n",
              "      <td>62.175392</td>\n",
              "      <td>61.442703</td>\n",
              "      <td>60.397530</td>\n",
              "      <td>60.012280</td>\n",
              "      <td>57.375595</td>\n",
              "      <td>50.074253</td>\n",
              "      <td>36.683922</td>\n",
              "      <td>26.186396</td>\n",
              "      <td>24.831192</td>\n",
              "      <td>28.058332</td>\n",
              "      <td>34.312706</td>\n",
              "      <td>53.216618</td>\n",
              "      <td>65.009567</td>\n",
              "      <td>68.646156</td>\n",
              "      <td>71.225609</td>\n",
              "      <td>72.482918</td>\n",
              "      <td>71.763748</td>\n",
              "      <td>71.775322</td>\n",
              "      <td>58.218502</td>\n",
              "      <td>48.581818</td>\n",
              "      <td>53.690239</td>\n",
              "      <td>59.333145</td>\n",
              "      <td>63.542301</td>\n",
              "      <td>67.761765</td>\n",
              "      <td>68.353409</td>\n",
              "      <td>67.149948</td>\n",
              "      <td>62.890793</td>\n",
              "      <td>61.357094</td>\n",
              "      <td>60.068024</td>\n",
              "      <td>...</td>\n",
              "      <td>65.222885</td>\n",
              "      <td>63.748131</td>\n",
              "      <td>64.395996</td>\n",
              "      <td>64.224686</td>\n",
              "      <td>68.322662</td>\n",
              "      <td>71.277534</td>\n",
              "      <td>87.316132</td>\n",
              "      <td>104.748436</td>\n",
              "      <td>110.894257</td>\n",
              "      <td>107.621773</td>\n",
              "      <td>93.474632</td>\n",
              "      <td>76.483879</td>\n",
              "      <td>104.946716</td>\n",
              "      <td>108.931549</td>\n",
              "      <td>110.039391</td>\n",
              "      <td>109.335297</td>\n",
              "      <td>99.604591</td>\n",
              "      <td>116.275513</td>\n",
              "      <td>128.538391</td>\n",
              "      <td>138.972244</td>\n",
              "      <td>154.279327</td>\n",
              "      <td>166.937195</td>\n",
              "      <td>176.176758</td>\n",
              "      <td>100.427361</td>\n",
              "      <td>28.915356</td>\n",
              "      <td>53.549980</td>\n",
              "      <td>57.775848</td>\n",
              "      <td>60.353367</td>\n",
              "      <td>63.220779</td>\n",
              "      <td>62.988071</td>\n",
              "      <td>62.173279</td>\n",
              "      <td>65.856018</td>\n",
              "      <td>73.982498</td>\n",
              "      <td>90.779922</td>\n",
              "      <td>102.481873</td>\n",
              "      <td>108.312592</td>\n",
              "      <td>112.868645</td>\n",
              "      <td>112.237625</td>\n",
              "      <td>106.178108</td>\n",
              "      <td>81.611908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>120.0</td>\n",
              "      <td>87.367783</td>\n",
              "      <td>44.252228</td>\n",
              "      <td>42.998890</td>\n",
              "      <td>43.155552</td>\n",
              "      <td>39.003334</td>\n",
              "      <td>37.627777</td>\n",
              "      <td>31.755554</td>\n",
              "      <td>24.042223</td>\n",
              "      <td>24.995556</td>\n",
              "      <td>25.740004</td>\n",
              "      <td>25.938887</td>\n",
              "      <td>27.435556</td>\n",
              "      <td>32.426666</td>\n",
              "      <td>37.054443</td>\n",
              "      <td>36.705555</td>\n",
              "      <td>36.101112</td>\n",
              "      <td>31.015556</td>\n",
              "      <td>17.106667</td>\n",
              "      <td>6.807778</td>\n",
              "      <td>5.871111</td>\n",
              "      <td>4.245555</td>\n",
              "      <td>3.800000</td>\n",
              "      <td>4.327778</td>\n",
              "      <td>3.558889</td>\n",
              "      <td>1.366667</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>0.008889</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>69.915558</td>\n",
              "      <td>41.303337</td>\n",
              "      <td>39.844444</td>\n",
              "      <td>39.778893</td>\n",
              "      <td>37.957779</td>\n",
              "      <td>38.945557</td>\n",
              "      <td>36.000004</td>\n",
              "      <td>30.776669</td>\n",
              "      <td>30.654446</td>\n",
              "      <td>30.538887</td>\n",
              "      <td>32.293335</td>\n",
              "      <td>...</td>\n",
              "      <td>10.851111</td>\n",
              "      <td>13.977777</td>\n",
              "      <td>16.779999</td>\n",
              "      <td>16.561110</td>\n",
              "      <td>11.821112</td>\n",
              "      <td>5.414444</td>\n",
              "      <td>3.492223</td>\n",
              "      <td>1.880000</td>\n",
              "      <td>0.687778</td>\n",
              "      <td>0.355556</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>55.023335</td>\n",
              "      <td>55.428886</td>\n",
              "      <td>54.921108</td>\n",
              "      <td>54.684444</td>\n",
              "      <td>51.985558</td>\n",
              "      <td>41.206665</td>\n",
              "      <td>22.921110</td>\n",
              "      <td>14.798889</td>\n",
              "      <td>13.552221</td>\n",
              "      <td>15.221110</td>\n",
              "      <td>18.818890</td>\n",
              "      <td>21.572224</td>\n",
              "      <td>23.287779</td>\n",
              "      <td>25.624445</td>\n",
              "      <td>26.713335</td>\n",
              "      <td>29.350000</td>\n",
              "      <td>30.437778</td>\n",
              "      <td>30.443335</td>\n",
              "      <td>30.677778</td>\n",
              "      <td>30.381111</td>\n",
              "      <td>28.037777</td>\n",
              "      <td>17.859999</td>\n",
              "      <td>4.543334</td>\n",
              "      <td>1.575556</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.511111</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>113.0</td>\n",
              "      <td>54.331192</td>\n",
              "      <td>55.017700</td>\n",
              "      <td>53.214897</td>\n",
              "      <td>52.273712</td>\n",
              "      <td>50.721905</td>\n",
              "      <td>47.942986</td>\n",
              "      <td>44.933353</td>\n",
              "      <td>43.029049</td>\n",
              "      <td>44.819176</td>\n",
              "      <td>49.170338</td>\n",
              "      <td>50.883236</td>\n",
              "      <td>51.591198</td>\n",
              "      <td>51.157883</td>\n",
              "      <td>54.688305</td>\n",
              "      <td>57.339882</td>\n",
              "      <td>57.780952</td>\n",
              "      <td>58.461903</td>\n",
              "      <td>60.181217</td>\n",
              "      <td>59.929203</td>\n",
              "      <td>56.568718</td>\n",
              "      <td>42.936485</td>\n",
              "      <td>13.303782</td>\n",
              "      <td>7.527840</td>\n",
              "      <td>6.555643</td>\n",
              "      <td>11.673115</td>\n",
              "      <td>34.487747</td>\n",
              "      <td>58.098282</td>\n",
              "      <td>72.280602</td>\n",
              "      <td>53.657372</td>\n",
              "      <td>55.231030</td>\n",
              "      <td>55.179661</td>\n",
              "      <td>53.752918</td>\n",
              "      <td>50.562927</td>\n",
              "      <td>48.674286</td>\n",
              "      <td>45.342159</td>\n",
              "      <td>43.821602</td>\n",
              "      <td>45.859032</td>\n",
              "      <td>49.336521</td>\n",
              "      <td>50.788944</td>\n",
              "      <td>...</td>\n",
              "      <td>111.266663</td>\n",
              "      <td>125.982460</td>\n",
              "      <td>110.725815</td>\n",
              "      <td>74.573418</td>\n",
              "      <td>80.093201</td>\n",
              "      <td>84.546646</td>\n",
              "      <td>88.125771</td>\n",
              "      <td>87.432053</td>\n",
              "      <td>87.222015</td>\n",
              "      <td>85.674995</td>\n",
              "      <td>81.925522</td>\n",
              "      <td>60.787766</td>\n",
              "      <td>25.679222</td>\n",
              "      <td>16.720181</td>\n",
              "      <td>10.011120</td>\n",
              "      <td>8.006108</td>\n",
              "      <td>7.443026</td>\n",
              "      <td>8.785966</td>\n",
              "      <td>10.615630</td>\n",
              "      <td>15.423056</td>\n",
              "      <td>43.353199</td>\n",
              "      <td>61.787842</td>\n",
              "      <td>70.092964</td>\n",
              "      <td>77.060143</td>\n",
              "      <td>74.133293</td>\n",
              "      <td>84.569504</td>\n",
              "      <td>102.229774</td>\n",
              "      <td>97.798416</td>\n",
              "      <td>148.469269</td>\n",
              "      <td>143.690353</td>\n",
              "      <td>123.058655</td>\n",
              "      <td>73.164536</td>\n",
              "      <td>79.665916</td>\n",
              "      <td>81.959122</td>\n",
              "      <td>83.609367</td>\n",
              "      <td>86.664421</td>\n",
              "      <td>87.334946</td>\n",
              "      <td>88.006737</td>\n",
              "      <td>85.108070</td>\n",
              "      <td>72.290154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>153.0</td>\n",
              "      <td>56.294254</td>\n",
              "      <td>53.946178</td>\n",
              "      <td>55.461021</td>\n",
              "      <td>54.806232</td>\n",
              "      <td>54.779617</td>\n",
              "      <td>54.067585</td>\n",
              "      <td>54.869755</td>\n",
              "      <td>50.098427</td>\n",
              "      <td>36.995815</td>\n",
              "      <td>19.157206</td>\n",
              "      <td>10.062541</td>\n",
              "      <td>17.512068</td>\n",
              "      <td>29.806700</td>\n",
              "      <td>35.252895</td>\n",
              "      <td>37.841393</td>\n",
              "      <td>38.201164</td>\n",
              "      <td>40.029861</td>\n",
              "      <td>38.341709</td>\n",
              "      <td>39.785080</td>\n",
              "      <td>38.720108</td>\n",
              "      <td>35.476227</td>\n",
              "      <td>34.785980</td>\n",
              "      <td>32.567646</td>\n",
              "      <td>31.863300</td>\n",
              "      <td>27.658125</td>\n",
              "      <td>25.083044</td>\n",
              "      <td>15.023539</td>\n",
              "      <td>1.990517</td>\n",
              "      <td>61.393906</td>\n",
              "      <td>58.421467</td>\n",
              "      <td>60.737030</td>\n",
              "      <td>63.169937</td>\n",
              "      <td>61.555901</td>\n",
              "      <td>60.859413</td>\n",
              "      <td>60.139008</td>\n",
              "      <td>51.613911</td>\n",
              "      <td>30.741552</td>\n",
              "      <td>11.024693</td>\n",
              "      <td>7.062754</td>\n",
              "      <td>...</td>\n",
              "      <td>31.890257</td>\n",
              "      <td>32.282497</td>\n",
              "      <td>32.951305</td>\n",
              "      <td>33.377209</td>\n",
              "      <td>34.551071</td>\n",
              "      <td>36.143024</td>\n",
              "      <td>36.052116</td>\n",
              "      <td>38.637192</td>\n",
              "      <td>39.432102</td>\n",
              "      <td>39.859631</td>\n",
              "      <td>35.025761</td>\n",
              "      <td>23.504038</td>\n",
              "      <td>55.406681</td>\n",
              "      <td>56.118633</td>\n",
              "      <td>57.022774</td>\n",
              "      <td>50.698879</td>\n",
              "      <td>46.994747</td>\n",
              "      <td>41.811443</td>\n",
              "      <td>38.261013</td>\n",
              "      <td>35.664406</td>\n",
              "      <td>35.155029</td>\n",
              "      <td>35.597767</td>\n",
              "      <td>37.047890</td>\n",
              "      <td>37.198776</td>\n",
              "      <td>36.832417</td>\n",
              "      <td>37.825966</td>\n",
              "      <td>36.637405</td>\n",
              "      <td>35.582256</td>\n",
              "      <td>34.157761</td>\n",
              "      <td>31.303387</td>\n",
              "      <td>33.277504</td>\n",
              "      <td>33.818916</td>\n",
              "      <td>34.243073</td>\n",
              "      <td>35.899273</td>\n",
              "      <td>36.455551</td>\n",
              "      <td>38.378834</td>\n",
              "      <td>39.430649</td>\n",
              "      <td>38.478706</td>\n",
              "      <td>33.101116</td>\n",
              "      <td>19.170235</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Width          0          1  ...         781         782        783\n",
              "0  152.0  50.378117  51.056786  ...  157.300552  108.574799  70.511772\n",
              "3  151.0  57.209335  50.073242  ...  112.237625  106.178108  81.611908\n",
              "4  120.0  87.367783  44.252228  ...    0.511111    0.000000   0.000000\n",
              "5  113.0  54.331192  55.017700  ...   88.006737   85.108070  72.290154\n",
              "8  153.0  56.294254  53.946178  ...   38.478706   33.101116  19.170235\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "Area = np.array(PSD_new['Area'])\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J705kDqsE8f",
        "outputId": "a69d835d-bddb-496b-c223-7d8ef859dd14"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(490, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wCFDX8esLoQ"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn-F050Hr9Ui"
      },
      "source": [
        ""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "Vfk_fNXGDK5_",
        "outputId": "0eff1a9e-2bd9-4d7e-aadf-1f5bcb43c789"
      },
      "source": [
        " wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        " wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        " X = pd.DataFrame([Diam1,Diameter_All])\n",
        " wts = pd.DataFrame([wt1,wt2])\n",
        "plt.hist(X,weights=wts)\n",
        "plt.legend(['Image J','CNN'])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f84b42b34d0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nGDbBEeiUij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "888f9702-4259-415e-9a87-fd79b32b692a"
      },
      "source": [
        "# plt.hist(x, bins=bins, density=True, histtype='step', cumulative=-1,label='Reversed emp.')\n",
        "plt.hist(X, density=True, histtype='step', cumulative=True,label='Reversed emp.')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.04210526, 0.14736842, 0.32631579, 0.66315789, 0.87368421,\n",
              "         0.95789474, 0.97894737, 0.97894737, 0.98947368, 1.        ],\n",
              "        [0.13888889, 0.25      , 0.36111111, 0.63888889, 0.75      ,\n",
              "         0.91666667, 0.97222222, 1.        , 1.        , 1.        ]]),\n",
              " array([0.65708494, 0.82312037, 0.98915579, 1.15519122, 1.32122664,\n",
              "        1.48726207, 1.65329749, 1.81933292, 1.98536834, 2.15140377,\n",
              "        2.31743919]),\n",
              " <a list of 2 Lists of Patches objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "9xENlBUUxfTu",
        "outputId": "eea406d7-5191-4006-a0c8-55bb3a861544"
      },
      "source": [
        "Obj = plt.hist(X, density=True, histtype='step', cumulative=True,label='Reversed emp.')\n",
        "Y1, Y2 = Obj[0]\n",
        "Rsquared = r2_score(Y1, Y2)\n",
        "print('r_squared =',Rsquared)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r_squared = 0.9698907750823322\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2XboMiFbkaa"
      },
      "source": [
        "acc_train = r.history['accuracy'][-1]\n",
        "acc_test = r.history['val_accuracy'][-1]\n",
        "loss_train = r.history['loss'][-1]\n",
        "loss_test = r.history['val_loss'][-1]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euTd_-CYN1v0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "60cf92e6-db90-4b78-ef00-08d7cf26421e"
      },
      "source": [
        "df = pd.DataFrame({'N1':N1, 'N2':N2,'R^2':Rsquared,\n",
        "                   'acc train':acc_train,'acc test':acc_test,\n",
        "                   'loss train':loss_train,'loss test':loss_test,\n",
        "                   'Details':Description},\n",
        "                  index= [0])\n",
        "Arq = \"output.xlsx\"\n",
        "df.to_excel(Arq)\n",
        "files.download(Arq)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_1dd270e4-568a-46ab-916b-36a35e8052d3\", \"output.xlsx\", 5151)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "-KukfpGTTKlj",
        "outputId": "2d099610-13da-4d16-ae89-49020ce32aef"
      },
      "source": [
        "df"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>N1</th>\n",
              "      <th>N2</th>\n",
              "      <th>R^2</th>\n",
              "      <th>acc train</th>\n",
              "      <th>acc test</th>\n",
              "      <th>loss train</th>\n",
              "      <th>loss test</th>\n",
              "      <th>Details</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>200</td>\n",
              "      <td>10</td>\n",
              "      <td>0.969891</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.972789</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.213473</td>\n",
              "      <td>3 layers of Convolution: 32, 64, 128</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    N1  N2  ...  loss test                                Details\n",
              "0  200  10  ...   0.213473  3 layers of Convolution: 32, 64, 128 \n",
              "\n",
              "[1 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "ZZHa1j4HT9Dq",
        "outputId": "50033e77-c446-47b8-da7d-58f3baad47af"
      },
      "source": [
        "counts, bins, bars = plt.hist(X,weights=wts)\n",
        "print(bars)\n",
        "print(bins)\n",
        "print(counts)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<a list of 2 Lists of Patches objects>\n",
            "[0.65708494 0.82312037 0.98915579 1.15519122 1.32122664 1.48726207\n",
            " 1.65329749 1.81933292 1.98536834 2.15140377 2.31743919]\n",
            "[[ 4.21052632 10.52631579 17.89473684 33.68421053 21.05263158  8.42105263\n",
            "   2.10526316  0.          1.05263158  1.05263158]\n",
            " [13.88888889 11.11111111 11.11111111 27.77777778 11.11111111 16.66666667\n",
            "   5.55555556  2.77777778  0.          0.        ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD6CAYAAAC4RRw1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAP2UlEQVR4nO3dfYxldX3H8fenPBRbKGB3SjaAHWvxgdiy0BGxGoNY7YJNwMSY0hapoV3bisHGGCl/VOxDgkmVpqnVrEKhjY9RFCpoS5CWWgUddFketlXE1UJXdnwA0Sa2C9/+cc/WdZjZe2bmnpn5se9XcrPnce8H9sxnf3vuOeemqpAktefH1jqAJGl5LHBJapQFLkmNssAlqVEWuCQ1ygKXpEaNLfAkhyX5XJI7ktyd5C3d8quSfDXJtu61afi4kqS9Du6xzQ+AM6rqe0kOAT6d5BPdujdW1Yf7vtmGDRtqenp6GTEl6cB1++23f7OqpuYvH1vgNbrT53vd7CHda1l3/0xPTzM7O7ucXSXpgJXkawst73UOPMlBSbYBu4Ebq+q2btWfJ9me5PIkPz6hrJKkHnoVeFU9WlWbgOOAU5M8G/gj4JnAc4AnA29aaN8kW5LMJpmdm5ubUGxJ0pKuQqmqh4Cbgc1VtatGfgD8LXDqIvtsraqZqpqZmnrcKRxJ0jL1uQplKslR3fSTgJcA/55kY7cswDnAXUMGlST9qD5XoWwErk5yEKPC/1BVfTzJp5JMAQG2Ab83YE5J0jx9rkLZDpy8wPIzBkkkSerFOzElqVEWuCQ1ygKXpEb1+RBTB6jpi69f8j47L3vZAEkkLcQRuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRo0t8CSHJflckjuS3J3kLd3ypya5Lcm9ST6Y5NDh40qS9uozAv8BcEZVnQRsAjYnOQ14K3B5Vf088B3gguFiSpLmG1vgNfK9bvaQ7lXAGcCHu+VXA+cMklCStKBe58CTHJRkG7AbuBH4CvBQVe3pNrkfOHaRfbckmU0yOzc3N4nMkiR6FnhVPVpVm4DjgFOBZ/Z9g6raWlUzVTUzNTW1zJiSpPmWdBVKVT0E3Aw8DzgqycHdquOAByacTZK0H32uQplKclQ3/STgJcAORkX+im6z84FrhwopSXq8g8dvwkbg6iQHMSr8D1XVx5PcA3wgyZ8BXwSuGDCnJGmesQVeVduBkxdYfh+j8+GSpDXgnZiS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRvW5E1Pq79Ijl7j9w8PkkA4AjsAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaNbbAkxyf5OYk9yS5O8lF3fJLkzyQZFv3Omv4uJKkvfo8TnYP8Iaq+kKSI4Dbk9zYrbu8qv5iuHiSpMWMLfCq2gXs6qYfSbIDOHboYJKk/VvSOfAk08DJwG3doguTbE9yZZKjF9lnS5LZJLNzc3MrCitJ+qHeBZ7kcOAjwOur6rvAO4GnAZsYjdDfttB+VbW1qmaqamZqamoCkSVJ0LPAkxzCqLzfW1XXAFTVg1X1aFU9BrwbOHW4mJKk+fpchRLgCmBHVb19n+Ub99ns5cBdk48nSVpMn6tQng+cB9yZZFu37BLg3CSbgAJ2Aq8ZJKEkaUF9rkL5NJAFVt0w+TiSpL68E1OSGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqP6fCemtOqmL75+yfvsvOxlAySR1i9H4JLUKAtckho1tsCTHJ/k5iT3JLk7yUXd8icnuTHJl7tfjx4+riRprz4j8D3AG6rqROA04LVJTgQuBm6qqhOAm7p5SdIqGVvgVbWrqr7QTT8C7ACOBc4Gru42uxo4Z6iQkqTHW9I58CTTwMnAbcAxVbWrW/UN4JiJJpMk7VfvAk9yOPAR4PVV9d1911VVAbXIfluSzCaZnZubW1FYSdIP9SrwJIcwKu/3VtU13eIHk2zs1m8Edi+0b1VtraqZqpqZmpqaRGZJEv2uQglwBbCjqt6+z6rrgPO76fOBaycfT5K0mD53Yj4fOA+4M8m2btklwGXAh5JcAHwNeOUwESVJCxlb4FX1aSCLrH7xZONIkvryTkxJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSo/xOzHXO74aUtBhH4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRXkYoTcKlRy5x+4eHyaEDiiNwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1KixBZ7kyiS7k9y1z7JLkzyQZFv3OmvYmJKk+fqMwK8CNi+w/PKq2tS9bphsLEnSOGMLvKpuAb69ClkkSUuwknPgFybZ3p1iOXpiiSRJvSy3wN8JPA3YBOwC3rbYhkm2JJlNMjs3N7fMt5MkzbesAq+qB6vq0ap6DHg3cOp+tt1aVTNVNTM1NbXcnJKkeZZV4Ek27jP7cuCuxbaVJA1j7NMIk7wfOB3YkOR+4M3A6Uk2AQXsBF4zYEZJ0gLGFnhVnbvA4isGyCJJWgLvxJSkRj2xv9DBh+xLegJzBC5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJatQT+wsd1tJafpnEgfpFFgfqf7cOWI7AJalRFrgkNcoCl6RGjT0HnuRK4NeA3VX17G7Zk4EPAtPATuCVVfWd4WKuremLr1/yPjsPGyCIJO2jzwj8KmDzvGUXAzdV1QnATd28JGkVjS3wqroF+Pa8xWcDV3fTVwPnTDiXJGmM5Z4DP6aqdnXT3wCOWWzDJFuSzCaZnZubW+bbSZLmW/GHmFVVQO1n/daqmqmqmampqZW+nSSps9wCfzDJRoDu192TiyRJ6mO5BX4dcH43fT5w7WTiSJL6GlvgSd4PfBZ4RpL7k1wAXAa8JMmXgV/p5iVJq2jsdeBVde4iq1484SySpCXwTkxJapQFLkmNssAlqVEWuCQ1yi90kObx4WVqhSNwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJatSKvpEnyU7gEeBRYE9VzUwilKQluPTIJW7/8DA5tOom8ZVqL6qqb07g95EkLYGnUCSpUSst8AL+KcntSbYstEGSLUlmk8zOzc2t8O0kSXuttMBfUFWnAGcCr03ywvkbVNXWqpqpqpmpqakVvp0kaa8VFXhVPdD9uhv4KHDqJEJJksZbdoEn+ckkR+ydBl4K3DWpYJKk/VvJVSjHAB9Nsvf3eV9VfXIiqSRJYy27wKvqPuCkCWbZr+mLr1/yPjsPGyCIJK0TXkYoSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJatQkngcu6UDll0msKUfgktQoC1ySGmWBS1KjPAcurTNLfXCbD207cDkCl6RGWeCS1CgLXJIaZYFLUqMscElqlFehSFoXlnz1zWUve0K890o4ApekRlngktSoFRV4ks1J/iPJvUkunlQoSdJ4yy7wJAcB7wDOBE4Ezk1y4qSCSZL2byUj8FOBe6vqvqr6H+ADwNmTiSVJGmclBX4s8J/7zN/fLZMkrYJU1fJ2TF4BbK6q3+nmzwOeW1UXzttuC7Clm30G8C3gm8tOPJwNrL9c6zETrM9cZupvPeZaj5lg/eT62aqamr9wJdeBPwAcv8/8cd2yH1FVW4Gte+eTzFbVzAredxDrMdd6zATrM5eZ+luPudZjJli/ufZaySmUzwMnJHlqkkOBXweum0wsSdI4yx6BV9WeJBcC/wgcBFxZVXdPLJkkab9WdCt9Vd0A3LDE3baO32RNrMdc6zETrM9cZupvPeZaj5lg/eYCVvAhpiRpbXkrvSQ1arAC73ObfZJXJrknyd1J3jdUlqXkSvKUJDcn+WKS7UnOGjjPlUl2J7lrkfVJ8ldd3u1JThkyzxJy/WaX584kn0ly0lpn2me75yTZ013qOrg+uZKcnmRbd6z/y1pnSnJkkn9IckeX6dWrkOn47mdr78/8RQtss6rHe89Mq36s91ZVE38x+lDzK8DPAYcCdwAnztvmBOCLwNHd/M8MkWUZubYCv99NnwjsHDjTC4FTgLsWWX8W8AkgwGnAbUP/f+qZ65f3+bM7czVyjcu0z5/xpxh9NvOKdfL/6ijgHuAp3fxqHOvjMl0CvLWbngK+DRw6cKaNwCnd9BHAlxb4+VvV471nplU/1vu+hhqB97nN/neBd1TVdwCqavdAWZaaq4Cf6qaPBP5ryEBVdQujH57FnA38XY3cChyVZOOQmfrkqqrP7P2zA25ldB/AmmbqvA74CLAaxxPQK9dvANdU1de77QfP1iNTAUckCXB4t+2egTPtqqovdNOPADt4/N3bq3q898m0Fsd6X0MVeJ/b7J8OPD3JvyW5NcnmgbIsNdelwG8luZ/RKO51q5Brf1p4ZMEFjEZNayrJscDLgXeudZZ5ng4cneSfk9ye5FVrHQj4a+BZjAYodwIXVdVjq/XmSaaBk4Hb5q1as+N9P5n2tS6O9b3W8ht5DmZ0GuV0Rn+j3ZLkF6rqoTXMBHAucFVVvS3J84C/T/Ls1Ty4W5LkRYwO6hesdRbgL4E3VdVjo4HlunEw8EvAi4EnAZ9NcmtVfWkNM/0qsA04A3gacGOSf62q7w79xkkOZ/SvpNevxvv10SfTOjvWgeEKvM9t9vczOpf0v8BXk3yJUaF/fqBMfXNdAGwGqKrPJjmM0fMQVu2f5PP0emTBWkjyi8B7gDOr6ltrnQeYAT7QlfcG4Kwke6rqY2sbi/uBb1XV94HvJ7kFOInR+da18mrgshqd2L03yVeBZwKfG/JNkxzCqCjfW1XXLLDJqh/vPTKtx2MdGO4USp/b7D/GaPRNkg2M/pl530B5lpLr64xGSiR5FnAYMDdwrv25DnhV9+n8acDDVbVrDfMAo6t1gGuA89Z4JPn/quqpVTVdVdPAh4E/WAflDXAt8IIkByf5CeC5jM61rqV9j/NjGD1obtCfv+58+xXAjqp6+yKbrerx3ifTejzW9xpkBF6L3Gaf5E+A2aq6rlv30iT3AI8Cbxz6b7aeud4AvDvJHzL6oOe3u1HKIJK8n9FfZBu68+5vBg7p8r6L0Xn4s4B7gf9mNHIaXI9cfwz8NPA33Yh3Tw380J8emdbEuFxVtSPJJ4HtwGPAe6pqv5dCDp0J+FPgqiR3Mrri401VNfRT954PnAfcmWRbt+wS4Cn75Frt471PplU/1vvyTkxJapR3YkpSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIa9X/mPSUAPAmWtwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o_vDGeWUwIZ",
        "outputId": "c4f3f7f6-b04e-4da0-d90e-21619d1f54f8"
      },
      "source": [
        "print(counts.sum())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200.00000000000006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "KcH52-6iJQ8t",
        "outputId": "1ee2fcda-c442-4816-a3cd-8170be9cf621"
      },
      "source": [
        "\n",
        "plt.hist([Diam1,Diameter_All])\n",
        "plt.legend(['Image J','CNN'])\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f84b82cb0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r11AxFK_JIii",
        "outputId": "8e28b0f0-be0a-47a6-84f5-34d6bffd8a8b"
      },
      "source": [
        "[Diam1,Diameter_All]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.59616801403081,\n",
              "  1.0217907939900581,\n",
              "  1.2716187407449044,\n",
              "  1.104429030701514,\n",
              "  1.2163487785097904,\n",
              "  1.6013445735058454,\n",
              "  1.1715597420637607,\n",
              "  1.2534662333717612,\n",
              "  1.2676073151634049,\n",
              "  1.309600575274104,\n",
              "  1.292966945531582,\n",
              "  1.7658322811231006,\n",
              "  1.3564037533648712,\n",
              "  1.2407040781688483,\n",
              "  2.130217298173151,\n",
              "  1.4228319915327,\n",
              "  1.0651086490865755,\n",
              "  1.3008210311003705,\n",
              "  1.336545951796433,\n",
              "  0.8927754224911278,\n",
              "  1.4494292838262302,\n",
              "  1.4052738287907582,\n",
              "  1.6421697097891788,\n",
              "  1.2329833804288621,\n",
              "  1.19042665178928,\n",
              "  1.1682948223612457,\n",
              "  1.1518314137121108,\n",
              "  0.9607802401865855,\n",
              "  2.317439190074449,\n",
              "  1.0591147430338594,\n",
              "  1.4308630919602832,\n",
              "  0.7535680705496237,\n",
              "  0.8608283307581511,\n",
              "  1.2776122636975893,\n",
              "  1.3745862957220916,\n",
              "  1.259546137598783,\n",
              "  1.2978813187979172,\n",
              "  1.2412170838050638,\n",
              "  1.6009469708743893,\n",
              "  1.3149369953539032,\n",
              "  1.417901703622935,\n",
              "  1.2478669653497139,\n",
              "  1.1055812783082735,\n",
              "  0.9561307405997607,\n",
              "  0.9487783503683882,\n",
              "  1.1238565871041026,\n",
              "  1.2058356273089446,\n",
              "  1.2801012827406097,\n",
              "  0.8733100751144249,\n",
              "  0.9194732501297403,\n",
              "  1.6425573339441792,\n",
              "  1.085826790250066,\n",
              "  1.0639125693728595,\n",
              "  1.0875842666474016,\n",
              "  1.417901703622935,\n",
              "  1.550443891425932,\n",
              "  0.7825779328716171,\n",
              "  1.4690612745308145,\n",
              "  1.053086721720641,\n",
              "  1.2676073151634049,\n",
              "  0.7744003006005755,\n",
              "  1.3787482149724068,\n",
              "  1.363892581861956,\n",
              "  1.299352006316543,\n",
              "  1.2870449283923413,\n",
              "  1.11817763925502,\n",
              "  0.9474354220939228,\n",
              "  1.5218484589055707,\n",
              "  1.3526437911676632,\n",
              "  1.1556938532445284,\n",
              "  1.6013445735058454,\n",
              "  1.274619025074578,\n",
              "  1.422384489715834,\n",
              "  1.3408259533459403,\n",
              "  1.172646028567008,\n",
              "  1.1490645795125545,\n",
              "  1.459060149136146,\n",
              "  1.2483770274864237,\n",
              "  1.336545951796433,\n",
              "  0.9601174044814821,\n",
              "  1.4867225193896279,\n",
              "  1.4277452542806772,\n",
              "  1.35028849808504,\n",
              "  0.7560982446653928,\n",
              "  1.259040600296622,\n",
              "  1.13456827900627,\n",
              "  1.6549133695530214,\n",
              "  1.1204526724091788,\n",
              "  1.1176081573544434,\n",
              "  0.9153095762832032,\n",
              "  1.1639273497938836,\n",
              "  1.3066806149514323,\n",
              "  1.1529362882239027,\n",
              "  1.3047303442899274,\n",
              "  1.3066806149514323],\n",
              " [1.3876393907268032,\n",
              "  1.2589897736666635,\n",
              "  0.7854232148325416,\n",
              "  0.9072011404871383,\n",
              "  1.2810138591413764,\n",
              "  1.1914056623672158,\n",
              "  1.0295103409399329,\n",
              "  1.5372005686323216,\n",
              "  1.8895340353043486,\n",
              "  1.220706416015174,\n",
              "  1.1283902993052688,\n",
              "  0.8973696540774816,\n",
              "  1.2902754077382714,\n",
              "  1.148830026959987,\n",
              "  0.7535375642008096,\n",
              "  1.6114245264756837,\n",
              "  1.2781022436431109,\n",
              "  1.2128702012701162,\n",
              "  1.5660313076555374,\n",
              "  1.4248888342313328,\n",
              "  1.2363626162391088,\n",
              "  0.6570849420891531,\n",
              "  1.6788939094735578,\n",
              "  1.0355477856570439,\n",
              "  0.8219190961900412,\n",
              "  0.7168683385609769,\n",
              "  0.9885209225740443,\n",
              "  1.8172642866279722,\n",
              "  1.5582098650606253,\n",
              "  0.8650338789700313,\n",
              "  1.519371122651826,\n",
              "  1.477638455423946,\n",
              "  1.3616222082781393,\n",
              "  1.6235457586117925,\n",
              "  1.2715838722598407,\n",
              "  1.2510440816593587]]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}