{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_CNN_B_five_layers_r_squared_jun_23_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/Qualificacao/Comparative_ANNs/PSD_histogram_CNN_B_five_layers_r_squared_jun_23_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from sklearn.metrics import r2_score\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZEvJvfoibE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22012518-d715-4b43-bcdf-f6075a23359d"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mahotas\n",
            "  Downloading mahotas-1.4.11-cp37-cp37m-manylinux2010_x86_64.whl (5.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7 MB 30.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.19.5)\n",
            "Installing collected packages: mahotas\n",
            "Successfully installed mahotas-1.4.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT"
      },
      "source": [
        "import mahotas.features.texture as mht\n",
        "import mahotas.features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VcTdaNVh9EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e9cfc2b-2ea4-48fd-da3f-81cc0bbf1e90"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_fev_2020'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 73 (delta 37), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n",
            "/content/marquesgabi_fev_2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v7SRrc8mH2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dd58b51-703c-4bee-e5a3-33e8a0c015af"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip'\n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Doutorado'...\n",
            "remote: Enumerating objects: 361, done.\u001b[K\n",
            "remote: Counting objects: 100% (111/111), done.\u001b[K\n",
            "remote: Compressing objects: 100% (110/110), done.\u001b[K\n",
            "remote: Total 361 (delta 38), reused 0 (delta 0), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (361/361), 165.33 MiB | 30.00 MiB/s, done.\n",
            "Resolving deltas: 100% (161/161), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[4] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHgqAnaFyCjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420bbabe-74cb-47b3-9de6-b76e4f293825"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (160/160), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 3 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 23.14 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from segment_filter_not_conclude import Segmenta  # got image provided segmented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5MN5a_v4np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a8e6791-56f4-48c4-c96b-1c9e37be4145"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     108   77.779152   83.769539  ...  112.827156  115.617287  117.630997\n",
            "1     178  119.169304  121.720497  ...  145.657501  148.665588  146.822372\n",
            "2     120    0.054444    0.654444  ...  118.911102  118.860008  121.654449\n",
            "3     131  167.028427  176.950684  ...    0.252375    0.527184    1.458482\n",
            "4     106  138.949097  140.595245  ...   36.273052   47.312569   49.625851\n",
            "5     101  134.352127  143.584366  ...    0.782178    0.185766    1.000000\n",
            "6     107  120.706528  126.153290  ...   77.054588   83.810989   93.601883\n",
            "7     107  233.626160  226.780334  ...   70.136429   68.023148   60.844002\n",
            "8     155  189.195892  176.486801  ...   54.658985   46.249741   42.515381\n",
            "9     189   95.666672   96.945129  ...  110.375854  113.183823  108.163246\n",
            "10    105  151.546692  170.106689  ...  183.768906  190.515564  198.737793\n",
            "11    136    0.836505    1.807094  ...  120.856407  111.673874  102.907440\n",
            "12    172  128.982697  131.496490  ...  162.514343  152.254730  145.615479\n",
            "13    194  162.472412  123.232742  ...    1.422893    0.175683    1.309491\n",
            "14    109  189.748322  192.829041  ...   65.339867   74.819626   84.026093\n",
            "15    164   45.076736   42.906601  ...  162.393219  168.194534  208.014877\n",
            "16    143  147.292526  151.530396  ...  192.308289  188.176483  185.837021\n",
            "17    108  153.106995  150.477356  ...  170.410156  171.231812  170.921814\n",
            "18    135   97.570854   92.845703  ...  105.871651  107.307541  119.943596\n",
            "19    139  113.485580  118.369904  ...  158.637222  176.920807  195.983292\n",
            "20    140  125.159996  119.719994  ...    0.440000    0.440000    1.440000\n",
            "21    140  136.319992  138.160004  ...  165.399994  177.959991  171.199997\n",
            "22    193  139.175125  135.451797  ...  193.018204  154.642532  128.518936\n",
            "23    143  118.834755  117.882927  ...  165.711426  164.536713  159.432571\n",
            "24    195  175.659210  179.879333  ...  131.246338  133.747955  135.617050\n",
            "25    163  112.834915  133.258423  ...    0.826339    0.959953    1.679514\n",
            "26    195  152.093002  153.827393  ...    1.436003    0.179698    1.307798\n",
            "27    120  152.055542  149.341125  ...  169.981110  168.354462  165.928894\n",
            "28    118   54.273487   60.086182  ...  128.929611  132.523697  133.575119\n",
            "29    153  173.664886  174.856125  ...  148.085480  127.876923  127.367035\n",
            "30    171  176.639923  179.599808  ...  124.214569  128.170761  133.956741\n",
            "31    130  133.387939  141.932785  ...  129.200958   99.016098   93.135391\n",
            "32    164  150.559769  150.732315  ...    1.424747    1.000000    1.000000\n",
            "33    136  127.730103  126.906578  ...    0.365917    0.483564    1.454152\n",
            "34    144  117.922836  118.496918  ...    0.510031    0.398920    1.426697\n",
            "35    127   89.929131   66.727074  ...  166.070129  166.677551  171.671539\n",
            "36    171  171.348251  153.748566  ...    1.356554    1.030300    0.875517\n",
            "37    152  117.729919  155.000000  ...  144.369110  165.639191  174.824799\n",
            "38    195    1.302669    1.312926  ...   93.397324  104.886047  111.046944\n",
            "39    112  155.500000  158.562500  ...  145.750000  174.312500  189.625000\n",
            "40    189  185.325119  164.754471  ...  131.390961  132.159119  131.956100\n",
            "41    166   27.867031   50.840466  ...  116.827972  116.821304  112.566116\n",
            "42    150  151.986847  163.944168  ...  167.610321  195.752350  197.016357\n",
            "43    147  208.065750  206.836761  ...   93.782318  118.687080  126.138321\n",
            "44    110  116.356689  124.057182  ...   31.022810    2.225124    2.778512\n",
            "45    173  106.250488  111.866852  ...  152.908875  162.854889  174.517761\n",
            "46    106  117.865799  118.046631  ...  162.383774  152.437531  140.996445\n",
            "47    167   50.014168   71.230202  ...    1.314246    8.282011   29.002081\n",
            "48    154   97.991745  115.380173  ...    1.000000    1.000000    0.553719\n",
            "49    107  146.081238  144.842346  ...   59.264652   62.124207   59.189972\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzpQ1Pz0fX5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0927f2c2-0041-4797-89ea-91b8396b59f7"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines\n",
        "# filename = 'model_ANN.pkl'\n",
        "filename = 'model_ANN_new.pkl'\n",
        "model = joblib.load(filename)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n!git clone https://github.com/ucfilho/MarquesGabi_Routines\\n%cd MarquesGabi_Routines\\n# filename = 'model_ANN.pkl'\\nfilename = 'model_ANN_new.pkl'\\nmodel = joblib.load(filename)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2emP4rNjQy",
        "outputId": "5a93e4dd-6718-4195-e306-908a839fa061"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 22.53 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "Description = '3 layers of Convolution: 32, 64, 128 '\n",
        "N1 = 200\n",
        "N2 = 10\n",
        "\n",
        "# make the CNN\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=32, kernel_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=64, kernel_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=(2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=512, kernel_size=(2,2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "#model.add(MaxPooling2D())\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=N1))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=N2))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbPQ1FSRG6A",
        "outputId": "a8336343-1c06-4e98-84f6-139f316012b2"
      },
      "source": [
        "\n",
        "# training the model\n",
        "r = model.fit(X_train, Y_train, validation_data=(X_test,Y_test), \n",
        "              epochs=200, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 51s 199ms/step - loss: 28.1963 - accuracy: 0.5632 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.7091 - accuracy: 0.4877 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.7021 - accuracy: 0.4899 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6898 - accuracy: 0.5042 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6989 - accuracy: 0.5049 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6953 - accuracy: 0.4910 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6921 - accuracy: 0.5570 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6930 - accuracy: 0.5085 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6943 - accuracy: 0.4751 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6912 - accuracy: 0.5045 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6941 - accuracy: 0.5007 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6929 - accuracy: 0.5185 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6921 - accuracy: 0.5356 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.6931 - accuracy: 0.5069 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6932 - accuracy: 0.5016 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6932 - accuracy: 0.4997 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6937 - accuracy: 0.4700 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6934 - accuracy: 0.4881 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6925 - accuracy: 0.5231 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.6932 - accuracy: 0.4992 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6936 - accuracy: 0.4782 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.6928 - accuracy: 0.5214 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6935 - accuracy: 0.4953 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6937 - accuracy: 0.4716 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6901 - accuracy: 0.5175 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6936 - accuracy: 0.4822 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6933 - accuracy: 0.4947 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6908 - accuracy: 0.5206 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6927 - accuracy: 0.5278 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6924 - accuracy: 0.5366 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6932 - accuracy: 0.5005 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6934 - accuracy: 0.4899 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6929 - accuracy: 0.5166 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6934 - accuracy: 0.4900 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6932 - accuracy: 0.4961 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6926 - accuracy: 0.5283 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6923 - accuracy: 0.5221 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6934 - accuracy: 0.4974 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6935 - accuracy: 0.4823 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6933 - accuracy: 0.4923 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.6931 - accuracy: 0.5039 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6934 - accuracy: 0.4906 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6938 - accuracy: 0.4655 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6878 - accuracy: 0.5106 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6925 - accuracy: 0.5355 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6933 - accuracy: 0.4976 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6935 - accuracy: 0.4863 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6937 - accuracy: 0.4788 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6934 - accuracy: 0.4924 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6891 - accuracy: 0.5356 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6873 - accuracy: 0.4956 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6820 - accuracy: 0.5176 - val_loss: 0.6887 - val_accuracy: 0.4966\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 1.0956 - accuracy: 0.5547 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6923 - accuracy: 0.5361 - val_loss: 0.6936 - val_accuracy: 0.4898\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.7214 - accuracy: 0.5092 - val_loss: 0.6936 - val_accuracy: 0.4898\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6924 - accuracy: 0.5037 - val_loss: 0.6936 - val_accuracy: 0.4898\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6964 - accuracy: 0.4848 - val_loss: 0.6936 - val_accuracy: 0.4898\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6928 - accuracy: 0.5139 - val_loss: 0.6936 - val_accuracy: 0.4898\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6921 - accuracy: 0.5385 - val_loss: 0.6936 - val_accuracy: 0.4898\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6932 - accuracy: 0.5031 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6933 - accuracy: 0.5391 - val_loss: 0.6936 - val_accuracy: 0.4898\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6930 - accuracy: 0.5086 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6936 - accuracy: 0.4891 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6933 - accuracy: 0.4963 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6929 - accuracy: 0.5121 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6931 - accuracy: 0.5051 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6936 - accuracy: 0.4839 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6927 - accuracy: 0.5232 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6929 - accuracy: 0.5124 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6922 - accuracy: 0.5467 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6924 - accuracy: 0.5341 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6930 - accuracy: 0.5110 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6932 - accuracy: 0.5010 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6923 - accuracy: 0.5454 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6930 - accuracy: 0.5110 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6939 - accuracy: 0.4623 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6935 - accuracy: 0.4817 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6935 - accuracy: 0.4824 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6926 - accuracy: 0.5360 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6936 - accuracy: 0.4800 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6939 - accuracy: 0.4614 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6928 - accuracy: 0.5229 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6933 - accuracy: 0.4960 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6928 - accuracy: 0.5217 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6930 - accuracy: 0.5088 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6937 - accuracy: 0.4756 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6929 - accuracy: 0.5177 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6932 - accuracy: 0.4994 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6939 - accuracy: 0.4647 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6930 - accuracy: 0.5092 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6930 - accuracy: 0.5107 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6925 - accuracy: 0.5370 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6925 - accuracy: 0.5374 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6932 - accuracy: 0.5002 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6928 - accuracy: 0.5196 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6930 - accuracy: 0.5111 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6932 - accuracy: 0.4998 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6923 - accuracy: 0.5512 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6934 - accuracy: 0.4892 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6933 - accuracy: 0.4946 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6933 - accuracy: 0.4916 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6925 - accuracy: 0.5420 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6923 - accuracy: 0.5510 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6934 - accuracy: 0.4891 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.6931 - accuracy: 0.5031 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6933 - accuracy: 0.4929 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6929 - accuracy: 0.5194 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6930 - accuracy: 0.5089 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6929 - accuracy: 0.5200 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6932 - accuracy: 0.4989 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6933 - accuracy: 0.4935 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6933 - accuracy: 0.4919 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6931 - accuracy: 0.5050 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6933 - accuracy: 0.4907 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6938 - accuracy: 0.4564 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6932 - accuracy: 0.4961 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6931 - accuracy: 0.5040 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6932 - accuracy: 0.5010 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6938 - accuracy: 0.4607 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6927 - accuracy: 0.5341 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6936 - accuracy: 0.4766 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6931 - accuracy: 0.5022 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6936 - accuracy: 0.4787 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6929 - accuracy: 0.5160 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6933 - accuracy: 0.4915 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6930 - accuracy: 0.5126 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6931 - accuracy: 0.5049 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6934 - accuracy: 0.4876 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6927 - accuracy: 0.5335 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6930 - accuracy: 0.5120 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6934 - accuracy: 0.4899 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6934 - accuracy: 0.4872 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6925 - accuracy: 0.5427 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6938 - accuracy: 0.4670 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6935 - accuracy: 0.4812 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6935 - accuracy: 0.4734 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6932 - accuracy: 0.4990 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6930 - accuracy: 0.5099 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6931 - accuracy: 0.5056 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6933 - accuracy: 0.4930 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6928 - accuracy: 0.5226 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6933 - accuracy: 0.4950 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6934 - accuracy: 0.4877 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6929 - accuracy: 0.5145 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6936 - accuracy: 0.4800 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6922 - accuracy: 0.5609 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6928 - accuracy: 0.5187 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6934 - accuracy: 0.4916 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6931 - accuracy: 0.5068 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6924 - accuracy: 0.5493 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6934 - accuracy: 0.4874 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6925 - accuracy: 0.5358 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6934 - accuracy: 0.4888 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6930 - accuracy: 0.5139 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6923 - accuracy: 0.5555 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6932 - accuracy: 0.4987 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6930 - accuracy: 0.5100 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6927 - accuracy: 0.5257 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6928 - accuracy: 0.5210 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6936 - accuracy: 0.4811 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6924 - accuracy: 0.5410 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6932 - accuracy: 0.5009 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6933 - accuracy: 0.4963 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6930 - accuracy: 0.5112 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6932 - accuracy: 0.4985 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6926 - accuracy: 0.5290 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 1s 53ms/step - loss: 0.6927 - accuracy: 0.5239 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6932 - accuracy: 0.4998 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6930 - accuracy: 0.5104 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6931 - accuracy: 0.5064 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6925 - accuracy: 0.5356 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6931 - accuracy: 0.5068 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6935 - accuracy: 0.4867 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6928 - accuracy: 0.5183 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6932 - accuracy: 0.5020 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6933 - accuracy: 0.4944 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6930 - accuracy: 0.5100 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6935 - accuracy: 0.4859 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6938 - accuracy: 0.4716 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6926 - accuracy: 0.5321 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6926 - accuracy: 0.5293 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6933 - accuracy: 0.4961 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6930 - accuracy: 0.5124 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6935 - accuracy: 0.4880 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6932 - accuracy: 0.5024 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6929 - accuracy: 0.5139 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6925 - accuracy: 0.5368 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 1s 51ms/step - loss: 0.6940 - accuracy: 0.4669 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6935 - accuracy: 0.4815 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 1s 52ms/step - loss: 0.6926 - accuracy: 0.5371 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6927 - accuracy: 0.5225 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 1s 48ms/step - loss: 0.6931 - accuracy: 0.5038 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 1s 49ms/step - loss: 0.6930 - accuracy: 0.5094 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6932 - accuracy: 0.5005 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 1s 50ms/step - loss: 0.6931 - accuracy: 0.5040 - val_loss: 0.6934 - val_accuracy: 0.4898\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpf0XlSARX78",
        "outputId": "2cf3d6df-9888-4c38-8623-b57c4e44a9ce"
      },
      "source": [
        "pred_test= model.predict_classes(X_test)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predict   0\n",
            "Actual     \n",
            "0        72\n",
            "1        75\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iFNNrlWV9tH",
        "outputId": "81d97cad-52a1-4d90-f1c8-a316344da092"
      },
      "source": [
        "pred_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QISvYcJBgWbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c369edb-da64-46ed-b3f7-abd9b376f998"
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[0] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  prediction = model.predict_classes(result)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0   155.0   93.947479   91.124756  ...  155.870850  131.065445  101.228271\n",
            "1   176.0   45.077995   47.535126  ...   84.467461   86.663734   86.839355\n",
            "2   134.0   93.528854   94.745155  ...  119.065712  121.515038  123.442856\n",
            "3   107.0   59.896500   58.935890  ...  173.585037  173.229370  171.274963\n",
            "4   152.0   95.075493   94.808159  ...   39.364956   40.569252   66.619804\n",
            "5   181.0   69.356277   65.302559  ...  104.417877  106.559021  105.024704\n",
            "6   166.0  101.481186  113.536072  ...  104.727386  100.819412  100.761932\n",
            "7   143.0   58.368916   59.049438  ...   10.192528    1.872610    0.889090\n",
            "8   152.0   65.108032   64.209145  ...  101.923134  107.261765  106.858719\n",
            "9   161.0  105.308128  104.699440  ...    2.818526    1.608696    1.279773\n",
            "10  152.0  107.265923   98.860809  ...  117.608719  108.867035   94.722298\n",
            "11  114.0   74.146500   72.806404  ...    0.000000    0.000000    0.000000\n",
            "12  107.0   84.178268   91.056595  ...   95.887070   92.626076   92.254341\n",
            "13  173.0   49.085964   53.711113  ...    1.358014    0.235658    0.609877\n",
            "14  102.0   74.019997   83.960014  ...   82.437912   80.192238   74.571320\n",
            "15  157.0   86.383102   93.861771  ...    6.569963    6.545296    6.694511\n",
            "16  114.0   45.749153   42.748844  ...   14.609111   10.799323    9.139735\n",
            "17  185.0   95.884026  104.076813  ...   20.093880   11.322600    0.719065\n",
            "18  151.0   83.829704   87.500237  ...    6.960791    6.445024    6.625543\n",
            "19  171.0  103.922119  109.524879  ...    3.017612    1.634212    1.235252\n",
            "20  117.0   69.031342   67.864128  ...  178.843094  156.845261  144.670685\n",
            "21  129.0   55.068443   52.556335  ...   58.119946   57.926445   77.070969\n",
            "22  124.0   52.878250   53.930279  ...    8.294485    8.200832    7.672216\n",
            "23  128.0  107.170898  108.347656  ...   81.860352   81.148438   83.035156\n",
            "24  107.0   79.623116   80.234001  ...    8.641541    8.952922    8.867325\n",
            "25  126.0    0.925926    0.827160  ...   50.666668   51.172840   53.395065\n",
            "26  139.0    7.801719    7.727705  ...  103.840378  105.789551  107.644844\n",
            "27  183.0  101.724091  103.917419  ...    3.083460    1.623757    1.178895\n",
            "28  199.0   96.846565   99.164932  ...    5.071942    5.239489    5.383728\n",
            "29  176.0  109.450409  115.328514  ...   24.988119   13.130680    1.402376\n",
            "30  172.0   91.615471  101.369934  ...  176.406708  185.376984  179.276901\n",
            "31  182.0  105.929001   91.088768  ...    1.686391    0.710059    0.698225\n",
            "32  183.0   97.030991   98.094330  ...    9.583535    6.626086    6.189644\n",
            "33  156.0  102.979630  105.783051  ...   30.423405   25.191322   19.445103\n",
            "34  166.0  112.132233  115.384949  ...   49.950066   51.839886   50.069237\n",
            "35  165.0   56.060020   56.656166  ...   11.053516    3.389752    0.599596\n",
            "36  109.0   54.434387   56.348873  ...   94.990479   93.241135   93.325302\n",
            "37  177.0   10.548437    2.100993  ...   45.030960   41.950363   27.852880\n",
            "38  126.0   82.086418   81.222229  ...   42.790123   42.370369   43.506172\n",
            "39  178.0    8.127762   12.641840  ...   85.041168   82.726181   82.852936\n",
            "40  136.0  115.505196  117.740501  ...   74.261246   75.352943   74.504333\n",
            "41  166.0   50.996223   60.650742  ...   81.826530   77.025543   67.376244\n",
            "42  175.0  112.854393  107.155197  ...   23.011198   37.079998   54.270397\n",
            "43  105.0   79.497787   80.333336  ...   54.293339   45.973339   36.328896\n",
            "44  197.0   73.953011   73.277039  ...  115.251083  122.150360  137.895935\n",
            "45  104.0   43.678993   43.841721  ...  120.479309  106.250008   96.875748\n",
            "46  176.0  225.285126  228.865189  ...   59.296486   59.710739   63.643593\n",
            "47  190.0  114.714790  107.224037  ...   42.949802   26.403654    2.976288\n",
            "48  135.0   88.918732   82.263260  ...   34.569214   30.562853   24.035883\n",
            "49  147.0   12.099773    5.165533  ...   63.569164   53.387756   47.242634\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRbWgmX_LFH",
        "outputId": "9e8c5893-f655-4d7f-b98e-f7ff5d78a2be"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "from GetBetterSegm import GetBetter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_paper_fev_2021'...\n",
            "remote: Enumerating objects: 650, done.\u001b[K\n",
            "remote: Counting objects: 100% (411/411), done.\u001b[K\n",
            "remote: Compressing objects: 100% (409/409), done.\u001b[K\n",
            "remote: Total 650 (delta 256), reused 0 (delta 0), pack-reused 239\u001b[K\n",
            "Receiving objects: 100% (650/650), 5.41 MiB | 13.86 MiB/s, done.\n",
            "Resolving deltas: 100% (393/393), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAG_I6FwCvFr",
        "outputId": "a9d0a7bb-028d-4cbc-d329-08f76a67cf87"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "%cd marquesgabi_out_2020\n",
        "PSD_imageJ = 'Areas_ImageJ.csv'\n",
        "PSD_new = pd.read_csv(PSD_imageJ)\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_out_2020'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 146 (delta 75), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (146/146), 1.00 MiB | 12.23 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020\n",
            "   Juntas   Area\n",
            "0       1  2.001\n",
            "1       2  0.820\n",
            "2       3  1.270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "PekBHQOT_6CP",
        "outputId": "cffac761-c183-4b8e-e272-2e588799ecf2"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>155.0</td>\n",
              "      <td>93.947479</td>\n",
              "      <td>91.124756</td>\n",
              "      <td>90.311386</td>\n",
              "      <td>89.152679</td>\n",
              "      <td>87.730118</td>\n",
              "      <td>84.029060</td>\n",
              "      <td>95.181190</td>\n",
              "      <td>101.519501</td>\n",
              "      <td>100.370537</td>\n",
              "      <td>87.022270</td>\n",
              "      <td>56.741608</td>\n",
              "      <td>23.561749</td>\n",
              "      <td>8.850739</td>\n",
              "      <td>2.890364</td>\n",
              "      <td>1.227305</td>\n",
              "      <td>0.704516</td>\n",
              "      <td>0.396753</td>\n",
              "      <td>0.360874</td>\n",
              "      <td>0.115796</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010489</td>\n",
              "      <td>0.603538</td>\n",
              "      <td>1.244745</td>\n",
              "      <td>0.696816</td>\n",
              "      <td>1.680666</td>\n",
              "      <td>1.139979</td>\n",
              "      <td>91.326630</td>\n",
              "      <td>92.203209</td>\n",
              "      <td>94.457901</td>\n",
              "      <td>95.135284</td>\n",
              "      <td>92.909134</td>\n",
              "      <td>90.477104</td>\n",
              "      <td>93.901939</td>\n",
              "      <td>103.464905</td>\n",
              "      <td>111.394714</td>\n",
              "      <td>110.062775</td>\n",
              "      <td>100.279266</td>\n",
              "      <td>...</td>\n",
              "      <td>107.763039</td>\n",
              "      <td>112.018532</td>\n",
              "      <td>117.332863</td>\n",
              "      <td>124.993393</td>\n",
              "      <td>136.927460</td>\n",
              "      <td>145.729874</td>\n",
              "      <td>144.812592</td>\n",
              "      <td>149.328964</td>\n",
              "      <td>157.254913</td>\n",
              "      <td>160.048737</td>\n",
              "      <td>142.973297</td>\n",
              "      <td>112.324135</td>\n",
              "      <td>67.671471</td>\n",
              "      <td>67.768372</td>\n",
              "      <td>75.516174</td>\n",
              "      <td>80.421982</td>\n",
              "      <td>84.323174</td>\n",
              "      <td>85.458649</td>\n",
              "      <td>83.714966</td>\n",
              "      <td>80.724083</td>\n",
              "      <td>76.618683</td>\n",
              "      <td>78.608124</td>\n",
              "      <td>81.041046</td>\n",
              "      <td>80.413452</td>\n",
              "      <td>80.702484</td>\n",
              "      <td>73.038216</td>\n",
              "      <td>59.531288</td>\n",
              "      <td>100.723785</td>\n",
              "      <td>106.091873</td>\n",
              "      <td>109.618774</td>\n",
              "      <td>115.457657</td>\n",
              "      <td>126.073593</td>\n",
              "      <td>135.948395</td>\n",
              "      <td>141.652054</td>\n",
              "      <td>138.628433</td>\n",
              "      <td>132.281464</td>\n",
              "      <td>148.410034</td>\n",
              "      <td>155.870850</td>\n",
              "      <td>131.065445</td>\n",
              "      <td>101.228271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>176.0</td>\n",
              "      <td>45.077995</td>\n",
              "      <td>47.535126</td>\n",
              "      <td>58.805782</td>\n",
              "      <td>75.419418</td>\n",
              "      <td>87.464874</td>\n",
              "      <td>91.532539</td>\n",
              "      <td>94.411156</td>\n",
              "      <td>90.193695</td>\n",
              "      <td>86.424080</td>\n",
              "      <td>83.565598</td>\n",
              "      <td>83.780991</td>\n",
              "      <td>84.907539</td>\n",
              "      <td>91.149788</td>\n",
              "      <td>94.155983</td>\n",
              "      <td>93.966934</td>\n",
              "      <td>89.601234</td>\n",
              "      <td>85.857956</td>\n",
              "      <td>83.086769</td>\n",
              "      <td>84.445763</td>\n",
              "      <td>81.489670</td>\n",
              "      <td>76.851761</td>\n",
              "      <td>71.121895</td>\n",
              "      <td>61.267563</td>\n",
              "      <td>57.176136</td>\n",
              "      <td>63.798553</td>\n",
              "      <td>68.549072</td>\n",
              "      <td>72.342964</td>\n",
              "      <td>73.894104</td>\n",
              "      <td>51.878101</td>\n",
              "      <td>61.814045</td>\n",
              "      <td>80.175629</td>\n",
              "      <td>90.320763</td>\n",
              "      <td>96.150826</td>\n",
              "      <td>96.000519</td>\n",
              "      <td>96.362091</td>\n",
              "      <td>92.741730</td>\n",
              "      <td>89.806305</td>\n",
              "      <td>90.309402</td>\n",
              "      <td>90.827988</td>\n",
              "      <td>...</td>\n",
              "      <td>104.212288</td>\n",
              "      <td>105.381195</td>\n",
              "      <td>103.617249</td>\n",
              "      <td>101.044922</td>\n",
              "      <td>102.027367</td>\n",
              "      <td>108.271172</td>\n",
              "      <td>117.722626</td>\n",
              "      <td>121.651337</td>\n",
              "      <td>108.616211</td>\n",
              "      <td>86.970047</td>\n",
              "      <td>91.174065</td>\n",
              "      <td>93.457115</td>\n",
              "      <td>52.734501</td>\n",
              "      <td>56.799068</td>\n",
              "      <td>59.082127</td>\n",
              "      <td>57.904961</td>\n",
              "      <td>54.403923</td>\n",
              "      <td>60.951447</td>\n",
              "      <td>65.779953</td>\n",
              "      <td>87.974686</td>\n",
              "      <td>100.711258</td>\n",
              "      <td>104.335747</td>\n",
              "      <td>109.127060</td>\n",
              "      <td>109.927170</td>\n",
              "      <td>110.603821</td>\n",
              "      <td>113.227783</td>\n",
              "      <td>111.650299</td>\n",
              "      <td>109.987091</td>\n",
              "      <td>113.127060</td>\n",
              "      <td>112.543900</td>\n",
              "      <td>111.559914</td>\n",
              "      <td>108.399796</td>\n",
              "      <td>101.166321</td>\n",
              "      <td>106.592972</td>\n",
              "      <td>116.275826</td>\n",
              "      <td>120.300095</td>\n",
              "      <td>104.063530</td>\n",
              "      <td>84.467461</td>\n",
              "      <td>86.663734</td>\n",
              "      <td>86.839355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>134.0</td>\n",
              "      <td>93.528854</td>\n",
              "      <td>94.745155</td>\n",
              "      <td>95.823799</td>\n",
              "      <td>97.385834</td>\n",
              "      <td>94.281136</td>\n",
              "      <td>88.558266</td>\n",
              "      <td>91.489197</td>\n",
              "      <td>104.861000</td>\n",
              "      <td>113.179779</td>\n",
              "      <td>121.268440</td>\n",
              "      <td>127.764099</td>\n",
              "      <td>131.944534</td>\n",
              "      <td>130.787476</td>\n",
              "      <td>125.387619</td>\n",
              "      <td>108.355530</td>\n",
              "      <td>88.773895</td>\n",
              "      <td>83.777908</td>\n",
              "      <td>84.343735</td>\n",
              "      <td>85.311432</td>\n",
              "      <td>83.626869</td>\n",
              "      <td>82.007126</td>\n",
              "      <td>80.309647</td>\n",
              "      <td>80.449989</td>\n",
              "      <td>74.853645</td>\n",
              "      <td>73.157944</td>\n",
              "      <td>73.812881</td>\n",
              "      <td>74.685234</td>\n",
              "      <td>75.837822</td>\n",
              "      <td>96.997772</td>\n",
              "      <td>98.721100</td>\n",
              "      <td>97.194252</td>\n",
              "      <td>96.449326</td>\n",
              "      <td>99.337494</td>\n",
              "      <td>98.527061</td>\n",
              "      <td>97.654938</td>\n",
              "      <td>105.274002</td>\n",
              "      <td>112.805527</td>\n",
              "      <td>121.669418</td>\n",
              "      <td>128.829819</td>\n",
              "      <td>...</td>\n",
              "      <td>110.405220</td>\n",
              "      <td>108.157051</td>\n",
              "      <td>112.108269</td>\n",
              "      <td>113.908218</td>\n",
              "      <td>112.885727</td>\n",
              "      <td>112.698158</td>\n",
              "      <td>117.957237</td>\n",
              "      <td>121.195374</td>\n",
              "      <td>119.698822</td>\n",
              "      <td>120.539101</td>\n",
              "      <td>124.363327</td>\n",
              "      <td>126.794617</td>\n",
              "      <td>90.585434</td>\n",
              "      <td>88.452217</td>\n",
              "      <td>85.456009</td>\n",
              "      <td>80.089111</td>\n",
              "      <td>76.043221</td>\n",
              "      <td>73.046112</td>\n",
              "      <td>72.308090</td>\n",
              "      <td>72.344620</td>\n",
              "      <td>73.503677</td>\n",
              "      <td>76.818008</td>\n",
              "      <td>80.018715</td>\n",
              "      <td>98.224106</td>\n",
              "      <td>109.857437</td>\n",
              "      <td>114.628654</td>\n",
              "      <td>111.957672</td>\n",
              "      <td>106.173317</td>\n",
              "      <td>107.141685</td>\n",
              "      <td>107.110489</td>\n",
              "      <td>107.693039</td>\n",
              "      <td>110.229889</td>\n",
              "      <td>112.461563</td>\n",
              "      <td>113.545563</td>\n",
              "      <td>117.357101</td>\n",
              "      <td>118.077972</td>\n",
              "      <td>117.948318</td>\n",
              "      <td>119.065712</td>\n",
              "      <td>121.515038</td>\n",
              "      <td>123.442856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>107.0</td>\n",
              "      <td>59.896500</td>\n",
              "      <td>58.935890</td>\n",
              "      <td>57.409466</td>\n",
              "      <td>52.052666</td>\n",
              "      <td>41.661976</td>\n",
              "      <td>66.513229</td>\n",
              "      <td>87.912224</td>\n",
              "      <td>99.820328</td>\n",
              "      <td>107.160805</td>\n",
              "      <td>110.299240</td>\n",
              "      <td>110.442139</td>\n",
              "      <td>107.627914</td>\n",
              "      <td>103.397240</td>\n",
              "      <td>99.457336</td>\n",
              "      <td>99.221588</td>\n",
              "      <td>99.747581</td>\n",
              "      <td>101.535851</td>\n",
              "      <td>102.844795</td>\n",
              "      <td>105.939125</td>\n",
              "      <td>107.425720</td>\n",
              "      <td>108.597603</td>\n",
              "      <td>109.187363</td>\n",
              "      <td>107.026817</td>\n",
              "      <td>108.830986</td>\n",
              "      <td>108.850555</td>\n",
              "      <td>105.320457</td>\n",
              "      <td>102.657700</td>\n",
              "      <td>105.743729</td>\n",
              "      <td>63.864182</td>\n",
              "      <td>64.153641</td>\n",
              "      <td>63.820683</td>\n",
              "      <td>57.843391</td>\n",
              "      <td>44.835529</td>\n",
              "      <td>45.116257</td>\n",
              "      <td>80.083237</td>\n",
              "      <td>97.191109</td>\n",
              "      <td>102.864708</td>\n",
              "      <td>103.157486</td>\n",
              "      <td>105.483276</td>\n",
              "      <td>...</td>\n",
              "      <td>152.423447</td>\n",
              "      <td>153.192413</td>\n",
              "      <td>155.151978</td>\n",
              "      <td>154.581284</td>\n",
              "      <td>154.845917</td>\n",
              "      <td>155.953445</td>\n",
              "      <td>159.185074</td>\n",
              "      <td>161.688538</td>\n",
              "      <td>163.226044</td>\n",
              "      <td>161.515152</td>\n",
              "      <td>160.095459</td>\n",
              "      <td>159.551147</td>\n",
              "      <td>58.132061</td>\n",
              "      <td>55.190319</td>\n",
              "      <td>58.214951</td>\n",
              "      <td>58.066643</td>\n",
              "      <td>57.286488</td>\n",
              "      <td>57.575775</td>\n",
              "      <td>59.487995</td>\n",
              "      <td>67.578651</td>\n",
              "      <td>105.212936</td>\n",
              "      <td>130.675430</td>\n",
              "      <td>137.196518</td>\n",
              "      <td>138.018600</td>\n",
              "      <td>134.831787</td>\n",
              "      <td>133.147964</td>\n",
              "      <td>136.114777</td>\n",
              "      <td>142.584152</td>\n",
              "      <td>146.912567</td>\n",
              "      <td>147.663025</td>\n",
              "      <td>150.380554</td>\n",
              "      <td>154.559265</td>\n",
              "      <td>157.294708</td>\n",
              "      <td>159.656113</td>\n",
              "      <td>165.472092</td>\n",
              "      <td>169.600479</td>\n",
              "      <td>172.063858</td>\n",
              "      <td>173.585037</td>\n",
              "      <td>173.229370</td>\n",
              "      <td>171.274963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>152.0</td>\n",
              "      <td>95.075493</td>\n",
              "      <td>94.808159</td>\n",
              "      <td>88.219528</td>\n",
              "      <td>75.263847</td>\n",
              "      <td>59.482685</td>\n",
              "      <td>55.363571</td>\n",
              "      <td>55.962601</td>\n",
              "      <td>57.328255</td>\n",
              "      <td>58.261776</td>\n",
              "      <td>57.989609</td>\n",
              "      <td>56.167583</td>\n",
              "      <td>55.117035</td>\n",
              "      <td>57.307476</td>\n",
              "      <td>58.394043</td>\n",
              "      <td>60.375343</td>\n",
              "      <td>61.085869</td>\n",
              "      <td>67.386421</td>\n",
              "      <td>73.182823</td>\n",
              "      <td>77.517311</td>\n",
              "      <td>80.611488</td>\n",
              "      <td>92.096260</td>\n",
              "      <td>104.535316</td>\n",
              "      <td>112.831024</td>\n",
              "      <td>115.657196</td>\n",
              "      <td>120.306793</td>\n",
              "      <td>128.715363</td>\n",
              "      <td>129.672424</td>\n",
              "      <td>117.229218</td>\n",
              "      <td>95.013855</td>\n",
              "      <td>94.591408</td>\n",
              "      <td>89.503464</td>\n",
              "      <td>80.101112</td>\n",
              "      <td>57.990303</td>\n",
              "      <td>55.275620</td>\n",
              "      <td>55.510380</td>\n",
              "      <td>55.873955</td>\n",
              "      <td>55.051247</td>\n",
              "      <td>55.550549</td>\n",
              "      <td>53.768700</td>\n",
              "      <td>...</td>\n",
              "      <td>76.862892</td>\n",
              "      <td>79.420364</td>\n",
              "      <td>83.409279</td>\n",
              "      <td>86.480614</td>\n",
              "      <td>87.565094</td>\n",
              "      <td>89.984070</td>\n",
              "      <td>88.318558</td>\n",
              "      <td>27.880194</td>\n",
              "      <td>28.410667</td>\n",
              "      <td>34.445984</td>\n",
              "      <td>47.481300</td>\n",
              "      <td>67.412735</td>\n",
              "      <td>70.362885</td>\n",
              "      <td>77.643356</td>\n",
              "      <td>78.078255</td>\n",
              "      <td>73.638504</td>\n",
              "      <td>77.721611</td>\n",
              "      <td>80.068558</td>\n",
              "      <td>81.695282</td>\n",
              "      <td>80.673820</td>\n",
              "      <td>77.801933</td>\n",
              "      <td>82.722305</td>\n",
              "      <td>83.253464</td>\n",
              "      <td>75.917587</td>\n",
              "      <td>67.584480</td>\n",
              "      <td>66.242386</td>\n",
              "      <td>69.461220</td>\n",
              "      <td>72.261078</td>\n",
              "      <td>75.141968</td>\n",
              "      <td>78.438362</td>\n",
              "      <td>82.469528</td>\n",
              "      <td>83.535316</td>\n",
              "      <td>84.959137</td>\n",
              "      <td>84.873268</td>\n",
              "      <td>77.702209</td>\n",
              "      <td>23.801939</td>\n",
              "      <td>30.641968</td>\n",
              "      <td>39.364956</td>\n",
              "      <td>40.569252</td>\n",
              "      <td>66.619804</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Width          0          1  ...         781         782         783\n",
              "0  155.0  93.947479  91.124756  ...  155.870850  131.065445  101.228271\n",
              "1  176.0  45.077995  47.535126  ...   84.467461   86.663734   86.839355\n",
              "2  134.0  93.528854  94.745155  ...  119.065712  121.515038  123.442856\n",
              "3  107.0  59.896500  58.935890  ...  173.585037  173.229370  171.274963\n",
              "4  152.0  95.075493  94.808159  ...   39.364956   40.569252   66.619804\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "Area = np.array(PSD_new['Area'])\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J705kDqsE8f",
        "outputId": "a5d03f8d-601d-4727-dbb0-1dfbb3f3e98e"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(490, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wCFDX8esLoQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn-F050Hr9Ui"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Vfk_fNXGDK5_",
        "outputId": "36681fce-8b09-4d7a-c0fb-2427bcbb880d"
      },
      "source": [
        " wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        " wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        " X = pd.DataFrame([Diam1,Diameter_All])\n",
        " wts = pd.DataFrame([wt1,wt2])\n",
        "plt.hist(X,weights=wts)\n",
        "plt.legend(['Image J','CNN'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f59cb513f90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nGDbBEeiUij",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "9b9c39ac-bfaa-4102-c98d-ced4d5b937c8"
      },
      "source": [
        "# plt.hist(x, bins=bins, density=True, histtype='step', cumulative=-1,label='Reversed emp.')\n",
        "plt.hist(X, density=True, histtype='step', cumulative=True,label='Reversed emp.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.04210526, 0.14736842, 0.33684211, 0.66315789, 0.87368421,\n",
              "         0.96842105, 0.97894737, 0.97894737, 0.98947368, 1.        ],\n",
              "        [0.04      , 0.14      , 0.38      , 0.58      , 0.78      ,\n",
              "         0.9       , 0.98      , 1.        , 1.        , 1.        ]]),\n",
              " array([0.66820915, 0.83313215, 0.99805516, 1.16297816, 1.32790116,\n",
              "        1.49282417, 1.65774717, 1.82267018, 1.98759318, 2.15251619,\n",
              "        2.31743919]),\n",
              " <a list of 2 Lists of Patches objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "9xENlBUUxfTu",
        "outputId": "5d67258a-3de8-48fa-b777-f6ed4f0bc960"
      },
      "source": [
        "Obj = plt.hist(X, density=True, histtype='step', cumulative=True,label='Reversed emp.')\n",
        "Y1, Y2 = Obj[0]\n",
        "Rsquared = r2_score(Y1, Y2)\n",
        "print('r_squared =',Rsquared)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r_squared = 0.9824644348262345\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2XboMiFbkaa"
      },
      "source": [
        "acc_train = r.history['accuracy'][-1]\n",
        "acc_test = r.history['val_accuracy'][-1]\n",
        "loss_train = r.history['loss'][-1]\n",
        "loss_test = r.history['val_loss'][-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euTd_-CYN1v0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f720aaca-7d85-4b2f-ff2f-a96e959da95f"
      },
      "source": [
        "df = pd.DataFrame({'N1':N1, 'N2':N2,'R^2':Rsquared,\n",
        "                   'acc train':acc_train,'acc test':acc_test,\n",
        "                   'loss train':loss_train,'loss test':loss_test,\n",
        "                   'Details':Description},\n",
        "                  index= [0])\n",
        "Arq = \"output.xlsx\"\n",
        "df.to_excel(Arq)\n",
        "files.download(Arq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_a8df2ad5-6320-427d-bcfc-0bed5789b903\", \"output.xlsx\", 5157)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "-KukfpGTTKlj",
        "outputId": "73abe071-74b2-401b-b00b-d3b3315d9ffa"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>N1</th>\n",
              "      <th>N2</th>\n",
              "      <th>R^2</th>\n",
              "      <th>acc train</th>\n",
              "      <th>acc test</th>\n",
              "      <th>loss train</th>\n",
              "      <th>loss test</th>\n",
              "      <th>Details</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>200</td>\n",
              "      <td>10</td>\n",
              "      <td>0.982464</td>\n",
              "      <td>0.504373</td>\n",
              "      <td>0.489796</td>\n",
              "      <td>0.693111</td>\n",
              "      <td>0.69338</td>\n",
              "      <td>3 layers of Convolution: 32, 64, 128</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    N1  N2  ...  loss test                                Details\n",
              "0  200  10  ...    0.69338  3 layers of Convolution: 32, 64, 128 \n",
              "\n",
              "[1 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "ZZHa1j4HT9Dq",
        "outputId": "257a7598-f060-4d6a-dc98-6649444bef4b"
      },
      "source": [
        "counts, bins, bars = plt.hist(X,weights=wts)\n",
        "print(bars)\n",
        "print(bins)\n",
        "print(counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<a list of 2 Lists of Patches objects>\n",
            "[0.66820915 0.83313215 0.99805516 1.16297816 1.32790116 1.49282417\n",
            " 1.65774717 1.82267018 1.98759318 2.15251619 2.31743919]\n",
            "[[ 4.21052632 10.52631579 18.94736842 32.63157895 21.05263158  9.47368421\n",
            "   1.05263158  0.          1.05263158  1.05263158]\n",
            " [ 4.         10.         24.         20.         20.         12.\n",
            "   8.          2.          0.          0.        ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o_vDGeWUwIZ",
        "outputId": "8d6ef65d-e60e-4c75-ad31-0a11a53ede81"
      },
      "source": [
        "print(counts.sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200.0000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "KcH52-6iJQ8t",
        "outputId": "e9555c4d-d521-4778-876e-041e618e1241"
      },
      "source": [
        "\n",
        "plt.hist([Diam1,Diameter_All])\n",
        "plt.legend(['Image J','CNN'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f596e141410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r11AxFK_JIii",
        "outputId": "86af5771-53a6-4cd1-ff5b-162392c5ad4a"
      },
      "source": [
        "[Diam1,Diameter_All]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.59616801403081,\n",
              "  1.0217907939900581,\n",
              "  1.2716187407449044,\n",
              "  1.104429030701514,\n",
              "  1.2163487785097904,\n",
              "  1.6013445735058454,\n",
              "  1.1715597420637607,\n",
              "  1.2534662333717612,\n",
              "  1.2676073151634049,\n",
              "  1.309600575274104,\n",
              "  1.292966945531582,\n",
              "  1.7658322811231006,\n",
              "  1.3564037533648712,\n",
              "  1.2407040781688483,\n",
              "  2.130217298173151,\n",
              "  1.4228319915327,\n",
              "  1.0651086490865755,\n",
              "  1.3008210311003705,\n",
              "  1.336545951796433,\n",
              "  0.8927754224911278,\n",
              "  1.4494292838262302,\n",
              "  1.4052738287907582,\n",
              "  1.6421697097891788,\n",
              "  1.2329833804288621,\n",
              "  1.19042665178928,\n",
              "  1.1682948223612457,\n",
              "  1.1518314137121108,\n",
              "  0.9607802401865855,\n",
              "  2.317439190074449,\n",
              "  1.0591147430338594,\n",
              "  1.4308630919602832,\n",
              "  0.7535680705496237,\n",
              "  0.8608283307581511,\n",
              "  1.2776122636975893,\n",
              "  1.3745862957220916,\n",
              "  1.259546137598783,\n",
              "  1.2978813187979172,\n",
              "  1.2412170838050638,\n",
              "  1.6009469708743893,\n",
              "  1.3149369953539032,\n",
              "  1.417901703622935,\n",
              "  1.2478669653497139,\n",
              "  1.1055812783082735,\n",
              "  0.9561307405997607,\n",
              "  0.9487783503683882,\n",
              "  1.1238565871041026,\n",
              "  1.2058356273089446,\n",
              "  1.2801012827406097,\n",
              "  0.8733100751144249,\n",
              "  0.9194732501297403,\n",
              "  1.6425573339441792,\n",
              "  1.085826790250066,\n",
              "  1.0639125693728595,\n",
              "  1.0875842666474016,\n",
              "  1.417901703622935,\n",
              "  1.550443891425932,\n",
              "  0.7825779328716171,\n",
              "  1.4690612745308145,\n",
              "  1.053086721720641,\n",
              "  1.2676073151634049,\n",
              "  0.7744003006005755,\n",
              "  1.3787482149724068,\n",
              "  1.363892581861956,\n",
              "  1.299352006316543,\n",
              "  1.2870449283923413,\n",
              "  1.11817763925502,\n",
              "  0.9474354220939228,\n",
              "  1.5218484589055707,\n",
              "  1.3526437911676632,\n",
              "  1.1556938532445284,\n",
              "  1.6013445735058454,\n",
              "  1.274619025074578,\n",
              "  1.422384489715834,\n",
              "  1.3408259533459403,\n",
              "  1.172646028567008,\n",
              "  1.1490645795125545,\n",
              "  1.459060149136146,\n",
              "  1.2483770274864237,\n",
              "  1.336545951796433,\n",
              "  0.9601174044814821,\n",
              "  1.4867225193896279,\n",
              "  1.4277452542806772,\n",
              "  1.35028849808504,\n",
              "  0.7560982446653928,\n",
              "  1.259040600296622,\n",
              "  1.13456827900627,\n",
              "  1.6549133695530214,\n",
              "  1.1204526724091788,\n",
              "  1.1176081573544434,\n",
              "  0.9153095762832032,\n",
              "  1.1639273497938836,\n",
              "  1.3066806149514323,\n",
              "  1.1529362882239027,\n",
              "  1.3047303442899274,\n",
              "  1.3066806149514323],\n",
              " [1.087935393855814,\n",
              "  1.2106732640648237,\n",
              "  1.0887189810800857,\n",
              "  1.0818687506209845,\n",
              "  1.0980500400249704,\n",
              "  1.442110443875572,\n",
              "  1.3937135438563717,\n",
              "  1.23655317474215,\n",
              "  1.4165182181603448,\n",
              "  1.4919570394732755,\n",
              "  1.1675515086449018,\n",
              "  1.0623886636202586,\n",
              "  0.7525091467462873,\n",
              "  1.4664607156647387,\n",
              "  0.9441427382260288,\n",
              "  1.2961188349107455,\n",
              "  1.181913075712612,\n",
              "  1.7717128257418353,\n",
              "  1.2868932127268289,\n",
              "  1.5683721829930668,\n",
              "  1.146358534347275,\n",
              "  1.1256133522578777,\n",
              "  1.2405503391757198,\n",
              "  1.1395104739649062,\n",
              "  1.0366172797001942,\n",
              "  0.9085259217262556,\n",
              "  1.0694555514704172,\n",
              "  1.691977163120794,\n",
              "  1.5976374066985888,\n",
              "  1.755635140526391,\n",
              "  1.2167634503574256,\n",
              "  1.3390750958414674,\n",
              "  1.611030538452511,\n",
              "  1.3472094871159672,\n",
              "  1.5136768631586262,\n",
              "  1.277822260673174,\n",
              "  0.961983539204892,\n",
              "  1.8433678600490109,\n",
              "  1.069504347495004,\n",
              "  1.4715828155162793,\n",
              "  1.1465592113197818,\n",
              "  1.405216189081912,\n",
              "  1.3662222837485787,\n",
              "  0.886422944395523,\n",
              "  1.6608247399264486,\n",
              "  0.6682091479058929,\n",
              "  1.5059749134052376,\n",
              "  1.5207670599678993,\n",
              "  1.267926318540793,\n",
              "  0.9859036440717683]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}