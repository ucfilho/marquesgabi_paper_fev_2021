{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_CNN_B_augmentation_r_squared_jul_13_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/Qualificacao/Comparative_ANNs/PSD_histogram_CNN_B_augmentation_r_squared_jul_13_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from keras.preprocessing import image\n",
        "from sklearn.metrics import r2_score\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZEvJvfoibE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ffac2be-e388-435d-f35c-c69429f81ef2"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mahotas\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/ad/553b246b0a35dccc3ed58dc8889a67124bf5ab858e9c6b7255d56086e70c/mahotas-1.4.11-cp37-cp37m-manylinux2010_x86_64.whl (5.7MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7MB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.19.5)\n",
            "Installing collected packages: mahotas\n",
            "Successfully installed mahotas-1.4.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT"
      },
      "source": [
        "import mahotas.features.texture as mht\n",
        "import mahotas.features"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VcTdaNVh9EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c601748b-2902-4877-a4de-6531b0d59189"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_fev_2020'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 73 (delta 37), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n",
            "/content/marquesgabi_fev_2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v7SRrc8mH2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe91835d-f05a-44fb-acd2-c97990ad7832"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip'\n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Doutorado'...\n",
            "remote: Enumerating objects: 361, done.\u001b[K\n",
            "remote: Counting objects: 100% (111/111), done.\u001b[K\n",
            "remote: Compressing objects: 100% (110/110), done.\u001b[K\n",
            "remote: Total 361 (delta 38), reused 0 (delta 0), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (361/361), 202.49 MiB | 20.53 MiB/s, done.\n",
            "Resolving deltas: 100% (155/155), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[4] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHgqAnaFyCjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc94b638-a794-4f84-f846-2f869cd7443c"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (160/160), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 3 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 22.02 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from segment_filter_not_conclude import Segmenta  # got image provided segmented"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5MN5a_v4np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2119714-8e25-4ca2-e576-ed2be73c39b5"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     184   89.517014   89.525978  ...   92.689980  132.370026  138.376175\n",
            "1     125  141.675079  137.414978  ...    1.263296    1.679296    0.799296\n",
            "2     189  168.561050  174.379974  ...  119.043900  113.015091  111.094650\n",
            "3     153  154.321457  157.486237  ...    0.654406    0.314537    1.399504\n",
            "4     129   47.635300   93.481216  ...  198.395706  187.200943  175.586014\n",
            "5     123    1.734748    0.861855  ...  118.872177  126.666008  103.231155\n",
            "6     139  145.962006  121.662750  ...  113.816933  127.923866  126.497025\n",
            "7     117  126.119080  123.296814  ...  146.031418  147.529556  143.822052\n",
            "8     184  132.791580  128.039688  ...    0.000000    0.604442    1.327504\n",
            "9     149  125.686646  131.302795  ...  118.891640  109.933525  108.449715\n",
            "10    166  177.168518  185.945129  ...  151.514145  154.581070  162.414703\n",
            "11    185   39.851044   63.968376  ...  156.719711  156.224213  158.861221\n",
            "12    138  176.638504  175.140106  ...  178.245514  184.111313  180.252869\n",
            "13    155   99.116348   86.458771  ...  174.499023  161.065781  163.849609\n",
            "14    178  123.499069  110.077278  ...  138.465744  153.786896  156.489838\n",
            "15    189  107.679016   97.939636  ...    1.355281    0.155007    1.318244\n",
            "16    161  129.317581  131.835556  ...  121.126648  124.756149  129.841217\n",
            "17    152  120.879494  142.060944  ...    0.653740    0.337950    1.416898\n",
            "18    197  220.517487  222.584778  ...  157.384338  155.049911  165.036621\n",
            "19    146  169.559753  169.202103  ...  130.898102  138.676865  145.080688\n",
            "20    146   23.038282   52.572903  ...  159.026642  165.696198  143.156113\n",
            "21    140  125.119995  125.680000  ...  172.879990  177.639999  149.319992\n",
            "22    176  158.294937  159.031494  ...  107.725204   99.558357  162.440094\n",
            "23    181  252.976105  253.956360  ...  129.002686   96.326187   93.706543\n",
            "24    121  212.694901  218.374725  ...  208.620651  194.323822  170.675430\n",
            "25    120   84.375557   74.993340  ...    1.000000    1.000000    0.782222\n",
            "26    172  150.497574  152.327209  ...  122.143333  154.503525  159.120056\n",
            "27    105  110.115562  118.871124  ...  144.746674  142.982239  161.382233\n",
            "28    179  143.162506  126.215416  ...  209.167648  211.528076  208.792328\n",
            "29    107   84.395416   89.708008  ...  124.791077  121.458466  118.292953\n",
            "30    200  177.520401  174.230408  ...    0.634800    1.341600    0.658400\n",
            "31    160  109.243118  108.662498  ...  173.537506  173.309998  158.505630\n",
            "32    117  127.556587   90.655411  ...  126.898827  107.368332   80.702240\n",
            "33    161  151.644623  137.119080  ...  171.820419  180.820419  179.803406\n",
            "34    119  100.456749   97.245674  ...    0.041522    0.702422    1.525952\n",
            "35    100  113.484802  116.974380  ...   65.579201   70.819199   66.600006\n",
            "36    157    1.497140    5.584933  ...   22.051929   36.368820   48.111042\n",
            "37    163  129.168015  127.079575  ...  224.830627  251.142075  253.866165\n",
            "38    199   50.595768  128.962067  ...   59.887753   57.956539  109.832253\n",
            "39    128  132.591797  119.322266  ...  137.583008  136.397461  135.400391\n",
            "40    160   85.028122   77.199371  ...    0.760000    0.255625    1.380625\n",
            "41    191  135.585388  138.935074  ...    1.617856    0.408925    0.775966\n",
            "42    123  124.370483  133.138016  ...    0.000000    0.271465    0.959284\n",
            "43    174  133.869202  137.378784  ...  138.705002  142.077042  146.982849\n",
            "44    167  145.942062  158.535614  ...  160.603867  169.186905  186.815048\n",
            "45    158  216.519623  208.102081  ...  128.571548  134.837372  134.418213\n",
            "46    119  174.301041  171.283737  ...  114.370239  116.723183  119.532867\n",
            "47    183  101.971809  105.383385  ...  105.401863  104.691154  112.004066\n",
            "48    102  248.159195  247.332565  ...  126.802399  128.245300  131.313354\n",
            "49    153  147.735626  155.404373  ...    1.229570    1.569439    0.484472\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzpQ1Pz0fX5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "72f9f82a-79b6-4b70-c424-b3162c76ddc1"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines\n",
        "# filename = 'model_ANN.pkl'\n",
        "filename = 'model_ANN_new.pkl'\n",
        "model = joblib.load(filename)\n",
        "'''"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n!git clone https://github.com/ucfilho/MarquesGabi_Routines\\n%cd MarquesGabi_Routines\\n# filename = 'model_ANN.pkl'\\nfilename = 'model_ANN_new.pkl'\\nmodel = joblib.load(filename)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2emP4rNjQy",
        "outputId": "14b2268c-aee7-4dc2-a91e-86253a3b6f6c"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (160/160), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 3 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 18.78 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "Description = 'Augmentation and 3 layers of Convolution: 32, 64, 128 '\n",
        "N1 = 200\n",
        "N2 = 10\n",
        "\n",
        "\n",
        "# make the CNN\n",
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=32, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=N1))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=N2))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbPQ1FSRG6A",
        "outputId": "d6b17aa8-9d75-4956-94e0-cc82f20cbdf3"
      },
      "source": [
        "\n",
        "# gives us back a <keras.callbacks.History object at 0x112e61a90>\n",
        "model.fit(X_train, Y_train, epochs=200, batch_size=32)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 48s 18ms/step - loss: 0.6476 - accuracy: 0.6750\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.3429 - accuracy: 0.8521\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.2848 - accuracy: 0.8812\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.1980 - accuracy: 0.9284\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0862 - accuracy: 0.9776\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0859 - accuracy: 0.9715\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0693 - accuracy: 0.9830\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0263 - accuracy: 0.9979\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0248 - accuracy: 0.9936\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0515 - accuracy: 0.9766\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0121 - accuracy: 0.9990\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0193 - accuracy: 0.9933\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0108 - accuracy: 0.9978\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0138 - accuracy: 0.9919\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0074 - accuracy: 1.0000\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0026 - accuracy: 0.9995\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0033 - accuracy: 0.9990\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0044 - accuracy: 0.9993\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0076 - accuracy: 0.9983\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0133 - accuracy: 0.9995\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0115 - accuracy: 0.9976\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0305 - accuracy: 0.9894\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0163 - accuracy: 0.9892\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0192 - accuracy: 0.9946\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0185 - accuracy: 0.9906\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0046 - accuracy: 0.9983\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0206 - accuracy: 0.9928\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0080 - accuracy: 0.9967\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0088 - accuracy: 0.9983\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0054 - accuracy: 0.9967\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0025 - accuracy: 0.9993\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0070 - accuracy: 0.9971\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0084 - accuracy: 0.9958\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0102 - accuracy: 0.9965\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0077 - accuracy: 0.9945\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 6.3524e-04 - accuracy: 1.0000\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 6.8570e-04 - accuracy: 1.0000\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.4242e-04 - accuracy: 1.0000\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0044 - accuracy: 0.9967\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0034 - accuracy: 0.9973\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 4.2717e-04 - accuracy: 1.0000\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 7.5737e-04 - accuracy: 1.0000\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.8755e-04 - accuracy: 1.0000\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.8934e-04 - accuracy: 1.0000\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.6940e-04 - accuracy: 1.0000\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.1857e-04 - accuracy: 1.0000\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.4350e-04 - accuracy: 1.0000\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.4943e-04 - accuracy: 1.0000\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.4039e-04 - accuracy: 1.0000\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.5909e-04 - accuracy: 1.0000\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 7.4620e-04 - accuracy: 1.0000\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.7307e-04 - accuracy: 1.0000\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0038 - accuracy: 0.9978\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0043 - accuracy: 0.9973\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0094 - accuracy: 0.9944\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0076 - accuracy: 0.9978\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 7.5682e-04 - accuracy: 1.0000\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.8262e-04 - accuracy: 1.0000\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 4.4635e-04 - accuracy: 1.0000\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.6500e-04 - accuracy: 1.0000\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 6.3962e-05 - accuracy: 1.0000\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 8.9893e-05 - accuracy: 1.0000\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 4.0591e-04 - accuracy: 1.0000\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 1.6812e-04 - accuracy: 1.0000\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.2002e-04 - accuracy: 1.0000\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 6.6884e-05 - accuracy: 1.0000\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 6.9813e-05 - accuracy: 1.0000\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 4.1661e-04 - accuracy: 1.0000\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.0112e-05 - accuracy: 1.0000\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.2948e-04 - accuracy: 1.0000\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.6720e-04 - accuracy: 1.0000\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 7.5818e-05 - accuracy: 1.0000\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 4.0333e-05 - accuracy: 1.0000\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 6.4938e-05 - accuracy: 1.0000\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.9591e-05 - accuracy: 1.0000\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 4.6242e-05 - accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 4.2030e-05 - accuracy: 1.0000\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 8.9547e-05 - accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.6502e-04 - accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.3344e-05 - accuracy: 1.0000\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.9501e-05 - accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 4.7825e-05 - accuracy: 1.0000\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.4953e-05 - accuracy: 1.0000\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.1989e-04 - accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.2365e-05 - accuracy: 1.0000\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.8697e-04 - accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.8842e-05 - accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 5.5769e-05 - accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 5.0466e-05 - accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 3.8759e-05 - accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.9696e-05 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.6192e-05 - accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.6996e-05 - accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.1081e-05 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 4.2607e-05 - accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.0613e-05 - accuracy: 1.0000\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 3.7823e-05 - accuracy: 1.0000\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.3320e-05 - accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.4580e-05 - accuracy: 1.0000\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.8441e-05 - accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.5754e-05 - accuracy: 1.0000\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.9159e-05 - accuracy: 1.0000\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.8229e-04 - accuracy: 1.0000\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.5271e-05 - accuracy: 1.0000\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.3964e-05 - accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.1069e-04 - accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 6.8934e-05 - accuracy: 1.0000\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.9610e-05 - accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.8586e-05 - accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.0183e-05 - accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.8235e-05 - accuracy: 1.0000\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 4.0260e-05 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.6967e-05 - accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.8033e-05 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.0496e-05 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 6.9551e-05 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 6.4515e-05 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.1041e-05 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.4200e-05 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 2.6507e-05 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 5.9771e-05 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.6522e-05 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.8110e-05 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.4500e-05 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 3.0838e-05 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.4741e-05 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 2.6521e-05 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.1000e-05 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.1269e-05 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.4179e-05 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.1234e-05 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.6027e-05 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.9615e-06 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 8.9737e-05 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.8358e-05 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.4734e-05 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.1694e-05 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.2780e-05 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.3920e-05 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.7162e-05 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.3057e-05 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.4305e-05 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 9.6449e-06 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.2984e-05 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.2738e-05 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 8.1470e-04 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.5696e-04 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0117 - accuracy: 0.9950\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.0436 - accuracy: 0.9787\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0641 - accuracy: 0.9713\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.1222 - accuracy: 0.9803\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.1140 - accuracy: 0.9674\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0867 - accuracy: 0.9790\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0788 - accuracy: 0.9706\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.0139 - accuracy: 0.9912\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0521 - accuracy: 0.9814\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0128 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 4.3724e-04 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 8.0558e-04 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.7775e-04 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 4.6884e-04 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 6.3231e-04 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 6.4167e-04 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.1805e-04 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.0011 - accuracy: 0.9995\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0012 - accuracy: 0.9995\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.6686e-04 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 9.4536e-04 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb5bfab0990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRaP8bHWNeZA",
        "outputId": "7f3d0772-3611-4a67-fc3a-06f8894fb60f"
      },
      "source": [
        "\n",
        "# Fit with data augmentation\n",
        "# Note: if you run this AFTER calling the previous model.fit(), it will CONTINUE training where it left off\n",
        "batch_size = 5\n",
        "data_generator = image.ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "train_generator = data_generator.flow(X_train, Y_train, batch_size)\n",
        "steps_per_epoch = X_train.shape[0] // batch_size\n",
        "\n",
        "model.fit(train_generator, validation_data=(X_test, Y_test), steps_per_epoch=steps_per_epoch, epochs=200)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "68/68 [==============================] - 3s 23ms/step - loss: 0.7968 - accuracy: 0.6361 - val_loss: 58.1959 - val_accuracy: 0.5102\n",
            "Epoch 2/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.4124 - accuracy: 0.8432 - val_loss: 177.0053 - val_accuracy: 0.5102\n",
            "Epoch 3/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.4157 - accuracy: 0.8609 - val_loss: 0.6967 - val_accuracy: 0.5102\n",
            "Epoch 4/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.2284 - accuracy: 0.9112 - val_loss: 9.2126 - val_accuracy: 0.5102\n",
            "Epoch 5/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.2370 - accuracy: 0.9083 - val_loss: 146.6022 - val_accuracy: 0.5102\n",
            "Epoch 6/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.3676 - accuracy: 0.8402 - val_loss: 0.7004 - val_accuracy: 0.5102\n",
            "Epoch 7/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.2574 - accuracy: 0.9083 - val_loss: 82.9100 - val_accuracy: 0.5102\n",
            "Epoch 8/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.2506 - accuracy: 0.9290 - val_loss: 27.6081 - val_accuracy: 0.5102\n",
            "Epoch 9/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.2545 - accuracy: 0.8935 - val_loss: 16.0913 - val_accuracy: 0.5102\n",
            "Epoch 10/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1971 - accuracy: 0.9201 - val_loss: 51.6852 - val_accuracy: 0.5102\n",
            "Epoch 11/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1563 - accuracy: 0.9408 - val_loss: 123.1060 - val_accuracy: 0.5102\n",
            "Epoch 12/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1840 - accuracy: 0.9231 - val_loss: 160.6210 - val_accuracy: 0.5102\n",
            "Epoch 13/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1991 - accuracy: 0.9201 - val_loss: 148.5739 - val_accuracy: 0.5102\n",
            "Epoch 14/200\n",
            "68/68 [==============================] - 1s 7ms/step - loss: 0.2928 - accuracy: 0.8941 - val_loss: 15.2561 - val_accuracy: 0.5102\n",
            "Epoch 15/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.2073 - accuracy: 0.9172 - val_loss: 17.3677 - val_accuracy: 0.5102\n",
            "Epoch 16/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.2299 - accuracy: 0.9083 - val_loss: 45.1716 - val_accuracy: 0.5102\n",
            "Epoch 17/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1893 - accuracy: 0.9231 - val_loss: 25.7386 - val_accuracy: 0.5102\n",
            "Epoch 18/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.2151 - accuracy: 0.9201 - val_loss: 38.3120 - val_accuracy: 0.5102\n",
            "Epoch 19/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.2328 - accuracy: 0.9320 - val_loss: 22.3607 - val_accuracy: 0.5102\n",
            "Epoch 20/200\n",
            "68/68 [==============================] - 1s 7ms/step - loss: 0.1899 - accuracy: 0.9172 - val_loss: 35.9645 - val_accuracy: 0.5102\n",
            "Epoch 21/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.2285 - accuracy: 0.9172 - val_loss: 3.8371 - val_accuracy: 0.5102\n",
            "Epoch 22/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1660 - accuracy: 0.9260 - val_loss: 6.3225 - val_accuracy: 0.5102\n",
            "Epoch 23/200\n",
            "68/68 [==============================] - 1s 7ms/step - loss: 0.2004 - accuracy: 0.9290 - val_loss: 11.3698 - val_accuracy: 0.5102\n",
            "Epoch 24/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1930 - accuracy: 0.9142 - val_loss: 2.9205 - val_accuracy: 0.5102\n",
            "Epoch 25/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.2048 - accuracy: 0.9260 - val_loss: 0.7151 - val_accuracy: 0.5102\n",
            "Epoch 26/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1805 - accuracy: 0.9290 - val_loss: 1.6683 - val_accuracy: 0.5102\n",
            "Epoch 27/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1758 - accuracy: 0.9201 - val_loss: 0.7152 - val_accuracy: 0.5102\n",
            "Epoch 28/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1291 - accuracy: 0.9438 - val_loss: 0.7147 - val_accuracy: 0.5102\n",
            "Epoch 29/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1751 - accuracy: 0.9379 - val_loss: 5.4580 - val_accuracy: 0.4898\n",
            "Epoch 30/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1282 - accuracy: 0.9408 - val_loss: 0.7114 - val_accuracy: 0.5170\n",
            "Epoch 31/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1702 - accuracy: 0.9320 - val_loss: 31.8458 - val_accuracy: 0.4898\n",
            "Epoch 32/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1429 - accuracy: 0.9379 - val_loss: 39.4672 - val_accuracy: 0.4898\n",
            "Epoch 33/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.2095 - accuracy: 0.9083 - val_loss: 14.4936 - val_accuracy: 0.4898\n",
            "Epoch 34/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.2056 - accuracy: 0.9320 - val_loss: 7.3838 - val_accuracy: 0.4898\n",
            "Epoch 35/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.2015 - accuracy: 0.9320 - val_loss: 7.9901 - val_accuracy: 0.4898\n",
            "Epoch 36/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1098 - accuracy: 0.9763 - val_loss: 6.3705 - val_accuracy: 0.4898\n",
            "Epoch 37/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1408 - accuracy: 0.9320 - val_loss: 0.7772 - val_accuracy: 0.4898\n",
            "Epoch 38/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1400 - accuracy: 0.9497 - val_loss: 11.3141 - val_accuracy: 0.4898\n",
            "Epoch 39/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1199 - accuracy: 0.9320 - val_loss: 6.8756 - val_accuracy: 0.5102\n",
            "Epoch 40/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1585 - accuracy: 0.9260 - val_loss: 0.7157 - val_accuracy: 0.5102\n",
            "Epoch 41/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1351 - accuracy: 0.9556 - val_loss: 13.3352 - val_accuracy: 0.4898\n",
            "Epoch 42/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1370 - accuracy: 0.9438 - val_loss: 9.2453 - val_accuracy: 0.4898\n",
            "Epoch 43/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1333 - accuracy: 0.9615 - val_loss: 21.8129 - val_accuracy: 0.4898\n",
            "Epoch 44/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1737 - accuracy: 0.9438 - val_loss: 7.7759 - val_accuracy: 0.5102\n",
            "Epoch 45/200\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.1502 - accuracy: 0.9467 - val_loss: 8.8620 - val_accuracy: 0.5102\n",
            "Epoch 46/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1186 - accuracy: 0.9497 - val_loss: 13.7920 - val_accuracy: 0.5102\n",
            "Epoch 47/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1141 - accuracy: 0.9675 - val_loss: 1.7859 - val_accuracy: 0.5306\n",
            "Epoch 48/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1522 - accuracy: 0.9408 - val_loss: 15.3972 - val_accuracy: 0.5102\n",
            "Epoch 49/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1372 - accuracy: 0.9497 - val_loss: 3.5849 - val_accuracy: 0.5102\n",
            "Epoch 50/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1616 - accuracy: 0.9349 - val_loss: 5.3398 - val_accuracy: 0.5102\n",
            "Epoch 51/200\n",
            "68/68 [==============================] - 1s 7ms/step - loss: 0.1294 - accuracy: 0.9586 - val_loss: 5.3909 - val_accuracy: 0.5102\n",
            "Epoch 52/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1082 - accuracy: 0.9497 - val_loss: 1.3636 - val_accuracy: 0.5306\n",
            "Epoch 53/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1219 - accuracy: 0.9497 - val_loss: 15.2132 - val_accuracy: 0.5102\n",
            "Epoch 54/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.2143 - accuracy: 0.9201 - val_loss: 23.8892 - val_accuracy: 0.5102\n",
            "Epoch 55/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1088 - accuracy: 0.9645 - val_loss: 6.8350 - val_accuracy: 0.5102\n",
            "Epoch 56/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1248 - accuracy: 0.9438 - val_loss: 5.3257 - val_accuracy: 0.5102\n",
            "Epoch 57/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1087 - accuracy: 0.9527 - val_loss: 5.1109 - val_accuracy: 0.5102\n",
            "Epoch 58/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1137 - accuracy: 0.9556 - val_loss: 16.5204 - val_accuracy: 0.5102\n",
            "Epoch 59/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0999 - accuracy: 0.9586 - val_loss: 18.6371 - val_accuracy: 0.5102\n",
            "Epoch 60/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1484 - accuracy: 0.9438 - val_loss: 9.0263 - val_accuracy: 0.5102\n",
            "Epoch 61/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1571 - accuracy: 0.9704 - val_loss: 6.6794 - val_accuracy: 0.5102\n",
            "Epoch 62/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1200 - accuracy: 0.9586 - val_loss: 6.8362 - val_accuracy: 0.4898\n",
            "Epoch 63/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1074 - accuracy: 0.9645 - val_loss: 6.0293 - val_accuracy: 0.4898\n",
            "Epoch 64/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1486 - accuracy: 0.9497 - val_loss: 1.7360 - val_accuracy: 0.4898\n",
            "Epoch 65/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1286 - accuracy: 0.9438 - val_loss: 1.1146 - val_accuracy: 0.5238\n",
            "Epoch 66/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0833 - accuracy: 0.9822 - val_loss: 3.9409 - val_accuracy: 0.4898\n",
            "Epoch 67/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1435 - accuracy: 0.9467 - val_loss: 11.8562 - val_accuracy: 0.4898\n",
            "Epoch 68/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0955 - accuracy: 0.9586 - val_loss: 3.4007 - val_accuracy: 0.4898\n",
            "Epoch 69/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1079 - accuracy: 0.9497 - val_loss: 8.5163 - val_accuracy: 0.5102\n",
            "Epoch 70/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1123 - accuracy: 0.9497 - val_loss: 2.1867 - val_accuracy: 0.4898\n",
            "Epoch 71/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1092 - accuracy: 0.9556 - val_loss: 13.8671 - val_accuracy: 0.5102\n",
            "Epoch 72/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1023 - accuracy: 0.9586 - val_loss: 4.8852 - val_accuracy: 0.4898\n",
            "Epoch 73/200\n",
            "68/68 [==============================] - 1s 7ms/step - loss: 0.0872 - accuracy: 0.9763 - val_loss: 23.5721 - val_accuracy: 0.4898\n",
            "Epoch 74/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1602 - accuracy: 0.9379 - val_loss: 26.6275 - val_accuracy: 0.4898\n",
            "Epoch 75/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1161 - accuracy: 0.9645 - val_loss: 41.1745 - val_accuracy: 0.4898\n",
            "Epoch 76/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1213 - accuracy: 0.9556 - val_loss: 9.6588 - val_accuracy: 0.4898\n",
            "Epoch 77/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1209 - accuracy: 0.9349 - val_loss: 2.5882 - val_accuracy: 0.4898\n",
            "Epoch 78/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0987 - accuracy: 0.9645 - val_loss: 22.1781 - val_accuracy: 0.4898\n",
            "Epoch 79/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0608 - accuracy: 0.9822 - val_loss: 51.6566 - val_accuracy: 0.4898\n",
            "Epoch 80/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1274 - accuracy: 0.9497 - val_loss: 18.7526 - val_accuracy: 0.4898\n",
            "Epoch 81/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1208 - accuracy: 0.9467 - val_loss: 12.2635 - val_accuracy: 0.4898\n",
            "Epoch 82/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0623 - accuracy: 0.9822 - val_loss: 20.4778 - val_accuracy: 0.4898\n",
            "Epoch 83/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1065 - accuracy: 0.9438 - val_loss: 0.6620 - val_accuracy: 0.5782\n",
            "Epoch 84/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1002 - accuracy: 0.9675 - val_loss: 7.0470 - val_accuracy: 0.5102\n",
            "Epoch 85/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1144 - accuracy: 0.9615 - val_loss: 2.8282 - val_accuracy: 0.4898\n",
            "Epoch 86/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0879 - accuracy: 0.9615 - val_loss: 5.0258 - val_accuracy: 0.4898\n",
            "Epoch 87/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1080 - accuracy: 0.9615 - val_loss: 3.6512 - val_accuracy: 0.5102\n",
            "Epoch 88/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0915 - accuracy: 0.9704 - val_loss: 29.7927 - val_accuracy: 0.5102\n",
            "Epoch 89/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0846 - accuracy: 0.9618 - val_loss: 4.4908 - val_accuracy: 0.5102\n",
            "Epoch 90/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0735 - accuracy: 0.9793 - val_loss: 12.3632 - val_accuracy: 0.5102\n",
            "Epoch 91/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0873 - accuracy: 0.9615 - val_loss: 48.2922 - val_accuracy: 0.4898\n",
            "Epoch 92/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1434 - accuracy: 0.9467 - val_loss: 132.4096 - val_accuracy: 0.4898\n",
            "Epoch 93/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1164 - accuracy: 0.9645 - val_loss: 83.4229 - val_accuracy: 0.4898\n",
            "Epoch 94/200\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0875 - accuracy: 0.9704 - val_loss: 28.2868 - val_accuracy: 0.4898\n",
            "Epoch 95/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1260 - accuracy: 0.9441 - val_loss: 2.8811 - val_accuracy: 0.5102\n",
            "Epoch 96/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1182 - accuracy: 0.9586 - val_loss: 16.3096 - val_accuracy: 0.4898\n",
            "Epoch 97/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1121 - accuracy: 0.9497 - val_loss: 0.5861 - val_accuracy: 0.7007\n",
            "Epoch 98/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0820 - accuracy: 0.9763 - val_loss: 8.8607 - val_accuracy: 0.4898\n",
            "Epoch 99/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1144 - accuracy: 0.9556 - val_loss: 6.1343 - val_accuracy: 0.5102\n",
            "Epoch 100/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0804 - accuracy: 0.9763 - val_loss: 16.8560 - val_accuracy: 0.4898\n",
            "Epoch 101/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0948 - accuracy: 0.9645 - val_loss: 31.7967 - val_accuracy: 0.5102\n",
            "Epoch 102/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1187 - accuracy: 0.9497 - val_loss: 6.1761 - val_accuracy: 0.4898\n",
            "Epoch 103/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0970 - accuracy: 0.9675 - val_loss: 0.8741 - val_accuracy: 0.5102\n",
            "Epoch 104/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0899 - accuracy: 0.9675 - val_loss: 5.8151 - val_accuracy: 0.4898\n",
            "Epoch 105/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1232 - accuracy: 0.9645 - val_loss: 12.5481 - val_accuracy: 0.4898\n",
            "Epoch 106/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1076 - accuracy: 0.9675 - val_loss: 0.7115 - val_accuracy: 0.6871\n",
            "Epoch 107/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0809 - accuracy: 0.9704 - val_loss: 11.8278 - val_accuracy: 0.4898\n",
            "Epoch 108/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0980 - accuracy: 0.9586 - val_loss: 5.7829 - val_accuracy: 0.4898\n",
            "Epoch 109/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0668 - accuracy: 0.9704 - val_loss: 10.0016 - val_accuracy: 0.5102\n",
            "Epoch 110/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0911 - accuracy: 0.9675 - val_loss: 12.0295 - val_accuracy: 0.4898\n",
            "Epoch 111/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0937 - accuracy: 0.9704 - val_loss: 16.7559 - val_accuracy: 0.5102\n",
            "Epoch 112/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0833 - accuracy: 0.9704 - val_loss: 10.6423 - val_accuracy: 0.4898\n",
            "Epoch 113/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1105 - accuracy: 0.9615 - val_loss: 57.7825 - val_accuracy: 0.4898\n",
            "Epoch 114/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0866 - accuracy: 0.9645 - val_loss: 68.3872 - val_accuracy: 0.4898\n",
            "Epoch 115/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0945 - accuracy: 0.9675 - val_loss: 78.3611 - val_accuracy: 0.4898\n",
            "Epoch 116/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1475 - accuracy: 0.9379 - val_loss: 3.1586 - val_accuracy: 0.5102\n",
            "Epoch 117/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1071 - accuracy: 0.9379 - val_loss: 10.7496 - val_accuracy: 0.4898\n",
            "Epoch 118/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0795 - accuracy: 0.9734 - val_loss: 3.1729 - val_accuracy: 0.4898\n",
            "Epoch 119/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1335 - accuracy: 0.9586 - val_loss: 0.6567 - val_accuracy: 0.5782\n",
            "Epoch 120/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0792 - accuracy: 0.9704 - val_loss: 3.2801 - val_accuracy: 0.4898\n",
            "Epoch 121/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1108 - accuracy: 0.9586 - val_loss: 4.1826 - val_accuracy: 0.4898\n",
            "Epoch 122/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0762 - accuracy: 0.9704 - val_loss: 0.7164 - val_accuracy: 0.5102\n",
            "Epoch 123/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0711 - accuracy: 0.9704 - val_loss: 6.9855 - val_accuracy: 0.4898\n",
            "Epoch 124/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1227 - accuracy: 0.9586 - val_loss: 15.0380 - val_accuracy: 0.5102\n",
            "Epoch 125/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1597 - accuracy: 0.9527 - val_loss: 12.6341 - val_accuracy: 0.4898\n",
            "Epoch 126/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0757 - accuracy: 0.9735 - val_loss: 42.3551 - val_accuracy: 0.4898\n",
            "Epoch 127/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0854 - accuracy: 0.9645 - val_loss: 13.0793 - val_accuracy: 0.4898\n",
            "Epoch 128/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0576 - accuracy: 0.9793 - val_loss: 8.4536 - val_accuracy: 0.4898\n",
            "Epoch 129/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1111 - accuracy: 0.9704 - val_loss: 2.9964 - val_accuracy: 0.4966\n",
            "Epoch 130/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0542 - accuracy: 0.9793 - val_loss: 28.7396 - val_accuracy: 0.4898\n",
            "Epoch 131/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1040 - accuracy: 0.9527 - val_loss: 3.2851 - val_accuracy: 0.5102\n",
            "Epoch 132/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0813 - accuracy: 0.9704 - val_loss: 9.4216 - val_accuracy: 0.5102\n",
            "Epoch 133/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0939 - accuracy: 0.9675 - val_loss: 14.7004 - val_accuracy: 0.5102\n",
            "Epoch 134/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0909 - accuracy: 0.9704 - val_loss: 55.2151 - val_accuracy: 0.4898\n",
            "Epoch 135/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1067 - accuracy: 0.9556 - val_loss: 17.8078 - val_accuracy: 0.4898\n",
            "Epoch 136/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0822 - accuracy: 0.9704 - val_loss: 0.7238 - val_accuracy: 0.5102\n",
            "Epoch 137/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0675 - accuracy: 0.9675 - val_loss: 17.1640 - val_accuracy: 0.5102\n",
            "Epoch 138/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1022 - accuracy: 0.9645 - val_loss: 7.8646 - val_accuracy: 0.5102\n",
            "Epoch 139/200\n",
            "68/68 [==============================] - 1s 7ms/step - loss: 0.0619 - accuracy: 0.9793 - val_loss: 8.8370 - val_accuracy: 0.5102\n",
            "Epoch 140/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1007 - accuracy: 0.9734 - val_loss: 0.6425 - val_accuracy: 0.6395\n",
            "Epoch 141/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0763 - accuracy: 0.9734 - val_loss: 5.4821 - val_accuracy: 0.4898\n",
            "Epoch 142/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0971 - accuracy: 0.9704 - val_loss: 2.8983 - val_accuracy: 0.5102\n",
            "Epoch 143/200\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0890 - accuracy: 0.9645 - val_loss: 12.5793 - val_accuracy: 0.4898\n",
            "Epoch 144/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0698 - accuracy: 0.9793 - val_loss: 39.0212 - val_accuracy: 0.4898\n",
            "Epoch 145/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0589 - accuracy: 0.9852 - val_loss: 2.8517 - val_accuracy: 0.5374\n",
            "Epoch 146/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0585 - accuracy: 0.9734 - val_loss: 0.6489 - val_accuracy: 0.5102\n",
            "Epoch 147/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0602 - accuracy: 0.9763 - val_loss: 8.4481 - val_accuracy: 0.4898\n",
            "Epoch 148/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0507 - accuracy: 0.9793 - val_loss: 20.4950 - val_accuracy: 0.4898\n",
            "Epoch 149/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0667 - accuracy: 0.9822 - val_loss: 0.8051 - val_accuracy: 0.6667\n",
            "Epoch 150/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0774 - accuracy: 0.9763 - val_loss: 18.9992 - val_accuracy: 0.4898\n",
            "Epoch 151/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0525 - accuracy: 0.9763 - val_loss: 32.2970 - val_accuracy: 0.5102\n",
            "Epoch 152/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0689 - accuracy: 0.9793 - val_loss: 11.6430 - val_accuracy: 0.4898\n",
            "Epoch 153/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0667 - accuracy: 0.9675 - val_loss: 23.8109 - val_accuracy: 0.4898\n",
            "Epoch 154/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1020 - accuracy: 0.9615 - val_loss: 2.0796 - val_accuracy: 0.5102\n",
            "Epoch 155/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1566 - accuracy: 0.9586 - val_loss: 10.2855 - val_accuracy: 0.4898\n",
            "Epoch 156/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0908 - accuracy: 0.9615 - val_loss: 5.9256 - val_accuracy: 0.4898\n",
            "Epoch 157/200\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.1059 - accuracy: 0.9675 - val_loss: 1.1240 - val_accuracy: 0.5102\n",
            "Epoch 158/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1152 - accuracy: 0.9734 - val_loss: 0.5328 - val_accuracy: 0.7483\n",
            "Epoch 159/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1479 - accuracy: 0.9527 - val_loss: 0.6343 - val_accuracy: 0.7755\n",
            "Epoch 160/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0459 - accuracy: 0.9882 - val_loss: 3.4677 - val_accuracy: 0.4966\n",
            "Epoch 161/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0865 - accuracy: 0.9704 - val_loss: 76.1184 - val_accuracy: 0.4898\n",
            "Epoch 162/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0535 - accuracy: 0.9882 - val_loss: 69.2475 - val_accuracy: 0.4898\n",
            "Epoch 163/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0805 - accuracy: 0.9734 - val_loss: 5.6867 - val_accuracy: 0.5102\n",
            "Epoch 164/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1017 - accuracy: 0.9734 - val_loss: 4.8992 - val_accuracy: 0.5102\n",
            "Epoch 165/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0704 - accuracy: 0.9734 - val_loss: 8.6604 - val_accuracy: 0.5102\n",
            "Epoch 166/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0960 - accuracy: 0.9586 - val_loss: 6.6106 - val_accuracy: 0.4898\n",
            "Epoch 167/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0606 - accuracy: 0.9822 - val_loss: 18.0491 - val_accuracy: 0.5102\n",
            "Epoch 168/200\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0619 - accuracy: 0.9822 - val_loss: 7.1207 - val_accuracy: 0.4898\n",
            "Epoch 169/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0542 - accuracy: 0.9852 - val_loss: 2.8589 - val_accuracy: 0.4898\n",
            "Epoch 170/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0545 - accuracy: 0.9763 - val_loss: 9.9029 - val_accuracy: 0.5102\n",
            "Epoch 171/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0938 - accuracy: 0.9615 - val_loss: 1.6585 - val_accuracy: 0.4966\n",
            "Epoch 172/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0503 - accuracy: 0.9763 - val_loss: 43.4603 - val_accuracy: 0.4898\n",
            "Epoch 173/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0948 - accuracy: 0.9586 - val_loss: 26.8404 - val_accuracy: 0.4898\n",
            "Epoch 174/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0926 - accuracy: 0.9734 - val_loss: 40.8795 - val_accuracy: 0.4898\n",
            "Epoch 175/200\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0512 - accuracy: 0.9704 - val_loss: 3.5423 - val_accuracy: 0.5102\n",
            "Epoch 176/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0851 - accuracy: 0.9704 - val_loss: 11.9854 - val_accuracy: 0.5102\n",
            "Epoch 177/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0624 - accuracy: 0.9704 - val_loss: 7.1574 - val_accuracy: 0.4898\n",
            "Epoch 178/200\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0608 - accuracy: 0.9763 - val_loss: 17.9732 - val_accuracy: 0.4898\n",
            "Epoch 179/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0690 - accuracy: 0.9704 - val_loss: 13.6342 - val_accuracy: 0.4898\n",
            "Epoch 180/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0455 - accuracy: 0.9882 - val_loss: 16.9778 - val_accuracy: 0.4898\n",
            "Epoch 181/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0699 - accuracy: 0.9793 - val_loss: 1.7534 - val_accuracy: 0.5102\n",
            "Epoch 182/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0779 - accuracy: 0.9675 - val_loss: 3.2226 - val_accuracy: 0.5102\n",
            "Epoch 183/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0655 - accuracy: 0.9763 - val_loss: 16.8043 - val_accuracy: 0.4898\n",
            "Epoch 184/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0458 - accuracy: 0.9852 - val_loss: 11.4805 - val_accuracy: 0.4898\n",
            "Epoch 185/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0656 - accuracy: 0.9882 - val_loss: 5.4450 - val_accuracy: 0.5102\n",
            "Epoch 186/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0388 - accuracy: 0.9911 - val_loss: 28.5881 - val_accuracy: 0.4898\n",
            "Epoch 187/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1143 - accuracy: 0.9645 - val_loss: 5.6867 - val_accuracy: 0.5102\n",
            "Epoch 188/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0646 - accuracy: 0.9704 - val_loss: 0.5517 - val_accuracy: 0.6259\n",
            "Epoch 189/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0495 - accuracy: 0.9852 - val_loss: 3.0572 - val_accuracy: 0.4898\n",
            "Epoch 190/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0726 - accuracy: 0.9763 - val_loss: 20.2785 - val_accuracy: 0.4898\n",
            "Epoch 191/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0765 - accuracy: 0.9763 - val_loss: 21.5486 - val_accuracy: 0.4898\n",
            "Epoch 192/200\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.0373 - accuracy: 0.9852 - val_loss: 3.5762 - val_accuracy: 0.4898\n",
            "Epoch 193/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0812 - accuracy: 0.9734 - val_loss: 4.5731 - val_accuracy: 0.5102\n",
            "Epoch 194/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0828 - accuracy: 0.9586 - val_loss: 21.4303 - val_accuracy: 0.5102\n",
            "Epoch 195/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.1168 - accuracy: 0.9527 - val_loss: 21.7517 - val_accuracy: 0.5102\n",
            "Epoch 196/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0314 - accuracy: 0.9882 - val_loss: 13.8279 - val_accuracy: 0.5102\n",
            "Epoch 197/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0211 - accuracy: 0.9911 - val_loss: 3.5411 - val_accuracy: 0.5102\n",
            "Epoch 198/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0301 - accuracy: 0.9852 - val_loss: 51.3894 - val_accuracy: 0.4898\n",
            "Epoch 199/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0765 - accuracy: 0.9734 - val_loss: 107.0959 - val_accuracy: 0.4898\n",
            "Epoch 200/200\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0671 - accuracy: 0.9793 - val_loss: 137.6881 - val_accuracy: 0.4898\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb5bf77ff90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-3hvIYqRcV5",
        "outputId": "cf77b87c-df59-44ab-a18a-e6c8689f25e0"
      },
      "source": [
        "# X_train.shape\n",
        "steps_per_epoch"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIgDLfqrNhp_",
        "outputId": "25aef1a1-57fe-464b-b4eb-79895b538565"
      },
      "source": [
        "# gives us back a <keras.callbacks.History object at 0x112e61a90>\n",
        "model.fit(X_train, Y_train, epochs=200, batch_size=32)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0137 - accuracy: 0.9971\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0103 - accuracy: 0.9971\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0092 - accuracy: 0.9971\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 6.9559e-04 - accuracy: 1.0000\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.0602e-04 - accuracy: 1.0000\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 5.0909e-04 - accuracy: 1.0000\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 7.5718e-04 - accuracy: 1.0000\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 7.8182e-04 - accuracy: 1.0000\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.4129e-04 - accuracy: 1.0000\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 6.4171e-04 - accuracy: 1.0000\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 7.0516e-04 - accuracy: 1.0000\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.3658e-04 - accuracy: 1.0000\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 8.2976e-04 - accuracy: 1.0000\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 7.2961e-04 - accuracy: 1.0000\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 6.5169e-04 - accuracy: 1.0000\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 7.7119e-04 - accuracy: 1.0000\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 8.7891e-04 - accuracy: 1.0000\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 5.1515e-04 - accuracy: 1.0000\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 8.8612e-04 - accuracy: 1.0000\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 4.4315e-04 - accuracy: 1.0000\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.5062e-04 - accuracy: 1.0000\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 4.3160e-04 - accuracy: 1.0000\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.8093e-04 - accuracy: 1.0000\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.7959e-04 - accuracy: 1.0000\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 3.1871e-04 - accuracy: 1.0000\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.9573e-04 - accuracy: 1.0000\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.4117e-04 - accuracy: 1.0000\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 3.8040e-04 - accuracy: 1.0000\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.2223e-04 - accuracy: 1.0000\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.5684e-04 - accuracy: 1.0000\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 3.8376e-04 - accuracy: 1.0000\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 2.6695e-04 - accuracy: 1.0000\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.2276e-04 - accuracy: 1.0000\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.1556e-04 - accuracy: 1.0000\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 4.6846e-04 - accuracy: 1.0000\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 5.4977e-04 - accuracy: 1.0000\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 4.5655e-04 - accuracy: 1.0000\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.9850e-04 - accuracy: 1.0000\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.9507e-04 - accuracy: 1.0000\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.3192e-04 - accuracy: 1.0000\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 5.4951e-04 - accuracy: 1.0000\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.6189e-04 - accuracy: 1.0000\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.8414e-04 - accuracy: 1.0000\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.7857e-04 - accuracy: 1.0000\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.9270e-05 - accuracy: 1.0000\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.3700e-04 - accuracy: 1.0000\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 1.0793e-04 - accuracy: 1.0000\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.6405e-04 - accuracy: 1.0000\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.7106e-04 - accuracy: 1.0000\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.3158e-04 - accuracy: 1.0000\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.1513e-04 - accuracy: 1.0000\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.0097e-04 - accuracy: 1.0000\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 5.9816e-04 - accuracy: 1.0000\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.9805e-04 - accuracy: 1.0000\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 7.5368e-05 - accuracy: 1.0000\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.7766e-05 - accuracy: 1.0000\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.1665e-04 - accuracy: 1.0000\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 7.6306e-05 - accuracy: 1.0000\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.2205e-04 - accuracy: 1.0000\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.0605e-04 - accuracy: 1.0000\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.5530e-04 - accuracy: 1.0000\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 8.0620e-05 - accuracy: 1.0000\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.2345e-04 - accuracy: 1.0000\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.4516e-04 - accuracy: 1.0000\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.5114e-04 - accuracy: 1.0000\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.0212e-04 - accuracy: 1.0000\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.2386e-04 - accuracy: 1.0000\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.0592e-04 - accuracy: 1.0000\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.5132e-05 - accuracy: 1.0000\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.0761e-04 - accuracy: 1.0000\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.1765e-04 - accuracy: 1.0000\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 7.6532e-05 - accuracy: 1.0000\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.0718e-04 - accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 6.7449e-05 - accuracy: 1.0000\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.5748e-04 - accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.7222e-05 - accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.7826e-04 - accuracy: 1.0000\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 5.3382e-05 - accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 4.9581e-05 - accuracy: 1.0000\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 5.8838e-05 - accuracy: 1.0000\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.6921e-05 - accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.5139e-04 - accuracy: 1.0000\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 5.1643e-05 - accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 8.6614e-05 - accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.7704e-04 - accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.5673e-05 - accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.2649e-04 - accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.4149e-05 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 4.9316e-05 - accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 6.2014e-05 - accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 4.7321e-05 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.4866e-04 - accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.8579e-05 - accuracy: 1.0000\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.8411e-05 - accuracy: 1.0000\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 8.7820e-05 - accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 4.7775e-05 - accuracy: 1.0000\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 4.0199e-05 - accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 6.9027e-05 - accuracy: 1.0000\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.0135e-04 - accuracy: 1.0000\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 6.8386e-05 - accuracy: 1.0000\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.5684e-05 - accuracy: 1.0000\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 4.4664e-05 - accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 5.2274e-05 - accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 6.4707e-05 - accuracy: 1.0000\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 4.7495e-04 - accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 6.9818e-05 - accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 4.8580e-05 - accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.0456e-04 - accuracy: 1.0000\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.2356e-04 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.2553e-05 - accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 6.6042e-05 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 5.7793e-05 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.4185e-05 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 8.7831e-05 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.3781e-05 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.9711e-05 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.7886e-05 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 8.8015e-05 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.4765e-05 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 2.8498e-05 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 3.1577e-05 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.3838e-05 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 6.4122e-05 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 4.3786e-05 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.4149e-05 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.1696e-05 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.0855e-05 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.1927e-05 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.7441e-05 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.8512e-05 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.7965e-05 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.7208e-04 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.8946e-05 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.4249e-05 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 2.9456e-05 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.7506e-05 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.3024e-05 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.3554e-05 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 3.9705e-05 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.8123e-05 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.5070e-05 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 4.8043e-05 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.2450e-05 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.1577e-05 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.5020e-05 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.8328e-05 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.7842e-05 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.4918e-05 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.0660e-05 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.8244e-05 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.1645e-05 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.5860e-05 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.0254e-05 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.8754e-05 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.1603e-05 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 3.0334e-05 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.0405e-05 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.8276e-05 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.1920e-05 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 8.9429e-06 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.5212e-05 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.4222e-05 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.1025e-05 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 6.5241e-05 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.6209e-05 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 2.1372e-05 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.7925e-05 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.7500e-05 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.0615e-05 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 9.7258e-06 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 1.5286e-05 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 8.1716e-06 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 5.6934e-06 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.2519e-05 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 1.3653e-05 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb5bf74fbd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpf0XlSARX78",
        "outputId": "1c6a2cc2-b4b8-4f07-856e-8ed6a7b97260"
      },
      "source": [
        "pred_test= model.predict_classes(X_test)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predict   0   1\n",
            "Actual         \n",
            "0        70   2\n",
            "1         1  74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iFNNrlWV9tH",
        "outputId": "42b7f21c-6b66-462a-ca89-218207c2fe32"
      },
      "source": [
        "pred_test"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QISvYcJBgWbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50fba92a-242d-4b89-e4b4-2d80797bb9af"
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[0] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  prediction = model.predict_classes(result)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0   179.0  128.355896  134.201019  ...    0.278518    0.905902    0.641990\n",
            "1   167.0   73.104454   73.209877  ...   93.896088   94.443657   88.234688\n",
            "3   148.0   53.941566   53.861214  ...    6.955443    1.388605    0.352812\n",
            "4   189.0   55.537720   56.871056  ...   78.513031   67.279839   39.524006\n",
            "5   104.0   56.096161   56.142017  ...   71.531067   75.616859   78.566574\n",
            "6   133.0   83.930748   82.711914  ...   60.193909   58.324100   65.814407\n",
            "7   177.0   74.865936   76.760979  ...   55.872543   61.668289   68.452484\n",
            "12  146.0    0.162882    0.142616  ...   77.893227   70.931320   65.676674\n",
            "14  171.0   62.799183   64.504395  ...    0.169488    0.000000    0.169488\n",
            "15  165.0   90.939613  102.888046  ...   66.648743   69.279121   72.063766\n",
            "16  188.0   64.773193   38.411499  ...    9.999093    3.201448    0.212766\n",
            "18  102.0  105.796242  131.442535  ...   58.624378   54.760098   44.154175\n",
            "22  160.0   83.107498   74.320000  ...    0.715000    0.175000    0.000000\n",
            "26  100.0  118.320000  118.737602  ...   91.400002   91.144005   90.657600\n",
            "27  180.0   80.550613   84.473587  ...   61.576797   56.872593   40.553585\n",
            "28  116.0   71.455406   69.621880  ...   77.023773   73.586205   68.470863\n",
            "31  181.0   47.887184   48.872746  ...   82.818375   76.146362   74.638474\n",
            "32  199.0   77.089539   76.645874  ...  136.351776  153.188416  168.855789\n",
            "33  114.0  125.740547  115.145279  ...  163.896286  162.182220  152.888901\n",
            "35  132.0   69.167130   70.120300  ...   45.344353   44.707077   44.791557\n",
            "37  154.0   74.495872   77.504135  ...   92.867775   37.239674   44.198353\n",
            "38  121.0  103.132782   96.392670  ...   67.963120   68.539108   68.296631\n",
            "39  149.0   86.530289   92.274452  ...  101.023331   95.595520   91.840820\n",
            "40  102.0  109.008469  106.367180  ...   85.271057   85.921577   89.530960\n",
            "41  190.0    1.042881    4.113019  ...   52.157005   51.468586   53.163876\n",
            "45  105.0   56.271118   38.466671  ...   66.724449   57.062225   45.826672\n",
            "47  175.0   38.966400   36.528000  ...   27.190397   27.815996   28.039997\n",
            "\n",
            "[27 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRbWgmX_LFH",
        "outputId": "02d58692-ad07-4752-ec34-73bbd5fe11ac"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "from GetBetterSegm import GetBetter"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_paper_fev_2021'...\n",
            "remote: Enumerating objects: 630, done.\u001b[K\n",
            "remote: Counting objects: 100% (391/391), done.\u001b[K\n",
            "remote: Compressing objects: 100% (389/389), done.\u001b[K\n",
            "remote: Total 630 (delta 241), reused 0 (delta 0), pack-reused 239\u001b[K\n",
            "Receiving objects: 100% (630/630), 5.29 MiB | 7.27 MiB/s, done.\n",
            "Resolving deltas: 100% (378/378), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAG_I6FwCvFr",
        "outputId": "05ba32e3-89ae-4bf7-d619-ef03641314ac"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "%cd marquesgabi_out_2020\n",
        "PSD_imageJ = 'Areas_ImageJ.csv'\n",
        "PSD_new = pd.read_csv(PSD_imageJ)\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_out_2020'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 146 (delta 75), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (146/146), 1.00 MiB | 6.08 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020\n",
            "   Juntas   Area\n",
            "0       1  2.001\n",
            "1       2  0.820\n",
            "2       3  1.270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "PekBHQOT_6CP",
        "outputId": "e3c95299-fc0c-4648-a62b-f06ac608291f"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>179.0</td>\n",
              "      <td>128.355896</td>\n",
              "      <td>134.201019</td>\n",
              "      <td>127.940483</td>\n",
              "      <td>107.498550</td>\n",
              "      <td>80.812714</td>\n",
              "      <td>67.831779</td>\n",
              "      <td>67.161636</td>\n",
              "      <td>66.719017</td>\n",
              "      <td>68.405540</td>\n",
              "      <td>74.711334</td>\n",
              "      <td>87.123505</td>\n",
              "      <td>81.718170</td>\n",
              "      <td>58.052055</td>\n",
              "      <td>48.020538</td>\n",
              "      <td>55.396397</td>\n",
              "      <td>67.284294</td>\n",
              "      <td>79.587875</td>\n",
              "      <td>89.900040</td>\n",
              "      <td>99.896477</td>\n",
              "      <td>101.354118</td>\n",
              "      <td>95.750290</td>\n",
              "      <td>64.779282</td>\n",
              "      <td>50.689804</td>\n",
              "      <td>47.925098</td>\n",
              "      <td>46.950092</td>\n",
              "      <td>45.763622</td>\n",
              "      <td>42.315067</td>\n",
              "      <td>52.879341</td>\n",
              "      <td>124.592270</td>\n",
              "      <td>135.129532</td>\n",
              "      <td>134.157959</td>\n",
              "      <td>119.649544</td>\n",
              "      <td>81.069130</td>\n",
              "      <td>64.120415</td>\n",
              "      <td>55.896820</td>\n",
              "      <td>56.287254</td>\n",
              "      <td>56.579445</td>\n",
              "      <td>63.535282</td>\n",
              "      <td>76.773132</td>\n",
              "      <td>...</td>\n",
              "      <td>20.274618</td>\n",
              "      <td>1.422584</td>\n",
              "      <td>0.701227</td>\n",
              "      <td>0.391374</td>\n",
              "      <td>0.465903</td>\n",
              "      <td>0.793546</td>\n",
              "      <td>0.553104</td>\n",
              "      <td>0.381886</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.701476</td>\n",
              "      <td>2.919322</td>\n",
              "      <td>0.591305</td>\n",
              "      <td>0.888143</td>\n",
              "      <td>1.306108</td>\n",
              "      <td>2.127711</td>\n",
              "      <td>1.454262</td>\n",
              "      <td>1.422115</td>\n",
              "      <td>1.856621</td>\n",
              "      <td>1.528167</td>\n",
              "      <td>3.025062</td>\n",
              "      <td>16.176180</td>\n",
              "      <td>51.993412</td>\n",
              "      <td>75.975807</td>\n",
              "      <td>88.637474</td>\n",
              "      <td>85.975807</td>\n",
              "      <td>75.475266</td>\n",
              "      <td>51.040886</td>\n",
              "      <td>11.227273</td>\n",
              "      <td>1.366031</td>\n",
              "      <td>0.551887</td>\n",
              "      <td>0.170407</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.024469</td>\n",
              "      <td>0.054243</td>\n",
              "      <td>0.078587</td>\n",
              "      <td>0.071658</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.278518</td>\n",
              "      <td>0.905902</td>\n",
              "      <td>0.641990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>167.0</td>\n",
              "      <td>73.104454</td>\n",
              "      <td>73.209877</td>\n",
              "      <td>73.980606</td>\n",
              "      <td>66.811249</td>\n",
              "      <td>58.485321</td>\n",
              "      <td>54.546135</td>\n",
              "      <td>50.789993</td>\n",
              "      <td>47.527847</td>\n",
              "      <td>40.913624</td>\n",
              "      <td>40.534195</td>\n",
              "      <td>42.793972</td>\n",
              "      <td>43.020439</td>\n",
              "      <td>43.194633</td>\n",
              "      <td>46.762920</td>\n",
              "      <td>47.774612</td>\n",
              "      <td>48.074299</td>\n",
              "      <td>48.514828</td>\n",
              "      <td>49.421497</td>\n",
              "      <td>48.154976</td>\n",
              "      <td>45.396538</td>\n",
              "      <td>41.271969</td>\n",
              "      <td>41.169064</td>\n",
              "      <td>39.637856</td>\n",
              "      <td>41.891861</td>\n",
              "      <td>46.859230</td>\n",
              "      <td>49.754421</td>\n",
              "      <td>49.392097</td>\n",
              "      <td>64.315178</td>\n",
              "      <td>72.787125</td>\n",
              "      <td>72.062973</td>\n",
              "      <td>68.277710</td>\n",
              "      <td>54.415691</td>\n",
              "      <td>50.091473</td>\n",
              "      <td>51.922161</td>\n",
              "      <td>49.544338</td>\n",
              "      <td>47.154369</td>\n",
              "      <td>44.890896</td>\n",
              "      <td>43.641010</td>\n",
              "      <td>42.931950</td>\n",
              "      <td>...</td>\n",
              "      <td>43.819141</td>\n",
              "      <td>41.978989</td>\n",
              "      <td>41.790386</td>\n",
              "      <td>43.811680</td>\n",
              "      <td>48.827248</td>\n",
              "      <td>54.756397</td>\n",
              "      <td>57.969204</td>\n",
              "      <td>57.387825</td>\n",
              "      <td>58.727531</td>\n",
              "      <td>57.572239</td>\n",
              "      <td>57.293812</td>\n",
              "      <td>52.555672</td>\n",
              "      <td>119.499809</td>\n",
              "      <td>117.823090</td>\n",
              "      <td>116.230171</td>\n",
              "      <td>113.747902</td>\n",
              "      <td>112.219414</td>\n",
              "      <td>102.465675</td>\n",
              "      <td>89.241104</td>\n",
              "      <td>71.294594</td>\n",
              "      <td>58.018970</td>\n",
              "      <td>39.911472</td>\n",
              "      <td>37.543625</td>\n",
              "      <td>41.032089</td>\n",
              "      <td>43.546989</td>\n",
              "      <td>43.136509</td>\n",
              "      <td>42.824345</td>\n",
              "      <td>45.194164</td>\n",
              "      <td>45.519958</td>\n",
              "      <td>50.029694</td>\n",
              "      <td>55.902401</td>\n",
              "      <td>64.442833</td>\n",
              "      <td>77.358353</td>\n",
              "      <td>86.508560</td>\n",
              "      <td>88.541046</td>\n",
              "      <td>88.091759</td>\n",
              "      <td>90.817604</td>\n",
              "      <td>93.896088</td>\n",
              "      <td>94.443657</td>\n",
              "      <td>88.234688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>148.0</td>\n",
              "      <td>53.941566</td>\n",
              "      <td>53.861214</td>\n",
              "      <td>50.330173</td>\n",
              "      <td>45.357197</td>\n",
              "      <td>43.165085</td>\n",
              "      <td>47.290726</td>\n",
              "      <td>51.508404</td>\n",
              "      <td>57.365238</td>\n",
              "      <td>62.281963</td>\n",
              "      <td>64.193573</td>\n",
              "      <td>65.493797</td>\n",
              "      <td>65.298027</td>\n",
              "      <td>60.482109</td>\n",
              "      <td>60.332359</td>\n",
              "      <td>61.048946</td>\n",
              "      <td>60.506214</td>\n",
              "      <td>62.016808</td>\n",
              "      <td>62.139523</td>\n",
              "      <td>60.027031</td>\n",
              "      <td>57.185543</td>\n",
              "      <td>55.705624</td>\n",
              "      <td>50.930611</td>\n",
              "      <td>43.647923</td>\n",
              "      <td>32.392262</td>\n",
              "      <td>15.822500</td>\n",
              "      <td>3.709277</td>\n",
              "      <td>1.900657</td>\n",
              "      <td>1.920380</td>\n",
              "      <td>54.287075</td>\n",
              "      <td>51.196491</td>\n",
              "      <td>47.835648</td>\n",
              "      <td>46.507675</td>\n",
              "      <td>45.780865</td>\n",
              "      <td>48.713661</td>\n",
              "      <td>54.492332</td>\n",
              "      <td>61.128567</td>\n",
              "      <td>65.268082</td>\n",
              "      <td>66.756027</td>\n",
              "      <td>64.518631</td>\n",
              "      <td>...</td>\n",
              "      <td>93.450706</td>\n",
              "      <td>88.845154</td>\n",
              "      <td>66.357201</td>\n",
              "      <td>31.314829</td>\n",
              "      <td>23.849525</td>\n",
              "      <td>24.035061</td>\n",
              "      <td>21.170197</td>\n",
              "      <td>17.123447</td>\n",
              "      <td>13.785976</td>\n",
              "      <td>8.831994</td>\n",
              "      <td>1.584368</td>\n",
              "      <td>0.623813</td>\n",
              "      <td>112.905785</td>\n",
              "      <td>106.017540</td>\n",
              "      <td>99.861221</td>\n",
              "      <td>99.520821</td>\n",
              "      <td>102.088394</td>\n",
              "      <td>103.799866</td>\n",
              "      <td>92.506218</td>\n",
              "      <td>14.993427</td>\n",
              "      <td>6.902849</td>\n",
              "      <td>11.928415</td>\n",
              "      <td>18.710739</td>\n",
              "      <td>18.374727</td>\n",
              "      <td>62.660336</td>\n",
              "      <td>91.247635</td>\n",
              "      <td>94.771362</td>\n",
              "      <td>93.864868</td>\n",
              "      <td>93.365967</td>\n",
              "      <td>95.013153</td>\n",
              "      <td>94.344788</td>\n",
              "      <td>79.799133</td>\n",
              "      <td>39.980282</td>\n",
              "      <td>24.511322</td>\n",
              "      <td>20.473339</td>\n",
              "      <td>17.693937</td>\n",
              "      <td>13.882397</td>\n",
              "      <td>6.955443</td>\n",
              "      <td>1.388605</td>\n",
              "      <td>0.352812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>189.0</td>\n",
              "      <td>55.537720</td>\n",
              "      <td>56.871056</td>\n",
              "      <td>60.448559</td>\n",
              "      <td>73.814812</td>\n",
              "      <td>92.522636</td>\n",
              "      <td>98.626900</td>\n",
              "      <td>102.274353</td>\n",
              "      <td>104.578880</td>\n",
              "      <td>107.165985</td>\n",
              "      <td>104.170097</td>\n",
              "      <td>101.032928</td>\n",
              "      <td>113.825798</td>\n",
              "      <td>117.234573</td>\n",
              "      <td>125.884781</td>\n",
              "      <td>128.846375</td>\n",
              "      <td>106.190674</td>\n",
              "      <td>101.735260</td>\n",
              "      <td>96.862831</td>\n",
              "      <td>86.454056</td>\n",
              "      <td>86.437584</td>\n",
              "      <td>89.005493</td>\n",
              "      <td>93.886147</td>\n",
              "      <td>93.345680</td>\n",
              "      <td>70.796982</td>\n",
              "      <td>58.812073</td>\n",
              "      <td>56.825783</td>\n",
              "      <td>54.835392</td>\n",
              "      <td>55.447189</td>\n",
              "      <td>57.776409</td>\n",
              "      <td>58.528126</td>\n",
              "      <td>61.894379</td>\n",
              "      <td>71.370377</td>\n",
              "      <td>84.421120</td>\n",
              "      <td>93.691360</td>\n",
              "      <td>102.460899</td>\n",
              "      <td>105.292183</td>\n",
              "      <td>105.953369</td>\n",
              "      <td>102.452682</td>\n",
              "      <td>101.213989</td>\n",
              "      <td>...</td>\n",
              "      <td>133.584366</td>\n",
              "      <td>129.257889</td>\n",
              "      <td>113.997253</td>\n",
              "      <td>82.908096</td>\n",
              "      <td>69.028801</td>\n",
              "      <td>70.598076</td>\n",
              "      <td>71.941017</td>\n",
              "      <td>72.486969</td>\n",
              "      <td>71.410149</td>\n",
              "      <td>68.820305</td>\n",
              "      <td>45.153637</td>\n",
              "      <td>35.773663</td>\n",
              "      <td>44.436214</td>\n",
              "      <td>43.769547</td>\n",
              "      <td>44.474625</td>\n",
              "      <td>41.629631</td>\n",
              "      <td>33.925926</td>\n",
              "      <td>22.984913</td>\n",
              "      <td>19.936901</td>\n",
              "      <td>21.495201</td>\n",
              "      <td>24.289440</td>\n",
              "      <td>34.004116</td>\n",
              "      <td>67.614540</td>\n",
              "      <td>82.733871</td>\n",
              "      <td>88.891632</td>\n",
              "      <td>104.532234</td>\n",
              "      <td>124.534988</td>\n",
              "      <td>146.255142</td>\n",
              "      <td>143.271606</td>\n",
              "      <td>134.632385</td>\n",
              "      <td>117.901245</td>\n",
              "      <td>88.743484</td>\n",
              "      <td>69.204391</td>\n",
              "      <td>69.237312</td>\n",
              "      <td>72.289436</td>\n",
              "      <td>74.980797</td>\n",
              "      <td>74.260628</td>\n",
              "      <td>78.513031</td>\n",
              "      <td>67.279839</td>\n",
              "      <td>39.524006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>104.0</td>\n",
              "      <td>56.096161</td>\n",
              "      <td>56.142017</td>\n",
              "      <td>54.931953</td>\n",
              "      <td>53.124268</td>\n",
              "      <td>53.279587</td>\n",
              "      <td>52.612434</td>\n",
              "      <td>50.954151</td>\n",
              "      <td>53.513321</td>\n",
              "      <td>59.434910</td>\n",
              "      <td>69.186401</td>\n",
              "      <td>76.770721</td>\n",
              "      <td>78.664215</td>\n",
              "      <td>78.400894</td>\n",
              "      <td>82.965981</td>\n",
              "      <td>93.096169</td>\n",
              "      <td>97.810654</td>\n",
              "      <td>103.997055</td>\n",
              "      <td>103.144974</td>\n",
              "      <td>90.639061</td>\n",
              "      <td>82.326927</td>\n",
              "      <td>75.491127</td>\n",
              "      <td>75.926041</td>\n",
              "      <td>71.571014</td>\n",
              "      <td>71.208580</td>\n",
              "      <td>72.215988</td>\n",
              "      <td>72.872787</td>\n",
              "      <td>74.022194</td>\n",
              "      <td>73.711548</td>\n",
              "      <td>54.940834</td>\n",
              "      <td>54.315094</td>\n",
              "      <td>53.190834</td>\n",
              "      <td>54.110954</td>\n",
              "      <td>53.828411</td>\n",
              "      <td>53.602077</td>\n",
              "      <td>50.636101</td>\n",
              "      <td>51.011841</td>\n",
              "      <td>51.778114</td>\n",
              "      <td>57.011837</td>\n",
              "      <td>65.690834</td>\n",
              "      <td>...</td>\n",
              "      <td>59.356514</td>\n",
              "      <td>58.383141</td>\n",
              "      <td>60.134621</td>\n",
              "      <td>62.948235</td>\n",
              "      <td>65.301773</td>\n",
              "      <td>68.133141</td>\n",
              "      <td>69.551781</td>\n",
              "      <td>69.936401</td>\n",
              "      <td>70.529594</td>\n",
              "      <td>73.488174</td>\n",
              "      <td>76.900894</td>\n",
              "      <td>77.158295</td>\n",
              "      <td>97.960068</td>\n",
              "      <td>97.679001</td>\n",
              "      <td>95.179001</td>\n",
              "      <td>93.856522</td>\n",
              "      <td>93.612434</td>\n",
              "      <td>93.920135</td>\n",
              "      <td>93.590248</td>\n",
              "      <td>94.795868</td>\n",
              "      <td>95.535515</td>\n",
              "      <td>98.196754</td>\n",
              "      <td>104.284035</td>\n",
              "      <td>107.986694</td>\n",
              "      <td>109.020721</td>\n",
              "      <td>106.914215</td>\n",
              "      <td>96.321014</td>\n",
              "      <td>80.505920</td>\n",
              "      <td>66.600594</td>\n",
              "      <td>60.211544</td>\n",
              "      <td>59.897934</td>\n",
              "      <td>61.630184</td>\n",
              "      <td>63.735214</td>\n",
              "      <td>65.695267</td>\n",
              "      <td>66.713028</td>\n",
              "      <td>68.491135</td>\n",
              "      <td>68.573975</td>\n",
              "      <td>71.531067</td>\n",
              "      <td>75.616859</td>\n",
              "      <td>78.566574</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Width           0           1  ...        781        782        783\n",
              "0  179.0  128.355896  134.201019  ...   0.278518   0.905902   0.641990\n",
              "1  167.0   73.104454   73.209877  ...  93.896088  94.443657  88.234688\n",
              "3  148.0   53.941566   53.861214  ...   6.955443   1.388605   0.352812\n",
              "4  189.0   55.537720   56.871056  ...  78.513031  67.279839  39.524006\n",
              "5  104.0   56.096161   56.142017  ...  71.531067  75.616859  78.566574\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "Area = np.array(PSD_new['Area'])\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Vfk_fNXGDK5_",
        "outputId": "22791414-74ef-4726-8778-88450c7d3f59"
      },
      "source": [
        " wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        " wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        " X = pd.DataFrame([Diam1,Diameter_All])\n",
        " wts = pd.DataFrame([wt1,wt2])\n",
        "plt.hist(X,weights=wts)\n",
        "plt.legend(['Image J','CNN'])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb5bf4e1c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "PLckZ0Db05Om",
        "outputId": "beff8c84-b806-4ea2-f9d2-f65c42d6acb4"
      },
      "source": [
        "Obj = plt.hist(X, density=True, histtype='step', cumulative=True,label='Reversed emp.')\n",
        "Y1, Y2 = Obj[0]\n",
        "Rsquared = r2_score(Y1, Y2)\n",
        "print('r_squared =',Rsquared)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r_squared = 0.95487865257035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "LWx5_fZvdPUO",
        "outputId": "0ff0a28d-492a-4dd3-99a4-8cbed07a24d0"
      },
      "source": [
        "df = pd.DataFrame({'N1':N1, 'N2':N2,'R^2':Rsquared,'Details':Description},\n",
        "                  index= [0])\n",
        "Arq = \"output.xlsx\"\n",
        "df.to_excel(Arq)\n",
        "files.download(Arq)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_758b8696-bfe6-48e8-954c-3d5fec3a971e\", \"output.xlsx\", 5063)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "ZZHa1j4HT9Dq",
        "outputId": "2772397a-ed43-471a-f595-7bddadea01a3"
      },
      "source": [
        "counts, bins, bars = plt.hist(X,weights=wts)\n",
        "print(bars)\n",
        "print(bins)\n",
        "print(counts)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<a list of 2 Lists of Patches objects>\n",
            "[0.61145114 0.78204995 0.95264875 1.12324756 1.29384636 1.46444517\n",
            " 1.63504397 1.80564278 1.97624158 2.14684039 2.31743919]\n",
            "[[ 3.15789474  8.42105263 15.78947368 30.52631579 27.36842105  8.42105263\n",
            "   4.21052632  0.          1.05263158  1.05263158]\n",
            " [ 7.40740741 14.81481481 18.51851852  3.7037037  29.62962963 18.51851852\n",
            "   3.7037037   3.7037037   0.          0.        ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o_vDGeWUwIZ",
        "outputId": "2f48eba9-61a1-4d36-f6e3-c92fcfde0776"
      },
      "source": [
        "print(counts.sum())"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200.0000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "KcH52-6iJQ8t",
        "outputId": "66342373-e768-472c-dd28-3c09abb20e2b"
      },
      "source": [
        "\n",
        "plt.hist([Diam1,Diameter_All])\n",
        "plt.legend(['Image J','CNN'])\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb5bfb39c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r11AxFK_JIii",
        "outputId": "4bb56dd0-e6b8-47ee-8d57-b892e8ce867a"
      },
      "source": [
        "[Diam1,Diameter_All]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.59616801403081,\n",
              "  1.0217907939900581,\n",
              "  1.2716187407449044,\n",
              "  1.104429030701514,\n",
              "  1.2163487785097904,\n",
              "  1.6013445735058454,\n",
              "  1.1715597420637607,\n",
              "  1.2534662333717612,\n",
              "  1.2676073151634049,\n",
              "  1.309600575274104,\n",
              "  1.292966945531582,\n",
              "  1.7658322811231006,\n",
              "  1.3564037533648712,\n",
              "  1.2407040781688483,\n",
              "  2.130217298173151,\n",
              "  1.4228319915327,\n",
              "  1.0651086490865755,\n",
              "  1.3008210311003705,\n",
              "  1.336545951796433,\n",
              "  0.8927754224911278,\n",
              "  1.4494292838262302,\n",
              "  1.4052738287907582,\n",
              "  1.6421697097891788,\n",
              "  1.2329833804288621,\n",
              "  1.19042665178928,\n",
              "  1.1682948223612457,\n",
              "  1.1518314137121108,\n",
              "  0.9607802401865855,\n",
              "  2.317439190074449,\n",
              "  1.0591147430338594,\n",
              "  1.4308630919602832,\n",
              "  0.7535680705496237,\n",
              "  0.8608283307581511,\n",
              "  1.2776122636975893,\n",
              "  1.3745862957220916,\n",
              "  1.259546137598783,\n",
              "  1.2978813187979172,\n",
              "  1.2412170838050638,\n",
              "  1.6009469708743893,\n",
              "  1.3149369953539032,\n",
              "  1.417901703622935,\n",
              "  1.2478669653497139,\n",
              "  1.1055812783082735,\n",
              "  0.9561307405997607,\n",
              "  0.9487783503683882,\n",
              "  1.1238565871041026,\n",
              "  1.2058356273089446,\n",
              "  1.2801012827406097,\n",
              "  0.8733100751144249,\n",
              "  0.9194732501297403,\n",
              "  1.6425573339441792,\n",
              "  1.085826790250066,\n",
              "  1.0639125693728595,\n",
              "  1.0875842666474016,\n",
              "  1.417901703622935,\n",
              "  1.550443891425932,\n",
              "  0.7825779328716171,\n",
              "  1.4690612745308145,\n",
              "  1.053086721720641,\n",
              "  1.2676073151634049,\n",
              "  0.7744003006005755,\n",
              "  1.3787482149724068,\n",
              "  1.363892581861956,\n",
              "  1.299352006316543,\n",
              "  1.2870449283923413,\n",
              "  1.11817763925502,\n",
              "  0.9474354220939228,\n",
              "  1.5218484589055707,\n",
              "  1.3526437911676632,\n",
              "  1.1556938532445284,\n",
              "  1.6013445735058454,\n",
              "  1.274619025074578,\n",
              "  1.422384489715834,\n",
              "  1.3408259533459403,\n",
              "  1.172646028567008,\n",
              "  1.1490645795125545,\n",
              "  1.459060149136146,\n",
              "  1.2483770274864237,\n",
              "  1.336545951796433,\n",
              "  0.9601174044814821,\n",
              "  1.4867225193896279,\n",
              "  1.4277452542806772,\n",
              "  1.35028849808504,\n",
              "  0.7560982446653928,\n",
              "  1.259040600296622,\n",
              "  1.13456827900627,\n",
              "  1.6549133695530214,\n",
              "  1.1204526724091788,\n",
              "  1.1176081573544434,\n",
              "  0.9153095762832032,\n",
              "  1.1639273497938836,\n",
              "  1.3066806149514323,\n",
              "  1.1529362882239027,\n",
              "  1.3047303442899274,\n",
              "  1.3066806149514323],\n",
              " [1.4129902570816042,\n",
              "  1.4079072668412025,\n",
              "  1.3352451323939396,\n",
              "  1.599914817689647,\n",
              "  0.8946152246339834,\n",
              "  1.1605160918627733,\n",
              "  1.504427863256319,\n",
              "  0.9134021544478228,\n",
              "  1.411696310635271,\n",
              "  1.5115890125213567,\n",
              "  1.9260314704209422,\n",
              "  0.8968226584695632,\n",
              "  1.6737835203559153,\n",
              "  0.6114511434670826,\n",
              "  1.5504084978271,\n",
              "  1.0749504135335402,\n",
              "  1.5048870277177024,\n",
              "  1.4402582233259626,\n",
              "  1.076586023720424,\n",
              "  1.0452327627617946,\n",
              "  1.3351360434770179,\n",
              "  1.1059236960435899,\n",
              "  1.0354359893513685,\n",
              "  0.9344368441041114,\n",
              "  1.3015872010702831,\n",
              "  0.6841143385494588,\n",
              "  1.4611362286696612]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}