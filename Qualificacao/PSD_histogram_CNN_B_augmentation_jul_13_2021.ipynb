{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_CNN_B_augmentation_jul_13_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/Qualificacao/PSD_histogram_CNN_B_augmentation_jul_13_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from keras.preprocessing import image"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZEvJvfoibE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8472bf23-4dfd-4c55-8a1f-fa33a69dc29a"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mahotas\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/ad/553b246b0a35dccc3ed58dc8889a67124bf5ab858e9c6b7255d56086e70c/mahotas-1.4.11-cp37-cp37m-manylinux2010_x86_64.whl (5.7MB)\n",
            "\u001b[K     |████████████████████████████████| 5.7MB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.19.5)\n",
            "Installing collected packages: mahotas\n",
            "Successfully installed mahotas-1.4.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT"
      },
      "source": [
        "import mahotas.features.texture as mht\n",
        "import mahotas.features"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VcTdaNVh9EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90a453f9-b89d-40f6-ff16-7fd63f223aff"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_fev_2020'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 73 (delta 37), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n",
            "/content/marquesgabi_fev_2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v7SRrc8mH2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f64a5cc-4fdc-49d8-99ac-0c6d42e9d971"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip'\n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Doutorado'...\n",
            "remote: Enumerating objects: 361, done.\u001b[K\n",
            "remote: Counting objects: 100% (111/111), done.\u001b[K\n",
            "remote: Compressing objects: 100% (110/110), done.\u001b[K\n",
            "remote: Total 361 (delta 38), reused 0 (delta 0), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (361/361), 202.49 MiB | 28.02 MiB/s, done.\n",
            "Resolving deltas: 100% (155/155), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[4] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHgqAnaFyCjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c198632f-8af0-4fad-a10d-7041a97c35a3"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (160/160), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 3 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 19.33 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from segment_filter_not_conclude import Segmenta  # got image provided segmented"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5MN5a_v4np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35d3c8bd-7fb3-46e6-c5b6-530b4b2fb01f"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     197  207.617859  213.103302  ...    0.297276    1.584375    0.762658\n",
            "1     103  252.052216  252.980850  ...  200.482315  192.536041  182.760193\n",
            "2     128  153.305664  168.249023  ...  125.667969  122.086914  120.923828\n",
            "3     131   92.519493   91.574554  ...  161.328949  150.257202  135.541397\n",
            "4     123  133.072189  125.464813  ...  129.923737  110.246613  134.161880\n",
            "5     130  149.885910  134.547226  ...  137.793854  129.984619  129.809464\n",
            "6     168  141.583328  141.305557  ...  105.777779  111.194443  117.750000\n",
            "7     140  189.479996  169.599991  ...  145.000000  143.679993  145.800003\n",
            "8     183  185.236069  168.600830  ...   70.325333   59.045029   19.702320\n",
            "9     150  153.931381  125.737602  ...    1.000000    1.000000    1.000000\n",
            "10    185   93.817291   88.901764  ...    1.298583    0.137706    1.325610\n",
            "11    137  141.275986  130.605194  ...  232.833557  227.453613  229.094315\n",
            "12    128  179.391602  169.509766  ...  127.042969  135.901367  138.471680\n",
            "13    189  179.831284  177.581635  ...   69.116600   53.323734    9.496572\n",
            "14    141  168.665192  174.408691  ...  190.783752  186.498871  187.924652\n",
            "15    187  216.523087  194.429779  ...  198.498886  192.216934  176.201263\n",
            "16    100  104.817604  104.991997  ...  103.062401   99.854401   93.427200\n",
            "17    143    1.234143    0.213164  ...   93.515190   99.087975  109.200645\n",
            "18    139  132.385071  123.098900  ...  117.422638  120.839142  120.226227\n",
            "19    189   70.030174   43.739372  ...    0.218107    1.144033    1.429355\n",
            "20    171  184.226334  179.351868  ...    3.383332   13.825997   41.980232\n",
            "21    110   82.069420   94.210251  ...    1.101488    3.392066    4.572892\n",
            "22    148  118.539085   94.546379  ...    0.617969    0.373265    1.146092\n",
            "23    191  144.407806  137.258560  ...  144.878723  143.959747  142.938934\n",
            "24    121  155.713425  152.216095  ...  243.293762  248.718323  248.617798\n",
            "25    103   82.912621   86.552361  ...  103.671875  102.926666  103.174759\n",
            "26    153   17.901405   28.186169  ...    0.654406    0.314537    1.399504\n",
            "27    173  175.123459  188.606964  ...    1.033379    0.174847    1.354572\n",
            "28    175  136.364807  134.393585  ...  219.382370  224.318405  231.675201\n",
            "29    174  175.834335  157.197800  ...    1.080988    1.035011    0.223808\n",
            "30    189   59.759953   62.725651  ...  158.379990  170.895752  178.939651\n",
            "31    124   46.976067   49.182098  ...  141.791885  157.783554  163.008316\n",
            "32    191  191.894363  177.631821  ...  157.237045  203.117981  245.946045\n",
            "33    197  113.571526  108.952797  ...  129.377975  129.990021  132.125488\n",
            "34    141  185.477859  182.204056  ...  173.007950  122.867020  136.014435\n",
            "35    164  139.089233  154.119583  ...  123.654373  138.903046  128.041641\n",
            "36    101  126.141953  119.684547  ...   69.560440   20.512205    9.876973\n",
            "37    141  131.266449  139.834229  ...  175.497162  173.404312  168.547913\n",
            "38    177  167.113876  167.030197  ...   89.120934   66.928749   45.826771\n",
            "39    198  139.560226  139.694916  ...  207.900604  215.544205  216.783264\n",
            "40    124  145.038483  144.665970  ...  151.978149  127.893860  112.671173\n",
            "41    109   69.488426   65.599525  ...    0.014140    0.850013    1.579749\n",
            "42    199  129.476761  141.750641  ...   90.363655   81.052467   74.639854\n",
            "43    179    1.619581    2.490840  ...  159.427765  156.336349  152.142166\n",
            "44    162   74.420670   73.985977  ...    0.793629    0.239750    1.375553\n",
            "45    157  252.740265  251.158798  ...  135.281845  134.961578  126.701607\n",
            "46    129  127.196861  123.071266  ...  150.437180  147.649231  140.509277\n",
            "47    132  135.118454  146.370071  ...  139.686874  144.914597  155.748398\n",
            "48    169  171.112900  171.419678  ...  123.907272  119.376236  132.668137\n",
            "49    139  111.685783  110.668442  ...    0.421873    0.450650    1.443455\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzpQ1Pz0fX5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "834a91b2-bcb9-42e0-9ec4-f9b474eadcf0"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines\n",
        "# filename = 'model_ANN.pkl'\n",
        "filename = 'model_ANN_new.pkl'\n",
        "model = joblib.load(filename)\n",
        "'''"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n!git clone https://github.com/ucfilho/MarquesGabi_Routines\\n%cd MarquesGabi_Routines\\n# filename = 'model_ANN.pkl'\\nfilename = 'model_ANN_new.pkl'\\nmodel = joblib.load(filename)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2emP4rNjQy",
        "outputId": "3ab7695f-878e-4c08-9bff-92c53d513a41"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 21.84 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "# make the CNN\n",
        "# model.add(Input(shape=(28, 28, 1)))\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=32, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=200))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=10))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbPQ1FSRG6A",
        "outputId": "6b540761-435e-43a3-ee68-43bc0f9de03a"
      },
      "source": [
        "\n",
        "# gives us back a <keras.callbacks.History object at 0x112e61a90>\n",
        "model.fit(X_train, Y_train, epochs=200, batch_size=32)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 46s 17ms/step - loss: 0.7511 - accuracy: 0.6436\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.5068 - accuracy: 0.7590\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.3495 - accuracy: 0.8200\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.1906 - accuracy: 0.9298\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1320 - accuracy: 0.9601\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0914 - accuracy: 0.9650\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0610 - accuracy: 0.9794\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0347 - accuracy: 0.9967\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0318 - accuracy: 0.9918\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0880 - accuracy: 0.9614\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0563 - accuracy: 0.9758\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.9949\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0176 - accuracy: 0.9958\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 0.0116 - accuracy: 1.0000\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.9973\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0075 - accuracy: 0.9966\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0085 - accuracy: 0.9973\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0090 - accuracy: 0.9965\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0084 - accuracy: 0.9919\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.7730e-04 - accuracy: 1.0000\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 0.9995\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.9993\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0073 - accuracy: 0.9919\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.6394e-04 - accuracy: 1.0000\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.2320e-04 - accuracy: 1.0000\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.6598e-04 - accuracy: 1.0000\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.1288e-04 - accuracy: 1.0000\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2752e-04 - accuracy: 1.0000\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0433e-04 - accuracy: 1.0000\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 8.0369e-04 - accuracy: 1.0000\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2.9772e-04 - accuracy: 1.0000\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 1.9154e-04 - accuracy: 1.0000\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0297e-04 - accuracy: 1.0000\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.6250e-04 - accuracy: 1.0000\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1663e-04 - accuracy: 1.0000\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 2.1148e-04 - accuracy: 1.0000\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0969e-04 - accuracy: 1.0000\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0015e-04 - accuracy: 1.0000\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.1924e-04 - accuracy: 1.0000\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.5072e-04 - accuracy: 1.0000\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4470e-04 - accuracy: 1.0000\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7882e-04 - accuracy: 1.0000\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2014e-04 - accuracy: 1.0000\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2022e-04 - accuracy: 1.0000\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6897e-04 - accuracy: 1.0000\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 3ms/step - loss: 7.1606e-04 - accuracy: 0.9995\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0067 - accuracy: 0.9945\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0211 - accuracy: 0.9951\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0151 - accuracy: 0.9904\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0202 - accuracy: 0.9979\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0311 - accuracy: 0.9909\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0082 - accuracy: 0.9972\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0145 - accuracy: 0.9896\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0411 - accuracy: 0.9899\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1911 - accuracy: 0.9654\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0434 - accuracy: 0.9849\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0342 - accuracy: 0.9958\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0358 - accuracy: 0.9877\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0169 - accuracy: 0.9916\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0225 - accuracy: 0.9958\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.9945\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0065 - accuracy: 0.9958\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3536e-04 - accuracy: 1.0000\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.6930e-04 - accuracy: 1.0000\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.5552e-04 - accuracy: 1.0000\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.8387e-04 - accuracy: 1.0000\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.1551e-04 - accuracy: 1.0000\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.7283e-04 - accuracy: 1.0000\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.5821e-04 - accuracy: 1.0000\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.3995e-04 - accuracy: 1.0000\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6153e-04 - accuracy: 1.0000\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.2832e-04 - accuracy: 1.0000\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1979e-04 - accuracy: 1.0000\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2904e-04 - accuracy: 1.0000\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1608e-04 - accuracy: 1.0000\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.4585e-04 - accuracy: 1.0000\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0786e-04 - accuracy: 1.0000\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6733e-04 - accuracy: 1.0000\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8889e-04 - accuracy: 1.0000\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5121e-04 - accuracy: 1.0000\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5226e-04 - accuracy: 1.0000\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.3705e-05 - accuracy: 1.0000\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1218e-04 - accuracy: 1.0000\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4871e-04 - accuracy: 1.0000\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5514e-05 - accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.9139e-04 - accuracy: 1.0000\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6719e-04 - accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.5893e-04 - accuracy: 1.0000\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9873e-04 - accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4364e-04 - accuracy: 1.0000\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.7920e-05 - accuracy: 1.0000\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1530e-04 - accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0622e-04 - accuracy: 1.0000\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3915e-04 - accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.7988e-05 - accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3319e-04 - accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.4453e-05 - accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.7002e-05 - accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.7247e-05 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2453e-04 - accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.4798e-05 - accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4509e-04 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3045e-04 - accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5551e-05 - accuracy: 1.0000\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9946e-04 - accuracy: 1.0000\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6438e-04 - accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.1226e-05 - accuracy: 1.0000\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5929e-04 - accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.4039e-05 - accuracy: 1.0000\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.4314e-05 - accuracy: 1.0000\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.2754e-05 - accuracy: 1.0000\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.3838e-05 - accuracy: 1.0000\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.9490e-05 - accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.5586e-05 - accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.8010e-05 - accuracy: 1.0000\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2726e-05 - accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7260e-04 - accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.3746e-05 - accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.4666e-05 - accuracy: 1.0000\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.1747e-05 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6916e-05 - accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.3813e-05 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.4565e-05 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.7307e-05 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.8685e-05 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1287e-05 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0556e-05 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.2572e-05 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2036e-05 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.3068e-05 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.8249e-05 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0111e-05 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7696e-05 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3351e-05 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.1758e-05 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.3931e-05 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5654e-05 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.9308e-05 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.4863e-05 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4015e-04 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6376e-05 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.1766e-05 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5549e-05 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3403e-05 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.5133e-05 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5961e-04 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.7803e-05 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5077e-05 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.6221e-05 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.0931e-05 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.8320e-05 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9920e-05 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2523e-05 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7766e-05 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.0731e-05 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5196e-05 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.2797e-05 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6686e-05 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.3798e-05 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1801e-05 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2403e-05 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6055e-05 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2300e-05 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7088e-05 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7170e-05 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3730e-05 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4771e-05 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.5939e-05 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0258e-05 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6378e-05 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6131e-05 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4333e-05 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.1123e-05 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.8223e-05 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.6533e-05 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5110e-04 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3724e-05 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1019e-05 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.0434e-05 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9855e-05 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2994e-05 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4446e-05 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4216e-05 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2586e-05 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2511e-05 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6366e-05 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb73b194d50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRaP8bHWNeZA",
        "outputId": "3b6b903f-75db-4db6-ee18-d3c0aa8e4b4d"
      },
      "source": [
        "\n",
        "# Fit with data augmentation\n",
        "# Note: if you run this AFTER calling the previous model.fit(), it will CONTINUE training where it left off\n",
        "batch_size = 5\n",
        "data_generator = image.ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "train_generator = data_generator.flow(X_train, Y_train, batch_size)\n",
        "steps_per_epoch = X_train.shape[0] // batch_size\n",
        "\n",
        "model.fit(train_generator, validation_data=(X_test, Y_test), steps_per_epoch=steps_per_epoch, epochs=200)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "68/68 [==============================] - 2s 17ms/step - loss: 0.8227 - accuracy: 0.5533 - val_loss: 0.6936 - val_accuracy: 0.4898\n",
            "Epoch 2/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6951 - accuracy: 0.5000 - val_loss: 0.6937 - val_accuracy: 0.4898\n",
            "Epoch 3/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5059 - val_loss: 0.6938 - val_accuracy: 0.4898\n",
            "Epoch 4/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5030 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 5/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6936 - accuracy: 0.4675 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 6/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5148 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 7/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6168 - accuracy: 0.7160 - val_loss: 9.3577 - val_accuracy: 0.5102\n",
            "Epoch 8/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.5962 - accuracy: 0.7515 - val_loss: 8.1628 - val_accuracy: 0.5102\n",
            "Epoch 9/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.5112 - accuracy: 0.8254 - val_loss: 0.7074 - val_accuracy: 0.4898\n",
            "Epoch 10/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.4583 - accuracy: 0.8609 - val_loss: 14.8245 - val_accuracy: 0.5102\n",
            "Epoch 11/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.4111 - accuracy: 0.8698 - val_loss: 13.1152 - val_accuracy: 0.5102\n",
            "Epoch 12/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.4123 - accuracy: 0.8609 - val_loss: 42.1100 - val_accuracy: 0.5102\n",
            "Epoch 13/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.4217 - accuracy: 0.8550 - val_loss: 8.9022 - val_accuracy: 0.5102\n",
            "Epoch 14/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.4268 - accuracy: 0.8698 - val_loss: 0.7453 - val_accuracy: 0.5170\n",
            "Epoch 15/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.3645 - accuracy: 0.8817 - val_loss: 5.7460 - val_accuracy: 0.5102\n",
            "Epoch 16/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.3522 - accuracy: 0.8905 - val_loss: 0.7926 - val_accuracy: 0.4966\n",
            "Epoch 17/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.3231 - accuracy: 0.8964 - val_loss: 17.4133 - val_accuracy: 0.5102\n",
            "Epoch 18/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.3151 - accuracy: 0.9083 - val_loss: 26.6155 - val_accuracy: 0.5102\n",
            "Epoch 19/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.3589 - accuracy: 0.8757 - val_loss: 8.4277 - val_accuracy: 0.5102\n",
            "Epoch 20/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2994 - accuracy: 0.9053 - val_loss: 31.1730 - val_accuracy: 0.5102\n",
            "Epoch 21/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.3018 - accuracy: 0.8935 - val_loss: 81.6311 - val_accuracy: 0.5102\n",
            "Epoch 22/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2858 - accuracy: 0.9053 - val_loss: 33.9933 - val_accuracy: 0.5102\n",
            "Epoch 23/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2793 - accuracy: 0.9024 - val_loss: 109.8185 - val_accuracy: 0.5102\n",
            "Epoch 24/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2556 - accuracy: 0.9142 - val_loss: 23.8852 - val_accuracy: 0.5102\n",
            "Epoch 25/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2361 - accuracy: 0.9497 - val_loss: 7.1868 - val_accuracy: 0.5102\n",
            "Epoch 26/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2662 - accuracy: 0.9083 - val_loss: 0.9448 - val_accuracy: 0.4966\n",
            "Epoch 27/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.3264 - accuracy: 0.8639 - val_loss: 0.9624 - val_accuracy: 0.4898\n",
            "Epoch 28/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2949 - accuracy: 0.8905 - val_loss: 3.5272 - val_accuracy: 0.5102\n",
            "Epoch 29/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2295 - accuracy: 0.9290 - val_loss: 5.5181 - val_accuracy: 0.5102\n",
            "Epoch 30/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2340 - accuracy: 0.9172 - val_loss: 4.5106 - val_accuracy: 0.5102\n",
            "Epoch 31/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.3331 - accuracy: 0.8846 - val_loss: 1.0139 - val_accuracy: 0.4898\n",
            "Epoch 32/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2503 - accuracy: 0.9290 - val_loss: 0.9963 - val_accuracy: 0.4898\n",
            "Epoch 33/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2100 - accuracy: 0.9408 - val_loss: 17.4800 - val_accuracy: 0.5102\n",
            "Epoch 34/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2059 - accuracy: 0.9408 - val_loss: 10.4989 - val_accuracy: 0.5102\n",
            "Epoch 35/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2649 - accuracy: 0.9024 - val_loss: 3.6614 - val_accuracy: 0.5102\n",
            "Epoch 36/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.2099 - accuracy: 0.9320 - val_loss: 7.4900 - val_accuracy: 0.5102\n",
            "Epoch 37/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1739 - accuracy: 0.9349 - val_loss: 12.7800 - val_accuracy: 0.5102\n",
            "Epoch 38/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1935 - accuracy: 0.9438 - val_loss: 8.4152 - val_accuracy: 0.5102\n",
            "Epoch 39/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1961 - accuracy: 0.9349 - val_loss: 0.5415 - val_accuracy: 0.7959\n",
            "Epoch 40/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1768 - accuracy: 0.9408 - val_loss: 7.9915 - val_accuracy: 0.5102\n",
            "Epoch 41/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1888 - accuracy: 0.9290 - val_loss: 1.1152 - val_accuracy: 0.4898\n",
            "Epoch 42/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1927 - accuracy: 0.9260 - val_loss: 25.6299 - val_accuracy: 0.5102\n",
            "Epoch 43/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1669 - accuracy: 0.9379 - val_loss: 11.4789 - val_accuracy: 0.5102\n",
            "Epoch 44/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1998 - accuracy: 0.9201 - val_loss: 25.0743 - val_accuracy: 0.5102\n",
            "Epoch 45/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1322 - accuracy: 0.9408 - val_loss: 17.0245 - val_accuracy: 0.5102\n",
            "Epoch 46/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1910 - accuracy: 0.9349 - val_loss: 7.7530 - val_accuracy: 0.5102\n",
            "Epoch 47/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1453 - accuracy: 0.9527 - val_loss: 0.5072 - val_accuracy: 0.7551\n",
            "Epoch 48/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1502 - accuracy: 0.9497 - val_loss: 43.5343 - val_accuracy: 0.5102\n",
            "Epoch 49/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1364 - accuracy: 0.9527 - val_loss: 67.2654 - val_accuracy: 0.5102\n",
            "Epoch 50/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1824 - accuracy: 0.9260 - val_loss: 35.3569 - val_accuracy: 0.4898\n",
            "Epoch 51/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1681 - accuracy: 0.9379 - val_loss: 5.6361 - val_accuracy: 0.5102\n",
            "Epoch 52/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1506 - accuracy: 0.9201 - val_loss: 4.1246 - val_accuracy: 0.5102\n",
            "Epoch 53/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1401 - accuracy: 0.9586 - val_loss: 0.4182 - val_accuracy: 0.8027\n",
            "Epoch 54/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1825 - accuracy: 0.9320 - val_loss: 1.1461 - val_accuracy: 0.5238\n",
            "Epoch 55/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1423 - accuracy: 0.9379 - val_loss: 1.9937 - val_accuracy: 0.5374\n",
            "Epoch 56/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1306 - accuracy: 0.9527 - val_loss: 9.2278 - val_accuracy: 0.5102\n",
            "Epoch 57/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1903 - accuracy: 0.9172 - val_loss: 0.6119 - val_accuracy: 0.6667\n",
            "Epoch 58/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1697 - accuracy: 0.9379 - val_loss: 6.0967 - val_accuracy: 0.4898\n",
            "Epoch 59/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1177 - accuracy: 0.9527 - val_loss: 0.8974 - val_accuracy: 0.5578\n",
            "Epoch 60/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1510 - accuracy: 0.9438 - val_loss: 9.4084 - val_accuracy: 0.4898\n",
            "Epoch 61/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1704 - accuracy: 0.9320 - val_loss: 13.2429 - val_accuracy: 0.4898\n",
            "Epoch 62/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1915 - accuracy: 0.9379 - val_loss: 19.2521 - val_accuracy: 0.5102\n",
            "Epoch 63/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1286 - accuracy: 0.9527 - val_loss: 25.8527 - val_accuracy: 0.5102\n",
            "Epoch 64/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1187 - accuracy: 0.9408 - val_loss: 17.2555 - val_accuracy: 0.5102\n",
            "Epoch 65/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1351 - accuracy: 0.9408 - val_loss: 42.0519 - val_accuracy: 0.5102\n",
            "Epoch 66/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1482 - accuracy: 0.9497 - val_loss: 2.1547 - val_accuracy: 0.5102\n",
            "Epoch 67/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1387 - accuracy: 0.9497 - val_loss: 29.3281 - val_accuracy: 0.4898\n",
            "Epoch 68/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1380 - accuracy: 0.9408 - val_loss: 12.6145 - val_accuracy: 0.4898\n",
            "Epoch 69/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1351 - accuracy: 0.9467 - val_loss: 22.3138 - val_accuracy: 0.4898\n",
            "Epoch 70/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0730 - accuracy: 0.9734 - val_loss: 13.1193 - val_accuracy: 0.4898\n",
            "Epoch 71/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1064 - accuracy: 0.9556 - val_loss: 26.7912 - val_accuracy: 0.4898\n",
            "Epoch 72/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1780 - accuracy: 0.9438 - val_loss: 11.5180 - val_accuracy: 0.4898\n",
            "Epoch 73/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1113 - accuracy: 0.9586 - val_loss: 7.5131 - val_accuracy: 0.4898\n",
            "Epoch 74/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1095 - accuracy: 0.9645 - val_loss: 9.6812 - val_accuracy: 0.4898\n",
            "Epoch 75/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1314 - accuracy: 0.9527 - val_loss: 1.0611 - val_accuracy: 0.4898\n",
            "Epoch 76/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0939 - accuracy: 0.9704 - val_loss: 10.8360 - val_accuracy: 0.5102\n",
            "Epoch 77/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1235 - accuracy: 0.9497 - val_loss: 0.8063 - val_accuracy: 0.5238\n",
            "Epoch 78/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0997 - accuracy: 0.9556 - val_loss: 1.1384 - val_accuracy: 0.4898\n",
            "Epoch 79/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1234 - accuracy: 0.9586 - val_loss: 1.1615 - val_accuracy: 0.4898\n",
            "Epoch 80/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1170 - accuracy: 0.9586 - val_loss: 18.9810 - val_accuracy: 0.4898\n",
            "Epoch 81/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1305 - accuracy: 0.9408 - val_loss: 1.3115 - val_accuracy: 0.4966\n",
            "Epoch 82/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1110 - accuracy: 0.9615 - val_loss: 1.2304 - val_accuracy: 0.5442\n",
            "Epoch 83/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1373 - accuracy: 0.9467 - val_loss: 1.0455 - val_accuracy: 0.5102\n",
            "Epoch 84/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1221 - accuracy: 0.9497 - val_loss: 8.0352 - val_accuracy: 0.5102\n",
            "Epoch 85/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1373 - accuracy: 0.9320 - val_loss: 10.0230 - val_accuracy: 0.5102\n",
            "Epoch 86/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0946 - accuracy: 0.9704 - val_loss: 5.3706 - val_accuracy: 0.5102\n",
            "Epoch 87/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1487 - accuracy: 0.9438 - val_loss: 5.9340 - val_accuracy: 0.4898\n",
            "Epoch 88/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0874 - accuracy: 0.9615 - val_loss: 7.5772 - val_accuracy: 0.5102\n",
            "Epoch 89/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0854 - accuracy: 0.9675 - val_loss: 0.5237 - val_accuracy: 0.7143\n",
            "Epoch 90/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1060 - accuracy: 0.9586 - val_loss: 1.7787 - val_accuracy: 0.4898\n",
            "Epoch 91/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1281 - accuracy: 0.9556 - val_loss: 24.8944 - val_accuracy: 0.4898\n",
            "Epoch 92/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0717 - accuracy: 0.9734 - val_loss: 22.5434 - val_accuracy: 0.4898\n",
            "Epoch 93/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1042 - accuracy: 0.9615 - val_loss: 13.9902 - val_accuracy: 0.4898\n",
            "Epoch 94/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0936 - accuracy: 0.9704 - val_loss: 10.0093 - val_accuracy: 0.4898\n",
            "Epoch 95/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1089 - accuracy: 0.9615 - val_loss: 5.1419 - val_accuracy: 0.4898\n",
            "Epoch 96/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0942 - accuracy: 0.9645 - val_loss: 7.0726 - val_accuracy: 0.4898\n",
            "Epoch 97/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0959 - accuracy: 0.9586 - val_loss: 17.7876 - val_accuracy: 0.4898\n",
            "Epoch 98/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1212 - accuracy: 0.9704 - val_loss: 8.1352 - val_accuracy: 0.4898\n",
            "Epoch 99/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0934 - accuracy: 0.9615 - val_loss: 2.5266 - val_accuracy: 0.5850\n",
            "Epoch 100/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0928 - accuracy: 0.9645 - val_loss: 3.5442 - val_accuracy: 0.4898\n",
            "Epoch 101/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1062 - accuracy: 0.9586 - val_loss: 0.9970 - val_accuracy: 0.5238\n",
            "Epoch 102/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1179 - accuracy: 0.9615 - val_loss: 1.7994 - val_accuracy: 0.5102\n",
            "Epoch 103/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0932 - accuracy: 0.9645 - val_loss: 18.4421 - val_accuracy: 0.5102\n",
            "Epoch 104/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1567 - accuracy: 0.9353 - val_loss: 20.9619 - val_accuracy: 0.5102\n",
            "Epoch 105/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0889 - accuracy: 0.9645 - val_loss: 1.6099 - val_accuracy: 0.5102\n",
            "Epoch 106/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0991 - accuracy: 0.9645 - val_loss: 3.0046 - val_accuracy: 0.5102\n",
            "Epoch 107/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1388 - accuracy: 0.9586 - val_loss: 6.6034 - val_accuracy: 0.4898\n",
            "Epoch 108/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0887 - accuracy: 0.9645 - val_loss: 25.1925 - val_accuracy: 0.5102\n",
            "Epoch 109/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0930 - accuracy: 0.9645 - val_loss: 19.8571 - val_accuracy: 0.5102\n",
            "Epoch 110/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1217 - accuracy: 0.9527 - val_loss: 20.6644 - val_accuracy: 0.5102\n",
            "Epoch 111/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1093 - accuracy: 0.9467 - val_loss: 0.7703 - val_accuracy: 0.7347\n",
            "Epoch 112/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1644 - accuracy: 0.9349 - val_loss: 8.8980 - val_accuracy: 0.5102\n",
            "Epoch 113/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1049 - accuracy: 0.9556 - val_loss: 3.0725 - val_accuracy: 0.5102\n",
            "Epoch 114/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1226 - accuracy: 0.9527 - val_loss: 8.4534 - val_accuracy: 0.4898\n",
            "Epoch 115/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0846 - accuracy: 0.9645 - val_loss: 8.2311 - val_accuracy: 0.4898\n",
            "Epoch 116/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0732 - accuracy: 0.9645 - val_loss: 22.4477 - val_accuracy: 0.4898\n",
            "Epoch 117/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1216 - accuracy: 0.9556 - val_loss: 3.9141 - val_accuracy: 0.4898\n",
            "Epoch 118/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0905 - accuracy: 0.9615 - val_loss: 2.8605 - val_accuracy: 0.5238\n",
            "Epoch 119/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0897 - accuracy: 0.9645 - val_loss: 7.4173 - val_accuracy: 0.5102\n",
            "Epoch 120/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1214 - accuracy: 0.9586 - val_loss: 10.9397 - val_accuracy: 0.4898\n",
            "Epoch 121/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1051 - accuracy: 0.9556 - val_loss: 1.7001 - val_accuracy: 0.5102\n",
            "Epoch 122/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0984 - accuracy: 0.9556 - val_loss: 0.9169 - val_accuracy: 0.6803\n",
            "Epoch 123/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0774 - accuracy: 0.9645 - val_loss: 1.5237 - val_accuracy: 0.4966\n",
            "Epoch 124/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0681 - accuracy: 0.9734 - val_loss: 49.1360 - val_accuracy: 0.4898\n",
            "Epoch 125/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1092 - accuracy: 0.9556 - val_loss: 1.1212 - val_accuracy: 0.4898\n",
            "Epoch 126/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0834 - accuracy: 0.9645 - val_loss: 1.4128 - val_accuracy: 0.5306\n",
            "Epoch 127/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0973 - accuracy: 0.9704 - val_loss: 7.4591 - val_accuracy: 0.5102\n",
            "Epoch 128/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0625 - accuracy: 0.9675 - val_loss: 25.9640 - val_accuracy: 0.5102\n",
            "Epoch 129/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1102 - accuracy: 0.9497 - val_loss: 25.4619 - val_accuracy: 0.5102\n",
            "Epoch 130/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0592 - accuracy: 0.9793 - val_loss: 7.9240 - val_accuracy: 0.5102\n",
            "Epoch 131/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1267 - accuracy: 0.9438 - val_loss: 10.3637 - val_accuracy: 0.4898\n",
            "Epoch 132/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0625 - accuracy: 0.9822 - val_loss: 3.5168 - val_accuracy: 0.5102\n",
            "Epoch 133/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1289 - accuracy: 0.9556 - val_loss: 13.4220 - val_accuracy: 0.5102\n",
            "Epoch 134/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0997 - accuracy: 0.9615 - val_loss: 4.3030 - val_accuracy: 0.5102\n",
            "Epoch 135/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0782 - accuracy: 0.9645 - val_loss: 9.7680 - val_accuracy: 0.4898\n",
            "Epoch 136/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0841 - accuracy: 0.9763 - val_loss: 24.4651 - val_accuracy: 0.5102\n",
            "Epoch 137/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1693 - accuracy: 0.9586 - val_loss: 7.5595 - val_accuracy: 0.4898\n",
            "Epoch 138/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1005 - accuracy: 0.9497 - val_loss: 4.4490 - val_accuracy: 0.4898\n",
            "Epoch 139/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1133 - accuracy: 0.9527 - val_loss: 1.1064 - val_accuracy: 0.4898\n",
            "Epoch 140/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0898 - accuracy: 0.9645 - val_loss: 3.2421 - val_accuracy: 0.4898\n",
            "Epoch 141/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0936 - accuracy: 0.9645 - val_loss: 6.5336 - val_accuracy: 0.4898\n",
            "Epoch 142/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0793 - accuracy: 0.9704 - val_loss: 6.3179 - val_accuracy: 0.4898\n",
            "Epoch 143/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0454 - accuracy: 0.9852 - val_loss: 0.8490 - val_accuracy: 0.6122\n",
            "Epoch 144/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0795 - accuracy: 0.9793 - val_loss: 0.7803 - val_accuracy: 0.5238\n",
            "Epoch 145/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0633 - accuracy: 0.9763 - val_loss: 4.8791 - val_accuracy: 0.4898\n",
            "Epoch 146/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0763 - accuracy: 0.9734 - val_loss: 2.9646 - val_accuracy: 0.5102\n",
            "Epoch 147/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0990 - accuracy: 0.9675 - val_loss: 1.6310 - val_accuracy: 0.5102\n",
            "Epoch 148/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0815 - accuracy: 0.9763 - val_loss: 16.1561 - val_accuracy: 0.5102\n",
            "Epoch 149/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0512 - accuracy: 0.9852 - val_loss: 0.6480 - val_accuracy: 0.7143\n",
            "Epoch 150/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0971 - accuracy: 0.9645 - val_loss: 12.9702 - val_accuracy: 0.5102\n",
            "Epoch 151/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0723 - accuracy: 0.9615 - val_loss: 11.1235 - val_accuracy: 0.4898\n",
            "Epoch 152/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1139 - accuracy: 0.9556 - val_loss: 7.9049 - val_accuracy: 0.4898\n",
            "Epoch 153/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0880 - accuracy: 0.9675 - val_loss: 12.0644 - val_accuracy: 0.5102\n",
            "Epoch 154/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0993 - accuracy: 0.9556 - val_loss: 3.9452 - val_accuracy: 0.5102\n",
            "Epoch 155/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1030 - accuracy: 0.9586 - val_loss: 0.7832 - val_accuracy: 0.5782\n",
            "Epoch 156/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0787 - accuracy: 0.9615 - val_loss: 6.6778 - val_accuracy: 0.5102\n",
            "Epoch 157/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0573 - accuracy: 0.9734 - val_loss: 0.6464 - val_accuracy: 0.7551\n",
            "Epoch 158/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1335 - accuracy: 0.9467 - val_loss: 37.3055 - val_accuracy: 0.5102\n",
            "Epoch 159/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0947 - accuracy: 0.9645 - val_loss: 10.1971 - val_accuracy: 0.5102\n",
            "Epoch 160/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0652 - accuracy: 0.9793 - val_loss: 1.9436 - val_accuracy: 0.5102\n",
            "Epoch 161/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1028 - accuracy: 0.9675 - val_loss: 2.7361 - val_accuracy: 0.4898\n",
            "Epoch 162/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0683 - accuracy: 0.9763 - val_loss: 8.1351 - val_accuracy: 0.4898\n",
            "Epoch 163/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0608 - accuracy: 0.9793 - val_loss: 1.1178 - val_accuracy: 0.4898\n",
            "Epoch 164/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0736 - accuracy: 0.9793 - val_loss: 34.2016 - val_accuracy: 0.5102\n",
            "Epoch 165/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.1085 - accuracy: 0.9704 - val_loss: 7.3881 - val_accuracy: 0.4898\n",
            "Epoch 166/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0856 - accuracy: 0.9615 - val_loss: 6.7563 - val_accuracy: 0.5102\n",
            "Epoch 167/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0659 - accuracy: 0.9763 - val_loss: 6.0967 - val_accuracy: 0.4898\n",
            "Epoch 168/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0616 - accuracy: 0.9793 - val_loss: 10.9095 - val_accuracy: 0.4898\n",
            "Epoch 169/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1187 - accuracy: 0.9704 - val_loss: 8.6407 - val_accuracy: 0.4898\n",
            "Epoch 170/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0590 - accuracy: 0.9704 - val_loss: 5.4695 - val_accuracy: 0.4898\n",
            "Epoch 171/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0575 - accuracy: 0.9822 - val_loss: 0.7540 - val_accuracy: 0.5918\n",
            "Epoch 172/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0557 - accuracy: 0.9793 - val_loss: 8.0921 - val_accuracy: 0.5102\n",
            "Epoch 173/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0573 - accuracy: 0.9822 - val_loss: 4.0399 - val_accuracy: 0.5102\n",
            "Epoch 174/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0805 - accuracy: 0.9675 - val_loss: 3.6993 - val_accuracy: 0.5102\n",
            "Epoch 175/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0439 - accuracy: 0.9793 - val_loss: 11.6856 - val_accuracy: 0.4898\n",
            "Epoch 176/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0821 - accuracy: 0.9675 - val_loss: 23.4853 - val_accuracy: 0.4898\n",
            "Epoch 177/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0622 - accuracy: 0.9734 - val_loss: 22.1844 - val_accuracy: 0.5102\n",
            "Epoch 178/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0889 - accuracy: 0.9586 - val_loss: 27.0699 - val_accuracy: 0.4898\n",
            "Epoch 179/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0787 - accuracy: 0.9734 - val_loss: 5.8498 - val_accuracy: 0.4898\n",
            "Epoch 180/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0851 - accuracy: 0.9615 - val_loss: 4.0392 - val_accuracy: 0.4898\n",
            "Epoch 181/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0702 - accuracy: 0.9645 - val_loss: 7.3034 - val_accuracy: 0.4898\n",
            "Epoch 182/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0774 - accuracy: 0.9734 - val_loss: 10.6831 - val_accuracy: 0.4898\n",
            "Epoch 183/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0638 - accuracy: 0.9704 - val_loss: 13.3251 - val_accuracy: 0.4898\n",
            "Epoch 184/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0525 - accuracy: 0.9793 - val_loss: 0.5782 - val_accuracy: 0.7007\n",
            "Epoch 185/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0541 - accuracy: 0.9852 - val_loss: 7.6346 - val_accuracy: 0.4898\n",
            "Epoch 186/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0570 - accuracy: 0.9822 - val_loss: 6.3615 - val_accuracy: 0.4898\n",
            "Epoch 187/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0522 - accuracy: 0.9704 - val_loss: 1.4107 - val_accuracy: 0.5170\n",
            "Epoch 188/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0947 - accuracy: 0.9822 - val_loss: 8.6872 - val_accuracy: 0.4898\n",
            "Epoch 189/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0638 - accuracy: 0.9763 - val_loss: 0.8810 - val_accuracy: 0.5850\n",
            "Epoch 190/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0814 - accuracy: 0.9615 - val_loss: 35.3776 - val_accuracy: 0.5102\n",
            "Epoch 191/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0446 - accuracy: 0.9793 - val_loss: 3.6657 - val_accuracy: 0.5102\n",
            "Epoch 192/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0957 - accuracy: 0.9645 - val_loss: 24.7016 - val_accuracy: 0.5102\n",
            "Epoch 193/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.1006 - accuracy: 0.9704 - val_loss: 17.7205 - val_accuracy: 0.4898\n",
            "Epoch 194/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0615 - accuracy: 0.9793 - val_loss: 14.7617 - val_accuracy: 0.4898\n",
            "Epoch 195/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0779 - accuracy: 0.9734 - val_loss: 10.8412 - val_accuracy: 0.5102\n",
            "Epoch 196/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.0254 - accuracy: 0.9911 - val_loss: 18.2543 - val_accuracy: 0.5102\n",
            "Epoch 197/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0680 - accuracy: 0.9704 - val_loss: 11.5374 - val_accuracy: 0.4898\n",
            "Epoch 198/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0899 - accuracy: 0.9675 - val_loss: 3.0256 - val_accuracy: 0.4898\n",
            "Epoch 199/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0542 - accuracy: 0.9793 - val_loss: 2.6836 - val_accuracy: 0.4898\n",
            "Epoch 200/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.0691 - accuracy: 0.9675 - val_loss: 27.3484 - val_accuracy: 0.4898\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb73b00a910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-3hvIYqRcV5",
        "outputId": "5fba4870-12a7-4648-f22c-34e34ce3edbd"
      },
      "source": [
        "# X_train.shape\n",
        "steps_per_epoch"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIgDLfqrNhp_",
        "outputId": "0a3acb51-1186-4710-93b9-bf0dae27da19"
      },
      "source": [
        "# gives us back a <keras.callbacks.History object at 0x112e61a90>\n",
        "model.fit(X_train, Y_train, epochs=200, batch_size=32)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0219 - accuracy: 0.9971\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 0.9971\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0087 - accuracy: 1.0000\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.9073e-04 - accuracy: 1.0000\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.1373e-04 - accuracy: 1.0000\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.3841e-04 - accuracy: 1.0000\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.6504e-04 - accuracy: 1.0000\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.1108e-04 - accuracy: 1.0000\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.7720e-04 - accuracy: 1.0000\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.6844e-04 - accuracy: 1.0000\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.0890e-04 - accuracy: 1.0000\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.3952e-04 - accuracy: 1.0000\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.7546e-04 - accuracy: 1.0000\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.8813e-04 - accuracy: 1.0000\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2310e-04 - accuracy: 1.0000\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.0004e-04 - accuracy: 1.0000\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.4859e-04 - accuracy: 1.0000\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.9518e-04 - accuracy: 1.0000\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.1520e-04 - accuracy: 1.0000\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6588e-04 - accuracy: 1.0000\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.1075e-04 - accuracy: 1.0000\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.1355e-04 - accuracy: 1.0000\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9586e-04 - accuracy: 1.0000\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5427e-04 - accuracy: 1.0000\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.3036e-04 - accuracy: 1.0000\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2470e-04 - accuracy: 1.0000\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9122e-04 - accuracy: 1.0000\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2404e-04 - accuracy: 1.0000\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.1151e-04 - accuracy: 1.0000\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.9311e-04 - accuracy: 1.0000\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4574e-04 - accuracy: 1.0000\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5383e-04 - accuracy: 1.0000\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8579e-04 - accuracy: 1.0000\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6276e-04 - accuracy: 1.0000\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8156e-04 - accuracy: 1.0000\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9473e-04 - accuracy: 1.0000\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9711e-04 - accuracy: 1.0000\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5605e-04 - accuracy: 1.0000\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7096e-04 - accuracy: 1.0000\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4710e-04 - accuracy: 1.0000\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0877e-04 - accuracy: 1.0000\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8854e-04 - accuracy: 1.0000\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.9774e-05 - accuracy: 1.0000\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3636e-04 - accuracy: 1.0000\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4001e-04 - accuracy: 1.0000\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.3421e-05 - accuracy: 1.0000\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4720e-04 - accuracy: 1.0000\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0597e-04 - accuracy: 1.0000\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7074e-04 - accuracy: 1.0000\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0591e-04 - accuracy: 1.0000\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0964e-04 - accuracy: 1.0000\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0286e-04 - accuracy: 1.0000\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6482e-05 - accuracy: 1.0000\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.1184e-04 - accuracy: 1.0000\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4870e-04 - accuracy: 1.0000\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4352e-05 - accuracy: 1.0000\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0274e-04 - accuracy: 1.0000\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4390e-04 - accuracy: 1.0000\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0700e-04 - accuracy: 1.0000\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2659e-04 - accuracy: 1.0000\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1466e-04 - accuracy: 1.0000\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5117e-04 - accuracy: 1.0000\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0324e-04 - accuracy: 1.0000\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1609e-04 - accuracy: 1.0000\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.0936e-05 - accuracy: 1.0000\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6628e-05 - accuracy: 1.0000\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4836e-04 - accuracy: 1.0000\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.8894e-04 - accuracy: 1.0000\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.2273e-05 - accuracy: 1.0000\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.9404e-04 - accuracy: 1.0000\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.2873e-05 - accuracy: 1.0000\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1163e-04 - accuracy: 1.0000\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.0397e-05 - accuracy: 1.0000\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.8487e-05 - accuracy: 1.0000\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3261e-04 - accuracy: 1.0000\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.7036e-05 - accuracy: 1.0000\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2123e-04 - accuracy: 1.0000\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.7481e-05 - accuracy: 1.0000\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4711e-05 - accuracy: 1.0000\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.9748e-05 - accuracy: 1.0000\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.7564e-05 - accuracy: 1.0000\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0890e-04 - accuracy: 1.0000\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.3630e-05 - accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.3427e-05 - accuracy: 1.0000\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.4937e-05 - accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.9565e-05 - accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.5173e-05 - accuracy: 1.0000\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3306e-05 - accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.6006e-05 - accuracy: 1.0000\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.9166e-05 - accuracy: 1.0000\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8659e-04 - accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.8391e-05 - accuracy: 1.0000\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.2027e-05 - accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.0517e-05 - accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.5338e-05 - accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6610e-05 - accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.9837e-05 - accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.2182e-05 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.8594e-05 - accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.5657e-05 - accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2796e-04 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3460e-05 - accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6496e-04 - accuracy: 1.0000\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0956e-05 - accuracy: 1.0000\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7157e-05 - accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.2367e-05 - accuracy: 1.0000\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.8442e-05 - accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.2706e-05 - accuracy: 1.0000\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2182e-04 - accuracy: 1.0000\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6447e-05 - accuracy: 1.0000\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.1520e-05 - accuracy: 1.0000\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.8152e-05 - accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2680e-05 - accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.6053e-05 - accuracy: 1.0000\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.3365e-05 - accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.8371e-05 - accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.8086e-05 - accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0122e-05 - accuracy: 1.0000\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4961e-05 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6507e-05 - accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3523e-05 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.7387e-05 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.7276e-05 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6941e-05 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.1516e-05 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.6977e-05 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.8853e-05 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.1030e-05 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.2971e-05 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.5632e-04 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.7466e-04 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.4920e-05 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0806e-05 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0425e-05 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.2219e-05 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.9296e-05 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8093e-05 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0417e-05 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.3328e-05 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0104e-05 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2236e-05 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.0384e-05 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7265e-05 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5982e-05 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5139e-05 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.8108e-05 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4630e-05 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6575e-05 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0537e-05 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5520e-05 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9503e-05 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8355e-05 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5576e-05 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.4766e-05 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2306e-05 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3541e-05 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.1937e-05 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6555e-05 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5813e-05 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7993e-05 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3951e-05 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 1.3059e-05 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.7979e-05 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4040e-05 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6981e-05 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4616e-05 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2517e-05 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6972e-05 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9148e-05 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.5708e-05 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5692e-05 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3720e-05 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4023e-05 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2793e-05 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0824e-04 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5190e-05 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.5491e-05 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.9298e-05 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3479e-05 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1396e-05 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.0662e-05 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7654e-05 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8220e-05 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8292e-05 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5175e-05 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb73afebbd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpf0XlSARX78",
        "outputId": "f7bedbb8-fe3c-4088-a701-2700fe1cb07a"
      },
      "source": [
        "pred_test= model.predict_classes(X_test)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predict   0   1\n",
            "Actual         \n",
            "0        70   2\n",
            "1         2  73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iFNNrlWV9tH",
        "outputId": "78fa61d1-bbfe-40d0-f54a-a054bcdbcd09"
      },
      "source": [
        "pred_test"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
              "       1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QISvYcJBgWbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f09175dc-080d-4827-dce1-1b20aab2f5fc"
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[0] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  prediction = model.predict_classes(result)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "5   172.0  101.557068   98.128723  ...    0.502975    2.043808    0.118442\n",
            "10  198.0   72.640129   75.233948  ...    0.902867    0.000000    0.000000\n",
            "14  132.0   99.912766   99.797989  ...  136.005508  142.307632  140.765854\n",
            "23  165.0   96.162613  122.159096  ...   54.862110   48.515007   37.005508\n",
            "26  188.0   63.377541   61.138977  ...   63.220913   66.860565   72.512451\n",
            "27  124.0   53.867844   38.016644  ...   12.810613    3.649323    2.382934\n",
            "28  191.0  124.580231   99.179665  ...   99.584961  112.032646  108.363564\n",
            "32  128.0   95.728516   98.484375  ...   22.014648   26.892578   32.234375\n",
            "34  113.0   61.291481   60.417965  ...   70.870468   68.101105   63.274414\n",
            "40  109.0   97.648178   99.220428  ...   89.935783   91.213615   89.774513\n",
            "44  200.0  110.669609  108.840805  ...   57.342003   62.326004   57.558006\n",
            "45  150.0   75.288887   83.270393  ...   41.807293   66.117332   84.013153\n",
            "48  154.0   99.289261  102.049591  ...   54.462807   49.305786   49.619835\n",
            "49  175.0   98.497589  100.295998  ...   57.137596   63.716801   66.862396\n",
            "1   159.0  100.195312  104.077843  ...    6.374036    2.387682    0.669950\n",
            "6   159.0  100.831650  100.502007  ...    0.000000    0.000000    0.000000\n",
            "14  131.0   84.871979   88.730614  ...   31.413494   40.464249   43.823959\n",
            "23  183.0   59.640301   52.836933  ...   40.276718   35.022129   17.828959\n",
            "24  106.0   29.978643   26.498398  ...   59.272343   58.386616   52.951229\n",
            "36  147.0   89.498863   88.437637  ...   61.120182   91.362808  105.875290\n",
            "46  160.0   76.475616   79.498123  ...   74.585625   77.834999   81.524368\n",
            "1   120.0   76.656670   78.447777  ...   78.134438   82.874451   84.712234\n",
            "2   186.0   38.748066   31.852932  ...    1.948433    0.573014    0.534166\n",
            "8   138.0   35.657001   34.527409  ...   24.633478   23.103340   22.470697\n",
            "11  154.0   90.595039   92.330589  ...   79.429756   82.099174   83.140495\n",
            "14  150.0   78.210136   78.293335  ...  108.435547  104.630402   77.679291\n",
            "18  100.0    0.044800    0.033600  ...   63.063999   65.310394   64.559998\n",
            "28  128.0   92.172852   90.476562  ...  108.044922  107.433594  106.232422\n",
            "32  151.0   79.384323   80.082718  ...   61.345333   71.617081   72.632034\n",
            "35  139.0  100.140778  103.294235  ...   75.789970   72.700531   42.140724\n",
            "37  196.0    0.346939    0.204082  ...    1.102041    0.102041    0.000000\n",
            "38  152.0  122.994461  122.863571  ...   61.519390   58.276314   56.029774\n",
            "39  130.0  107.019165  112.726151  ...  100.585320  100.101067   97.058228\n",
            "41  142.0   39.835350   44.454277  ...   18.674868   16.351122   11.278318\n",
            "43  185.0  104.555733  109.735924  ...    4.823871    5.008765    4.476990\n",
            "45  106.0   80.765404   80.240654  ...  118.384125  122.361343  130.364197\n",
            "46  177.0   25.939833   25.536179  ...  138.065994  132.156616  129.724808\n",
            "49  192.0  110.755196  112.287323  ...   71.799469   84.728722   83.314232\n",
            "\n",
            "[38 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRbWgmX_LFH",
        "outputId": "c9421121-62c9-43a5-aec5-84df9663ec79"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "from GetBetterSegm import GetBetter"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_paper_fev_2021'...\n",
            "remote: Enumerating objects: 592, done.\u001b[K\n",
            "remote: Counting objects: 100% (353/353), done.\u001b[K\n",
            "remote: Compressing objects: 100% (352/352), done.\u001b[K\n",
            "remote: Total 592 (delta 215), reused 0 (delta 0), pack-reused 239\u001b[K\n",
            "Receiving objects: 100% (592/592), 5.09 MiB | 13.50 MiB/s, done.\n",
            "Resolving deltas: 100% (352/352), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAG_I6FwCvFr",
        "outputId": "4aac415b-29f7-4051-c6c1-3b9da16f4ceb"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "%cd marquesgabi_out_2020\n",
        "PSD_imageJ = 'Areas_ImageJ.csv'\n",
        "PSD_new = pd.read_csv(PSD_imageJ)\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_out_2020'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 146 (delta 75), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (146/146), 1.00 MiB | 7.23 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020\n",
            "   Juntas   Area\n",
            "0       1  2.001\n",
            "1       2  0.820\n",
            "2       3  1.270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "PekBHQOT_6CP",
        "outputId": "eae88705-d099-46b6-d690-88f05c69d1f3"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>172.0</td>\n",
              "      <td>101.557068</td>\n",
              "      <td>98.128723</td>\n",
              "      <td>101.747978</td>\n",
              "      <td>107.835602</td>\n",
              "      <td>115.239601</td>\n",
              "      <td>129.891296</td>\n",
              "      <td>143.020569</td>\n",
              "      <td>133.254211</td>\n",
              "      <td>31.388319</td>\n",
              "      <td>29.560844</td>\n",
              "      <td>29.686317</td>\n",
              "      <td>47.357491</td>\n",
              "      <td>98.188217</td>\n",
              "      <td>134.784210</td>\n",
              "      <td>132.199036</td>\n",
              "      <td>118.964844</td>\n",
              "      <td>108.418610</td>\n",
              "      <td>106.641441</td>\n",
              "      <td>103.562469</td>\n",
              "      <td>97.923744</td>\n",
              "      <td>102.407257</td>\n",
              "      <td>103.601944</td>\n",
              "      <td>105.022186</td>\n",
              "      <td>114.480263</td>\n",
              "      <td>124.139008</td>\n",
              "      <td>115.123848</td>\n",
              "      <td>26.545704</td>\n",
              "      <td>19.326122</td>\n",
              "      <td>97.285019</td>\n",
              "      <td>98.623032</td>\n",
              "      <td>103.339645</td>\n",
              "      <td>108.153069</td>\n",
              "      <td>114.670647</td>\n",
              "      <td>123.277458</td>\n",
              "      <td>134.581406</td>\n",
              "      <td>122.151436</td>\n",
              "      <td>29.331532</td>\n",
              "      <td>31.099516</td>\n",
              "      <td>70.305573</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.725257</td>\n",
              "      <td>2.277988</td>\n",
              "      <td>0.722012</td>\n",
              "      <td>0.000541</td>\n",
              "      <td>0.019470</td>\n",
              "      <td>0.019470</td>\n",
              "      <td>0.009735</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.502975</td>\n",
              "      <td>2.043808</td>\n",
              "      <td>0.118442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>198.0</td>\n",
              "      <td>72.640129</td>\n",
              "      <td>75.233948</td>\n",
              "      <td>72.447395</td>\n",
              "      <td>67.467705</td>\n",
              "      <td>69.026115</td>\n",
              "      <td>72.809608</td>\n",
              "      <td>73.448013</td>\n",
              "      <td>73.057640</td>\n",
              "      <td>75.735130</td>\n",
              "      <td>77.437401</td>\n",
              "      <td>78.845718</td>\n",
              "      <td>76.451477</td>\n",
              "      <td>72.934692</td>\n",
              "      <td>73.592583</td>\n",
              "      <td>72.875931</td>\n",
              "      <td>74.201912</td>\n",
              "      <td>74.254555</td>\n",
              "      <td>75.561775</td>\n",
              "      <td>74.992645</td>\n",
              "      <td>79.474434</td>\n",
              "      <td>77.371895</td>\n",
              "      <td>65.220085</td>\n",
              "      <td>45.122330</td>\n",
              "      <td>34.849911</td>\n",
              "      <td>34.279766</td>\n",
              "      <td>35.749615</td>\n",
              "      <td>27.364861</td>\n",
              "      <td>17.316906</td>\n",
              "      <td>68.165283</td>\n",
              "      <td>68.440147</td>\n",
              "      <td>66.656967</td>\n",
              "      <td>62.060093</td>\n",
              "      <td>60.395565</td>\n",
              "      <td>62.779606</td>\n",
              "      <td>63.176201</td>\n",
              "      <td>65.234970</td>\n",
              "      <td>67.916542</td>\n",
              "      <td>71.989174</td>\n",
              "      <td>73.636658</td>\n",
              "      <td>...</td>\n",
              "      <td>62.274460</td>\n",
              "      <td>63.938572</td>\n",
              "      <td>63.205181</td>\n",
              "      <td>60.147739</td>\n",
              "      <td>56.609631</td>\n",
              "      <td>52.195793</td>\n",
              "      <td>51.117435</td>\n",
              "      <td>44.720535</td>\n",
              "      <td>23.095703</td>\n",
              "      <td>3.115498</td>\n",
              "      <td>0.052240</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>65.724609</td>\n",
              "      <td>65.919693</td>\n",
              "      <td>64.303940</td>\n",
              "      <td>62.969994</td>\n",
              "      <td>63.528202</td>\n",
              "      <td>54.787872</td>\n",
              "      <td>44.970406</td>\n",
              "      <td>35.255478</td>\n",
              "      <td>36.441788</td>\n",
              "      <td>39.483311</td>\n",
              "      <td>47.880722</td>\n",
              "      <td>59.015095</td>\n",
              "      <td>64.890106</td>\n",
              "      <td>67.982651</td>\n",
              "      <td>66.435455</td>\n",
              "      <td>60.475456</td>\n",
              "      <td>59.160488</td>\n",
              "      <td>59.560249</td>\n",
              "      <td>58.247520</td>\n",
              "      <td>55.958164</td>\n",
              "      <td>54.721050</td>\n",
              "      <td>51.715633</td>\n",
              "      <td>50.210587</td>\n",
              "      <td>40.451176</td>\n",
              "      <td>11.934597</td>\n",
              "      <td>0.902867</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>132.0</td>\n",
              "      <td>99.912766</td>\n",
              "      <td>99.797989</td>\n",
              "      <td>100.451797</td>\n",
              "      <td>100.994492</td>\n",
              "      <td>100.347115</td>\n",
              "      <td>100.647392</td>\n",
              "      <td>101.647392</td>\n",
              "      <td>96.535362</td>\n",
              "      <td>88.325073</td>\n",
              "      <td>85.586784</td>\n",
              "      <td>85.370071</td>\n",
              "      <td>85.859512</td>\n",
              "      <td>89.146011</td>\n",
              "      <td>94.407715</td>\n",
              "      <td>101.170807</td>\n",
              "      <td>102.494957</td>\n",
              "      <td>102.423332</td>\n",
              "      <td>108.142334</td>\n",
              "      <td>118.017448</td>\n",
              "      <td>120.629028</td>\n",
              "      <td>112.847572</td>\n",
              "      <td>101.179985</td>\n",
              "      <td>97.884308</td>\n",
              "      <td>97.184570</td>\n",
              "      <td>96.930222</td>\n",
              "      <td>94.718094</td>\n",
              "      <td>93.443535</td>\n",
              "      <td>91.565659</td>\n",
              "      <td>95.144173</td>\n",
              "      <td>98.837471</td>\n",
              "      <td>100.873276</td>\n",
              "      <td>99.912773</td>\n",
              "      <td>99.112038</td>\n",
              "      <td>103.507812</td>\n",
              "      <td>102.374657</td>\n",
              "      <td>96.018372</td>\n",
              "      <td>88.084496</td>\n",
              "      <td>87.130394</td>\n",
              "      <td>88.266304</td>\n",
              "      <td>...</td>\n",
              "      <td>60.625343</td>\n",
              "      <td>60.050507</td>\n",
              "      <td>57.190083</td>\n",
              "      <td>57.614323</td>\n",
              "      <td>79.516075</td>\n",
              "      <td>105.895325</td>\n",
              "      <td>116.855835</td>\n",
              "      <td>122.889824</td>\n",
              "      <td>128.810852</td>\n",
              "      <td>136.778702</td>\n",
              "      <td>140.403137</td>\n",
              "      <td>136.890732</td>\n",
              "      <td>48.263542</td>\n",
              "      <td>65.259872</td>\n",
              "      <td>74.134987</td>\n",
              "      <td>75.704315</td>\n",
              "      <td>71.694221</td>\n",
              "      <td>68.882462</td>\n",
              "      <td>64.726357</td>\n",
              "      <td>65.693298</td>\n",
              "      <td>64.406799</td>\n",
              "      <td>63.956848</td>\n",
              "      <td>63.575764</td>\n",
              "      <td>60.968781</td>\n",
              "      <td>60.447201</td>\n",
              "      <td>60.820023</td>\n",
              "      <td>60.112030</td>\n",
              "      <td>59.411392</td>\n",
              "      <td>58.667587</td>\n",
              "      <td>55.205696</td>\n",
              "      <td>54.824615</td>\n",
              "      <td>84.233246</td>\n",
              "      <td>107.350784</td>\n",
              "      <td>115.174477</td>\n",
              "      <td>119.729111</td>\n",
              "      <td>122.254372</td>\n",
              "      <td>124.577606</td>\n",
              "      <td>136.005508</td>\n",
              "      <td>142.307632</td>\n",
              "      <td>140.765854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>165.0</td>\n",
              "      <td>96.162613</td>\n",
              "      <td>122.159096</td>\n",
              "      <td>104.441803</td>\n",
              "      <td>80.908722</td>\n",
              "      <td>75.215210</td>\n",
              "      <td>73.983910</td>\n",
              "      <td>77.536972</td>\n",
              "      <td>74.520706</td>\n",
              "      <td>73.810471</td>\n",
              "      <td>74.590851</td>\n",
              "      <td>67.162758</td>\n",
              "      <td>58.590267</td>\n",
              "      <td>40.729515</td>\n",
              "      <td>31.236183</td>\n",
              "      <td>28.356730</td>\n",
              "      <td>25.483418</td>\n",
              "      <td>23.301746</td>\n",
              "      <td>22.571829</td>\n",
              "      <td>23.049955</td>\n",
              "      <td>20.024281</td>\n",
              "      <td>16.043526</td>\n",
              "      <td>15.264536</td>\n",
              "      <td>13.073608</td>\n",
              "      <td>8.908173</td>\n",
              "      <td>4.645546</td>\n",
              "      <td>3.435335</td>\n",
              "      <td>2.765179</td>\n",
              "      <td>1.732966</td>\n",
              "      <td>91.609596</td>\n",
              "      <td>116.088936</td>\n",
              "      <td>95.835297</td>\n",
              "      <td>80.947510</td>\n",
              "      <td>73.386154</td>\n",
              "      <td>74.141273</td>\n",
              "      <td>71.512367</td>\n",
              "      <td>70.920326</td>\n",
              "      <td>70.490585</td>\n",
              "      <td>67.746857</td>\n",
              "      <td>66.505569</td>\n",
              "      <td>...</td>\n",
              "      <td>66.131905</td>\n",
              "      <td>65.387810</td>\n",
              "      <td>61.957397</td>\n",
              "      <td>62.608707</td>\n",
              "      <td>61.170948</td>\n",
              "      <td>58.741482</td>\n",
              "      <td>58.896271</td>\n",
              "      <td>60.181046</td>\n",
              "      <td>57.885582</td>\n",
              "      <td>50.330544</td>\n",
              "      <td>30.378881</td>\n",
              "      <td>14.581416</td>\n",
              "      <td>46.333191</td>\n",
              "      <td>44.344135</td>\n",
              "      <td>44.817226</td>\n",
              "      <td>43.328594</td>\n",
              "      <td>42.659325</td>\n",
              "      <td>46.244263</td>\n",
              "      <td>47.307991</td>\n",
              "      <td>50.649185</td>\n",
              "      <td>53.093586</td>\n",
              "      <td>53.073608</td>\n",
              "      <td>53.428757</td>\n",
              "      <td>56.542145</td>\n",
              "      <td>59.796989</td>\n",
              "      <td>59.758644</td>\n",
              "      <td>60.710449</td>\n",
              "      <td>62.767899</td>\n",
              "      <td>64.839645</td>\n",
              "      <td>65.972198</td>\n",
              "      <td>61.798054</td>\n",
              "      <td>61.364040</td>\n",
              "      <td>59.450836</td>\n",
              "      <td>60.052711</td>\n",
              "      <td>59.311554</td>\n",
              "      <td>59.909603</td>\n",
              "      <td>58.802979</td>\n",
              "      <td>54.862110</td>\n",
              "      <td>48.515007</td>\n",
              "      <td>37.005508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>188.0</td>\n",
              "      <td>63.377541</td>\n",
              "      <td>61.138977</td>\n",
              "      <td>54.995930</td>\n",
              "      <td>55.511997</td>\n",
              "      <td>56.181526</td>\n",
              "      <td>58.811676</td>\n",
              "      <td>60.364872</td>\n",
              "      <td>62.856949</td>\n",
              "      <td>67.607513</td>\n",
              "      <td>76.126747</td>\n",
              "      <td>87.439102</td>\n",
              "      <td>96.983246</td>\n",
              "      <td>100.165230</td>\n",
              "      <td>102.084648</td>\n",
              "      <td>102.209152</td>\n",
              "      <td>99.465813</td>\n",
              "      <td>98.174278</td>\n",
              "      <td>101.521500</td>\n",
              "      <td>104.813034</td>\n",
              "      <td>99.619736</td>\n",
              "      <td>93.987328</td>\n",
              "      <td>91.268440</td>\n",
              "      <td>89.120407</td>\n",
              "      <td>87.675407</td>\n",
              "      <td>84.735168</td>\n",
              "      <td>78.175186</td>\n",
              "      <td>77.901764</td>\n",
              "      <td>76.271614</td>\n",
              "      <td>64.451790</td>\n",
              "      <td>61.996826</td>\n",
              "      <td>54.884563</td>\n",
              "      <td>55.419197</td>\n",
              "      <td>57.492531</td>\n",
              "      <td>58.986874</td>\n",
              "      <td>59.654140</td>\n",
              "      <td>62.015842</td>\n",
              "      <td>66.053413</td>\n",
              "      <td>67.291077</td>\n",
              "      <td>71.432770</td>\n",
              "      <td>...</td>\n",
              "      <td>79.081932</td>\n",
              "      <td>84.594376</td>\n",
              "      <td>94.827980</td>\n",
              "      <td>110.645538</td>\n",
              "      <td>132.808960</td>\n",
              "      <td>125.062469</td>\n",
              "      <td>111.710274</td>\n",
              "      <td>97.973289</td>\n",
              "      <td>80.211853</td>\n",
              "      <td>65.286102</td>\n",
              "      <td>68.080574</td>\n",
              "      <td>72.929375</td>\n",
              "      <td>37.095062</td>\n",
              "      <td>39.439114</td>\n",
              "      <td>38.795383</td>\n",
              "      <td>39.126297</td>\n",
              "      <td>41.763241</td>\n",
              "      <td>42.715256</td>\n",
              "      <td>41.398369</td>\n",
              "      <td>42.689453</td>\n",
              "      <td>42.092346</td>\n",
              "      <td>38.393845</td>\n",
              "      <td>27.993664</td>\n",
              "      <td>21.026709</td>\n",
              "      <td>20.718878</td>\n",
              "      <td>21.768673</td>\n",
              "      <td>27.988230</td>\n",
              "      <td>57.079674</td>\n",
              "      <td>74.552742</td>\n",
              "      <td>77.274788</td>\n",
              "      <td>77.289719</td>\n",
              "      <td>69.959259</td>\n",
              "      <td>67.230423</td>\n",
              "      <td>73.701668</td>\n",
              "      <td>76.865097</td>\n",
              "      <td>76.063370</td>\n",
              "      <td>71.020821</td>\n",
              "      <td>63.220913</td>\n",
              "      <td>66.860565</td>\n",
              "      <td>72.512451</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Width           0           1  ...         781         782         783\n",
              "5   172.0  101.557068   98.128723  ...    0.502975    2.043808    0.118442\n",
              "10  198.0   72.640129   75.233948  ...    0.902867    0.000000    0.000000\n",
              "14  132.0   99.912766   99.797989  ...  136.005508  142.307632  140.765854\n",
              "23  165.0   96.162613  122.159096  ...   54.862110   48.515007   37.005508\n",
              "26  188.0   63.377541   61.138977  ...   63.220913   66.860565   72.512451\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "Area = np.array(PSD_new['Area'])\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "Vfk_fNXGDK5_",
        "outputId": "0f832cea-4ad5-4087-c063-0a3b2992e170"
      },
      "source": [
        " wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        " wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        " X = pd.DataFrame([Diam1,Diameter_All])\n",
        " wts = pd.DataFrame([wt1,wt2])\n",
        "plt.hist(X,weights=wts)\n",
        "plt.legend(['Image J','CNN'])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb73af4fc10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "ZZHa1j4HT9Dq",
        "outputId": "ad2b9f17-937f-4416-f60d-f8f3a72a2c01"
      },
      "source": [
        "counts, bins, bars = plt.hist(X,weights=wts)\n",
        "print(bars)\n",
        "print(bins)\n",
        "print(counts)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<a list of 2 Lists of Patches objects>\n",
            "[0.70783943 0.86879941 1.02975939 1.19071936 1.35167934 1.51263931\n",
            " 1.67359929 1.83455926 1.99551924 2.15647921 2.31743919]\n",
            "[[ 5.26315789 10.52631579 23.15789474 31.57894737 16.84210526  9.47368421\n",
            "   1.05263158  0.          1.05263158  1.05263158]\n",
            " [13.15789474 10.52631579 23.68421053 21.05263158 10.52631579 13.15789474\n",
            "   2.63157895  5.26315789  0.          0.        ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o_vDGeWUwIZ",
        "outputId": "38e79423-5bb6-4c87-d904-3c7b635ccf9d"
      },
      "source": [
        "print(counts.sum())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200.00000000000014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "KcH52-6iJQ8t",
        "outputId": "bc602419-b6e9-4318-faa1-017551bfdfe0"
      },
      "source": [
        "\n",
        "plt.hist([Diam1,Diameter_All])\n",
        "plt.legend(['Image J','CNN'])\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb6ee22d0d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATbUlEQVR4nO3dfZBV9X3H8fdXXN20UqWyEiLiolIVagCzYqxMQyFa1JlYZ8yDaY12dDAPOqHJHxKdqaTNjJqQaExtLFZHSsiDk2ijNU3jWGxq4kNAEdGdGDXErEWeNOahUYN8+8de6Yq73Lu79+kH79fMzp577jn3fFj2fObs755zbmQmkqTy7NPqAJKkkbHAJalQFrgkFcoCl6RCWeCSVKh9m7mx8ePHZ3d3dzM3KUnFW7NmzdbM7Np1flMLvLu7m9WrVzdzk5JUvIj42WDzHUKRpEJZ4JJUKAtckgrV1DFwSXu33/3ud/T19fHyyy+3Okpb6uzsZNKkSXR0dNS0vAUuqWn6+voYO3Ys3d3dRESr47SVzGTbtm309fUxZcqUmtZxCEVS07z88sscfPDBlvcgIoKDDz54WH+dVC3wiOiMiIci4tGIeDwiPl2ZPyUiHoyIpyLiGxGx3yiyS9pLWN5DG+7PppYj8FeAeZk5A5gJLIiIdwJXA9dk5lHAi8AFw8wqSRqFqmPg2X/D8F9XHnZUvhKYB3ywMn85sAT4cv0jStpTdS++q66vt+GqM6ouc8ABB/DrX/+66nKNNnfuXJYuXUpPT8+IX6OmNzEjYgywBjgKuB54GvhFZm6vLNIHHDrEuguBhQCTJ08ecVA130h2rlp2IEn1UdObmJn5WmbOBCYBs4Fjat1AZi7LzJ7M7OnqetOl/JLUEvfeey/vete7OPPMMzniiCNYvHgxK1euZPbs2Rx33HE8/fTTANx5552ceOKJzJo1i3e/+91s2rQJgC1btnDKKacwffp0LrzwQg4//HC2bt0KwFe+8hVmz57NzJkzueiii3jttdca8m8Y1lkomfkLYBVwEnBQRLx+BD8JeK7O2SSpoR599FFuuOEGent7WbFiBU8++SQPPfQQF154IV/60pcAmDNnDg888ACPPPIIH/jAB/jsZz8LwKc//WnmzZvH448/ztlnn82zzz4LQG9vL9/4xjf4wQ9+wNq1axkzZgwrV65sSP6qQygR0QX8LjN/ERFvAU6h/w3MVcDZwNeB84BvNyShJDXICSecwMSJEwE48sgjOfXUUwE47rjjWLVqFdB/7vr73/9+Nm7cyKuvvrrzHO377ruP22+/HYAFCxYwbtw4AO655x7WrFnDCSecAMBvf/tbDjnkkIbkr2UMfCKwvDIOvg9wa2b+W0Q8AXw9Ij4DPALc1JCEktQg+++//87pffbZZ+fjffbZh+3b+9/iu+SSS/jEJz7Be97zHu69916WLFmy29fMTM477zyuvPLKhuV+XdUhlMxcl5mzMvPtmfnHmfl3lfnPZObszDwqM9+bma80PK0kNdlLL73EoYf2n6OxfPnynfNPPvlkbr31VgC+973v8eKLLwIwf/58vvnNb7J582YAXnjhBX72s0HvBjtqXkovqWVKOGtpyZIlvPe972XcuHHMmzePn/70pwBcccUVnHPOOaxYsYKTTjqJt771rYwdO5bx48fzmc98hlNPPZUdO3bQ0dHB9ddfz+GHH/6G192+ffsb/gIYieg/zbs5enp60g90KIenEareent7OfbYY1sdoy5eeeUVxowZw7777sv999/PRz7yEdauXVvzukcddRTr16/nwAMPfMNzg/2MImJNZr7phHGPwCVpBJ599lne9773sWPHDvbbbz9uvPHGmtZbvXo15557Lh/96EffVN7DZYFL0ghMnTqVRx55ZNjr9fT00NvbW5cM3o1QkgplgUtSoSxwSSqUBS5JhfJNTEmts2R0Z2G8+fVeqrrI888/z6JFi/jRj37EQQcdxIQJE7j22ms5+uijue6667jkkksAuPjii+np6eH888/n/PPP5+677+aZZ55h//33Z+vWrfT09LBhw4b65h8mj8Al7TUyk7POOou5c+fy9NNPs2bNGq688ko2bdrEIYccwhe/+EVeffXVQdcdM2YMN998c5MT754FLmmvsWrVKjo6Ovjwhz+8c96MGTM47LDD6OrqYv78+W+4XH6gRYsWcc011+y8R0o7sMAl7TXWr1/PO97xjiGfv/TSS1m6dOmg9++ePHkyc+bMYcWKFY2MOCwWuCRVHHHEEZx44ol89atfHfT5T33qU3zuc59jx44dTU42OAtc0l5j+vTprFmzZrfLXHbZZVx99dUMdp+oqVOnMnPmzJ13IWw1C1zSXmPevHm88sorLFu2bOe8devW8fOf/3zn42OOOYZp06Zx5513Dvoal19+OUuXLm141lp4GqGk1qnhtL96ighuv/12Fi1axNVXX01nZyfd3d1ce+21b1ju8ssvZ9asWYO+xvTp0zn++ON5+OGHmxF5tyxwSXuVt73tbYMOgaxfv37n9IwZM94wzn3LLbe8YdnbbrutYfmGwyEUSSqUBS5JhbLAJTVVMz8FrDTD/dlY4JKaprOzk23btlnig8hMtm3bRmdnZ83r+CampKaZNGkSfX19bNmypdVR2lJnZyeTJk2qeXkLXFLTdHR0MGXKlFbH2GM4hCJJhbLAJalQVQs8Ig6LiFUR8UREPB4RH6/MXxIRz0XE2srX6Y2PK0l6XS1j4NuBT2bmwxExFlgTEXdXnrsmM9vjpgCStJepWuCZuRHYWJn+VUT0Aoc2OpgkafeGNQYeEd3ALODByqyLI2JdRNwcEeOGWGdhRKyOiNWeOiRJ9VNzgUfEAcC3gEWZ+Uvgy8CRwEz6j9A/P9h6mbksM3sys6erq6sOkSVJUGOBR0QH/eW9MjNvA8jMTZn5WmbuAG4EZjcupiRpV7WchRLATUBvZn5hwPyJAxY7C1i/67qSpMap5SyUk4FzgcciYm1l3mXAORExE0hgA3BRQxJKkgZVy1ko9wExyFPfqX8cSVKtvBJTkgplgUtSoSxwSSqUBS5JhbLAJalQFrgkFcoCl6RCWeCSVCgLXJIKZYFLUqEscEkqlAUuSYWywCWpUBa4JBXKApekQtXygQ5qoe7Fdw17nQ1XndGAJJLajUfgklQoC1ySCmWBS1KhLHBJKpQFLkmFssAlqVAWuCQVygKXpEJZ4JJUKAtckgpVtcAj4rCIWBURT0TE4xHx8cr8P4yIuyPiJ5Xv4xofV5L0ulqOwLcDn8zMacA7gY9FxDRgMXBPZk4F7qk8liQ1SdUCz8yNmflwZfpXQC9wKHAmsLyy2HLgLxoVUpL0ZsMaA4+IbmAW8CAwITM3Vp56HpgwxDoLI2J1RKzesmXLKKJKkgaqucAj4gDgW8CizPzlwOcyM4EcbL3MXJaZPZnZ09XVNaqwkqT/V1OBR0QH/eW9MjNvq8zeFBETK89PBDY3JqIkaTC1nIUSwE1Ab2Z+YcBTdwDnVabPA75d/3iSpKHU8ok8JwPnAo9FxNrKvMuAq4BbI+IC4GfA+xoTUZI0mKoFnpn3ATHE0/PrG0eSVCuvxJSkQlngklQoC1ySCmWBS1KhLHBJKpQFLkmFssAlqVAWuCQVygKXpEJZ4JJUKAtckgplgUtSoSxwSSqUBS5JhbLAJalQFrgkFcoCl6RCWeCSVCgLXJIKZYFLUqEscEkqVNVPpZdaoXvxXcNeZ8NVZzQgidS+PAKXpEJZ4JJUKAtckgpVtcAj4uaI2BwR6wfMWxIRz0XE2srX6Y2NKUnaVS1H4LcACwaZf01mzqx8fae+sSRJ1VQt8Mz8PvBCE7JIkoZhNGPgF0fEusoQy7ihFoqIhRGxOiJWb9myZRSbkyQNNNIC/zJwJDAT2Ah8fqgFM3NZZvZkZk9XV9cINydJ2tWICjwzN2Xma5m5A7gRmF3fWJKkakZU4BExccDDs4D1Qy0rSWqMqpfSR8TXgLnA+IjoA64A5kbETCCBDcBFDcwoSRpE1QLPzHMGmX1TA7JIkobBKzElqVAWuCQVygKXpEJZ4JJUKAtckgplgUtSoSxwSSqUBS5JhbLAJalQFrgkFcoCl6RCWeCSVCgLXJIKZYFLUqEscEkqlAUuSYWywCWpUBa4JBXKApekQlngklQoC1ySCmWBS1KhLHBJKpQFLkmFssAlqVAWuCQVqmqBR8TNEbE5ItYPmPeHEXF3RPyk8n1cY2NKknZVyxH4LcCCXeYtBu7JzKnAPZXHkqQmqlrgmfl94IVdZp8JLK9MLwf+os65JElV7DvC9SZk5sbK9PPAhKEWjIiFwEKAyZMnj3BzrdW9+K5hr7PhqjMakESS/t+o38TMzARyN88vy8yezOzp6uoa7eYkSRUjLfBNETERoPJ9c/0iSZJqMdICvwM4rzJ9HvDt+sSRJNWqltMIvwbcDxwdEX0RcQFwFXBKRPwEeHflsSSpiaq+iZmZ5wzx1Pw6Z5EkDYNXYkpSoUZ6GqHa2ZIDh7n8S43JUShPG1UpPAKXpEJZ4JJUKAtckgplgUtSoSxwSSqUBS5JhfI0QtWXpzBKTeMRuCQVygKXpEJZ4JJUKAtckgplgUtSoSxwSSqUBS5JhbLAJalQFrgkFcoCl6RCWeCSVCgLXJIKZYFLUqEscEkqlAUuSYWywCWpUKP6QIeI2AD8CngN2J6ZPfUIJUmqrh6fyPNnmbm1Dq8jSRoGh1AkqVCjPQJP4HsRkcA/ZeayXReIiIXAQoDJkyePeEPdi+8a9jobOj84vBX8fEaNlJ8FqhYY7RH4nMw8HjgN+FhE/OmuC2Tmsszsycyerq6uUW5OkvS6URV4Zj5X+b4ZuB2YXY9QkqTqRlzgEfH7ETH29WngVGB9vYJJknZvNGPgE4DbI+L11/lqZn63LqkkSVWNuMAz8xlgRh2zSJKGwdMIJalQ9biQR4PxtLLm82euvYxH4JJUKAtckgplgUtSoSxwSSqUBS5JhbLAJalQFrgkFcoCl6RCWeCSVCgLXJIKZYFLUqEscEkqlAUuSYXyboSSRs47QLaUR+CSVCgLXJIKZYFLUqEscEkqlAUuSYWywCWpUJ5GKLWZ7sV3DWv5DZ0fHN4G2vRUvmH/u686Y4/Y9mh4BC5JhbLAJalQFrgkFWpUBR4RCyLixxHxVEQsrlcoSVJ1Iy7wiBgDXA+cBkwDzomIafUKJknavdEcgc8GnsrMZzLzVeDrwJn1iSVJqiYyc2QrRpwNLMjMCyuPzwVOzMyLd1luIbCw8vBo4Mcjjzti44GtLdhuNe2aC8w2Eu2aC9o3W7vmgvbKdnhmdu06s+HngWfmMmBZo7ezOxGxOjN7WplhMO2aC8w2Eu2aC9o3W7vmgvbO9rrRDKE8Bxw24PGkyjxJUhOMpsB/BEyNiCkRsR/wAeCO+sSSJFUz4iGUzNweERcD/wGMAW7OzMfrlqy+WjqEsxvtmgvMNhLtmgvaN1u75oL2zgaM4k1MSVJreSWmJBXKApekQu1RBV7t0v6ImBwRqyLikYhYFxGnNynXzRGxOSLWD/F8RMR1ldzrIuL4Nsn1l5U8j0XEDyNiRjNy1ZJtwHInRMT2ynUJbZErIuZGxNqIeDwi/qsZuWrJFhEHRsSdEfFoJdtfNynXYZX97onKdj8+yDKt2gdqyday/aCqzNwjvuh/I/Vp4AhgP+BRYNouyywDPlKZngZsaFK2PwWOB9YP8fzpwL8DAbwTeLBNcv0JMK4yfVqzctWSbcD/+X8C3wHObodcwEHAE8DkyuND2uVnBlwGXF2Z7gJeAPZrQq6JwPGV6bHAk4Psm63aB2rJ1rL9oNrXnnQEXsul/Qn8QWX6QOB/mhEsM79P/84ylDOBf8l+DwAHRcTEVufKzB9m5ouVhw/Qf65/U9TwMwO4BPgWsLnxifrVkOuDwG2Z+Wxl+XbKlsDYiAjggMqy25uQa2NmPlyZ/hXQCxy6y2Kt2geqZmvlflDNnlTghwI/H/C4jzf/kiwB/ioi+ug/arukOdGqqiV7q11A/xFSW4iIQ4GzgC+3Ossu/ggYFxH3RsSaiPhQqwMN8A/AsfQfuDwGfDwzdzQzQER0A7OAB3d5quX7wG6yDdRW+8He9pFq5wC3ZObnI+IkYEVE/HGzf4lLExF/Rv8v7pxWZxngWuDSzNzRf0DZNvYF3gHMB94C3B8RD2Tmk62NBcCfA2uBecCRwN0R8d+Z+ctmbDwiDqD/L6ZFzdpmrWrJ1o77wZ5U4LVc2n8BsAAgM++PiE76b1jTtD9zh9C2tyWIiLcD/wyclpnbWp1ngB7g65XyHg+cHhHbM/NfWxuLPmBbZv4G+E1EfB+YQf/Yaqv9NXBV9g/mPhURPwWOAR5q9IYjooP+glyZmbcNskjL9oEasrXtfrAnDaHUcmn/s/QfGRERxwKdwJamphzcHcCHKu/EvxN4KTM3tjpUREwGbgPObZMjyJ0yc0pmdmdmN/BN4KNtUN4A3wbmRMS+EfF7wIn0j6u2g4G//xPovzvoM43eaGXM/SagNzO/MMRiLdkHasnWzvvBHnMEnkNc2h8Rfweszsw7gE8CN0bE39D/hs75laORhoqIrwFzgfGV8fcrgI5K7hvoH48/HXgK+F/6j5QaroZcfwscDPxj5Uh3ezbp7mw1ZGuJarkyszcivgusA3YA/5yZuz0VslnZgL8HbomIx+g/2+PSzGzG7VJPBs4FHouItZV5lwGTB2RryT5QY7aW7QfVeCm9JBVqTxpCkaS9igUuSYWywCWpUBa4JBXKApekQlngklQoC1ySCvV/eFVMu9SnS9AAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r11AxFK_JIii",
        "outputId": "09295cfc-1dd2-4d07-bb14-ccbd64d5e811"
      },
      "source": [
        "[Diam1,Diameter_All]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.59616801403081,\n",
              "  1.0217907939900581,\n",
              "  1.2716187407449044,\n",
              "  1.104429030701514,\n",
              "  1.2163487785097904,\n",
              "  1.6013445735058454,\n",
              "  1.1715597420637607,\n",
              "  1.2534662333717612,\n",
              "  1.2676073151634049,\n",
              "  1.309600575274104,\n",
              "  1.292966945531582,\n",
              "  1.7658322811231006,\n",
              "  1.3564037533648712,\n",
              "  1.2407040781688483,\n",
              "  2.130217298173151,\n",
              "  1.4228319915327,\n",
              "  1.0651086490865755,\n",
              "  1.3008210311003705,\n",
              "  1.336545951796433,\n",
              "  0.8927754224911278,\n",
              "  1.4494292838262302,\n",
              "  1.4052738287907582,\n",
              "  1.6421697097891788,\n",
              "  1.2329833804288621,\n",
              "  1.19042665178928,\n",
              "  1.1682948223612457,\n",
              "  1.1518314137121108,\n",
              "  0.9607802401865855,\n",
              "  2.317439190074449,\n",
              "  1.0591147430338594,\n",
              "  1.4308630919602832,\n",
              "  0.7535680705496237,\n",
              "  0.8608283307581511,\n",
              "  1.2776122636975893,\n",
              "  1.3745862957220916,\n",
              "  1.259546137598783,\n",
              "  1.2978813187979172,\n",
              "  1.2412170838050638,\n",
              "  1.6009469708743893,\n",
              "  1.3149369953539032,\n",
              "  1.417901703622935,\n",
              "  1.2478669653497139,\n",
              "  1.1055812783082735,\n",
              "  0.9561307405997607,\n",
              "  0.9487783503683882,\n",
              "  1.1238565871041026,\n",
              "  1.2058356273089446,\n",
              "  1.2801012827406097,\n",
              "  0.8733100751144249,\n",
              "  0.9194732501297403,\n",
              "  1.6425573339441792,\n",
              "  1.085826790250066,\n",
              "  1.0639125693728595,\n",
              "  1.0875842666474016,\n",
              "  1.417901703622935,\n",
              "  1.550443891425932,\n",
              "  0.7825779328716171,\n",
              "  1.4690612745308145,\n",
              "  1.053086721720641,\n",
              "  1.2676073151634049,\n",
              "  0.7744003006005755,\n",
              "  1.3787482149724068,\n",
              "  1.363892581861956,\n",
              "  1.299352006316543,\n",
              "  1.2870449283923413,\n",
              "  1.11817763925502,\n",
              "  0.9474354220939228,\n",
              "  1.5218484589055707,\n",
              "  1.3526437911676632,\n",
              "  1.1556938532445284,\n",
              "  1.6013445735058454,\n",
              "  1.274619025074578,\n",
              "  1.422384489715834,\n",
              "  1.3408259533459403,\n",
              "  1.172646028567008,\n",
              "  1.1490645795125545,\n",
              "  1.459060149136146,\n",
              "  1.2483770274864237,\n",
              "  1.336545951796433,\n",
              "  0.9601174044814821,\n",
              "  1.4867225193896279,\n",
              "  1.4277452542806772,\n",
              "  1.35028849808504,\n",
              "  0.7560982446653928,\n",
              "  1.259040600296622,\n",
              "  1.13456827900627,\n",
              "  1.6549133695530214,\n",
              "  1.1204526724091788,\n",
              "  1.1176081573544434,\n",
              "  0.9153095762832032,\n",
              "  1.1639273497938836,\n",
              "  1.3066806149514323,\n",
              "  1.1529362882239027,\n",
              "  1.3047303442899274,\n",
              "  1.3066806149514323],\n",
              " [1.3513542915012873,\n",
              "  1.6531712758662451,\n",
              "  1.1591314016515095,\n",
              "  1.146623746597153,\n",
              "  1.534200250931635,\n",
              "  0.8244147643546641,\n",
              "  1.3849254511223998,\n",
              "  0.896646767585667,\n",
              "  0.8933920979850992,\n",
              "  0.8468733999355552,\n",
              "  1.5983727420273053,\n",
              "  1.2541504274128499,\n",
              "  1.163230224824979,\n",
              "  1.3684030070578461,\n",
              "  1.259030373326707,\n",
              "  1.5767177637336556,\n",
              "  1.1313105308473828,\n",
              "  1.4758270641002809,\n",
              "  0.7078394338399785,\n",
              "  1.0776631257858533,\n",
              "  1.3414816326526937,\n",
              "  0.9619430542380497,\n",
              "  1.864442277013294,\n",
              "  1.119563119841409,\n",
              "  1.1731134119849211,\n",
              "  1.191447502684958,\n",
              "  0.7383974068224853,\n",
              "  1.0894130835864608,\n",
              "  1.325976073101348,\n",
              "  1.1573133060467067,\n",
              "  1.875064223413523,\n",
              "  1.3827676165632712,\n",
              "  0.9650453383676673,\n",
              "  1.26103460094294,\n",
              "  1.7566019926196204,\n",
              "  0.8561294092512274,\n",
              "  1.306963410075799,\n",
              "  1.603075176597571]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    }
  ]
}