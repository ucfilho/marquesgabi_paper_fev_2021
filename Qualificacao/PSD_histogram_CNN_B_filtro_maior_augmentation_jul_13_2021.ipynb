{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_CNN_B_filtro_maior_augmentation_jul_13_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/Qualificacao/PSD_histogram_CNN_B_filtro_maior_augmentation_jul_13_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from keras.preprocessing import image"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZEvJvfoibE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db38f265-0ea1-4d00-fb56-07defcdda60c"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mahotas in /usr/local/lib/python3.7/dist-packages (1.4.11)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT"
      },
      "source": [
        "import mahotas.features.texture as mht\n",
        "import mahotas.features"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VcTdaNVh9EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8466e4d8-812a-4c1d-8998-5c1d6ee66e06"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_fev_2020'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 73 (delta 37), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020/marquesgabi_fev_2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v7SRrc8mH2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c74ebbb1-bfc6-419c-e335-628972e89bb6"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip'\n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Doutorado'...\n",
            "remote: Enumerating objects: 361, done.\u001b[K\n",
            "remote: Counting objects: 100% (111/111), done.\u001b[K\n",
            "remote: Compressing objects: 100% (110/110), done.\u001b[K\n",
            "remote: Total 361 (delta 38), reused 0 (delta 0), pack-reused 250\u001b[K\n",
            "Receiving objects: 100% (361/361), 202.49 MiB | 10.38 MiB/s, done.\n",
            "Resolving deltas: 100% (155/155), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020/marquesgabi_fev_2020/Doutorado\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[4] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHgqAnaFyCjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "915ddfd3-8a77-44dd-98de-e98820aeecd7"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 22.87 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from segment_filter_not_conclude import Segmenta  # got image provided segmented"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5MN5a_v4np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cc571ef-5123-418e-d929-52c05b10e7dd"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     104  138.569534  135.673080  ...   95.250008   94.887589   87.875748\n",
            "1     171  128.432159  130.897003  ...  169.105255  169.962173  163.393036\n",
            "2     179  174.372589  174.479080  ...  185.615494  222.606689  227.637161\n",
            "3     138   80.082130  113.349922  ...  177.112366  168.226425  147.537903\n",
            "4     130  183.983429  175.303192  ...  201.512436  180.657990  135.562134\n",
            "5     124   93.677414   89.108215  ...  190.247650  206.214355  220.683655\n",
            "6     188  136.789490  152.617462  ...  119.186501  110.069710  159.071960\n",
            "7     131  163.205750  168.807755  ...    0.267059    0.541868    1.473166\n",
            "8     157  169.597824  168.340668  ...  253.766754  249.267380  238.964111\n",
            "9     121  128.040024  126.870026  ...  250.625763  252.245941  250.515762\n",
            "10    126  194.592606  190.172836  ...  138.382721  138.604935  123.419754\n",
            "11    157    1.318431    1.458558  ...  254.909851  249.914734  250.406265\n",
            "12    122   72.921516   71.494484  ...    0.223596    1.147541    1.364149\n",
            "13    145   61.476334   61.110676  ...  153.768860  153.141922  148.893448\n",
            "14    200  149.466797  170.693207  ...    1.499600    0.216400    1.282400\n",
            "15    141  133.161804  125.019867  ...  134.414062  144.308884  164.056976\n",
            "16    178  194.541992  209.538345  ...  251.528610  253.248230  252.215256\n",
            "17    173  208.652481  198.210526  ...  123.780945  124.025162  129.082001\n",
            "18    191  138.121475  141.678223  ...    0.529344    0.637894    1.310847\n",
            "19    162  191.523224  192.004578  ...  184.682510  184.847427  182.734955\n",
            "20    129  127.068321  150.343842  ...    1.151974    0.516495    0.330329\n",
            "21    128  180.034180  182.173828  ...  178.358398  189.827148  184.335938\n",
            "22    162  152.665909  144.682358  ...  163.609207  161.093582  163.073166\n",
            "23    187  157.449738  158.845016  ...  154.674576  154.657700  169.350601\n",
            "24    179  139.103363  149.206619  ...    0.395306    1.613183    1.002903\n",
            "25    102  154.941956  160.128418  ...  100.785858   98.050369   90.951569\n",
            "26    100  107.211205  108.817604  ...    1.240000    1.398400    0.798400\n",
            "27    138  152.482239  156.181458  ...  137.974380  161.373230  158.328491\n",
            "28    113  126.489075  131.655960  ...  180.425232  175.326874  173.673981\n",
            "29    114  141.879654  136.709442  ...  156.707306  161.880280  159.364426\n",
            "30    110  228.637360  222.281647  ...  167.668762  180.760330  216.252548\n",
            "31    106   89.870064   93.931297  ...    0.029904    0.888928    1.598078\n",
            "32    122    3.964525   23.926363  ...  123.558174  127.416824  139.494751\n",
            "33    131  187.865921  197.780258  ...  127.254761  140.652405  151.728333\n",
            "34    127  149.388916  148.153259  ...  127.976250  125.043648  122.576294\n",
            "35    189  101.879288  101.961594  ...  142.150894  148.109741  153.388214\n",
            "36    118  106.794022  102.352768  ...  171.108032  172.787415  174.385803\n",
            "37    112  108.562500  117.375000  ...    1.000000    1.000000    1.000000\n",
            "38    140  223.039993  237.039993  ...  176.839996  198.759995  203.639999\n",
            "39    175  153.147202  146.782394  ...    1.078400    0.158400    1.345600\n",
            "40    168  119.500000  109.500000  ...   39.277779    2.277778    0.777778\n",
            "41    183  122.566124  130.039917  ...   89.369316   87.518433   93.055000\n",
            "42    130  163.578705  166.403320  ...  131.373505  128.052078  128.814209\n",
            "43    116  155.701523  157.711044  ...  133.888214  124.951248  119.332932\n",
            "44    200   46.441200   48.893600  ...    1.289600    1.000000    1.000000\n",
            "45    165  190.908829  194.399292  ...    0.404555    1.246832    1.000000\n",
            "46    148  154.365234  140.473343  ...    0.495252    1.549306    1.279036\n",
            "47    134  128.444870  124.175766  ...  159.552689  146.075089  128.381592\n",
            "48    175  162.228806  179.435196  ...  160.147171  167.076782  160.036774\n",
            "49    141  123.194954  152.541779  ...  135.530716  131.358780  126.263977\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzpQ1Pz0fX5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "75c42e79-4544-4502-92fb-604285d6fb46"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines\n",
        "# filename = 'model_ANN.pkl'\n",
        "filename = 'model_ANN_new.pkl'\n",
        "model = joblib.load(filename)\n",
        "'''"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n!git clone https://github.com/ucfilho/MarquesGabi_Routines\\n%cd MarquesGabi_Routines\\n# filename = 'model_ANN.pkl'\\nfilename = 'model_ANN_new.pkl'\\nmodel = joblib.load(filename)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2emP4rNjQy",
        "outputId": "5af67997-f7ad-4d9a-a3b6-f18df31c4013"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'MarquesGabi_Routines'...\n",
            "remote: Enumerating objects: 163, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (160/160), done.\u001b[K\n",
            "remote: Total 163 (delta 65), reused 3 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (163/163), 211.71 MiB | 16.14 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n",
            "Checking out files: 100% (46/46), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQO8d2QbNqj0"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "# make the CNN\n",
        "# model.add(Input(shape=(28, 28, 1)))\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=64, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=200))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=10))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbPQ1FSRG6A",
        "outputId": "6f2fc0ab-a8ae-4eb3-b071-89f082cf32db"
      },
      "source": [
        "\n",
        "# gives us back a <keras.callbacks.History object at 0x112e61a90>\n",
        "model.fit(X_train, Y_train, epochs=200, batch_size=32)\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 2s 18ms/step - loss: 0.6281 - accuracy: 0.6735\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.3806 - accuracy: 0.8350\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.2226 - accuracy: 0.9156\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1622 - accuracy: 0.9298\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0632 - accuracy: 0.9889\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0530 - accuracy: 0.9700\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0319 - accuracy: 0.9902\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0248 - accuracy: 0.9916\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0243 - accuracy: 0.9926\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0149 - accuracy: 0.9983\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0115 - accuracy: 0.9946\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0177 - accuracy: 0.9870\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0332 - accuracy: 0.9859\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0617 - accuracy: 0.9809\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0244 - accuracy: 0.9885\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0108 - accuracy: 0.9973\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0113 - accuracy: 0.9935\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0126 - accuracy: 0.9968\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0170 - accuracy: 0.9914\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0571 - accuracy: 0.9854\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.9511e-04 - accuracy: 1.0000\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.5616e-04 - accuracy: 1.0000\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.8884e-04 - accuracy: 1.0000\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6335e-04 - accuracy: 1.0000\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.8548e-04 - accuracy: 1.0000\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6980e-04 - accuracy: 1.0000\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7342e-04 - accuracy: 1.0000\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0623e-04 - accuracy: 1.0000\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2265e-04 - accuracy: 1.0000\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7571e-04 - accuracy: 1.0000\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2247e-04 - accuracy: 1.0000\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.1403e-04 - accuracy: 1.0000\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5034e-04 - accuracy: 1.0000\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.9640e-04 - accuracy: 1.0000\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1690e-04 - accuracy: 1.0000\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5214e-04 - accuracy: 1.0000\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3146e-04 - accuracy: 1.0000\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.9102e-04 - accuracy: 1.0000\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.6937e-04 - accuracy: 1.0000\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8731e-04 - accuracy: 1.0000\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6270e-04 - accuracy: 1.0000\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3009e-04 - accuracy: 1.0000\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.3662e-05 - accuracy: 1.0000\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1248e-04 - accuracy: 1.0000\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.4943e-04 - accuracy: 1.0000\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.6712e-05 - accuracy: 1.0000\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5069e-04 - accuracy: 1.0000\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.2770e-04 - accuracy: 1.0000\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8159e-04 - accuracy: 1.0000\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4868e-04 - accuracy: 1.0000\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2209e-04 - accuracy: 1.0000\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.9647e-04 - accuracy: 1.0000\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.9549e-05 - accuracy: 1.0000\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.7651e-05 - accuracy: 1.0000\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.8257e-05 - accuracy: 1.0000\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2236e-04 - accuracy: 1.0000\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.7299e-05 - accuracy: 1.0000\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5357e-05 - accuracy: 1.0000\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.5818e-05 - accuracy: 1.0000\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.9952e-05 - accuracy: 1.0000\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.3850e-05 - accuracy: 1.0000\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.3663e-05 - accuracy: 1.0000\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.8671e-05 - accuracy: 1.0000\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.0090e-05 - accuracy: 1.0000\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.4364e-05 - accuracy: 1.0000\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.9918e-04 - accuracy: 1.0000\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.8174e-05 - accuracy: 1.0000\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.5231e-05 - accuracy: 1.0000\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.4141e-05 - accuracy: 1.0000\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.2849e-05 - accuracy: 1.0000\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.7901e-05 - accuracy: 1.0000\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8230e-04 - accuracy: 1.0000\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7347e-05 - accuracy: 1.0000\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.2128e-05 - accuracy: 1.0000\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.9223e-05 - accuracy: 1.0000\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.1742e-05 - accuracy: 1.0000\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.0445e-05 - accuracy: 1.0000\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.3243e-05 - accuracy: 1.0000\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.9185e-05 - accuracy: 1.0000\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.7754e-05 - accuracy: 1.0000\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.6737e-05 - accuracy: 1.0000\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3300e-05 - accuracy: 1.0000\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2718e-05 - accuracy: 1.0000\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.3967e-05 - accuracy: 1.0000\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.3591e-05 - accuracy: 1.0000\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.3160e-05 - accuracy: 1.0000\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.7036e-05 - accuracy: 1.0000\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 2.2394e-05 - accuracy: 1.0000\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.8907e-05 - accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.6662e-05 - accuracy: 1.0000\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.2670e-05 - accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.5276e-05 - accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.7629e-05 - accuracy: 1.0000\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.2238e-05 - accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.8800e-05 - accuracy: 1.0000\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.6270e-05 - accuracy: 1.0000\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.9074e-05 - accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.7013e-04 - accuracy: 1.0000\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.3700e-05 - accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.8979e-05 - accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.9722e-05 - accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.4260e-05 - accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1519e-05 - accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1815e-05 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.0790e-05 - accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.0967e-04 - accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9251e-04 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.8747e-05 - accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 0.9967\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0187 - accuracy: 0.9961\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.4444 - accuracy: 0.9029\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1012 - accuracy: 0.9622\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0678 - accuracy: 0.9790\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 0.9958\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0079 - accuracy: 0.9979\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0130 - accuracy: 0.9941\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0307 - accuracy: 0.9925\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0214 - accuracy: 0.9897\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0058 - accuracy: 0.9978\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0182 - accuracy: 0.9990\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0215 - accuracy: 0.9904\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0142 - accuracy: 0.9962\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0141 - accuracy: 0.9978\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0048 - accuracy: 0.9993\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0037 - accuracy: 0.9990\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 0.9983\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 0.9951\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.7948e-04 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.8424e-04 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.5250e-04 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.5738e-04 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6665e-04 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.2953e-04 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0269e-04 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7454e-04 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6144e-04 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6051e-04 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1023e-04 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4615e-04 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8480e-04 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4111e-04 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.9396e-04 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.2799e-04 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.5281e-05 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.9358e-05 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.0348e-05 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.8037e-05 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.7333e-05 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.9844e-05 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.0066e-05 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.1296e-04 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.9703e-04 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.3845e-05 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2803e-04 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.8907e-05 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5344e-04 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.9597e-05 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.9191e-05 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.3669e-04 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.4846e-04 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4819e-04 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.5256e-04 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0246e-04 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.7674e-05 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.0076e-05 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3994e-04 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4038e-05 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4705e-05 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.7912e-05 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.3512e-05 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.3868e-04 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.3120e-05 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.6703e-04 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1048e-05 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5562e-04 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.1410e-05 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2746e-04 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.3508e-05 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.8331e-05 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7694e-05 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.1815e-05 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb73b2aa790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRaP8bHWNeZA",
        "outputId": "253f5897-31e1-4776-f97a-c14702c50d35"
      },
      "source": [
        "\n",
        "# Fit with data augmentation\n",
        "# Note: if you run this AFTER calling the previous model.fit(), it will CONTINUE training where it left off\n",
        "batch_size = 5\n",
        "data_generator = image.ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "train_generator = data_generator.flow(X_train, Y_train, batch_size)\n",
        "steps_per_epoch = X_train.shape[0] // batch_size\n",
        "\n",
        "model.fit(train_generator, validation_data=(X_test, Y_test), steps_per_epoch=steps_per_epoch, epochs=200)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "68/68 [==============================] - 2s 14ms/step - loss: 0.8774 - accuracy: 0.6006 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
            "Epoch 2/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6941 - accuracy: 0.5000 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
            "Epoch 3/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6939 - accuracy: 0.4970 - val_loss: 0.6929 - val_accuracy: 0.5102\n",
            "Epoch 4/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6938 - accuracy: 0.4911 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
            "Epoch 5/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6930 - val_accuracy: 0.5102\n",
            "Epoch 6/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.4970 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 7/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 8/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6936 - accuracy: 0.4793 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 9/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 10/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 11/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.4822 - val_loss: 0.6931 - val_accuracy: 0.5102\n",
            "Epoch 12/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.4852 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 13/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.4615 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 14/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6953 - accuracy: 0.5118 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 15/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 16/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 17/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 18/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 19/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 20/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 21/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6931 - accuracy: 0.5089 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 22/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 23/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 24/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5089 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 25/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 26/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 27/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 28/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 29/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 30/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6935 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 31/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 32/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 33/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 34/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 35/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 36/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5059 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 37/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 38/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 39/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5059 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 40/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.4556 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 41/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 42/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5089 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 43/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 44/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6935 - accuracy: 0.4408 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 45/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.4970 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 46/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 47/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.4763 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 48/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.4734 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 49/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6936 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 50/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5089 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 51/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 52/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6936 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 53/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 54/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 55/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5089 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 56/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 57/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 58/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 59/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 60/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5089 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 61/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 62/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 63/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6931 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 64/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6930 - accuracy: 0.5118 - val_loss: 0.6936 - val_accuracy: 0.4898\n",
            "Epoch 65/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5089 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 66/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 67/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6931 - accuracy: 0.5089 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 68/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 69/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 70/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 71/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6931 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 72/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 73/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 74/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 75/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 76/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 77/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 78/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5089 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 79/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 80/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5089 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 81/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 82/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 83/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 84/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5089 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 85/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 86/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 87/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 88/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 89/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 90/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6937 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 91/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 92/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6931 - accuracy: 0.5089 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 93/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 94/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5089 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 95/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 96/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 97/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5118 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 98/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 99/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 100/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 101/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 102/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5089 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 103/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 104/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 105/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 106/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5030 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 107/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 108/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5089 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 109/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 110/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 111/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.4970 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 112/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 113/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5089 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 114/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6935 - accuracy: 0.5029 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 115/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 116/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 117/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.4970 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 118/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.4970 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 119/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 120/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5089 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 121/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 122/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 123/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 124/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5030 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 125/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 126/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 127/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 128/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 129/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5089 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 130/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6935 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 131/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5089 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 132/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6934 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 133/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 134/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 135/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 136/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 137/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 138/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 139/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 140/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 141/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 142/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 143/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6935 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 144/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 145/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5089 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 146/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 147/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 148/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 149/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 150/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.4970 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 151/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 152/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.4970 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 153/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 154/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6936 - accuracy: 0.4290 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 155/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5089 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 156/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 157/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 158/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6937 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 159/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 160/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 161/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 162/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 163/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6931 - accuracy: 0.5089 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 164/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 165/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.4645 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 166/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 167/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6931 - accuracy: 0.5089 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 168/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 169/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 170/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 171/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 172/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 173/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 174/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5059 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 175/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 176/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 177/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 178/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6935 - accuracy: 0.4675 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 179/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.4615 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 180/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 181/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 182/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5089 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 183/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6935 - accuracy: 0.4970 - val_loss: 0.6932 - val_accuracy: 0.4898\n",
            "Epoch 184/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 185/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 186/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 187/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 188/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 189/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6935 - accuracy: 0.5059 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 190/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5089 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 191/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5118 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 192/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6936 - accuracy: 0.5030 - val_loss: 0.6933 - val_accuracy: 0.4898\n",
            "Epoch 193/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6932 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 194/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 195/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6934 - accuracy: 0.5030 - val_loss: 0.6935 - val_accuracy: 0.4898\n",
            "Epoch 196/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6931 - accuracy: 0.5089 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 197/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 198/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 199/200\n",
            "68/68 [==============================] - 0s 6ms/step - loss: 0.6931 - accuracy: 0.5059 - val_loss: 0.6934 - val_accuracy: 0.4898\n",
            "Epoch 200/200\n",
            "68/68 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4898\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb73b73efd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-3hvIYqRcV5",
        "outputId": "f0114e45-3411-4cff-b7e7-afa855b4806a"
      },
      "source": [
        "# X_train.shape\n",
        "steps_per_epoch"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "68"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIgDLfqrNhp_",
        "outputId": "0d4d0eea-ad7c-4264-97d9-2884e538b783"
      },
      "source": [
        "# gives us back a <keras.callbacks.History object at 0x112e61a90>\n",
        "model.fit(X_train, Y_train, epochs=200, batch_size=32)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.6815 - accuracy: 0.5510\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.3941 - accuracy: 0.8601\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.2433 - accuracy: 0.9125\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.1510 - accuracy: 0.9446\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0629 - accuracy: 0.9854\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0654 - accuracy: 0.9825\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0484 - accuracy: 0.9825\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 0.0314 - accuracy: 0.9825\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0149 - accuracy: 0.9971\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0044 - accuracy: 0.9971\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0032 - accuracy: 0.9971\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0083 - accuracy: 0.9971\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0147 - accuracy: 0.9942\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0181 - accuracy: 0.9913\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0042 - accuracy: 0.9971\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0483 - accuracy: 0.9913\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0150 - accuracy: 0.9971\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0067 - accuracy: 0.9971\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0062 - accuracy: 0.9971\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0064 - accuracy: 0.9942\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.9998e-04 - accuracy: 1.0000\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.4754e-04 - accuracy: 1.0000\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0045 - accuracy: 0.9971\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0050 - accuracy: 0.9971\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0167 - accuracy: 0.9971\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0356 - accuracy: 0.9796\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.1014 - accuracy: 0.9796\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0187 - accuracy: 1.0000\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.8909e-04 - accuracy: 1.0000\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.2727e-04 - accuracy: 1.0000\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5116e-04 - accuracy: 1.0000\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0038 - accuracy: 0.9971\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0232e-04 - accuracy: 1.0000\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.5664e-04 - accuracy: 1.0000\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2266e-04 - accuracy: 1.0000\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7254e-04 - accuracy: 1.0000\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.8716e-05 - accuracy: 1.0000\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7720e-04 - accuracy: 1.0000\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.9246e-05 - accuracy: 1.0000\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.7980e-05 - accuracy: 1.0000\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.6115e-05 - accuracy: 1.0000\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5762e-05 - accuracy: 1.0000\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.0874e-04 - accuracy: 1.0000\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 0.9971\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 0.9971\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.6295e-04 - accuracy: 1.0000\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.9371e-04 - accuracy: 1.0000\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.1895e-05 - accuracy: 1.0000\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1985e-04 - accuracy: 1.0000\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.7205e-04 - accuracy: 1.0000\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0057e-04 - accuracy: 1.0000\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.0852e-04 - accuracy: 1.0000\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.1937e-05 - accuracy: 1.0000\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.9748e-05 - accuracy: 1.0000\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4601e-04 - accuracy: 1.0000\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.6849e-05 - accuracy: 1.0000\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.6335e-04 - accuracy: 1.0000\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.9118e-05 - accuracy: 1.0000\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5311e-05 - accuracy: 1.0000\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4265e-05 - accuracy: 1.0000\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.2793e-04 - accuracy: 1.0000\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.3836e-05 - accuracy: 1.0000\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7958e-05 - accuracy: 1.0000\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4749e-04 - accuracy: 1.0000\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.3568e-05 - accuracy: 1.0000\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.5662e-06 - accuracy: 1.0000\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1399e-04 - accuracy: 1.0000\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.3240e-05 - accuracy: 1.0000\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1560e-04 - accuracy: 1.0000\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.0587e-06 - accuracy: 1.0000\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 0.9971\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4286e-04 - accuracy: 1.0000\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.9327e-04 - accuracy: 1.0000\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.8630e-05 - accuracy: 1.0000\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.4187e-05 - accuracy: 1.0000\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0744e-04 - accuracy: 1.0000\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.5623e-05 - accuracy: 1.0000\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0101 - accuracy: 0.9971\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0073 - accuracy: 0.9971\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1508e-04 - accuracy: 1.0000\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0424e-04 - accuracy: 1.0000\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.5967e-04 - accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0465e-04 - accuracy: 1.0000\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.2999e-05 - accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.7744e-04 - accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.3184e-04 - accuracy: 1.0000\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8623e-05 - accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.4086e-05 - accuracy: 1.0000\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.4550e-05 - accuracy: 1.0000\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9477e-04 - accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.4518e-05 - accuracy: 1.0000\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0768e-05 - accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.1700e-05 - accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.7007e-04 - accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.8797e-06 - accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4625e-05 - accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7257e-05 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.0663e-05 - accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.9511e-06 - accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.5104e-06 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2574e-04 - accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.1648e-05 - accuracy: 1.0000\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.7298e-05 - accuracy: 1.0000\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.2235e-05 - accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.6845e-05 - accuracy: 1.0000\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.2217e-06 - accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8.2668e-05 - accuracy: 1.0000\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4.0346e-05 - accuracy: 1.0000\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.8887e-05 - accuracy: 1.0000\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.7496e-06 - accuracy: 1.0000\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.0752e-05 - accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.2818e-07 - accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.3717e-06 - accuracy: 1.0000\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4058e-05 - accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.4114e-06 - accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.9949e-06 - accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.7064e-06 - accuracy: 1.0000\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3952e-05 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.1626e-06 - accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.0087e-05 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.3864e-06 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3915e-06 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9.2468e-06 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.6605e-05 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5163e-05 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6504e-06 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7327e-05 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.2795e-06 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.5887e-06 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.6004e-05 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 7.5506e-06 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.9885e-06 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5.3618e-06 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.9681e-06 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4760e-06 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.4438e-05 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.6509e-04 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.9815e-05 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.4449e-06 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.1660e-06 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.3647e-06 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.3730e-05 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.2301e-04 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.1134e-06 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2252e-05 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7.6355e-06 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3.8152e-05 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.4994e-05 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.4843e-05 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3.2887e-04 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.2752e-06 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.8752e-05 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.3337e-06 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5811e-06 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3228e-05 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.0075e-06 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.6015e-07 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.9449e-06 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.6724e-06 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.3000e-07 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.5951e-06 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3535e-05 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5.1295e-07 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.3120e-05 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9.4883e-04 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2.9502e-04 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.2987e-04 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.7167e-04 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.1356e-04 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2.6612e-05 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.1480e-07 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.1558e-06 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.3578e-04 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.0628e-07 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.0262e-05 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6.6185e-06 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.2341e-06 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.6244e-05 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.7361e-04 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.9353e-05 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8.3964e-07 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1.5427e-06 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4.6397e-07 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6.8776e-07 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb662268a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpf0XlSARX78",
        "outputId": "748e7817-b010-42e5-e7a2-38563ee6e0c4"
      },
      "source": [
        "pred_test= model.predict_classes(X_test)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predict   0   1\n",
            "Actual         \n",
            "0        47  25\n",
            "1         0  75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iFNNrlWV9tH",
        "outputId": "0630b12c-852c-457a-9a73-ec7d61fe4af4"
      },
      "source": [
        "pred_test"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
              "       1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "       1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
              "       0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QISvYcJBgWbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b39994c8-d98a-45e0-8312-0eefcaceaca2"
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[0] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  prediction = model.predict_classes(result)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "6   117.0   55.128059   59.700489  ...   87.601509   89.989555   91.943817\n",
            "8   148.0    2.585829    3.181154  ...   80.189194   80.206726   81.015343\n",
            "20  144.0   31.922840   40.058640  ...   68.333336   68.954475   70.943680\n",
            "25  149.0   55.310799   74.156624  ...    5.885906    5.885906    5.885906\n",
            "37  166.0   54.139061   73.410355  ...   72.352448   71.138763   67.800987\n",
            "40  106.0  105.626556  105.499115  ...  145.628693  129.074768   59.719124\n",
            "3   166.0    1.882131    2.013935  ...   78.332405   77.287254   75.720123\n",
            "7   189.0   89.677643   89.397804  ...  132.677643  138.436203  143.042526\n",
            "12  135.0    0.000000    0.000000  ...   59.093109   72.329102   85.764053\n",
            "25  133.0   40.288090   51.091415  ...  104.373962  116.684204  130.396118\n",
            "42  191.0    1.564047    0.690003  ...   74.990021   73.721558   76.497963\n",
            "47  168.0  135.083328  127.861115  ...   47.777779   46.277779   46.805557\n",
            "7   129.0    0.254612    0.567574  ...   45.372578   49.290543   52.559219\n",
            "24  157.0  104.335838  106.633698  ...  117.020126  119.303261  122.582748\n",
            "29  184.0    0.499527    0.148866  ...    0.711248    0.029773    0.000000\n",
            "44  147.0  137.775513  141.335617  ...   63.358276   64.238098   66.034012\n",
            "48  199.0   55.509502   57.527939  ...  114.432312  109.991302  112.840225\n",
            "1   100.0   29.342400   43.753601  ...   46.486401   37.867199   21.195200\n",
            "8   119.0   95.716263   94.491356  ...    0.775087    0.941176    0.083045\n",
            "10  159.0   58.604404   36.870140  ...   61.770817   65.862152   65.547203\n",
            "14  196.0   92.224487   88.816322  ...   47.693878   50.714283   49.163265\n",
            "21  154.0   91.429749   88.966949  ...   56.123966   48.140495   42.752071\n",
            "25  184.0    2.206994   34.629013  ...   78.638939   83.293472   84.082230\n",
            "39  123.0  182.862335   18.205566  ...   60.681015   58.629848   54.302269\n",
            "41  183.0   98.224129   98.302666  ...    4.741528    4.459764    4.155424\n",
            "42  122.0  100.932281  100.414131  ...  133.598495  131.887131  103.041656\n",
            "\n",
            "[26 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRbWgmX_LFH",
        "outputId": "10e687a5-571c-4220-ebe0-6c633ea36d63"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "from GetBetterSegm import GetBetter"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_paper_fev_2021'...\n",
            "remote: Enumerating objects: 596, done.\u001b[K\n",
            "remote: Counting objects: 100% (357/357), done.\u001b[K\n",
            "remote: Compressing objects: 100% (356/356), done.\u001b[K\n",
            "remote: Total 596 (delta 218), reused 0 (delta 0), pack-reused 239\u001b[K\n",
            "Receiving objects: 100% (596/596), 5.12 MiB | 15.70 MiB/s, done.\n",
            "Resolving deltas: 100% (355/355), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAG_I6FwCvFr",
        "outputId": "28917c32-a411-47c4-faad-018679491d36"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "%cd marquesgabi_out_2020\n",
        "PSD_imageJ = 'Areas_ImageJ.csv'\n",
        "PSD_new = pd.read_csv(PSD_imageJ)\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'marquesgabi_out_2020'...\n",
            "remote: Enumerating objects: 146, done.\u001b[K\n",
            "remote: Counting objects: 100% (146/146), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 146 (delta 75), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (146/146), 1.00 MiB | 15.81 MiB/s, done.\n",
            "Resolving deltas: 100% (75/75), done.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020\n",
            "   Juntas   Area\n",
            "0       1  2.001\n",
            "1       2  0.820\n",
            "2       3  1.270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "PekBHQOT_6CP",
        "outputId": "eac632b5-ccf1-4ef5-9d65-b19615da6091"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>117.0</td>\n",
              "      <td>55.128059</td>\n",
              "      <td>59.700489</td>\n",
              "      <td>62.096722</td>\n",
              "      <td>67.018929</td>\n",
              "      <td>68.180298</td>\n",
              "      <td>71.295052</td>\n",
              "      <td>75.167648</td>\n",
              "      <td>79.615166</td>\n",
              "      <td>81.438461</td>\n",
              "      <td>82.256042</td>\n",
              "      <td>82.315292</td>\n",
              "      <td>80.995544</td>\n",
              "      <td>82.627365</td>\n",
              "      <td>85.075310</td>\n",
              "      <td>85.369781</td>\n",
              "      <td>84.836807</td>\n",
              "      <td>81.623573</td>\n",
              "      <td>75.328003</td>\n",
              "      <td>81.173569</td>\n",
              "      <td>88.036598</td>\n",
              "      <td>89.673164</td>\n",
              "      <td>92.582512</td>\n",
              "      <td>88.769958</td>\n",
              "      <td>83.303825</td>\n",
              "      <td>87.347588</td>\n",
              "      <td>97.606468</td>\n",
              "      <td>101.811745</td>\n",
              "      <td>109.706406</td>\n",
              "      <td>64.105347</td>\n",
              "      <td>70.146980</td>\n",
              "      <td>70.722191</td>\n",
              "      <td>71.725922</td>\n",
              "      <td>73.687637</td>\n",
              "      <td>74.654976</td>\n",
              "      <td>75.737450</td>\n",
              "      <td>76.554604</td>\n",
              "      <td>78.803925</td>\n",
              "      <td>78.979553</td>\n",
              "      <td>79.778656</td>\n",
              "      <td>...</td>\n",
              "      <td>34.294762</td>\n",
              "      <td>37.960625</td>\n",
              "      <td>47.765289</td>\n",
              "      <td>66.304993</td>\n",
              "      <td>73.844475</td>\n",
              "      <td>73.650597</td>\n",
              "      <td>76.161804</td>\n",
              "      <td>75.998024</td>\n",
              "      <td>78.379059</td>\n",
              "      <td>83.292801</td>\n",
              "      <td>84.391335</td>\n",
              "      <td>80.918762</td>\n",
              "      <td>137.971161</td>\n",
              "      <td>143.745117</td>\n",
              "      <td>124.361237</td>\n",
              "      <td>65.200455</td>\n",
              "      <td>61.825554</td>\n",
              "      <td>67.770325</td>\n",
              "      <td>70.025711</td>\n",
              "      <td>88.518082</td>\n",
              "      <td>108.782379</td>\n",
              "      <td>58.494125</td>\n",
              "      <td>25.751406</td>\n",
              "      <td>27.866243</td>\n",
              "      <td>32.204182</td>\n",
              "      <td>33.088394</td>\n",
              "      <td>32.763168</td>\n",
              "      <td>33.652641</td>\n",
              "      <td>35.915627</td>\n",
              "      <td>44.141647</td>\n",
              "      <td>60.940830</td>\n",
              "      <td>73.167137</td>\n",
              "      <td>78.268028</td>\n",
              "      <td>78.564468</td>\n",
              "      <td>78.539482</td>\n",
              "      <td>79.932281</td>\n",
              "      <td>82.373367</td>\n",
              "      <td>87.601509</td>\n",
              "      <td>89.989555</td>\n",
              "      <td>91.943817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>148.0</td>\n",
              "      <td>2.585829</td>\n",
              "      <td>3.181154</td>\n",
              "      <td>4.813733</td>\n",
              "      <td>11.298028</td>\n",
              "      <td>28.473341</td>\n",
              "      <td>40.308254</td>\n",
              "      <td>48.465305</td>\n",
              "      <td>50.249821</td>\n",
              "      <td>46.991234</td>\n",
              "      <td>47.047482</td>\n",
              "      <td>46.133678</td>\n",
              "      <td>43.435360</td>\n",
              "      <td>36.070126</td>\n",
              "      <td>27.554420</td>\n",
              "      <td>14.807890</td>\n",
              "      <td>6.843682</td>\n",
              "      <td>2.528123</td>\n",
              "      <td>1.308985</td>\n",
              "      <td>1.364500</td>\n",
              "      <td>0.937180</td>\n",
              "      <td>0.869248</td>\n",
              "      <td>0.338203</td>\n",
              "      <td>0.579255</td>\n",
              "      <td>0.639153</td>\n",
              "      <td>1.019722</td>\n",
              "      <td>1.414901</td>\n",
              "      <td>2.219869</td>\n",
              "      <td>2.583638</td>\n",
              "      <td>16.620892</td>\n",
              "      <td>14.543464</td>\n",
              "      <td>23.941565</td>\n",
              "      <td>38.271004</td>\n",
              "      <td>48.111031</td>\n",
              "      <td>53.463116</td>\n",
              "      <td>55.622356</td>\n",
              "      <td>56.090580</td>\n",
              "      <td>55.243248</td>\n",
              "      <td>54.312641</td>\n",
              "      <td>52.041645</td>\n",
              "      <td>...</td>\n",
              "      <td>51.429512</td>\n",
              "      <td>49.382771</td>\n",
              "      <td>47.637691</td>\n",
              "      <td>46.747993</td>\n",
              "      <td>52.384960</td>\n",
              "      <td>68.521553</td>\n",
              "      <td>77.360123</td>\n",
              "      <td>79.306801</td>\n",
              "      <td>80.959824</td>\n",
              "      <td>80.615784</td>\n",
              "      <td>81.522285</td>\n",
              "      <td>81.686638</td>\n",
              "      <td>63.689556</td>\n",
              "      <td>54.276123</td>\n",
              "      <td>47.401028</td>\n",
              "      <td>47.392990</td>\n",
              "      <td>50.487949</td>\n",
              "      <td>51.490143</td>\n",
              "      <td>53.562458</td>\n",
              "      <td>54.634781</td>\n",
              "      <td>55.661800</td>\n",
              "      <td>57.126373</td>\n",
              "      <td>58.495987</td>\n",
              "      <td>57.191387</td>\n",
              "      <td>55.467499</td>\n",
              "      <td>54.777214</td>\n",
              "      <td>54.639889</td>\n",
              "      <td>54.469692</td>\n",
              "      <td>52.190655</td>\n",
              "      <td>48.513519</td>\n",
              "      <td>47.004387</td>\n",
              "      <td>49.403217</td>\n",
              "      <td>56.295837</td>\n",
              "      <td>75.466774</td>\n",
              "      <td>78.703438</td>\n",
              "      <td>80.750916</td>\n",
              "      <td>80.635506</td>\n",
              "      <td>80.189194</td>\n",
              "      <td>80.206726</td>\n",
              "      <td>81.015343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>144.0</td>\n",
              "      <td>31.922840</td>\n",
              "      <td>40.058640</td>\n",
              "      <td>43.809414</td>\n",
              "      <td>46.358795</td>\n",
              "      <td>48.128857</td>\n",
              "      <td>46.524693</td>\n",
              "      <td>46.600307</td>\n",
              "      <td>44.966816</td>\n",
              "      <td>46.654324</td>\n",
              "      <td>46.530094</td>\n",
              "      <td>45.635036</td>\n",
              "      <td>45.173611</td>\n",
              "      <td>46.406635</td>\n",
              "      <td>48.280865</td>\n",
              "      <td>52.158176</td>\n",
              "      <td>49.524693</td>\n",
              "      <td>46.083336</td>\n",
              "      <td>44.951389</td>\n",
              "      <td>60.157410</td>\n",
              "      <td>84.212959</td>\n",
              "      <td>101.346451</td>\n",
              "      <td>108.281639</td>\n",
              "      <td>108.256172</td>\n",
              "      <td>110.099541</td>\n",
              "      <td>111.829483</td>\n",
              "      <td>116.014679</td>\n",
              "      <td>121.021614</td>\n",
              "      <td>124.025467</td>\n",
              "      <td>29.902781</td>\n",
              "      <td>41.169754</td>\n",
              "      <td>46.537037</td>\n",
              "      <td>47.598766</td>\n",
              "      <td>49.429790</td>\n",
              "      <td>48.115746</td>\n",
              "      <td>48.654324</td>\n",
              "      <td>47.791672</td>\n",
              "      <td>49.034725</td>\n",
              "      <td>49.067131</td>\n",
              "      <td>47.865742</td>\n",
              "      <td>...</td>\n",
              "      <td>63.231483</td>\n",
              "      <td>62.994598</td>\n",
              "      <td>64.329475</td>\n",
              "      <td>64.090286</td>\n",
              "      <td>65.973000</td>\n",
              "      <td>66.614975</td>\n",
              "      <td>66.449081</td>\n",
              "      <td>69.574081</td>\n",
              "      <td>68.054787</td>\n",
              "      <td>66.013893</td>\n",
              "      <td>66.150467</td>\n",
              "      <td>65.165131</td>\n",
              "      <td>89.532410</td>\n",
              "      <td>100.482262</td>\n",
              "      <td>105.212975</td>\n",
              "      <td>110.816360</td>\n",
              "      <td>114.717598</td>\n",
              "      <td>119.288582</td>\n",
              "      <td>122.728394</td>\n",
              "      <td>125.775475</td>\n",
              "      <td>126.516975</td>\n",
              "      <td>124.794754</td>\n",
              "      <td>121.002312</td>\n",
              "      <td>113.189819</td>\n",
              "      <td>102.015442</td>\n",
              "      <td>89.750000</td>\n",
              "      <td>77.584106</td>\n",
              "      <td>68.653549</td>\n",
              "      <td>65.919762</td>\n",
              "      <td>64.588737</td>\n",
              "      <td>64.329475</td>\n",
              "      <td>65.192139</td>\n",
              "      <td>66.145836</td>\n",
              "      <td>67.267754</td>\n",
              "      <td>66.091049</td>\n",
              "      <td>66.176704</td>\n",
              "      <td>65.969910</td>\n",
              "      <td>68.333336</td>\n",
              "      <td>68.954475</td>\n",
              "      <td>70.943680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>149.0</td>\n",
              "      <td>55.310799</td>\n",
              "      <td>74.156624</td>\n",
              "      <td>80.154770</td>\n",
              "      <td>81.541641</td>\n",
              "      <td>83.940010</td>\n",
              "      <td>85.073547</td>\n",
              "      <td>87.572548</td>\n",
              "      <td>86.519890</td>\n",
              "      <td>88.669106</td>\n",
              "      <td>91.522499</td>\n",
              "      <td>91.330925</td>\n",
              "      <td>90.879875</td>\n",
              "      <td>88.929962</td>\n",
              "      <td>86.997612</td>\n",
              "      <td>84.017937</td>\n",
              "      <td>80.838257</td>\n",
              "      <td>66.636818</td>\n",
              "      <td>29.278502</td>\n",
              "      <td>24.292377</td>\n",
              "      <td>24.450476</td>\n",
              "      <td>17.870052</td>\n",
              "      <td>63.473225</td>\n",
              "      <td>76.659935</td>\n",
              "      <td>78.877487</td>\n",
              "      <td>79.531151</td>\n",
              "      <td>77.714203</td>\n",
              "      <td>72.798347</td>\n",
              "      <td>72.293861</td>\n",
              "      <td>31.079723</td>\n",
              "      <td>47.704563</td>\n",
              "      <td>68.700424</td>\n",
              "      <td>73.618492</td>\n",
              "      <td>75.393364</td>\n",
              "      <td>78.651505</td>\n",
              "      <td>81.962265</td>\n",
              "      <td>83.393631</td>\n",
              "      <td>84.487320</td>\n",
              "      <td>85.933372</td>\n",
              "      <td>87.052834</td>\n",
              "      <td>...</td>\n",
              "      <td>1.017567</td>\n",
              "      <td>0.879195</td>\n",
              "      <td>0.879195</td>\n",
              "      <td>0.879195</td>\n",
              "      <td>0.879195</td>\n",
              "      <td>0.841359</td>\n",
              "      <td>0.740462</td>\n",
              "      <td>0.879195</td>\n",
              "      <td>0.879195</td>\n",
              "      <td>0.879195</td>\n",
              "      <td>0.879195</td>\n",
              "      <td>0.879195</td>\n",
              "      <td>7.307283</td>\n",
              "      <td>7.259358</td>\n",
              "      <td>7.073960</td>\n",
              "      <td>7.702762</td>\n",
              "      <td>7.991397</td>\n",
              "      <td>6.722085</td>\n",
              "      <td>6.591235</td>\n",
              "      <td>6.773479</td>\n",
              "      <td>6.946579</td>\n",
              "      <td>7.056304</td>\n",
              "      <td>7.094140</td>\n",
              "      <td>6.757399</td>\n",
              "      <td>6.972569</td>\n",
              "      <td>6.351786</td>\n",
              "      <td>5.328454</td>\n",
              "      <td>5.548264</td>\n",
              "      <td>5.925904</td>\n",
              "      <td>5.512589</td>\n",
              "      <td>5.149363</td>\n",
              "      <td>5.164497</td>\n",
              "      <td>5.695465</td>\n",
              "      <td>6.035990</td>\n",
              "      <td>5.689158</td>\n",
              "      <td>5.315843</td>\n",
              "      <td>5.769875</td>\n",
              "      <td>5.885906</td>\n",
              "      <td>5.885906</td>\n",
              "      <td>5.885906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>166.0</td>\n",
              "      <td>54.139061</td>\n",
              "      <td>73.410355</td>\n",
              "      <td>81.971687</td>\n",
              "      <td>83.499924</td>\n",
              "      <td>87.490334</td>\n",
              "      <td>87.758591</td>\n",
              "      <td>84.550148</td>\n",
              "      <td>80.134995</td>\n",
              "      <td>62.355633</td>\n",
              "      <td>64.369286</td>\n",
              "      <td>69.948174</td>\n",
              "      <td>72.555809</td>\n",
              "      <td>77.742767</td>\n",
              "      <td>79.246620</td>\n",
              "      <td>72.772095</td>\n",
              "      <td>71.759468</td>\n",
              "      <td>81.803162</td>\n",
              "      <td>89.945702</td>\n",
              "      <td>92.965302</td>\n",
              "      <td>96.838142</td>\n",
              "      <td>96.938301</td>\n",
              "      <td>97.186218</td>\n",
              "      <td>97.358673</td>\n",
              "      <td>94.203217</td>\n",
              "      <td>97.248062</td>\n",
              "      <td>105.752060</td>\n",
              "      <td>115.599792</td>\n",
              "      <td>117.989693</td>\n",
              "      <td>73.078812</td>\n",
              "      <td>80.144569</td>\n",
              "      <td>79.925247</td>\n",
              "      <td>82.814476</td>\n",
              "      <td>91.974884</td>\n",
              "      <td>89.059509</td>\n",
              "      <td>84.950493</td>\n",
              "      <td>84.918556</td>\n",
              "      <td>76.281319</td>\n",
              "      <td>65.483231</td>\n",
              "      <td>69.890396</td>\n",
              "      <td>...</td>\n",
              "      <td>36.699081</td>\n",
              "      <td>41.614162</td>\n",
              "      <td>47.486423</td>\n",
              "      <td>54.436344</td>\n",
              "      <td>59.468861</td>\n",
              "      <td>63.127445</td>\n",
              "      <td>68.692841</td>\n",
              "      <td>73.429375</td>\n",
              "      <td>76.621857</td>\n",
              "      <td>74.945557</td>\n",
              "      <td>72.898956</td>\n",
              "      <td>71.141090</td>\n",
              "      <td>92.144867</td>\n",
              "      <td>90.999557</td>\n",
              "      <td>81.667870</td>\n",
              "      <td>66.164322</td>\n",
              "      <td>55.846561</td>\n",
              "      <td>56.200752</td>\n",
              "      <td>55.388443</td>\n",
              "      <td>56.274490</td>\n",
              "      <td>58.371605</td>\n",
              "      <td>58.931915</td>\n",
              "      <td>59.767597</td>\n",
              "      <td>55.044121</td>\n",
              "      <td>49.341557</td>\n",
              "      <td>47.764545</td>\n",
              "      <td>50.521118</td>\n",
              "      <td>51.877190</td>\n",
              "      <td>30.020901</td>\n",
              "      <td>16.729570</td>\n",
              "      <td>24.197994</td>\n",
              "      <td>40.715050</td>\n",
              "      <td>53.128025</td>\n",
              "      <td>61.212219</td>\n",
              "      <td>67.020897</td>\n",
              "      <td>68.372330</td>\n",
              "      <td>70.854988</td>\n",
              "      <td>72.352448</td>\n",
              "      <td>71.138763</td>\n",
              "      <td>67.800987</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Width          0          1  ...        781        782        783\n",
              "6   117.0  55.128059  59.700489  ...  87.601509  89.989555  91.943817\n",
              "8   148.0   2.585829   3.181154  ...  80.189194  80.206726  81.015343\n",
              "20  144.0  31.922840  40.058640  ...  68.333336  68.954475  70.943680\n",
              "25  149.0  55.310799  74.156624  ...   5.885906   5.885906   5.885906\n",
              "37  166.0  54.139061  73.410355  ...  72.352448  71.138763  67.800987\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "Area = np.array(PSD_new['Area'])\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "Vfk_fNXGDK5_",
        "outputId": "ec45e178-954d-4579-8341-8ad1f4b84c9f"
      },
      "source": [
        " wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        " wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        " X = pd.DataFrame([Diam1,Diameter_All])\n",
        " wts = pd.DataFrame([wt1,wt2])\n",
        "plt.hist(X,weights=wts)\n",
        "plt.legend(['Image J','CNN'])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb73b5d5410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "ZZHa1j4HT9Dq",
        "outputId": "cde46ce6-920b-479b-e34e-c750182ebe35"
      },
      "source": [
        "counts, bins, bars = plt.hist(X,weights=wts)\n",
        "print(bars)\n",
        "print(bins)\n",
        "print(counts)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<a list of 2 Lists of Patches objects>\n",
            "[0.61145114 0.78204995 0.95264875 1.12324756 1.29384636 1.46444517\n",
            " 1.63504397 1.80564278 1.97624158 2.14684039 2.31743919]\n",
            "[[ 3.15789474  8.42105263 15.78947368 30.52631579 27.36842105  8.42105263\n",
            "   4.21052632  0.          1.05263158  1.05263158]\n",
            " [ 3.84615385 15.38461538 11.53846154 26.92307692 15.38461538  7.69230769\n",
            "  15.38461538  3.84615385  0.          0.        ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o_vDGeWUwIZ",
        "outputId": "ff9ef13f-6b3e-4862-89e4-a0c202981e7d"
      },
      "source": [
        "print(counts.sum())"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200.00000000000006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "KcH52-6iJQ8t",
        "outputId": "6eb35c21-f714-4d92-a974-4c9c4407d0b9"
      },
      "source": [
        "\n",
        "plt.hist([Diam1,Diameter_All])\n",
        "plt.legend(['Image J','CNN'])\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb662164350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r11AxFK_JIii",
        "outputId": "4e25c2fd-0fbf-4a57-86f9-06ff02da3f2e"
      },
      "source": [
        "[Diam1,Diameter_All]"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.59616801403081,\n",
              "  1.0217907939900581,\n",
              "  1.2716187407449044,\n",
              "  1.104429030701514,\n",
              "  1.2163487785097904,\n",
              "  1.6013445735058454,\n",
              "  1.1715597420637607,\n",
              "  1.2534662333717612,\n",
              "  1.2676073151634049,\n",
              "  1.309600575274104,\n",
              "  1.292966945531582,\n",
              "  1.7658322811231006,\n",
              "  1.3564037533648712,\n",
              "  1.2407040781688483,\n",
              "  2.130217298173151,\n",
              "  1.4228319915327,\n",
              "  1.0651086490865755,\n",
              "  1.3008210311003705,\n",
              "  1.336545951796433,\n",
              "  0.8927754224911278,\n",
              "  1.4494292838262302,\n",
              "  1.4052738287907582,\n",
              "  1.6421697097891788,\n",
              "  1.2329833804288621,\n",
              "  1.19042665178928,\n",
              "  1.1682948223612457,\n",
              "  1.1518314137121108,\n",
              "  0.9607802401865855,\n",
              "  2.317439190074449,\n",
              "  1.0591147430338594,\n",
              "  1.4308630919602832,\n",
              "  0.7535680705496237,\n",
              "  0.8608283307581511,\n",
              "  1.2776122636975893,\n",
              "  1.3745862957220916,\n",
              "  1.259546137598783,\n",
              "  1.2978813187979172,\n",
              "  1.2412170838050638,\n",
              "  1.6009469708743893,\n",
              "  1.3149369953539032,\n",
              "  1.417901703622935,\n",
              "  1.2478669653497139,\n",
              "  1.1055812783082735,\n",
              "  0.9561307405997607,\n",
              "  0.9487783503683882,\n",
              "  1.1238565871041026,\n",
              "  1.2058356273089446,\n",
              "  1.2801012827406097,\n",
              "  0.8733100751144249,\n",
              "  0.9194732501297403,\n",
              "  1.6425573339441792,\n",
              "  1.085826790250066,\n",
              "  1.0639125693728595,\n",
              "  1.0875842666474016,\n",
              "  1.417901703622935,\n",
              "  1.550443891425932,\n",
              "  0.7825779328716171,\n",
              "  1.4690612745308145,\n",
              "  1.053086721720641,\n",
              "  1.2676073151634049,\n",
              "  0.7744003006005755,\n",
              "  1.3787482149724068,\n",
              "  1.363892581861956,\n",
              "  1.299352006316543,\n",
              "  1.2870449283923413,\n",
              "  1.11817763925502,\n",
              "  0.9474354220939228,\n",
              "  1.5218484589055707,\n",
              "  1.3526437911676632,\n",
              "  1.1556938532445284,\n",
              "  1.6013445735058454,\n",
              "  1.274619025074578,\n",
              "  1.422384489715834,\n",
              "  1.3408259533459403,\n",
              "  1.172646028567008,\n",
              "  1.1490645795125545,\n",
              "  1.459060149136146,\n",
              "  1.2483770274864237,\n",
              "  1.336545951796433,\n",
              "  0.9601174044814821,\n",
              "  1.4867225193896279,\n",
              "  1.4277452542806772,\n",
              "  1.35028849808504,\n",
              "  0.7560982446653928,\n",
              "  1.259040600296622,\n",
              "  1.13456827900627,\n",
              "  1.6549133695530214,\n",
              "  1.1204526724091788,\n",
              "  1.1176081573544434,\n",
              "  0.9153095762832032,\n",
              "  1.1639273497938836,\n",
              "  1.3066806149514323,\n",
              "  1.1529362882239027,\n",
              "  1.3047303442899274,\n",
              "  1.3066806149514323],\n",
              " [0.8927556410739327,\n",
              "  1.204237982586011,\n",
              "  1.2866652376400842,\n",
              "  1.32971653602165,\n",
              "  1.403305629137748,\n",
              "  1.1245555975218615,\n",
              "  1.1582151449753184,\n",
              "  1.6887481244026106,\n",
              "  0.8297471714638734,\n",
              "  1.1811098727141622,\n",
              "  1.410339047030947,\n",
              "  1.249336990782272,\n",
              "  0.8366190185755455,\n",
              "  1.4729229089157536,\n",
              "  1.710883568111203,\n",
              "  1.3214554616708363,\n",
              "  1.8188331860551605,\n",
              "  0.6114511434670826,\n",
              "  1.115183614017463,\n",
              "  1.0937332328767442,\n",
              "  1.7746318284338831,\n",
              "  1.1391607895763858,\n",
              "  1.538424524293096,\n",
              "  0.9494606533632395,\n",
              "  1.6996680080432691,\n",
              "  1.0480707068747352]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    }
  ]
}