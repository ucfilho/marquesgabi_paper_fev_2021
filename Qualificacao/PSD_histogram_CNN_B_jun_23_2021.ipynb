{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PSD_histogram_CNN_B_jun_23_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ucfilho/marquesgabi_paper_fev_2021/blob/main/Qualificacao/PSD_histogram_CNN_B_jun_23_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sog7Z9pyhUD_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import zipfile\n",
        "#import random\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "#import scikit-image\n",
        "import skimage\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZEvJvfoibE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d61ffc65-b191-430d-93fc-f81f1bf3a3a1"
      },
      "source": [
        "!pip install mahotas"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mahotas in /usr/local/lib/python3.7/dist-packages (1.4.11)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mahotas) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf_a6PJ1iUnT"
      },
      "source": [
        "import mahotas.features.texture as mht\n",
        "import mahotas.features"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VcTdaNVh9EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceb94ed0-5bb0-4fdc-b5d3-ee37eebe8709"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_fev_2020 #clonar do Github\n",
        "%cd marquesgabi_fev_2020\n",
        "import Go2BlackWhite\n",
        "import Go2Mahotas"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'marquesgabi_fev_2020' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v7SRrc8mH2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3f1aa39-fc0f-41c9-f9a9-210af04df26c"
      },
      "source": [
        "!git clone https://github.com/marquesgabi/Doutorado\n",
        "%cd Doutorado\n",
        "\n",
        "Transfere='Fotos_Grandes_3cdAmostra.zip'\n",
        "file_name = zipfile.ZipFile(Transfere, 'r')\n",
        "file_name.extractall()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Doutorado' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kA4IWSmasoD"
      },
      "source": [
        "Size=1200 # tamanho da foto\n",
        "ww,img_name=Go2BlackWhite.BlackWhite(Transfere,Size) #Pegamos a primeira foto Grande\n",
        "img=ww[4] \n",
        "# this is the big image we want to segment \n",
        "# ww[0], change it if you want to segment another picture"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHgqAnaFyCjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38dfdc1e-5120-433d-8ab8-fa23e7985bf8"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'MarquesGabi_Routines' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4rFvzkyWCi"
      },
      "source": [
        "from segment_filter_not_conclude import Segmenta  # got image provided segmented"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnTtH3KDP863"
      },
      "source": [
        "df=Segmenta(img)\n",
        "Img_Size = 28"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5MN5a_v4np",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3452fb49-d40f-43ba-d895-452129187ba6"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "0     191  173.630325  167.554810  ...  107.287117  112.845810  122.996414\n",
            "1     119    1.290658    0.467128  ...    0.041522    0.702422    1.525952\n",
            "2     174   90.534409   91.254990  ...    1.052187    0.163430    1.347734\n",
            "3     100  102.416000  106.201599  ...    1.000000    1.000000    1.000000\n",
            "4     155  111.213242  114.444542  ...    7.121208    1.657732    1.618855\n",
            "5     170  132.322632  159.447357  ...    0.899239    0.157232    0.141453\n",
            "6     164  159.761444  149.698990  ...  191.011322  210.444397  217.311722\n",
            "7     130  197.958344  192.264145  ...  199.774200  179.581299  167.961899\n",
            "8     159   78.188637   67.890236  ...    0.742811    0.263716    1.383213\n",
            "9     132   65.763084   75.588615  ...    0.274564    0.516988    1.456382\n",
            "10    166  204.519379  233.171417  ...  170.152695  166.214096  155.828552\n",
            "11    144  150.335663  151.279327  ...  138.179016  150.580261  153.921295\n",
            "12    137  181.581848  173.642059  ...  135.714264  130.712021  128.484299\n",
            "13    178  125.732750  138.184448  ...    1.155410    0.143542    1.339351\n",
            "14    134  165.035202  166.486755  ...    0.327244    0.506349    1.461573\n",
            "15    122   48.309055   37.570007  ...    1.000000    1.000000    1.000000\n",
            "16    153  133.316086  115.784546  ...   85.115852   82.393402   86.626122\n",
            "17    126  236.962967  242.938278  ...   86.395065   84.814819   83.456787\n",
            "18    120  248.094452  248.732224  ...  184.828888  188.944458  190.375549\n",
            "19    185  142.818024  153.302124  ...  140.758209   81.906212   54.326309\n",
            "20    140  159.039993  157.399994  ...  195.599991  208.879990  208.399994\n",
            "21    146  114.314880  117.958710  ...    0.543629    0.379246    1.420341\n",
            "22    154  157.347137  146.983475  ...   16.826447    9.917356    1.586777\n",
            "23    184  132.155472  130.145081  ...  232.667267  179.314728  118.636093\n",
            "24    109    1.000000    1.972477  ...   60.421349   63.454166   64.139130\n",
            "25    167   54.007530   58.047550  ...  167.722229  168.201126  165.923889\n",
            "26    156  157.426041  167.681152  ...  193.416183  198.404358  183.084167\n",
            "27    117  172.572723  174.691650  ...  156.921753  154.527740  145.770035\n",
            "28    154  129.818192  116.148773  ...  127.512398  127.181824  126.851250\n",
            "29    174    0.936451    2.583961  ...    1.000000    1.000000    0.689259\n",
            "30    175  143.611191  127.755196  ...  169.491211  172.315201  168.884781\n",
            "31    106  184.526520  182.220367  ...  149.844421  123.597733  119.362061\n",
            "32    129    0.481221    0.310678  ...  167.475876  161.251602  145.191757\n",
            "33    103  150.493546  151.724380  ...  144.293335  145.162781  159.943451\n",
            "34    124  124.870972  110.479698  ...   87.883453   89.978134   90.524445\n",
            "35    130  162.324280  151.810165  ...    1.000000    0.972781    0.089467\n",
            "36    164  124.289703  129.648422  ...  132.514572  157.619873  153.795944\n",
            "37    127   64.933968   67.162994  ...  161.733154  142.449188  132.788452\n",
            "38    141    1.047935    1.828077  ...  128.312408  128.813538  122.917160\n",
            "39    170  127.954468  126.713776  ...    0.948097    0.186159    1.358478\n",
            "40    193  181.967346  172.874374  ...    1.229268    0.171629    1.311203\n",
            "41    123   71.286072   72.443451  ...    0.092471    0.645317    1.507106\n",
            "42    118   90.941399   96.655266  ...    1.883654    1.158000    0.344441\n",
            "43    187   68.844513  104.320793  ...    2.079213    2.599445   36.146870\n",
            "44    181  161.563904  161.614273  ...    1.230060    0.129025    1.333323\n",
            "45    182   71.745567   76.502960  ...  155.698242  156.372787  158.360962\n",
            "46    198  156.153748  151.371170  ...   83.189873   98.861740  107.971832\n",
            "47    193  191.952759  205.925629  ...  198.728394  209.185181  210.143768\n",
            "48    161  161.964096  177.862000  ...  141.707001  142.098297  136.930054\n",
            "49    101  187.654938  179.348694  ...   89.970596   81.276253   80.859039\n",
            "\n",
            "[50 rows x 785 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzpQ1Pz0fX5L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4230b45c-704c-4f6d-8b6d-7a94d0278a18"
      },
      "source": [
        "'''\n",
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines\n",
        "# filename = 'model_ANN.pkl'\n",
        "filename = 'model_ANN_new.pkl'\n",
        "model = joblib.load(filename)\n",
        "'''"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n!git clone https://github.com/ucfilho/MarquesGabi_Routines\\n%cd MarquesGabi_Routines\\n# filename = 'model_ANN.pkl'\\nfilename = 'model_ANN_new.pkl'\\nmodel = joblib.load(filename)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR2emP4rNjQy",
        "outputId": "8d29a250-b1fb-4394-962e-039ab6209696"
      },
      "source": [
        "!git clone https://github.com/ucfilho/MarquesGabi_Routines\n",
        "%cd MarquesGabi_Routines"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'MarquesGabi_Routines' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6cMHOrlNliO"
      },
      "source": [
        "# leitura dos dados\n",
        "df=pd.read_excel(\"FotosTreinoRede.xlsx\")\n",
        "y = df['y']\n",
        "df.drop(['Unnamed: 0','y'], axis='columns', inplace=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "zQO8d2QbNqj0",
        "outputId": "6166750f-5f6d-4436-fda9-130241bb363d"
      },
      "source": [
        "X =np.array(df.copy())/255.0 \n",
        "\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.30, shuffle=True, random_state=42)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nmodel = MLPClassifier(hidden_layer_sizes=(200,10), activation='tanh', \\n                      solver='adam',random_state=1, max_iter=300).fit(X_train,y_train)\\n  \\nprediction = model.predict(X_test)\\n  \\ny =np.copy(y_test)\\ndata = {'y_true': y_test,'y_predict': prediction}  # este dado esta no formato de dicionario\\n\\ndf = pd.DataFrame(data, columns=['y_true','y_predict'])\\n\\n\\nconfusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\\nprint(confusion_matrix)\\n\\ny_true = df['y_true']\\ny_pred = df['y_predict']\\n\\n  \\nMETRICS=sklearn.metrics.classification_report(y_true, y_pred)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23iKv6bDPWQl"
      },
      "source": [
        "# helper\n",
        "def ynindicator(Y):\n",
        "  N = len(Y)\n",
        "  K = len(set(Y))\n",
        "  I = np.zeros((N, K))\n",
        "  I[np.arange(N), Y] = 1\n",
        "  return I\n",
        "\n",
        "def yback(Y_test):\n",
        "  nrow, ncol = Y_test.shape\n",
        "  y_class = np.zeros(nrow,dtype=int)\n",
        "  y_resp = Y_test\n",
        "  for k in range(nrow):\n",
        "    for kk in range(K):\n",
        "      if(y_resp[k,kk] == 1):\n",
        "        y_class[k] = kk\n",
        "  Y_test = y_class.copy()\n",
        "  return Y_test\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "Y_train = np.array(Y_train)\n",
        "K = len(set(Y_train))\n",
        "\n",
        "X_train = X_train.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_train = Y_train.astype(np.int32)\n",
        "Y_train = ynindicator(Y_train)\n",
        "\n",
        "X_test = np.array(X_test )\n",
        "Y_test = np.array(Y_test)\n",
        "X_test = X_test.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "Y_test = Y_test.astype(np.int32)\n",
        "Y_test = ynindicator(Y_test)\n",
        "\n",
        "# the model will be a sequence of layers\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "# make the CNN\n",
        "# model.add(Input(shape=(28, 28, 1)))\n",
        "model.add(Conv2D(input_shape=(Img_Size, Img_Size, 1), filters=32, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=200))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=10))\n",
        "model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.2))\n",
        "model.add(Dense(units=K))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "# list of losses: https://keras.io/losses/\n",
        "# list of optimizers: https://keras.io/optimizers/\n",
        "# list of metrics: https://keras.io/metrics/\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpbPQ1FSRG6A",
        "outputId": "27857f7a-4109-43e4-eee0-630ec01cdc81"
      },
      "source": [
        "\n",
        "# gives us back a <keras.callbacks.History object at 0x112e61a90>\n",
        "model.fit(X_train, Y_train, epochs=200, batch_size=32)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 18s 40ms/step - loss: 0.7983 - accuracy: 0.5576\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.4030 - accuracy: 0.8289\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.3302 - accuracy: 0.8948\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.2239 - accuracy: 0.9089\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.1261 - accuracy: 0.9542\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 0.1248 - accuracy: 0.9438\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 0.0766 - accuracy: 0.9756\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.0629 - accuracy: 0.9872\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 0.0359 - accuracy: 0.9892\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.0286 - accuracy: 0.9861\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.0180 - accuracy: 0.9978\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.0136 - accuracy: 0.9952\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 38ms/step - loss: 0.0111 - accuracy: 1.0000\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 7.7867e-04 - accuracy: 1.0000\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 0.0018 - accuracy: 0.9995\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.0165 - accuracy: 0.9978\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 0.0140 - accuracy: 0.9959\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 0.0055 - accuracy: 0.9993\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.0307 - accuracy: 0.9860\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 0.0517 - accuracy: 0.9866\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 38ms/step - loss: 0.0364 - accuracy: 0.9866\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 0.0208 - accuracy: 0.9924\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.0115 - accuracy: 1.0000\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 0.0037 - accuracy: 0.9986\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.0066 - accuracy: 0.9973\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.0070 - accuracy: 0.9958\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.0360 - accuracy: 0.9908\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 0.0371 - accuracy: 0.9811\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.0131 - accuracy: 0.9967\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 0.0246 - accuracy: 0.9957\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 0.0048 - accuracy: 0.9983\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 42ms/step - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 0.0117 - accuracy: 0.9903\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.0030 - accuracy: 0.9983\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 7.6019e-04 - accuracy: 1.0000\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 8.5358e-04 - accuracy: 1.0000\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 2.9104e-04 - accuracy: 1.0000\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 2.4890e-04 - accuracy: 1.0000\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 2.4270e-04 - accuracy: 1.0000\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 4.5879e-04 - accuracy: 1.0000\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 2.1907e-04 - accuracy: 1.0000\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 1.6391e-04 - accuracy: 1.0000\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 1.5991e-04 - accuracy: 1.0000\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 9.0430e-05 - accuracy: 1.0000\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 8.5543e-05 - accuracy: 1.0000\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 1.0020e-04 - accuracy: 1.0000\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 42ms/step - loss: 1.2541e-04 - accuracy: 1.0000\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 8.4627e-05 - accuracy: 1.0000\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 2.0274e-04 - accuracy: 1.0000\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 1.1452e-04 - accuracy: 1.0000\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 4.3028e-05 - accuracy: 1.0000\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 4.8853e-04 - accuracy: 1.0000\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 1.8760e-04 - accuracy: 1.0000\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 43ms/step - loss: 0.0165 - accuracy: 0.9956\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 42ms/step - loss: 0.0052 - accuracy: 0.9973\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 0.0105 - accuracy: 0.9958\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 42ms/step - loss: 2.7017e-04 - accuracy: 1.0000\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 7.7605e-04 - accuracy: 1.0000\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 2.7401e-04 - accuracy: 1.0000\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 1.4101e-04 - accuracy: 1.0000\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 1.1202e-04 - accuracy: 1.0000\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 1.1109e-04 - accuracy: 1.0000\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 42ms/step - loss: 1.2306e-04 - accuracy: 1.0000\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 8.4449e-04 - accuracy: 1.0000\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 42ms/step - loss: 5.9021e-05 - accuracy: 1.0000\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 2.0374e-04 - accuracy: 1.0000\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 7.8321e-05 - accuracy: 1.0000\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 7.7718e-05 - accuracy: 1.0000\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 3.3910e-04 - accuracy: 1.0000\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 1.6031e-05 - accuracy: 1.0000\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 6.7983e-05 - accuracy: 1.0000\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 2.1734e-04 - accuracy: 1.0000\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 4.8143e-05 - accuracy: 1.0000\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 43ms/step - loss: 4.8931e-05 - accuracy: 1.0000\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 3.9982e-05 - accuracy: 1.0000\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 1.2352e-04 - accuracy: 1.0000\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 4.8911e-05 - accuracy: 1.0000\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 2.0854e-05 - accuracy: 1.0000\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 5.9419e-05 - accuracy: 1.0000\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 3.2518e-05 - accuracy: 1.0000\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 3.5552e-05 - accuracy: 1.0000\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 6.4856e-05 - accuracy: 1.0000\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 4.7710e-05 - accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 3.3306e-05 - accuracy: 1.0000\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 6.6770e-05 - accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 1.5544e-05 - accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 42ms/step - loss: 1.9597e-05 - accuracy: 1.0000\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 2.9215e-05 - accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 2.3489e-05 - accuracy: 1.0000\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 42ms/step - loss: 1.2827e-05 - accuracy: 1.0000\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 1.6865e-05 - accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 4.3596e-05 - accuracy: 1.0000\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 1.9294e-05 - accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 7.3459e-05 - accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 5.4216e-05 - accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 3.5717e-05 - accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 42ms/step - loss: 4.2631e-05 - accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 1.8248e-05 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 5.1805e-05 - accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 42ms/step - loss: 1.7602e-05 - accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 1.3811e-05 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 1.3725e-05 - accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 1.4510e-05 - accuracy: 1.0000\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 2.5713e-05 - accuracy: 1.0000\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 2.0782e-05 - accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 8.5906e-05 - accuracy: 1.0000\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 3.1173e-05 - accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 1.8764e-05 - accuracy: 1.0000\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 2.0311e-05 - accuracy: 1.0000\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 1.2744e-05 - accuracy: 1.0000\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 1.6139e-05 - accuracy: 1.0000\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 3.3163e-05 - accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 1.3675e-05 - accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 3.0305e-05 - accuracy: 1.0000\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 1.5215e-05 - accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 1.1526e-05 - accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 8.1032e-06 - accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 9.6668e-06 - accuracy: 1.0000\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 42ms/step - loss: 1.2161e-05 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 7.3150e-06 - accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 2.0041e-05 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 1.8097e-05 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 1.4212e-05 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 9.5009e-06 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 2.0223e-05 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 8.8444e-06 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 6.5801e-06 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 1.3926e-05 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 5.3224e-05 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 6.2230e-06 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 2.1423e-05 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 1.8787e-05 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 1.0388e-05 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 42ms/step - loss: 7.8187e-06 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 4.7269e-06 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 2.1056e-05 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 38ms/step - loss: 2.2654e-05 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 1.4745e-05 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 9.1526e-06 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 6.0187e-06 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 1.5307e-05 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 2.3770e-05 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 2.6823e-05 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 1.4995e-05 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 5.4366e-06 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 8.6188e-06 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 6.2870e-06 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 42ms/step - loss: 1.5367e-05 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 2.1090e-05 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 42ms/step - loss: 7.1672e-05 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 5.7893e-06 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 1.0063e-05 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 1.4733e-05 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 42ms/step - loss: 3.6713e-06 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 1.0875e-05 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 6.4629e-06 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 7.3221e-06 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 1.9271e-05 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 44ms/step - loss: 5.8443e-06 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 43ms/step - loss: 8.8667e-06 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 42ms/step - loss: 5.4537e-05 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 9.2107e-06 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 1.9324e-05 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 2.5262e-05 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 42ms/step - loss: 6.8811e-06 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 4.1881e-06 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 4.8466e-06 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 7.3402e-06 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 8.7148e-06 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 5.1534e-06 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 1.4803e-05 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 42ms/step - loss: 2.9372e-06 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 1.0787e-05 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 5.1086e-06 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 4.0498e-06 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 9.9009e-06 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 4.8885e-06 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 7.2655e-06 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 42ms/step - loss: 2.6341e-06 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 2.5319e-05 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 41ms/step - loss: 7.1968e-06 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 3.5293e-06 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 40ms/step - loss: 1.1697e-05 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 39ms/step - loss: 4.5477e-06 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 42ms/step - loss: 7.7883e-06 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efd727b0f50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSTlOSUBWMpj"
      },
      "source": [
        "Y_test = yback(Y_test)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpf0XlSARX78",
        "outputId": "96833c5b-ee95-43dc-ba9a-6e03eaea0c68"
      },
      "source": [
        "pred_test= model.predict_classes(X_test)\n",
        "\n",
        "data = {'y_true': Y_test,'y_predict': pred_test}  # este dado esta no formato de dicionario\n",
        "\n",
        "df = pd.DataFrame(data, columns=['y_true','y_predict'])\n",
        "\n",
        "\n",
        "confusion_matrix = pd.crosstab(df['y_true'], df['y_predict'], rownames=['Actual'], colnames=['Predict'])\n",
        "print(confusion_matrix)\n",
        "\n",
        "y_true = df['y_true']\n",
        "y_pred = df['y_predict']\n",
        "\n",
        "  \n",
        "METRICS=sklearn.metrics.classification_report(y_true, y_pred)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Predict   0   1\n",
            "Actual         \n",
            "0        68   4\n",
            "1         1  74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iFNNrlWV9tH",
        "outputId": "f387ced2-acd9-420d-a674-5070c6639415"
      },
      "source": [
        "pred_test"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
              "       1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
              "       0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
              "       1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
              "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QISvYcJBgWbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63382564-7933-4745-c246-d5deb696a4cb"
      },
      "source": [
        "cont = 0; num =25\n",
        "img_graos = []\n",
        "Width_new = []\n",
        "img=ww[0] \n",
        "while( cont < num):\n",
        "  df=Segmenta(img)\n",
        "  df_ann =df.copy()\n",
        "  Width = df['Width']\n",
        "  del df_ann['Width']\n",
        "  result = np.array(df_ann)\n",
        "  result = result.reshape(-1, Img_Size, Img_Size, 1) / 255.0\n",
        "  prediction = model.predict_classes(result)\n",
        "  loc_grao =[];k=0\n",
        "  for i in prediction:\n",
        "    if( i == 0):\n",
        "      img_graos.append(df.iloc[k,:])\n",
        "      Width_new.append(Width.iloc[k])\n",
        "      cont = cont + 1\n",
        "    k = k +1\n",
        "img_graos = pd.DataFrame(img_graos)\n",
        "print(img_graos)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Width           0           1  ...         781         782         783\n",
            "1   137.0  103.430649   53.760666  ...  121.954170  121.712982  117.591713\n",
            "2   141.0   23.447813   23.533422  ...   40.927670   36.023842   34.149090\n",
            "3   186.0   45.336224   65.845879  ...   75.484688   78.045212   82.606659\n",
            "4   190.0    1.671690    1.006205  ...  137.572510   79.255615   65.631241\n",
            "5   156.0   62.159767   77.289284  ...  119.680473  121.566734  134.122955\n",
            "6   112.0   82.750000   79.062500  ...   52.312500   52.437500   53.000000\n",
            "7   149.0  141.593094  168.861313  ...   65.930321   71.665054   76.468178\n",
            "8   163.0   74.655502   76.005493  ...   69.232376   90.095490   99.536865\n",
            "9   177.0   82.647163   87.161888  ...    5.344473    5.964728    5.378435\n",
            "10  146.0  122.257652  117.968285  ...  102.014259  103.250519  107.032455\n",
            "11  192.0   43.239582   48.967010  ...   89.809891   93.657547   95.496956\n",
            "12  103.0   26.547459   23.771324  ...   83.099815   80.416153   77.386841\n",
            "13  158.0  114.291939  117.784645  ...    0.000000    0.000000    0.000000\n",
            "14  116.0   75.781204   73.453026  ...   34.713436   34.818069   36.255646\n",
            "15  127.0   60.126732   59.616463  ...   96.138504   95.768860   93.085129\n",
            "16  165.0   76.977707   78.468239  ...    6.349054    6.558862    6.260606\n",
            "17  187.0    0.120106    0.255426  ...   60.619724   63.335041   66.017532\n",
            "18  116.0   84.650421   84.030907  ...   51.646847   52.317474   53.979782\n",
            "19  131.0   90.889458   95.860382  ...   91.823608   88.230461   84.437096\n",
            "22  134.0  110.137222  112.418800  ...  135.010681   62.855427   66.076187\n",
            "23  110.0   79.202644   81.191406  ...  105.303131  109.471733  113.800987\n",
            "24  145.0   92.360428   93.181839  ...   73.827400   79.984299   77.402618\n",
            "28  135.0   84.701950   84.523399  ...   84.102066   86.622055   88.538208\n",
            "30  132.0  126.939407  125.575775  ...   52.134991   47.104687   55.494034\n",
            "31  198.0   99.951225   97.851433  ...    3.124273    1.654117    1.114172\n",
            "32  188.0  110.685829  108.328194  ...   63.089180   76.130371   87.359436\n",
            "33  143.0   93.828842   98.014374  ...    0.000000    0.000000    0.000000\n",
            "34  148.0   56.319942   54.302418  ...   23.969322   22.558073   16.865597\n",
            "36  169.0   50.262661   48.717758  ...  112.232941  108.521057   91.204155\n",
            "37  137.0   75.716072   75.181999  ...  107.802711  111.841171  124.704712\n",
            "39  104.0   52.536987   52.918640  ...   70.078415   67.955627   63.585804\n",
            "40  100.0   92.905602   93.094406  ...   93.742401   94.270401  101.265594\n",
            "41  144.0  117.878860  112.180557  ...  101.425926  103.756943  106.517746\n",
            "42  144.0   73.256180   71.280090  ...    2.290895    1.608796    1.708333\n",
            "44  166.0   92.489319   89.156479  ...  123.429230  131.825073  159.900558\n",
            "46  101.0   26.713360   53.758949  ...    0.000000    0.000000    0.000000\n",
            "47  158.0   46.181541   48.421246  ...   90.667519   72.272865   68.878860\n",
            "48  163.0   74.447365   41.205540  ...   85.307045   87.264977   88.616623\n",
            "49  120.0   78.940002   80.409996  ...   84.739998   86.767776   88.318878\n",
            "\n",
            "[39 rows x 785 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LkA4vHp-f6_"
      },
      "source": [
        "Width=np.array(Width_new)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRbWgmX_LFH",
        "outputId": "1753918e-5c1d-4dd6-bf8a-b8e4e746f293"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_paper_fev_2021\n",
        "%cd marquesgabi_paper_fev_2021\n",
        "\n",
        "from Get_PSDArea_New import PSDArea\n",
        "from histogram_fev_2021 import PSD\n",
        "from GetBetterSegm import GetBetter"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'marquesgabi_paper_fev_2021' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAG_I6FwCvFr",
        "outputId": "94e9fe68-3efe-4a7f-e0cb-24d846bc8d0f"
      },
      "source": [
        "!git clone https://github.com/ucfilho/marquesgabi_out_2020\n",
        "%cd marquesgabi_out_2020\n",
        "PSD_imageJ = 'Areas_ImageJ.csv'\n",
        "PSD_new = pd.read_csv(PSD_imageJ)\n",
        "print(PSD_new.head(3))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'marquesgabi_out_2020' already exists and is not an empty directory.\n",
            "/content/marquesgabi_fev_2020/Doutorado/MarquesGabi_Routines/MarquesGabi_Routines/marquesgabi_paper_fev_2021/marquesgabi_out_2020\n",
            "   Juntas   Area\n",
            "0       1  2.001\n",
            "1       2  0.820\n",
            "2       3  1.270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_1WIM8w7poO"
      },
      "source": [
        "Area_All, Diameter_All=PSDArea(img_graos) "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "PekBHQOT_6CP",
        "outputId": "65ad15b4-da67-42f7-db84-98da258e272f"
      },
      "source": [
        "img_graos.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Width</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "      <th>768</th>\n",
              "      <th>769</th>\n",
              "      <th>770</th>\n",
              "      <th>771</th>\n",
              "      <th>772</th>\n",
              "      <th>773</th>\n",
              "      <th>774</th>\n",
              "      <th>775</th>\n",
              "      <th>776</th>\n",
              "      <th>777</th>\n",
              "      <th>778</th>\n",
              "      <th>779</th>\n",
              "      <th>780</th>\n",
              "      <th>781</th>\n",
              "      <th>782</th>\n",
              "      <th>783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>137.0</td>\n",
              "      <td>103.430649</td>\n",
              "      <td>53.760666</td>\n",
              "      <td>50.854012</td>\n",
              "      <td>48.629387</td>\n",
              "      <td>48.991104</td>\n",
              "      <td>50.751984</td>\n",
              "      <td>51.366028</td>\n",
              "      <td>51.754322</td>\n",
              "      <td>47.253292</td>\n",
              "      <td>41.794289</td>\n",
              "      <td>38.823006</td>\n",
              "      <td>43.286324</td>\n",
              "      <td>47.026901</td>\n",
              "      <td>47.882145</td>\n",
              "      <td>48.977409</td>\n",
              "      <td>51.775959</td>\n",
              "      <td>53.746758</td>\n",
              "      <td>66.033409</td>\n",
              "      <td>77.187271</td>\n",
              "      <td>74.138733</td>\n",
              "      <td>73.802010</td>\n",
              "      <td>74.759125</td>\n",
              "      <td>66.350204</td>\n",
              "      <td>61.699875</td>\n",
              "      <td>56.952099</td>\n",
              "      <td>55.496616</td>\n",
              "      <td>51.931267</td>\n",
              "      <td>51.791145</td>\n",
              "      <td>141.071442</td>\n",
              "      <td>88.594269</td>\n",
              "      <td>53.134796</td>\n",
              "      <td>51.065742</td>\n",
              "      <td>50.616974</td>\n",
              "      <td>50.126694</td>\n",
              "      <td>50.236931</td>\n",
              "      <td>53.912994</td>\n",
              "      <td>54.083057</td>\n",
              "      <td>47.462997</td>\n",
              "      <td>36.585060</td>\n",
              "      <td>...</td>\n",
              "      <td>97.989662</td>\n",
              "      <td>101.417435</td>\n",
              "      <td>103.848213</td>\n",
              "      <td>106.690025</td>\n",
              "      <td>109.368370</td>\n",
              "      <td>111.995781</td>\n",
              "      <td>120.132927</td>\n",
              "      <td>122.853477</td>\n",
              "      <td>122.099197</td>\n",
              "      <td>121.737862</td>\n",
              "      <td>116.729019</td>\n",
              "      <td>115.773399</td>\n",
              "      <td>88.029083</td>\n",
              "      <td>77.175339</td>\n",
              "      <td>61.633541</td>\n",
              "      <td>47.889923</td>\n",
              "      <td>54.749054</td>\n",
              "      <td>60.679466</td>\n",
              "      <td>64.601898</td>\n",
              "      <td>65.095474</td>\n",
              "      <td>63.113697</td>\n",
              "      <td>60.078800</td>\n",
              "      <td>53.200863</td>\n",
              "      <td>47.334698</td>\n",
              "      <td>58.793167</td>\n",
              "      <td>81.732910</td>\n",
              "      <td>95.975380</td>\n",
              "      <td>100.270874</td>\n",
              "      <td>103.105652</td>\n",
              "      <td>101.620850</td>\n",
              "      <td>102.480202</td>\n",
              "      <td>105.471207</td>\n",
              "      <td>108.071777</td>\n",
              "      <td>110.565987</td>\n",
              "      <td>116.203583</td>\n",
              "      <td>119.996109</td>\n",
              "      <td>120.310509</td>\n",
              "      <td>121.954170</td>\n",
              "      <td>121.712982</td>\n",
              "      <td>117.591713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>141.0</td>\n",
              "      <td>23.447813</td>\n",
              "      <td>23.533422</td>\n",
              "      <td>25.956593</td>\n",
              "      <td>32.852417</td>\n",
              "      <td>40.458683</td>\n",
              "      <td>47.837681</td>\n",
              "      <td>49.195515</td>\n",
              "      <td>46.405914</td>\n",
              "      <td>48.771992</td>\n",
              "      <td>53.455563</td>\n",
              "      <td>62.639660</td>\n",
              "      <td>76.738953</td>\n",
              "      <td>85.865150</td>\n",
              "      <td>91.481064</td>\n",
              "      <td>92.484581</td>\n",
              "      <td>95.350235</td>\n",
              "      <td>98.233444</td>\n",
              "      <td>104.846100</td>\n",
              "      <td>107.735168</td>\n",
              "      <td>112.431221</td>\n",
              "      <td>109.115433</td>\n",
              "      <td>97.371857</td>\n",
              "      <td>77.612892</td>\n",
              "      <td>68.930138</td>\n",
              "      <td>68.383080</td>\n",
              "      <td>67.217041</td>\n",
              "      <td>61.611694</td>\n",
              "      <td>60.560741</td>\n",
              "      <td>23.445202</td>\n",
              "      <td>21.373171</td>\n",
              "      <td>21.284695</td>\n",
              "      <td>29.941200</td>\n",
              "      <td>45.184799</td>\n",
              "      <td>50.399933</td>\n",
              "      <td>57.280319</td>\n",
              "      <td>60.685982</td>\n",
              "      <td>62.867062</td>\n",
              "      <td>60.470295</td>\n",
              "      <td>70.841156</td>\n",
              "      <td>...</td>\n",
              "      <td>75.014191</td>\n",
              "      <td>68.376396</td>\n",
              "      <td>70.092705</td>\n",
              "      <td>70.579704</td>\n",
              "      <td>70.051407</td>\n",
              "      <td>70.209854</td>\n",
              "      <td>68.515419</td>\n",
              "      <td>59.612442</td>\n",
              "      <td>40.193604</td>\n",
              "      <td>33.166092</td>\n",
              "      <td>32.602386</td>\n",
              "      <td>30.536896</td>\n",
              "      <td>23.647905</td>\n",
              "      <td>25.010410</td>\n",
              "      <td>31.294853</td>\n",
              "      <td>59.181885</td>\n",
              "      <td>75.590668</td>\n",
              "      <td>83.673958</td>\n",
              "      <td>88.470657</td>\n",
              "      <td>98.061813</td>\n",
              "      <td>106.783249</td>\n",
              "      <td>113.709061</td>\n",
              "      <td>134.895889</td>\n",
              "      <td>134.038071</td>\n",
              "      <td>132.119659</td>\n",
              "      <td>127.714050</td>\n",
              "      <td>117.626488</td>\n",
              "      <td>90.912033</td>\n",
              "      <td>76.880692</td>\n",
              "      <td>68.061516</td>\n",
              "      <td>70.142593</td>\n",
              "      <td>72.434738</td>\n",
              "      <td>71.526077</td>\n",
              "      <td>72.533379</td>\n",
              "      <td>71.471802</td>\n",
              "      <td>70.429207</td>\n",
              "      <td>63.168705</td>\n",
              "      <td>40.927670</td>\n",
              "      <td>36.023842</td>\n",
              "      <td>34.149090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>186.0</td>\n",
              "      <td>45.336224</td>\n",
              "      <td>65.845879</td>\n",
              "      <td>79.964966</td>\n",
              "      <td>85.256676</td>\n",
              "      <td>86.166969</td>\n",
              "      <td>98.538795</td>\n",
              "      <td>104.769569</td>\n",
              "      <td>102.922424</td>\n",
              "      <td>98.610252</td>\n",
              "      <td>97.216789</td>\n",
              "      <td>99.147888</td>\n",
              "      <td>99.763443</td>\n",
              "      <td>98.694542</td>\n",
              "      <td>80.078285</td>\n",
              "      <td>43.280842</td>\n",
              "      <td>32.650017</td>\n",
              "      <td>27.754309</td>\n",
              "      <td>32.027172</td>\n",
              "      <td>42.404442</td>\n",
              "      <td>65.972481</td>\n",
              "      <td>74.085800</td>\n",
              "      <td>76.244194</td>\n",
              "      <td>76.103951</td>\n",
              "      <td>75.316574</td>\n",
              "      <td>76.387566</td>\n",
              "      <td>83.750961</td>\n",
              "      <td>90.281891</td>\n",
              "      <td>91.950752</td>\n",
              "      <td>56.572208</td>\n",
              "      <td>88.322014</td>\n",
              "      <td>97.333237</td>\n",
              "      <td>96.743561</td>\n",
              "      <td>92.959885</td>\n",
              "      <td>99.968788</td>\n",
              "      <td>106.593605</td>\n",
              "      <td>105.474869</td>\n",
              "      <td>102.413925</td>\n",
              "      <td>102.502258</td>\n",
              "      <td>100.569778</td>\n",
              "      <td>...</td>\n",
              "      <td>35.033882</td>\n",
              "      <td>63.701008</td>\n",
              "      <td>72.859299</td>\n",
              "      <td>75.401787</td>\n",
              "      <td>74.556374</td>\n",
              "      <td>72.922195</td>\n",
              "      <td>73.759857</td>\n",
              "      <td>74.374962</td>\n",
              "      <td>74.745178</td>\n",
              "      <td>77.676147</td>\n",
              "      <td>83.028442</td>\n",
              "      <td>88.038040</td>\n",
              "      <td>93.469994</td>\n",
              "      <td>94.815125</td>\n",
              "      <td>91.845421</td>\n",
              "      <td>94.186508</td>\n",
              "      <td>92.504112</td>\n",
              "      <td>89.756042</td>\n",
              "      <td>87.402603</td>\n",
              "      <td>83.990646</td>\n",
              "      <td>85.640541</td>\n",
              "      <td>87.466423</td>\n",
              "      <td>95.065445</td>\n",
              "      <td>111.889603</td>\n",
              "      <td>130.777206</td>\n",
              "      <td>142.554535</td>\n",
              "      <td>135.302353</td>\n",
              "      <td>113.845764</td>\n",
              "      <td>63.602966</td>\n",
              "      <td>77.149269</td>\n",
              "      <td>76.018280</td>\n",
              "      <td>70.648170</td>\n",
              "      <td>72.511734</td>\n",
              "      <td>73.635338</td>\n",
              "      <td>74.253105</td>\n",
              "      <td>73.914101</td>\n",
              "      <td>75.129272</td>\n",
              "      <td>75.484688</td>\n",
              "      <td>78.045212</td>\n",
              "      <td>82.606659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>190.0</td>\n",
              "      <td>1.671690</td>\n",
              "      <td>1.006205</td>\n",
              "      <td>2.783380</td>\n",
              "      <td>14.304265</td>\n",
              "      <td>47.044868</td>\n",
              "      <td>57.798332</td>\n",
              "      <td>61.084759</td>\n",
              "      <td>68.334396</td>\n",
              "      <td>72.408524</td>\n",
              "      <td>74.224159</td>\n",
              "      <td>80.250969</td>\n",
              "      <td>81.011086</td>\n",
              "      <td>55.751019</td>\n",
              "      <td>48.844208</td>\n",
              "      <td>58.780380</td>\n",
              "      <td>99.528961</td>\n",
              "      <td>93.562775</td>\n",
              "      <td>78.545586</td>\n",
              "      <td>71.597565</td>\n",
              "      <td>93.185585</td>\n",
              "      <td>89.788147</td>\n",
              "      <td>62.056950</td>\n",
              "      <td>61.819500</td>\n",
              "      <td>69.638893</td>\n",
              "      <td>81.090744</td>\n",
              "      <td>91.767525</td>\n",
              "      <td>92.837334</td>\n",
              "      <td>84.311691</td>\n",
              "      <td>1.862936</td>\n",
              "      <td>1.315346</td>\n",
              "      <td>7.970526</td>\n",
              "      <td>46.236229</td>\n",
              "      <td>58.421490</td>\n",
              "      <td>62.564430</td>\n",
              "      <td>65.028915</td>\n",
              "      <td>69.075455</td>\n",
              "      <td>73.128532</td>\n",
              "      <td>76.592896</td>\n",
              "      <td>79.752800</td>\n",
              "      <td>...</td>\n",
              "      <td>108.948479</td>\n",
              "      <td>118.623367</td>\n",
              "      <td>117.191360</td>\n",
              "      <td>113.733192</td>\n",
              "      <td>117.368637</td>\n",
              "      <td>120.622047</td>\n",
              "      <td>117.452393</td>\n",
              "      <td>120.918770</td>\n",
              "      <td>119.837334</td>\n",
              "      <td>79.981377</td>\n",
              "      <td>65.669128</td>\n",
              "      <td>68.052734</td>\n",
              "      <td>71.589577</td>\n",
              "      <td>84.667915</td>\n",
              "      <td>84.201103</td>\n",
              "      <td>84.698837</td>\n",
              "      <td>81.891747</td>\n",
              "      <td>78.200661</td>\n",
              "      <td>94.454834</td>\n",
              "      <td>97.146698</td>\n",
              "      <td>88.432686</td>\n",
              "      <td>84.396339</td>\n",
              "      <td>80.696617</td>\n",
              "      <td>77.774742</td>\n",
              "      <td>79.222824</td>\n",
              "      <td>83.990242</td>\n",
              "      <td>87.589912</td>\n",
              "      <td>95.181267</td>\n",
              "      <td>109.269135</td>\n",
              "      <td>113.968536</td>\n",
              "      <td>113.017838</td>\n",
              "      <td>115.935730</td>\n",
              "      <td>120.696281</td>\n",
              "      <td>122.123650</td>\n",
              "      <td>115.969643</td>\n",
              "      <td>120.344147</td>\n",
              "      <td>131.231461</td>\n",
              "      <td>137.572510</td>\n",
              "      <td>79.255615</td>\n",
              "      <td>65.631241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>156.0</td>\n",
              "      <td>62.159767</td>\n",
              "      <td>77.289284</td>\n",
              "      <td>77.316902</td>\n",
              "      <td>71.280083</td>\n",
              "      <td>64.382645</td>\n",
              "      <td>66.715981</td>\n",
              "      <td>66.957268</td>\n",
              "      <td>66.984879</td>\n",
              "      <td>69.023010</td>\n",
              "      <td>73.035507</td>\n",
              "      <td>72.118340</td>\n",
              "      <td>68.239319</td>\n",
              "      <td>61.300461</td>\n",
              "      <td>38.900066</td>\n",
              "      <td>24.124262</td>\n",
              "      <td>32.921108</td>\n",
              "      <td>46.180805</td>\n",
              "      <td>57.689682</td>\n",
              "      <td>59.341221</td>\n",
              "      <td>58.666664</td>\n",
              "      <td>58.898754</td>\n",
              "      <td>57.330044</td>\n",
              "      <td>57.500328</td>\n",
              "      <td>56.453648</td>\n",
              "      <td>54.955948</td>\n",
              "      <td>53.652863</td>\n",
              "      <td>54.796848</td>\n",
              "      <td>58.391853</td>\n",
              "      <td>48.805393</td>\n",
              "      <td>73.519402</td>\n",
              "      <td>74.827095</td>\n",
              "      <td>68.830376</td>\n",
              "      <td>57.382645</td>\n",
              "      <td>56.249836</td>\n",
              "      <td>63.276798</td>\n",
              "      <td>66.837608</td>\n",
              "      <td>67.597641</td>\n",
              "      <td>68.487839</td>\n",
              "      <td>69.124268</td>\n",
              "      <td>...</td>\n",
              "      <td>94.725182</td>\n",
              "      <td>107.577911</td>\n",
              "      <td>113.159111</td>\n",
              "      <td>113.178833</td>\n",
              "      <td>113.325439</td>\n",
              "      <td>116.502968</td>\n",
              "      <td>120.946754</td>\n",
              "      <td>124.216301</td>\n",
              "      <td>121.569366</td>\n",
              "      <td>115.334656</td>\n",
              "      <td>121.843529</td>\n",
              "      <td>133.457596</td>\n",
              "      <td>83.775154</td>\n",
              "      <td>79.368179</td>\n",
              "      <td>79.216301</td>\n",
              "      <td>80.493752</td>\n",
              "      <td>79.650238</td>\n",
              "      <td>81.558189</td>\n",
              "      <td>81.327423</td>\n",
              "      <td>81.480606</td>\n",
              "      <td>78.415520</td>\n",
              "      <td>76.041428</td>\n",
              "      <td>73.376732</td>\n",
              "      <td>71.885605</td>\n",
              "      <td>72.534523</td>\n",
              "      <td>77.800797</td>\n",
              "      <td>83.468773</td>\n",
              "      <td>88.693626</td>\n",
              "      <td>97.356354</td>\n",
              "      <td>108.513481</td>\n",
              "      <td>113.086121</td>\n",
              "      <td>113.134109</td>\n",
              "      <td>115.357658</td>\n",
              "      <td>118.438522</td>\n",
              "      <td>120.647606</td>\n",
              "      <td>123.435905</td>\n",
              "      <td>124.548332</td>\n",
              "      <td>119.680473</td>\n",
              "      <td>121.566734</td>\n",
              "      <td>134.122955</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Width           0          1  ...         781         782         783\n",
              "1  137.0  103.430649  53.760666  ...  121.954170  121.712982  117.591713\n",
              "2  141.0   23.447813  23.533422  ...   40.927670   36.023842   34.149090\n",
              "3  186.0   45.336224  65.845879  ...   75.484688   78.045212   82.606659\n",
              "4  190.0    1.671690   1.006205  ...  137.572510   79.255615   65.631241\n",
              "5  156.0   62.159767  77.289284  ...  119.680473  121.566734  134.122955\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vmhG2LgCabC"
      },
      "source": [
        "Area = np.array(PSD_new['Area'])\n",
        "diam_teste = []\n",
        "for A in Area:\n",
        "  diam_teste.append((4*A/np.pi)**0.5) \n",
        "\n",
        "Diam1 = [ (4*A/np.pi)**0.5 for A in Area]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "Vfk_fNXGDK5_",
        "outputId": "a286a209-83c3-44a2-f40f-8723074eed88"
      },
      "source": [
        " wt1 = np.ones(len(Diam1)) / len(Diam1)*100\n",
        " wt2 = np.ones(len(Diameter_All)) / len(Diameter_All)*100\n",
        " X = pd.DataFrame([Diam1,Diameter_All])\n",
        " wts = pd.DataFrame([wt1,wt2])\n",
        "plt.hist(X,weights=wts)\n",
        "plt.legend(['Image J','CNN'])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7efd6cb07f10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "ZZHa1j4HT9Dq",
        "outputId": "7d18c067-e862-4927-e502-618b6ef78fdc"
      },
      "source": [
        "counts, bins, bars = plt.hist(X,weights=wts)\n",
        "print(bars)\n",
        "print(bins)\n",
        "print(counts)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<a list of 2 Lists of Patches objects>\n",
            "[0.70327958 0.86469554 1.0261115  1.18752746 1.34894342 1.51035938\n",
            " 1.67177534 1.83319131 1.99460727 2.15602323 2.31743919]\n",
            "[[ 5.26315789 10.52631579 22.10526316 31.57894737 17.89473684  9.47368421\n",
            "   1.05263158  0.          1.05263158  1.05263158]\n",
            " [ 7.69230769 17.94871795 17.94871795 33.33333333 12.82051282  7.69230769\n",
            "   0.          2.56410256  0.          0.        ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o_vDGeWUwIZ",
        "outputId": "1c9f67bd-54ed-4f76-c30a-1c572437bb39"
      },
      "source": [
        "print(counts.sum())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200.00000000000014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "KcH52-6iJQ8t",
        "outputId": "54ca01bb-a801-4af8-b700-0ad1dbf0f5c9"
      },
      "source": [
        "\n",
        "plt.hist([Diam1,Diameter_All])\n",
        "plt.legend(['Image J','CNN'])\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7efd6ca8f290>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r11AxFK_JIii",
        "outputId": "49d887f6-6ff8-4211-94fe-4943eb8cfcfc"
      },
      "source": [
        "[Diam1,Diameter_All]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.59616801403081,\n",
              "  1.0217907939900581,\n",
              "  1.2716187407449044,\n",
              "  1.104429030701514,\n",
              "  1.2163487785097904,\n",
              "  1.6013445735058454,\n",
              "  1.1715597420637607,\n",
              "  1.2534662333717612,\n",
              "  1.2676073151634049,\n",
              "  1.309600575274104,\n",
              "  1.292966945531582,\n",
              "  1.7658322811231006,\n",
              "  1.3564037533648712,\n",
              "  1.2407040781688483,\n",
              "  2.130217298173151,\n",
              "  1.4228319915327,\n",
              "  1.0651086490865755,\n",
              "  1.3008210311003705,\n",
              "  1.336545951796433,\n",
              "  0.8927754224911278,\n",
              "  1.4494292838262302,\n",
              "  1.4052738287907582,\n",
              "  1.6421697097891788,\n",
              "  1.2329833804288621,\n",
              "  1.19042665178928,\n",
              "  1.1682948223612457,\n",
              "  1.1518314137121108,\n",
              "  0.9607802401865855,\n",
              "  2.317439190074449,\n",
              "  1.0591147430338594,\n",
              "  1.4308630919602832,\n",
              "  0.7535680705496237,\n",
              "  0.8608283307581511,\n",
              "  1.2776122636975893,\n",
              "  1.3745862957220916,\n",
              "  1.259546137598783,\n",
              "  1.2978813187979172,\n",
              "  1.2412170838050638,\n",
              "  1.6009469708743893,\n",
              "  1.3149369953539032,\n",
              "  1.417901703622935,\n",
              "  1.2478669653497139,\n",
              "  1.1055812783082735,\n",
              "  0.9561307405997607,\n",
              "  0.9487783503683882,\n",
              "  1.1238565871041026,\n",
              "  1.2058356273089446,\n",
              "  1.2801012827406097,\n",
              "  0.8733100751144249,\n",
              "  0.9194732501297403,\n",
              "  1.6425573339441792,\n",
              "  1.085826790250066,\n",
              "  1.0639125693728595,\n",
              "  1.0875842666474016,\n",
              "  1.417901703622935,\n",
              "  1.550443891425932,\n",
              "  0.7825779328716171,\n",
              "  1.4690612745308145,\n",
              "  1.053086721720641,\n",
              "  1.2676073151634049,\n",
              "  0.7744003006005755,\n",
              "  1.3787482149724068,\n",
              "  1.363892581861956,\n",
              "  1.299352006316543,\n",
              "  1.2870449283923413,\n",
              "  1.11817763925502,\n",
              "  0.9474354220939228,\n",
              "  1.5218484589055707,\n",
              "  1.3526437911676632,\n",
              "  1.1556938532445284,\n",
              "  1.6013445735058454,\n",
              "  1.274619025074578,\n",
              "  1.422384489715834,\n",
              "  1.3408259533459403,\n",
              "  1.172646028567008,\n",
              "  1.1490645795125545,\n",
              "  1.459060149136146,\n",
              "  1.2483770274864237,\n",
              "  1.336545951796433,\n",
              "  0.9601174044814821,\n",
              "  1.4867225193896279,\n",
              "  1.4277452542806772,\n",
              "  1.35028849808504,\n",
              "  0.7560982446653928,\n",
              "  1.259040600296622,\n",
              "  1.13456827900627,\n",
              "  1.6549133695530214,\n",
              "  1.1204526724091788,\n",
              "  1.1176081573544434,\n",
              "  0.9153095762832032,\n",
              "  1.1639273497938836,\n",
              "  1.3066806149514323,\n",
              "  1.1529362882239027,\n",
              "  1.3047303442899274,\n",
              "  1.3066806149514323],\n",
              " [1.0712640683824282,\n",
              "  1.1854509043968062,\n",
              "  1.4451234164037916,\n",
              "  1.5114940036320106,\n",
              "  1.3312865569219583,\n",
              "  0.9896949542929336,\n",
              "  1.2833937009790997,\n",
              "  1.4259195608811435,\n",
              "  1.4402035332278642,\n",
              "  1.1542942150957918,\n",
              "  1.6602671996537957,\n",
              "  0.9392091379839669,\n",
              "  1.2569265924939879,\n",
              "  0.8924952803638015,\n",
              "  1.0439497761554166,\n",
              "  1.3126132136804303,\n",
              "  1.264988813006276,\n",
              "  1.060224369760797,\n",
              "  0.9356923708220968,\n",
              "  1.2501718251633478,\n",
              "  0.8308787320178422,\n",
              "  1.2021717053594376,\n",
              "  1.0162285928020562,\n",
              "  1.2301342420866153,\n",
              "  1.8698599359354728,\n",
              "  1.6022246622029392,\n",
              "  1.2461763017810088,\n",
              "  1.2979901906268694,\n",
              "  1.2231338680506918,\n",
              "  1.299429440378171,\n",
              "  0.7895694603681274,\n",
              "  0.7032795763983994,\n",
              "  1.124205764913116,\n",
              "  1.0575807172667684,\n",
              "  1.4868523486677239,\n",
              "  0.8812941750236097,\n",
              "  1.4220501072099185,\n",
              "  1.3223825256097803,\n",
              "  0.9677554518690367]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    }
  ]
}